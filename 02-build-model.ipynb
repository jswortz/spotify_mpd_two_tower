{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2804db33-2adb-4845-a6bb-087f168eb6a5",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "# Build baseline tfrs model \n",
    "\n",
    "Look inside of `two_tower_src/` for the source code and model code\n",
    "\n",
    "This notebook constructs the two tower model and saves the model to GCS\n",
    "\n",
    "We will use managed Tensorboard for training. Before beginning, create a new tensorboard instance by going to Vertex -> Experiments -> Tensorboard Instances -> Create\n",
    "\n",
    "![](img/create-a-tb.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "059c0d02-8de1-446e-9b2c-7366dfff042d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow-recommenders google-cloud-aiplatform --user"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d2a891-1129-480d-ad11-f8f78ce062b8",
   "metadata": {},
   "source": [
    "#### Restart kernel after installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9289ca69-cd66-4161-88b2-db58e9218b91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = 'hybrid-vertex' \n",
    "LOCATION = 'us-central1' \n",
    "path = 'gs://jt-tfrs-central' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2d6dd36-f5ae-40cb-b76e-480b9782cd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "os.environ['TF_GPU_THREAD_MODE']='gpu_private'\n",
    "os.environ['TF_GPU_ALLOCATOR']='cuda_malloc_async'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef3417e6-72a8-48e2-ba9d-8fd207ab6229",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-04 15:32:14.834192: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "Copying gs://two-tower-models/vocabs/vocab_dict.pkl...\n",
      "/ [1 files][ 18.5 MiB/ 18.5 MiB]                                                \n",
      "Operation completed over 1 objects/18.5 MiB.                                     \n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "\n",
    "import tensorflow as tf\n",
    "import logging\n",
    "import time\n",
    "\n",
    "import tensorflow_recommenders as tfrs\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "\n",
    "from google.cloud import storage\n",
    "\n",
    "from src.two_tower_jt import two_tower as tt\n",
    "#inside this tt module the data parsing functions, candidate dataset and model classes are found"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5015b9c5-f6f8-46b3-998c-9ceb0ca65962",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create Dataset for local training and testing\n",
    "\n",
    "Inspect the contents of the directory - you can change parameters in the header of the `two_tower.py` script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2e48504-5598-4f8d-b996-9f0f9192e471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34mtwo_tower_src\u001b[00m\n",
      "├── __init__.py\n",
      "├── \u001b[01;34m__pycache__\u001b[00m\n",
      "│   ├── __init__.cpython-37.pyc\n",
      "│   └── two_tower.cpython-37.pyc\n",
      "├── task.py\n",
      "└── two_tower.py\n",
      "\n",
      "1 directory, 5 files\n"
     ]
    }
   ],
   "source": [
    "!tree two_tower_src"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9d8dc5-a796-4295-a1da-5474827ef859",
   "metadata": {},
   "source": [
    "## Create Dataset objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c26cf3b-6f23-476f-9806-27dd9697d22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_client = storage.Client()\n",
    "\n",
    "from google.cloud import aiplatform as vertex_ai\n",
    "\n",
    "\n",
    "options = tf.data.Options()\n",
    "options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.AUTO\n",
    "\n",
    "def full_parse(data):\n",
    "    # used for interleave - takes tensors and returns a tf.dataset\n",
    "    data = tf.data.TFRecordDataset(data)\n",
    "    return data\n",
    "\n",
    "batch_size = 1024 #*16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984676ac-e3bc-4c34-9875-3113130e418b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Create Train dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30df741a-fbb5-416d-97db-b952f703e6de",
   "metadata": {},
   "source": [
    "### Option 1: outer paralellism\n",
    "\n",
    "* parallelize transforms\n",
    "* run multiple copies of the input pipeline over sharded inputs and combine the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93fc839d-81c1-41d0-929d-4f70e4f45264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_path='gs://spotify-data-regimes/jtv10/valid_v9/*.tfrecords'\n",
    "\n",
    "# filenames = tf.data.Dataset.list_files(file_path, shuffle=None)\n",
    "# filenames.cache()\n",
    "\n",
    "# NUM_SHARDS=2\n",
    "\n",
    "# def make_dataset(shard_index):\n",
    "#     files = filenames.shard(NUM_SHARDS, shard_index)\n",
    "#     dataset = tf.data.TFRecordDataset(files)\n",
    "#     return dataset.batch(batch_size)\n",
    "\n",
    "# indices = tf.data.Dataset.range(NUM_SHARDS)\n",
    "\n",
    "# train_dataset = indices.interleave(\n",
    "#     make_dataset,\n",
    "#     num_parallel_calls=tf.data.AUTOTUNE\n",
    "# ).map(\n",
    "#     tt.parse_tfrecord, \n",
    "#     num_parallel_calls=tf.data.AUTOTUNE\n",
    "# ).repeat(\n",
    "#     args.num_epochs\n",
    "# ).prefetch(\n",
    "#     tf.data.AUTOTUNE\n",
    "# ).with_options(\n",
    "#     options\n",
    "# )\n",
    "# # train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c1aef5f7-9c8b-4166-98fb-aa812f54910a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'gs://spotify-data-regimes/jtv10/valid_v9/-00000-of-00006.tfrecords', shape=(), dtype=string)\n",
      "tf.Tensor(b'gs://spotify-data-regimes/jtv10/valid_v9/-00005-of-00006.tfrecords', shape=(), dtype=string)\n",
      "tf.Tensor(b'gs://spotify-data-regimes/jtv10/valid_v9/-00004-of-00006.tfrecords', shape=(), dtype=string)\n",
      "tf.Tensor(b'gs://spotify-data-regimes/jtv10/valid_v9/-00002-of-00006.tfrecords', shape=(), dtype=string)\n",
      "tf.Tensor(b'gs://spotify-data-regimes/jtv10/valid_v9/-00003-of-00006.tfrecords', shape=(), dtype=string)\n",
      "tf.Tensor(b'gs://spotify-data-regimes/jtv10/valid_v9/-00001-of-00006.tfrecords', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "# for file in filenames:\n",
    "#     print(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6827209-b916-4291-8f56-6afdb33c26ee",
   "metadata": {},
   "source": [
    "### Option 2: interleave --> map --> batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b03553df-06ea-4e76-b766-a8171acb2c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = 'spotify-data-regimes'\n",
    "train_dir_prefix = 'jtv10/valid_v9' # valid_v9 | train_v9\n",
    "\n",
    "train_files = []\n",
    "for blob in storage_client.list_blobs(f'{train_dir}', prefix=f'{train_dir_prefix}'):\n",
    "    if '.tfrecords' in blob.name:\n",
    "        train_files.append(blob.public_url.replace(\"https://storage.googleapis.com/\", \"gs://\"))\n",
    "    \n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_files).prefetch(\n",
    "    tf.data.AUTOTUNE,\n",
    ")\n",
    "\n",
    "train_dataset = train_dataset.interleave(\n",
    "    full_parse,\n",
    "    cycle_length=tf.data.AUTOTUNE, \n",
    "    num_parallel_calls=tf.data.AUTOTUNE,\n",
    "    deterministic=False,\n",
    ").map(\n",
    "    tt.parse_tfrecord, \n",
    "    num_parallel_calls=tf.data.AUTOTUNE\n",
    ").batch(\n",
    "    batch_size \n",
    ").prefetch(\n",
    "    tf.data.AUTOTUNE,\n",
    ").with_options(\n",
    "    options\n",
    ")\n",
    "\n",
    "# train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d0ccf9-4602-49b8-bde7-dbb962d6cc25",
   "metadata": {},
   "source": [
    "### Create Validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4983881f-099b-43d6-b296-381cf3e1d874",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dir = 'spotify-data-regimes'\n",
    "valid_dir_prefix = 'jtv10/valid_v9'\n",
    "\n",
    "valid_files = []\n",
    "for blob in storage_client.list_blobs(f'{valid_dir}', prefix=f'{valid_dir_prefix}'):\n",
    "    if '.tfrecords' in blob.name:\n",
    "        valid_files.append(blob.public_url.replace(\"https://storage.googleapis.com/\", \"gs://\"))\n",
    "\n",
    "\n",
    "valid_dataset = tf.data.Dataset.from_tensor_slices(valid_files).prefetch(\n",
    "    tf.data.AUTOTUNE,\n",
    ")\n",
    "\n",
    "valid_dataset = valid_dataset.interleave(\n",
    "    full_parse,\n",
    "    num_parallel_calls=tf.data.AUTOTUNE,\n",
    "    cycle_length=tf.data.AUTOTUNE, \n",
    "    deterministic=False,\n",
    ").map(\n",
    "    tt.parse_tfrecord, \n",
    "    num_parallel_calls=tf.data.AUTOTUNE\n",
    ").batch(\n",
    "    batch_size\n",
    ").prefetch(\n",
    "    tf.data.AUTOTUNE,\n",
    ").with_options(\n",
    "    options\n",
    ")\n",
    "\n",
    "# valid_dataset = valid_dataset.cache() #1gb machine mem + 400 MB in candidate ds (src/two-tower.py)\n",
    "\n",
    "# valid_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf15fc9-5d2d-470c-83d6-29354ff813d9",
   "metadata": {},
   "source": [
    "### Create Candidates dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b115e4b4-f616-4314-ac65-b956bbc2d9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "CANDIDATE_FILE_DIR = 'spotify-data-regimes'\n",
    "CANDIDATE_PREFIX = 'jtv10/candidates' \n",
    "\n",
    "candidate_files = []\n",
    "for blob in storage_client.list_blobs(f\"{CANDIDATE_FILE_DIR}\", prefix=f'{CANDIDATE_PREFIX}'):\n",
    "    if '.tfrecords' in blob.name:\n",
    "        candidate_files.append(blob.public_url.replace(\"https://storage.googleapis.com/\", \"gs://\"))\n",
    "\n",
    "candidate_dataset = tf.data.Dataset.from_tensor_slices(candidate_files)\n",
    "\n",
    "parsed_candidate_dataset = candidate_dataset.interleave(\n",
    "    full_parse,\n",
    "    cycle_length=tf.data.AUTOTUNE, \n",
    "    num_parallel_calls=tf.data.AUTOTUNE,\n",
    "    deterministic=False\n",
    ").map(\n",
    "    tt.parse_candidate_tfrecord_fn, \n",
    "    num_parallel_calls=tf.data.AUTOTUNE\n",
    ").with_options(\n",
    "    options\n",
    ")\n",
    "\n",
    "parsed_candidate_dataset = parsed_candidate_dataset.cache() #400 MB on machine mem\n",
    "\n",
    "# parsed_candidate_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa2e448-f833-421f-977d-f52b0db198c2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Local Training\n",
    "\n",
    "Compile the model\n",
    "Review the details of the model layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051a914a-e3d4-4c5c-a42a-9d39a6bde63c",
   "metadata": {},
   "source": [
    "## Adapt Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd6fa77-6401-47bc-b198-14c1891ddcac",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Adapt the text vectorizors - copy/paste to run one time\n",
    "\n",
    "We are accessing the `TextVectorizor` layers in the model via the layer print-outs above\n",
    "\n",
    "```python\n",
    "# adpat the text vectorizors\n",
    "\n",
    "MAX_PLAYLIST_LENGTH = 5 # this is set upstream by the BigQuery max length\n",
    "    \n",
    "model.query_tower.layers[0].layers[0].adapt(\n",
    "    train_dataset.unbatch().batch(10000).map(lambda x: x['pl_name_src']))\n",
    "print('pl_name_src adapts complete')\n",
    "model.query_tower.layers[7].layers[0].adapt(\n",
    "    train_dataset.unbatch().batch(10000).map(lambda x: tf.reshape(x['track_name_pl'], [-1, MAX_PLAYLIST_LENGTH, 1])))\n",
    "print('track_name_pl adapts complete')\n",
    "model.query_tower.layers[9].layers[0].adapt(\n",
    "    train_dataset.unbatch().batch(10000).map(lambda x: tf.reshape(x['artist_name_pl'], [-1, MAX_PLAYLIST_LENGTH, 1]))) \n",
    "print('artist_name_pl adapts complete')\n",
    "model.query_tower.layers[11].layers[0].adapt(\n",
    "    train_dataset.unbatch().batch(10000).map(lambda x: tf.reshape(x['album_name_pl'], [-1, MAX_PLAYLIST_LENGTH, 1])))\n",
    "print('album_name_pl adapts complete')\n",
    "model.query_tower.layers[12].layers[0].adapt(\n",
    "    train_dataset.unbatch().batch(10000).map(lambda x: tf.reshape(x['artist_genres_pl'], [-1, MAX_PLAYLIST_LENGTH, 1])))\n",
    "print('artist_genres_pl adapts complete')\n",
    "# model.query_tower.layers[13].layers[0].adapt(\n",
    "#     train_dataset.unbatch().batch(10000).map(lambda x: tf.reshape(x['tracks_playlist_titles_pl'], [-1, MAX_PLAYLIST_LENGTH, 1])))\n",
    "# print('tracks_playlist_titles_pl adapts complete')\n",
    "\n",
    "model.candidate_tower.layers[1].layers[0].adapt(\n",
    "    train_dataset.unbatch().batch(10000).map(lambda x: x['track_name_can'])) \n",
    "print('track_name_can adapts complete')\n",
    "model.candidate_tower.layers[3].layers[0].adapt(\n",
    "    train_dataset.unbatch().batch(10000).map(lambda x: x['artist_name_can'])) \n",
    "print('artist_name_can adapts complete')\n",
    "model.candidate_tower.layers[5].layers[0].adapt(\n",
    "    train_dataset.unbatch().batch(10000).map(lambda x: x['album_name_can'])) \n",
    "print('album_name_can adapts complete')\n",
    "model.candidate_tower.layers[9].layers[0].adapt(\n",
    "    train_dataset.unbatch().batch(10000).map(lambda x: x['artist_genres_can'])) \n",
    "print('artist_genres_can adapts complete')\n",
    "# model.candidate_tower.layers[11].layers[0].adapt(\n",
    "#     train_dataset.unbatch().batch(10000).map(lambda x: x['track_pl_titles_can'])) \n",
    "# print('track_pl_titles_can adapts complete')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c78fde-5d37-4fb9-a4b7-d4d4d9fd2648",
   "metadata": {},
   "source": [
    "### save vocab dict \n",
    "\n",
    "Save the vocab dictionary for later so you will not have to adapt\n",
    "\n",
    "```python\n",
    "vocab_dict = {\n",
    "    'pl_name_src' : model.query_tower.layers[0].layers[0].get_vocabulary(),\n",
    "    'track_name_pl' : model.query_tower.layers[7].layers[0].get_vocabulary(),\n",
    "    'artist_name_pl' : model.query_tower.layers[9].layers[0].get_vocabulary(),\n",
    "    'album_name_pl' : model.query_tower.layers[11].layers[0].get_vocabulary(),\n",
    "    'artist_genres_pl' : model.query_tower.layers[12].layers[0].get_vocabulary(),\n",
    "    # 'tracks_playlist_titles_pl' : model.query_tower.layers[13].layers[0].get_vocabulary(),\n",
    "\n",
    "    'track_name_can' : model.candidate_tower.layers[1].layers[0].get_vocabulary(),\n",
    "    'artist_name_can' : model.candidate_tower.layers[3].layers[0].get_vocabulary(),\n",
    "    'album_name_can' : model.candidate_tower.layers[5].layers[0].get_vocabulary(),\n",
    "    'artist_genres_can' : model.candidate_tower.layers[9].layers[0].get_vocabulary(),\n",
    "    # 'track_pl_titles_can' : model.candidate_tower.layers[11].layers[0].get_vocabulary(),\n",
    "}\n",
    "```\n",
    "\n",
    "```python\n",
    "import pickle as pkl\n",
    "\n",
    "filehandler = open('vocab_dict.pkl', 'wb')\n",
    "pkl.dump(vocab_dict, filehandler)\n",
    "\n",
    "filehandler.close()\n",
    "\n",
    "tt.upload_blob('two-tower-models', 'vocab_dict.pkl', 'vocabs/vocab_dict.pkl')\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83159b76-b529-4297-ae89-443e15da477b",
   "metadata": {},
   "source": [
    "### load saved vocab dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be17f6f9-1305-4d7d-ac8c-a5777e79e891",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.system('gsutil cp gs://two-tower-models/vocabs/vocab_dict.pkl .')  # TODO - paramterize\n",
    "\n",
    "filehandler = open('vocab_dict.pkl', 'rb')\n",
    "vocab_dict = pkl.load(filehandler)\n",
    "filehandler.close()\n",
    "\n",
    "# vocab_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1873a9d8-7dbd-4168-b47c-a9196b6793aa",
   "metadata": {},
   "source": [
    "## Build and Compile model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ec5d0c8-a4ae-451c-9056-fe5d9a8e4964",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_sizes=[512,256]\n",
    "\n",
    "model = tt.TheTwoTowers(layer_sizes, vocab_dict, parsed_candidate_dataset)\n",
    "\n",
    "LR = .1\n",
    "opt = tf.keras.optimizers.Adagrad(LR)\n",
    "model.compile(optimizer=opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec331ba6-0b38-4089-9432-097c43ed3bb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<src.two_tower_jt.two_tower.TheTwoTowers at 0x7f3daa1a3b10>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3414bbf-43af-4e95-96db-95b36a980ce9",
   "metadata": {},
   "source": [
    "### inspect layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "11401be6-0f5f-4454-997a-862a58df2a81",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Playlist (query) Tower:\n",
      "0 pl_name_src_text_embedding\n",
      "1 pl_collaborative_emb_model\n",
      "2 pl_duration_ms_new_emb_model\n",
      "3 num_pl_songs_new_emb_model\n",
      "4 num_pl_artists_new_emb_model\n",
      "5 num_pl_albums_new_emb_model\n",
      "6 track_uri_pl_emb_model\n",
      "7 track_name_pl_emb_model\n",
      "8 artist_uri_pl_emb_model\n",
      "9 artist_name_pl_emb_model\n",
      "10 album_uri_pl_emb_model\n",
      "11 album_name_pl_emb_model\n",
      "12 artist_genres_pl_emb_model\n",
      "13 duration_ms_songs_pl_emb_model\n",
      "14 track_pop_pl_emb_model\n",
      "15 artist_pop_pl_emb_model\n",
      "16 artists_followers_pl_emb_model\n",
      "17 track_danceability_pl_emb_model\n",
      "18 track_energy_pl_emb_model\n",
      "19 track_key_pl_emb_model\n",
      "20 track_loudness_pl_emb_model\n",
      "21 track_mode_pl_emb_model\n",
      "22 track_speechiness_pl_emb_model\n",
      "23 track_acousticness_pl_emb_model\n",
      "24 track_instrumentalness_pl_emb_model\n",
      "25 track_liveness_pl_emb_model\n",
      "26 track_valence_pl_emb_model\n",
      "27 track_tempo_pl_emb_model\n",
      "28 time_signature_pl_emb_model\n",
      "29 pl_dense_layers\n"
     ]
    }
   ],
   "source": [
    "## Quick look at the layers\n",
    "print(\"Playlist (query) Tower:\")\n",
    "\n",
    "for i, l in enumerate(model.query_tower.layers):\n",
    "    print(i, l.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0d4fb669-7c98-448a-a2bf-1559b0a4d67e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Track (candidate) Tower:\n",
      "0 track_uri_can_emb_model\n",
      "1 track_name_can_emb_model\n",
      "2 artist_uri_can_emb_model\n",
      "3 artist_name_can_emb_model\n",
      "4 album_uri_can_emb_model\n",
      "5 album_name_can_emb_model\n",
      "6 duration_ms_can_emb_model\n",
      "7 track_pop_can_emb_model\n",
      "8 artist_pop_can_emb_model\n",
      "9 artist_genres_can_emb_model\n",
      "10 artists_followers_can_emb_model\n",
      "11 track_danceability_can_emb_model\n",
      "12 track_energy_can_emb_model\n",
      "13 track_key_can_emb_model\n",
      "14 track_loudness_can_emb_model\n",
      "15 track_mode_can_emb_model\n",
      "16 track_speechiness_can_emb_model\n",
      "17 track_acousticness_can_emb_model\n",
      "18 track_instrumentalness_can_emb_model\n",
      "19 track_liveness_can_emb_model\n",
      "20 track_valence_can_emb_model\n",
      "21 track_tempo_can_emb_model\n",
      "22 time_signature_can_emb_model\n",
      "23 candidate_dense_layers\n"
     ]
    }
   ],
   "source": [
    "print(\"Track (candidate) Tower:\")\n",
    "for i, l in enumerate(model.candidate_tower.layers):\n",
    "    print(i, l.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4de78f-a955-43a2-9940-b8d9f8aa4911",
   "metadata": {},
   "source": [
    "### setup Vertex Exeperiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "68c84c47-bd82-4ca3-9f37-d3160e3ca376",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_NAME = f'build-local-v2'\n",
    "\n",
    "vertex_ai.init(\n",
    "    project=PROJECT_ID,\n",
    "    location='us-central1',\n",
    "    # experiment=EXPERIMENT_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425a17df-2e2a-41e5-a0c6-ac2abc2ad135",
   "metadata": {
    "tags": []
   },
   "source": [
    "### setup Tensorboard callbacks\n",
    "\n",
    "Setup tensorboard below so training is visible and we can inspect the graph\n",
    "\n",
    "**TODO:** clean up notebook section \n",
    "\n",
    "> *Note:* While profiling does not work for managed Tensorboard at this time, you can inspect the profiler with an [inline Tensorboard in another notebook](https://www.tensorflow.org/tensorboard/tensorboard_in_notebooks). You may be prompted to install the tensorflow profiler library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8238845-82bb-45a2-98b6-df8fb7589d90",
   "metadata": {},
   "source": [
    "#### Managed Tensorboard Resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6c0f152f-930f-4640-ab6d-7b540e6084b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TB_RESOURCE_NAME: projects/934903580331/locations/us-central1/tensorboards/472921941339013120\n"
     ]
    }
   ],
   "source": [
    "# use existing TB instance\n",
    "# TB_RESOURCE_NAME = 'projects/934903580331/locations/us-central1/tensorboards/472921941339013120'\n",
    "\n",
    "# # create new TB instance\n",
    "TENSORBOARD_DISPLAY_NAME=f\"{EXPERIMENT_NAME}\"\n",
    "tensorboard = vertex_ai.Tensorboard.create(display_name=TENSORBOARD_DISPLAY_NAME)\n",
    "TB_RESOURCE_NAME = tensorboard.resource_name\n",
    "\n",
    "\n",
    "print(f\"TB_RESOURCE_NAME: {TB_RESOURCE_NAME}\")\n",
    "# projects/934903580331/locations/us-central1/tensorboards/472921941339013120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6d62d539-6e29-4783-a358-1321f9521528",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_upload_logs_to_manged_tb_command(ttl_hrs, oneshot=\"false\"):\n",
    "    \"\"\"\n",
    "    Run this and copy/paste the command into terminal to have \n",
    "    upload the tensorboard logs from this machine to the managed tb instance\n",
    "    Note that the log dir is at the granularity of the run to help select the proper\n",
    "    timestamped run in Tensorboard\n",
    "    You can also run this in one-shot mode after training is done \n",
    "    to upload all tb objects at once\n",
    "    \"\"\"\n",
    "    return(f\"\"\"tb-gcp-uploader --tensorboard_resource_name={TB_RESOURCE_NAME} \\\n",
    "      --logdir={LOG_DIR} \\\n",
    "      --experiment_name={EXPERIMENT_NAME} \\\n",
    "      --one_shot={oneshot} \\\n",
    "      --event_file_inactive_secs={60*60*ttl_hrs}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360a034b-7959-4bd0-bf9c-45da1b695055",
   "metadata": {},
   "source": [
    "#### train config\n",
    "\n",
    "* consider experiment and experiment-run naming convention so names don't collide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d2eeffe1-efc0-4a2d-b168-6f10a43b1e76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUN_NAME: run-20230104-153318\n",
      "LOG_DIR: gs://jt-tfrs-central/build-local-v2/run-20230104-153318/tb-logs\n"
     ]
    }
   ],
   "source": [
    "invoke_time = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "RUN_NAME = f'run-{invoke_time}'\n",
    "\n",
    "LOG_DIR = f\"{path}/{EXPERIMENT_NAME}/{RUN_NAME}/tb-logs\"\n",
    "\n",
    "NUM_EPOCHS = 1\n",
    "VALID_FREQUENCY=5\n",
    "\n",
    "HIST_FREQ = 0\n",
    "EMBED_FREQ = 0\n",
    "\n",
    "print(f\"RUN_NAME: {RUN_NAME}\")\n",
    "print(f\"LOG_DIR: {LOG_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50177537-964d-4f49-9770-a880dd47fe10",
   "metadata": {},
   "source": [
    "### Train model\n",
    "\n",
    "* train model in-notebook\n",
    "* write metrics to Vertex AI Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5d9f592c-913c-45a9-a9eb-3ae1becefddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 15s 263ms/step - batch_categorical_accuracy_at_1: 0.0000e+00 - batch_categorical_accuracy_at_5: 0.0000e+00 - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - loss: 14208.3160 - regularization_loss: 0.0000e+00 - total_loss: 14208.3160\n"
     ]
    }
   ],
   "source": [
    "# tensorboard callback\n",
    "class UploadTBLogsBatchEnd(tf.keras.callbacks.Callback):\n",
    "    '''\n",
    "    ecapsulates one-shot log uploader via a custom callback\n",
    "\n",
    "    '''\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        os.system(get_upload_logs_to_manged_tb_command(ttl_hrs = 5, oneshot=\"true\"))\n",
    "        \n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "        log_dir=LOG_DIR,\n",
    "        histogram_freq=HIST_FREQ, \n",
    "        write_graph=True,\n",
    "        embeddings_freq=EMBED_FREQ,\n",
    "        # profile_batch=(20,50) #run profiler on steps 20-40 - enable this line if you want to run profiler from the utils/ notebook\n",
    "    )\n",
    "\n",
    "#start the timer and training\n",
    "start_time = time.time()\n",
    "\n",
    "layer_history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=valid_dataset,\n",
    "    validation_freq=VALID_FREQUENCY,\n",
    "    epochs=NUM_EPOCHS,\n",
    "    steps_per_epoch=10,\n",
    "    validation_steps=100,\n",
    "    callbacks=[\n",
    "        tensorboard_callback,\n",
    "        UploadTBLogsBatchEnd()\n",
    "    ], \n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "val_keys = [v for v in layer_history.history.keys()]\n",
    "runtime_mins = int((end_time - start_time) / 60)\n",
    "\n",
    "\n",
    "vertex_ai.start_run(RUN_NAME, tensorboard=TB_RESOURCE_NAME)\n",
    "\n",
    "vertex_ai.log_params(\n",
    "    {\n",
    "        \"layers\": str(layer_sizes), \n",
    "        \"learning_rate\": LR,\n",
    "        \"num_epochs\": NUM_EPOCHS,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"valid_freq\": VALID_FREQUENCY,\n",
    "    }\n",
    ")\n",
    "\n",
    "gather the metrics for the last epoch to be saved in metrics\n",
    "metrics_dict = {\"train-time-minutes\": runtime_mins}\n",
    "\n",
    "_ = [metrics_dict.update({key: layer_history.history[key][-1]}) for key in val_keys]\n",
    "\n",
    "vertex_ai.log_metrics(metrics_dict) # JT TODO removed for 'total_loss' getting nan\n",
    "\n",
    "vertex_ai.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c3a789c8-598f-47e1-a72c-2f3dc30b6be7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fdcdc23e750>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d1b5e576-db76-46ee-954e-5b6742db8a73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<src.two_tower_jt.two_tower.TheTwoTowers at 0x7f3daa1a3b10>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2450b70a-d6dd-44e9-b7ce-dc439594d0bc",
   "metadata": {},
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0f08e0-7fb7-48d2-99d2-640cd7405b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "eval_dict_v1 = model.evaluate(valid_dataset, return_dict=True)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "elapsed_mins = int((end_time - start_time) / 60)\n",
    "print(f\"elapsed_mins: {elapsed_mins}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9b0c75-7eaf-466c-b452-680addc20a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dict_v1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa43294b-3a5d-482c-a0fc-ea37f1d6cb78",
   "metadata": {},
   "source": [
    "### Efficient eval\n",
    "\n",
    "* approximate with scann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6cca1457-4917-4aa3-81c1-6227e4bea23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "scann = tfrs.layers.factorized_top_k.ScaNN(\n",
    "    num_reordering_candidates=500,\n",
    "    num_leaves_to_search=30\n",
    ")\n",
    "scann.index_from_dataset(candidates=parsed_candidate_dataset.batch(128).cache().map(lambda x: (x['track_uri_can'], model.candidate_tower(x))))\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "elapsed_scann_mins = int((end_time - start_time) / 60)\n",
    "print(f\"elapsed_scann_mins: {elapsed_scann_mins}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0607f90f-843e-4d11-b05c-5085d6e61910",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "model.task.factorized_metrics = tfrs.metrics.FactorizedTopK(\n",
    "    candidates=scann\n",
    ")\n",
    "model.compile()\n",
    "\n",
    "scann_result = model.evaluate(valid_dataset, return_dict=True, verbose=1)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "elapsed_scann_eval_mins = int((end_time - start_time) / 60)\n",
    "print(f\"elapsed_scann_eval_mins: {elapsed_scann_eval_mins}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c6e1cd-0e65-450b-9490-2f497ce65744",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### You can access the experiment from the console via the experiment name you just declared:\n",
    "\n",
    "![](img/experiment-console.png)\n",
    "\n",
    "![](img/tensorboard.png)\n",
    "\n",
    "### Also, while this is running - check out the Tensorboard profiler in `utils`.\n",
    "\n",
    "![](img/tb-profiler.png)\n",
    "\n",
    "### Run `nvtop` - check out the installation script in `utils` - `install_nvtop.sh`\n",
    "\n",
    "![](img/nvtop-optimized.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "75a8b573-ba18-4b2d-a507-daf95f9c9e71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total runtime: 105 minutes\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total runtime: {runtime_mins} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5ceb80-2365-4b85-8218-3f721ddebada",
   "metadata": {},
   "source": [
    "### When complete you get a decent model with around 30-40 hit rate for top 1\n",
    "\n",
    "![](img/tb-metrics.png)\n",
    "![](img/tb-loss.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "49e6170e-970e-4c7d-912f-3998adbd37ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train-time-minutes': 222,\n",
       " 'factorized_top_k/top_1_categorical_accuracy': 0.0,\n",
       " 'factorized_top_k/top_5_categorical_accuracy': 0.0,\n",
       " 'factorized_top_k/top_10_categorical_accuracy': 0.0,\n",
       " 'factorized_top_k/top_50_categorical_accuracy': 0.0,\n",
       " 'factorized_top_k/top_100_categorical_accuracy': 0.0,\n",
       " 'loss': 100245.734375,\n",
       " 'regularization_loss': 0,\n",
       " 'total_loss': 100245.734375}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get metrics for the Vertex Experiment\n",
    "metrics_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c559139c-d669-43b1-8ac2-6155b10ef83e",
   "metadata": {},
   "source": [
    "### Now, save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56f1957-1883-4527-83ee-fd5c9857f7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, create the bucket to store the tensorflow models\n",
    "# ! gsutil mb -l us-central1 $path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "559d57ed-540c-41f4-b7cf-d3dad716f063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: gs://two-tower-models/run-spotify-nb-train-full-jt-20221227-214932/query_model/assets\n",
      "INFO:tensorflow:Assets written to: gs://two-tower-models/run-spotify-nb-train-full-jt-20221227-214932/candidate_model/assets\n"
     ]
    }
   ],
   "source": [
    "#save the models\n",
    "\n",
    "tf.saved_model.save(model.query_tower, export_dir=path + f\"/{RUN_NAME}/query_model\")\n",
    "tf.saved_model.save(model.candidate_tower, export_dir=path + f\"/{RUN_NAME}/candidate_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ea4dd5-2003-472a-a501-19e029cd14df",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Save the candidate embeddings to GCS for use in Matching Engine later\n",
    "These will be the files we use for the index\n",
    "\n",
    "This does the following\n",
    "1) Create a tf pipeline to convert embeddings to numpy\n",
    "2) Serialize the candidate song emgeddings with the song_uri index and save to gcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e820278f-5dbf-4aca-858e-df32ee4abe4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a tf function to convert any bad null values\n",
    "def tf_if_null_return_zero(val):\n",
    "    \"\"\"\n",
    "    this function fills in nans to zeros - sometimes happens in embedding calcs.\n",
    "    this will clean the embedding inputs downstream\n",
    "    \"\"\"\n",
    "    return(tf.clip_by_value(val, -1e12, 1e12)) # a trick to remove NANs post tf2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ee8e4e21-c560-4148-93ea-d30f5a946861",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_embeddings = tt.parsed_candidate_dataset.batch(10000).map(lambda x: [x['track_uri_can'], tf_if_null_return_zero(model.candidate_tower(x))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8c7d452a-3137-41a5-a48a-92e686abafde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'candidate_embeddings.json': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# Save to the required format\n",
    "# make sure you start out with a clean empty file for the append write\n",
    "!rm candidate_embeddings.json > /dev/null \n",
    "!touch candidate_embeddings.json\n",
    "for batch in candidate_embeddings:\n",
    "    songs, embeddings = batch\n",
    "    with open(\"candidate_embeddings.json\", 'a') as f:\n",
    "        for song, emb in zip(songs.numpy(), embeddings.numpy()):\n",
    "            f.write('{\"id\":\"' + str(song) + '\",\"embedding\":[' + \",\".join(str(x) for x in list(emb)) + ']}')\n",
    "            f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ced7ce65-b1e5-49b9-824e-3cabedd656f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File candidate_embeddings.json uploaded to run-spotify-nb-train-full-jt-20221227-214932/candidates/candidate_embeddings.json.\n"
     ]
    }
   ],
   "source": [
    "tt.upload_blob(\n",
    "    'two-tower-models', \n",
    "    'candidate_embeddings.json', \n",
    "    f'{RUN_NAME}/candidates/candidate_embeddings.json'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab8cb39-707d-4dc9-9d80-32b198b5cdc3",
   "metadata": {
    "tags": []
   },
   "source": [
    "Do a quick line count from terminal - should look like this:\n",
    "\n",
    "```\n",
    "(base) jupyter@tf28-jsw-sep-a100:~/spotify_mpd_two_tower$ wc -l candidate_embeddings.json \n",
    "2249561 candidate_embeddings.json\n",
    "```\n",
    "\n",
    "### Finished\n",
    "\n",
    "Go on to the [03 notebook](03-matching-engine.ipynb)\n",
    "\n",
    "You should see results similar to the screenshot below\n",
    "![](img/embeddings.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "59c2e2b2-74a8-4485-9db0-22146e4a5159",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_OptionsDataset element_spec={'album_name_can': TensorSpec(shape=(None,), dtype=tf.string, name=None), 'album_name_pl': TensorSpec(shape=(None, 5), dtype=tf.string, name=None), 'album_uri_can': TensorSpec(shape=(None,), dtype=tf.string, name=None), 'album_uri_pl': TensorSpec(shape=(None, 5), dtype=tf.string, name=None), 'artist_followers_can': TensorSpec(shape=(None,), dtype=tf.float32, name=None), 'artist_genres_can': TensorSpec(shape=(None,), dtype=tf.string, name=None), 'artist_genres_pl': TensorSpec(shape=(None, 5), dtype=tf.string, name=None), 'artist_name_can': TensorSpec(shape=(None,), dtype=tf.string, name=None), 'artist_name_pl': TensorSpec(shape=(None, 5), dtype=tf.string, name=None), 'artist_pop_can': TensorSpec(shape=(None,), dtype=tf.float32, name=None), 'artist_pop_pl': TensorSpec(shape=(None, 5), dtype=tf.float32, name=None), 'artist_uri_can': TensorSpec(shape=(None,), dtype=tf.string, name=None), 'artist_uri_pl': TensorSpec(shape=(None, 5), dtype=tf.string, name=None), 'artists_followers_pl': TensorSpec(shape=(None, 5), dtype=tf.float32, name=None), 'duration_ms_can': TensorSpec(shape=(None,), dtype=tf.float32, name=None), 'duration_ms_songs_pl': TensorSpec(shape=(None, 5), dtype=tf.float32, name=None), 'num_pl_albums_new': TensorSpec(shape=(None,), dtype=tf.float32, name=None), 'num_pl_artists_new': TensorSpec(shape=(None,), dtype=tf.float32, name=None), 'num_pl_songs_new': TensorSpec(shape=(None,), dtype=tf.float32, name=None), 'pl_collaborative_src': TensorSpec(shape=(None,), dtype=tf.string, name=None), 'pl_duration_ms_new': TensorSpec(shape=(None,), dtype=tf.float32, name=None), 'pl_name_src': TensorSpec(shape=(None,), dtype=tf.string, name=None), 'time_signature_can': TensorSpec(shape=(None,), dtype=tf.string, name=None), 'time_signature_pl': TensorSpec(shape=(None, 5), dtype=tf.string, name=None), 'track_acousticness_can': TensorSpec(shape=(None,), dtype=tf.float32, name=None), 'track_acousticness_pl': TensorSpec(shape=(None, 5), dtype=tf.float32, name=None), 'track_danceability_can': TensorSpec(shape=(None,), dtype=tf.float32, name=None), 'track_danceability_pl': TensorSpec(shape=(None, 5), dtype=tf.float32, name=None), 'track_energy_can': TensorSpec(shape=(None,), dtype=tf.float32, name=None), 'track_energy_pl': TensorSpec(shape=(None, 5), dtype=tf.float32, name=None), 'track_instrumentalness_can': TensorSpec(shape=(None,), dtype=tf.float32, name=None), 'track_instrumentalness_pl': TensorSpec(shape=(None, 5), dtype=tf.float32, name=None), 'track_key_can': TensorSpec(shape=(None,), dtype=tf.string, name=None), 'track_key_pl': TensorSpec(shape=(None, 5), dtype=tf.string, name=None), 'track_liveness_can': TensorSpec(shape=(None,), dtype=tf.float32, name=None), 'track_liveness_pl': TensorSpec(shape=(None, 5), dtype=tf.float32, name=None), 'track_loudness_can': TensorSpec(shape=(None,), dtype=tf.float32, name=None), 'track_loudness_pl': TensorSpec(shape=(None, 5), dtype=tf.float32, name=None), 'track_mode_can': TensorSpec(shape=(None,), dtype=tf.string, name=None), 'track_mode_pl': TensorSpec(shape=(None, 5), dtype=tf.string, name=None), 'track_name_can': TensorSpec(shape=(None,), dtype=tf.string, name=None), 'track_name_pl': TensorSpec(shape=(None, 5), dtype=tf.string, name=None), 'track_pop_can': TensorSpec(shape=(None,), dtype=tf.float32, name=None), 'track_pop_pl': TensorSpec(shape=(None, 5), dtype=tf.float32, name=None), 'track_speechiness_can': TensorSpec(shape=(None,), dtype=tf.float32, name=None), 'track_speechiness_pl': TensorSpec(shape=(None, 5), dtype=tf.float32, name=None), 'track_tempo_can': TensorSpec(shape=(None,), dtype=tf.float32, name=None), 'track_tempo_pl': TensorSpec(shape=(None, 5), dtype=tf.float32, name=None), 'track_uri_can': TensorSpec(shape=(None,), dtype=tf.string, name=None), 'track_uri_pl': TensorSpec(shape=(None, 5), dtype=tf.string, name=None), 'track_valence_can': TensorSpec(shape=(None,), dtype=tf.float32, name=None), 'track_valence_pl': TensorSpec(shape=(None, 5), dtype=tf.float32, name=None)}>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "728b2fb7-f01d-4804-9416-e52373cd5749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'album_name_can': b'Capoeira Electronica', 'album_name_pl': array([b'Odilara', b'Capoeira Electronica', b'Capoeira Ultimate',\n",
      "       b'Festa Popular', b'Capoeira Electronica'], dtype=object), 'album_uri_can': b'spotify:album:2FsSSHGt8JM0JgRy6ZX3kR', 'album_uri_pl': array([b'spotify:album:4Y8RfvZzCiApBCIZswj9Ry',\n",
      "       b'spotify:album:2FsSSHGt8JM0JgRy6ZX3kR',\n",
      "       b'spotify:album:55HHBqZ2SefPeaENOgWxYK',\n",
      "       b'spotify:album:150L1V6UUT7fGUI3PbxpkE',\n",
      "       b'spotify:album:2FsSSHGt8JM0JgRy6ZX3kR'], dtype=object), 'artist_followers_can': 5170.0, 'artist_genres_can': b\"'capoeira'\", 'artist_genres_pl': array([b\"'samba moderno'\", b\"'capoeira'\", b\"'capoeira'\", b'NONE',\n",
      "       b\"'capoeira'\"], dtype=object), 'artist_name_can': b'Capoeira Experience', 'artist_name_pl': array([b'Odilara', b'Capoeira Experience', b'Denis Porto', b'Zambe',\n",
      "       b'Capoeira Experience'], dtype=object), 'artist_pop_can': 24.0, 'artist_pop_pl': array([ 4., 24.,  2.,  0., 24.], dtype=float32), 'artist_uri_can': b'spotify:artist:5SKEXbgzIdRl3gQJ23CnUP', 'artist_uri_pl': array([b'spotify:artist:72oameojLOPWYB7nB8rl6c',\n",
      "       b'spotify:artist:5SKEXbgzIdRl3gQJ23CnUP',\n",
      "       b'spotify:artist:67p5GMYQZOgaAfx1YyttQk',\n",
      "       b'spotify:artist:4fH3OXCRcPsaHFE5KhgqZS',\n",
      "       b'spotify:artist:5SKEXbgzIdRl3gQJ23CnUP'], dtype=object), 'artists_followers_pl': array([ 316., 5170.,  448.,   19., 5170.], dtype=float32), 'duration_ms_can': 192640.0, 'duration_ms_songs_pl': array([234612., 226826., 203480., 287946., 271920.], dtype=float32), 'num_pl_albums_new': 9.0, 'num_pl_artists_new': 5.0, 'num_pl_songs_new': 85.0, 'pl_collaborative_src': b'false', 'pl_duration_ms_new': 17971314.0, 'pl_name_src': b'Capoeira', 'time_signature_can': b'4', 'time_signature_pl': array([b'4', b'4', b'4', b'4', b'4'], dtype=object), 'track_acousticness_can': 0.478, 'track_acousticness_pl': array([0.238 , 0.105 , 0.0242, 0.125 , 0.304 ], dtype=float32), 'track_danceability_can': 0.709, 'track_danceability_pl': array([0.703, 0.712, 0.806, 0.529, 0.821], dtype=float32), 'track_energy_can': 0.742, 'track_energy_pl': array([0.743, 0.41 , 0.794, 0.776, 0.947], dtype=float32), 'track_instrumentalness_can': 0.00297, 'track_instrumentalness_pl': array([4.84e-06, 4.30e-01, 7.42e-04, 4.01e-01, 5.07e-03], dtype=float32), 'track_key_can': b'0', 'track_key_pl': array([b'5', b'0', b'1', b'10', b'10'], dtype=object), 'track_liveness_can': 0.0346, 'track_liveness_pl': array([0.128 , 0.0725, 0.191 , 0.105 , 0.0552], dtype=float32), 'track_loudness_can': -7.295, 'track_loudness_pl': array([-8.638, -8.754, -9.084, -7.04 , -6.694], dtype=float32), 'track_mode_can': b'1', 'track_mode_pl': array([b'0', b'1', b'1', b'0', b'1'], dtype=object), 'track_name_can': b'Bezouro Preto - Studio', 'track_name_pl': array([b'O Telefone Tocou Novamente', b'Bem Devagar - Studio',\n",
      "       b'Angola Dream', b'Janaina', b'Louco Berimbau - Studio'],\n",
      "      dtype=object), 'track_pop_can': 3.0, 'track_pop_pl': array([5., 1., 0., 0., 1.], dtype=float32), 'track_speechiness_can': 0.0802, 'track_speechiness_pl': array([0.0367, 0.0272, 0.0407, 0.132 , 0.0734], dtype=float32), 'track_tempo_can': 172.238, 'track_tempo_pl': array([100.039,  89.089, 123.999, 119.963, 119.214], dtype=float32), 'track_uri_can': b'spotify:track:0tlhK4OvpHCYpReTABvKFb', 'track_uri_pl': array([b'spotify:track:1pQkOdcTDfLr84TDCrmGy7',\n",
      "       b'spotify:track:39grEDsAHAjmo2QFo4G8D9',\n",
      "       b'spotify:track:5vxSLdJXqbKYH487YO8LSL',\n",
      "       b'spotify:track:6T9GbmZ6voDM4aTBsG5VDh',\n",
      "       b'spotify:track:7ELt9eslVvWo276pX2garN'], dtype=object), 'track_valence_can': 0.844, 'track_valence_pl': array([0.966, 0.667, 0.696, 0.876, 0.655], dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "for tensor_dict in valid_dataset.unbatch().skip(1000).take(1):\n",
    "    td_keys = tensor_dict.keys()\n",
    "    list_dict = {}\n",
    "    for k in td_keys:\n",
    "        list_dict.update({k: tensor_dict[k].numpy()})\n",
    "    print(list_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1318107a-0747-4b7a-9f17-fcf83c975de7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'album_name_can': b'Capoeira Electronica',\n",
       " 'album_name_pl': array([b'Odilara', b'Capoeira Electronica', b'Capoeira Ultimate',\n",
       "        b'Festa Popular', b'Capoeira Electronica'], dtype=object),\n",
       " 'album_uri_can': b'spotify:album:2FsSSHGt8JM0JgRy6ZX3kR',\n",
       " 'album_uri_pl': array([b'spotify:album:4Y8RfvZzCiApBCIZswj9Ry',\n",
       "        b'spotify:album:2FsSSHGt8JM0JgRy6ZX3kR',\n",
       "        b'spotify:album:55HHBqZ2SefPeaENOgWxYK',\n",
       "        b'spotify:album:150L1V6UUT7fGUI3PbxpkE',\n",
       "        b'spotify:album:2FsSSHGt8JM0JgRy6ZX3kR'], dtype=object),\n",
       " 'artist_followers_can': 5170.0,\n",
       " 'artist_genres_can': b\"'capoeira'\",\n",
       " 'artist_genres_pl': array([b\"'samba moderno'\", b\"'capoeira'\", b\"'capoeira'\", b'NONE',\n",
       "        b\"'capoeira'\"], dtype=object),\n",
       " 'artist_name_can': b'Capoeira Experience',\n",
       " 'artist_name_pl': array([b'Odilara', b'Capoeira Experience', b'Denis Porto', b'Zambe',\n",
       "        b'Capoeira Experience'], dtype=object),\n",
       " 'artist_pop_can': 24.0,\n",
       " 'artist_pop_pl': array([ 4., 24.,  2.,  0., 24.], dtype=float32),\n",
       " 'artist_uri_can': b'spotify:artist:5SKEXbgzIdRl3gQJ23CnUP',\n",
       " 'artist_uri_pl': array([b'spotify:artist:72oameojLOPWYB7nB8rl6c',\n",
       "        b'spotify:artist:5SKEXbgzIdRl3gQJ23CnUP',\n",
       "        b'spotify:artist:67p5GMYQZOgaAfx1YyttQk',\n",
       "        b'spotify:artist:4fH3OXCRcPsaHFE5KhgqZS',\n",
       "        b'spotify:artist:5SKEXbgzIdRl3gQJ23CnUP'], dtype=object),\n",
       " 'artists_followers_pl': array([ 316., 5170.,  448.,   19., 5170.], dtype=float32),\n",
       " 'duration_ms_can': 192640.0,\n",
       " 'duration_ms_songs_pl': array([234612., 226826., 203480., 287946., 271920.], dtype=float32),\n",
       " 'num_pl_albums_new': 9.0,\n",
       " 'num_pl_artists_new': 5.0,\n",
       " 'num_pl_songs_new': 85.0,\n",
       " 'pl_collaborative_src': b'false',\n",
       " 'pl_duration_ms_new': 17971314.0,\n",
       " 'pl_name_src': b'Capoeira',\n",
       " 'time_signature_can': b'4',\n",
       " 'time_signature_pl': array([b'4', b'4', b'4', b'4', b'4'], dtype=object),\n",
       " 'track_acousticness_can': 0.478,\n",
       " 'track_acousticness_pl': array([0.238 , 0.105 , 0.0242, 0.125 , 0.304 ], dtype=float32),\n",
       " 'track_danceability_can': 0.709,\n",
       " 'track_danceability_pl': array([0.703, 0.712, 0.806, 0.529, 0.821], dtype=float32),\n",
       " 'track_energy_can': 0.742,\n",
       " 'track_energy_pl': array([0.743, 0.41 , 0.794, 0.776, 0.947], dtype=float32),\n",
       " 'track_instrumentalness_can': 0.00297,\n",
       " 'track_instrumentalness_pl': array([4.84e-06, 4.30e-01, 7.42e-04, 4.01e-01, 5.07e-03], dtype=float32),\n",
       " 'track_key_can': b'0',\n",
       " 'track_key_pl': array([b'5', b'0', b'1', b'10', b'10'], dtype=object),\n",
       " 'track_liveness_can': 0.0346,\n",
       " 'track_liveness_pl': array([0.128 , 0.0725, 0.191 , 0.105 , 0.0552], dtype=float32),\n",
       " 'track_loudness_can': -7.295,\n",
       " 'track_loudness_pl': array([-8.638, -8.754, -9.084, -7.04 , -6.694], dtype=float32),\n",
       " 'track_mode_can': b'1',\n",
       " 'track_mode_pl': array([b'0', b'1', b'1', b'0', b'1'], dtype=object),\n",
       " 'track_name_can': b'Bezouro Preto - Studio',\n",
       " 'track_name_pl': array([b'O Telefone Tocou Novamente', b'Bem Devagar - Studio',\n",
       "        b'Angola Dream', b'Janaina', b'Louco Berimbau - Studio'],\n",
       "       dtype=object),\n",
       " 'track_pop_can': 3.0,\n",
       " 'track_pop_pl': array([5., 1., 0., 0., 1.], dtype=float32),\n",
       " 'track_speechiness_can': 0.0802,\n",
       " 'track_speechiness_pl': array([0.0367, 0.0272, 0.0407, 0.132 , 0.0734], dtype=float32),\n",
       " 'track_tempo_can': 172.238,\n",
       " 'track_tempo_pl': array([100.039,  89.089, 123.999, 119.963, 119.214], dtype=float32),\n",
       " 'track_uri_can': b'spotify:track:0tlhK4OvpHCYpReTABvKFb',\n",
       " 'track_uri_pl': array([b'spotify:track:1pQkOdcTDfLr84TDCrmGy7',\n",
       "        b'spotify:track:39grEDsAHAjmo2QFo4G8D9',\n",
       "        b'spotify:track:5vxSLdJXqbKYH487YO8LSL',\n",
       "        b'spotify:track:6T9GbmZ6voDM4aTBsG5VDh',\n",
       "        b'spotify:track:7ELt9eslVvWo276pX2garN'], dtype=object),\n",
       " 'track_valence_can': 0.844,\n",
       " 'track_valence_pl': array([0.966, 0.667, 0.696, 0.876, 0.655], dtype=float32)}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_dict"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m97",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m97"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
