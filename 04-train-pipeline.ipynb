{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "152d43dc-e7c4-4085-b214-b576e1b4a94e",
   "metadata": {},
   "source": [
    "# Training pipeline for TFRS  2tower model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58440717-f517-42f4-bfb1-d22676d04044",
   "metadata": {},
   "source": [
    "```\n",
    "tensorflow==2.10.1\n",
    "tensorflow-cloud==0.1.16\n",
    "tensorflow-datasets==4.6.0\n",
    "tensorflow-estimator==2.10.0\n",
    "tensorflow-hub==0.12.0\n",
    "tensorflow-io==0.27.0\n",
    "tensorflow-io-gcs-filesystem==0.27.0\n",
    "tensorflow-metadata==1.8.0\n",
    "tensorflow-probability==0.18.0\n",
    "tensorflow-recommenders==0.7.2\n",
    "tensorflow-serving-api==2.8.3\n",
    "tensorflow-transform==1.8.0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6a2a17-2a69-447f-a050-4e5ec3d261fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install kfp==1.8.18 --user\n",
    "# !pip install google-cloud-pipeline-components==1.0.32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43201bb1-8b40-4ae4-a415-b15eee0323cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! python3 -c \"import kfp; print('KFP SDK version: {}'.format(kfp.__version__))\"\n",
    "# ! python3 -c \"import google_cloud_pipeline_components; print('google_cloud_pipeline_components version: {}'.format(google_cloud_pipeline_components.__version__))\"\n",
    "# ! python3 -c \"import google.cloud.aiplatform; print('aiplatform SDK version: {}'.format(google.cloud.aiplatform.__version__))\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e3d319c-84e1-4cd2-8882-0f390bc21e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ID: hybrid-vertex\n",
      "PROJECT_NUM: 934903580331\n",
      "REGION: us-central1\n"
     ]
    }
   ],
   "source": [
    "GCP_PROJECTS = !gcloud config get-value project\n",
    "PROJECT_ID = GCP_PROJECTS[0]\n",
    "PROJECT_NUM = !gcloud projects list --filter=\"$PROJECT_ID\" --format=\"value(PROJECT_NUMBER)\"\n",
    "PROJECT_NUM = PROJECT_NUM[0]\n",
    "REGION = 'us-central1'\n",
    "\n",
    "print(f\"PROJECT_ID: {PROJECT_ID}\")\n",
    "print(f\"PROJECT_NUM: {PROJECT_NUM}\")\n",
    "print(f\"REGION: {REGION}\")\n",
    "\n",
    "VERTEX_SA = '934903580331-compute@developer.gserviceaccount.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68a4fb97-bed6-4533-92d3-dbc79e134f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "from time import time\n",
    "import pandas as pd\n",
    "# disable INFO and DEBUG logging everywhere\n",
    "import logging\n",
    "import time\n",
    "from pprint import pprint\n",
    "import pickle as pkl\n",
    "\n",
    "logging.disable(logging.WARNING)\n",
    "\n",
    "from google.cloud import aiplatform as vertex_ai\n",
    "from google.cloud import storage\n",
    "\n",
    "# Pipelines\n",
    "from typing import Any, Callable, Dict, NamedTuple, Optional, List\n",
    "from google_cloud_pipeline_components import aiplatform as gcc_aip\n",
    "from google_cloud_pipeline_components.types import artifact_types\n",
    "\n",
    "# Kubeflow SDK\n",
    "# TODO: fix these\n",
    "from kfp.v2 import dsl\n",
    "import kfp\n",
    "import kfp.v2.dsl\n",
    "from kfp.v2.google import client as pipelines_client\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output,\n",
    "                        OutputPath, component)\n",
    "\n",
    "storage_client = storage.Client(project=PROJECT_ID)\n",
    "\n",
    "vertex_ai.init(project=PROJECT_ID,location=REGION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b3d61c4-83ba-42d8-9b7f-e7aacea5e684",
   "metadata": {},
   "outputs": [],
   "source": [
    "VERSION= \"jtv5\"\n",
    "\n",
    "# TODO: update\n",
    "# BUCKET_DATA_DIR = 'spotify-data-regimes'\n",
    "# TRAIN_DIR_PREFIX = f'{VERSION}/train'                 # subset: valid_v9 | train_v9\n",
    "# VALID_DIR_PREFIX = f'{VERSION}/valid'                 # valid_v9 | train_v9\n",
    "# CANDIDATE_PREFIX = f'{VERSION}/candidates' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd32b85b-b44a-4e17-9c23-98fad8efe064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL_ROOT_NAME: sp-2tower-tfrs-jtv5-pipev3\n"
     ]
    }
   ],
   "source": [
    "# PREFIX = 'spotify-2tower'\n",
    "APP='sp'\n",
    "MODEL_TYPE='2tower'\n",
    "FRAMEWORK = 'tfrs'\n",
    "PIPELINE_VERSION = 'pipev3'\n",
    "MODEL_ROOT_NAME = f'{APP}-{MODEL_TYPE}-{FRAMEWORK}-{VERSION}-{PIPELINE_VERSION}'\n",
    "\n",
    "print(f\"MODEL_ROOT_NAME: {MODEL_ROOT_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e816a896-4015-4d0b-8a39-04d9a5a5b924",
   "metadata": {},
   "source": [
    "## Write Train files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "022ba9e5-95fb-47b2-8718-76687ad0dcc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "REPO_DOCKER_PATH_PREFIX = 'src'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c02857dd-731b-40ba-ac58-74286774fbf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMAGE_URI: gcr.io/hybrid-vertex/sp-2tower-tfrs-jtv5-pipev3-training\n"
     ]
    }
   ],
   "source": [
    "# Docker definitions for training\n",
    "IMAGE_NAME = f'{MODEL_ROOT_NAME}-training'\n",
    "IMAGE_URI = f'gcr.io/{PROJECT_ID}/{IMAGE_NAME}'\n",
    "\n",
    "DOCKERNAME = 'tfrs'\n",
    "REPO_DOCKER_PATH_PREFIX = 'src'\n",
    "MACHINE_TYPE ='e2-highcpu-32'\n",
    "FILE_LOCATION = './src'\n",
    "\n",
    "print(f\"IMAGE_URI: {IMAGE_URI}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46e4a95f-40f2-47b4-af3f-2d49cccd82d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/cloudbuild.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/cloudbuild.yaml\n",
    "\n",
    "steps:\n",
    "- name: 'gcr.io/cloud-builders/docker'\n",
    "  args: ['build', '-t', '$_IMAGE_URI', '$_FILE_LOCATION', '-f', '$_FILE_LOCATION/Dockerfile.$_DOCKERNAME']\n",
    "images:\n",
    "- '$_IMAGE_URI'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad807fb4-b7a4-4ce0-967c-deb73f32536b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/two_tower_jt/train_config.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/two_tower_jt/train_config.py\n",
    "\n",
    "# PROJECT_ID = 'hybrid-vertex'\n",
    "# NEW_ADAPTS = 'True'\n",
    "# USE_CROSS_LAYER = True\n",
    "# USE_DROPOUT = 'True'\n",
    "# SEED = 1234\n",
    "MAX_PLAYLIST_LENGTH = 15\n",
    "# EMBEDDING_DIM = 128   \n",
    "# PROJECTION_DIM = 25  \n",
    "# SEED = 1234\n",
    "# DROPOUT_RATE = 0.33\n",
    "# MAX_TOKENS = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d80ee63-a858-4ffe-9f6d-39e17afb486c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/two_tower_jt/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/two_tower_jt/requirements.txt\n",
    "google-cloud-aiplatform>=1.21.0\n",
    "tensorflow-recommenders==0.7.2\n",
    "tensorboard==2.10.1\n",
    "tensorboard-data-server==0.6.1\n",
    "tensorboard-plugin-profile==2.11.1\n",
    "tensorflow-io==0.27.0\n",
    "google-cloud-aiplatform[cloud_profiler]>=1.20.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b81e499-9dd2-42c6-a2af-91f6849a5b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/Dockerfile.tfrs\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/Dockerfile.{DOCKERNAME}\n",
    "\n",
    "# FROM tensorflow/tensorflow:2.10.1-gpu\n",
    "FROM gcr.io/deeplearning-platform-release/tf-gpu.2-10\n",
    "\n",
    "WORKDIR /src\n",
    "\n",
    "# Copies the trainer code to the docker image.\n",
    "COPY two_tower_jt/* two_tower_jt/ \n",
    "\n",
    "RUN pip install -r two_tower_jt/requirements.txt\n",
    "\n",
    "RUN apt update && apt -y install nvtop\n",
    "\n",
    "# # Sets up the entry point to invoke the trainer.\n",
    "# # ENTRYPOINT [\"python\", \"-m\", \"two_tower_jt.task\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85623af4-764f-4316-bde2-d4ae8cbbdf60",
   "metadata": {},
   "source": [
    "## Build Custom Train Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c76bfb28-6fd5-4d96-8d17-ba5e8212a43b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOCKERNAME: tfrs\n",
      "IMAGE_URI: gcr.io/hybrid-vertex/sp-2tower-tfrs-jtv5-pipev3-training\n",
      "FILE_LOCATION: ./src\n",
      "MACHINE_TYPE: e2-highcpu-32\n"
     ]
    }
   ],
   "source": [
    "print(f\"DOCKERNAME: {DOCKERNAME}\")\n",
    "print(f\"IMAGE_URI: {IMAGE_URI}\")\n",
    "print(f\"FILE_LOCATION: {FILE_LOCATION}\")\n",
    "print(f\"MACHINE_TYPE: {MACHINE_TYPE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b85202c-6495-4ff4-82d0-f0ffcc709725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/jw-repo/spotify_mpd_two_tower\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "351cbc55-3efc-48c1-ac1b-5917d10faaf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m/home/jupyter/jw-repo/spotify_mpd_two_tower/src\u001b[00m\n",
      "├── Dockerfile.tfrs\n",
      "├── cloudbuild.yaml\n",
      "├── \u001b[01;34mtrain_pipes\u001b[00m\n",
      "│   ├── \u001b[01;34m__pycache__\u001b[00m\n",
      "│   │   ├── adapt_fixed_text_layer_vocab.cpython-37.pyc\n",
      "│   │   ├── adapt_ragged_text_layer_vocab.cpython-37.pyc\n",
      "│   │   ├── build_custom_image.cpython-37.pyc\n",
      "│   │   ├── create_ann_index.cpython-37.pyc\n",
      "│   │   ├── create_ann_index_endpoint_vpc.cpython-37.pyc\n",
      "│   │   ├── create_brute_force_index.cpython-37.pyc\n",
      "│   │   ├── create_brute_index_endpoint_vpc.cpython-37.pyc\n",
      "│   │   ├── create_master_vocab.cpython-37.pyc\n",
      "│   │   ├── create_tensorboard.cpython-37.pyc\n",
      "│   │   ├── deploy_ann_index.cpython-37.pyc\n",
      "│   │   ├── deploy_brute_index.cpython-37.pyc\n",
      "│   │   ├── generate_candidates.cpython-37.pyc\n",
      "│   │   ├── pipeline_config.cpython-37.pyc\n",
      "│   │   ├── test_index_recall.cpython-37.pyc\n",
      "│   │   └── train_custom_model.cpython-37.pyc\n",
      "│   ├── adapt_fixed_text_layer_vocab.py\n",
      "│   ├── adapt_ragged_text_layer_vocab.py\n",
      "│   ├── build_custom_image.py\n",
      "│   ├── create_ann_index.py\n",
      "│   ├── create_ann_index_endpoint_vpc.py\n",
      "│   ├── create_brute_force_index.py\n",
      "│   ├── create_brute_index_endpoint_vpc.py\n",
      "│   ├── create_master_vocab.py\n",
      "│   ├── create_tensorboard.py\n",
      "│   ├── deploy_ann_index.py\n",
      "│   ├── deploy_brute_index.py\n",
      "│   ├── generate_candidates.py\n",
      "│   ├── pipeline_config.py\n",
      "│   ├── test_index_recall.py\n",
      "│   └── train_custom_model.py\n",
      "├── \u001b[01;34mtwo_tower_jt\u001b[00m\n",
      "│   ├── __init__.py\n",
      "│   ├── \u001b[01;34m__pycache__\u001b[00m\n",
      "│   │   ├── __init__.cpython-37.pyc\n",
      "│   │   ├── test_instances.cpython-37.pyc\n",
      "│   │   ├── train_config.cpython-37.pyc\n",
      "│   │   └── two_tower.cpython-37.pyc\n",
      "│   ├── data-pipeline.py\n",
      "│   ├── interactive_train.py\n",
      "│   ├── requirements.txt\n",
      "│   ├── task.py\n",
      "│   ├── test_instances.py\n",
      "│   ├── train_config.py\n",
      "│   ├── two_tower.py\n",
      "│   └── two_tower_lite.py\n",
      "└── \u001b[01;34mvocab_pipes\u001b[00m\n",
      "    ├── adapt_fixed_text_layer_vocab.py\n",
      "    ├── adapt_ragged_text_layer_vocab.py\n",
      "    ├── config.py\n",
      "    └── create_master_vocab.py\n",
      "\n",
      "5 directories, 49 files\n"
     ]
    }
   ],
   "source": [
    "!tree /home/jupyter/jw-repo/spotify_mpd_two_tower/src"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fcdae75-a4cb-476f-8d15-89c2f69ea417",
   "metadata": {},
   "source": [
    "### Optionally include a `.gcloudignore` file \n",
    "\n",
    "* limits the files submitted to Cloud Build\n",
    "* see [gcloudignore](https://cloud.google.com/sdk/gcloud/reference/topic/gcloudignore) for details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1433a749-df25-4862-8f27-b9c84442b1ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [gcloudignore/enabled].\n"
     ]
    }
   ],
   "source": [
    "! gcloud config set gcloudignore/enabled true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2c430b9f-556f-40ec-bb68-1ff8d2d9bd9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting .gcloudignore\n"
     ]
    }
   ],
   "source": [
    "%%writefile .gcloudignore\n",
    ".gcloudignore\n",
    "/local_files/\n",
    "/img/\n",
    "*.pkl\n",
    "*.png\n",
    ".git\n",
    ".github\n",
    ".ipynb_checkpoints/*\n",
    "*__pycache__\n",
    "*cpython-37.pyc\n",
    "spotipy_secret_creds.py\n",
    "candidate_embs_local_20230130-180710.json\n",
    "vocab_dict.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d6c3cd53-5595-4524-88cd-a2f6b4c31c7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !gcloud meta list-files-for-upload\n",
    "# !ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "94546ac1-8b61-4fff-981a-9dae7f976fdb",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 77 file(s) totalling 1.8 MiB before compression.\n",
      "Uploading tarball of [.] to [gs://hybrid-vertex_cloudbuild/source/1676353668.064571-9739bcfbd4ff4ace83214bef350f82a7.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/hybrid-vertex/locations/global/builds/6d6b4738-4edf-4fcf-aff8-5309a9860b0e].\n",
      "Logs are available at [ https://console.cloud.google.com/cloud-build/builds/6d6b4738-4edf-4fcf-aff8-5309a9860b0e?project=934903580331 ].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"6d6b4738-4edf-4fcf-aff8-5309a9860b0e\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://hybrid-vertex_cloudbuild/source/1676353668.064571-9739bcfbd4ff4ace83214bef350f82a7.tgz#1676353668665894\n",
      "Copying gs://hybrid-vertex_cloudbuild/source/1676353668.064571-9739bcfbd4ff4ace83214bef350f82a7.tgz#1676353668665894...\n",
      "/ [1 files][295.2 KiB/295.2 KiB]                                                \n",
      "Operation completed over 1 objects/295.2 KiB.\n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  258.6kB\n",
      "Step 1/5 : FROM gcr.io/deeplearning-platform-release/tf-gpu.2-10\n",
      "latest: Pulling from deeplearning-platform-release/tf-gpu.2-10\n",
      "846c0b181fff: Pulling fs layer\n",
      "6fc9dd88827c: Pulling fs layer\n",
      "0b311d7060d0: Pulling fs layer\n",
      "326d76058f67: Pulling fs layer\n",
      "ed7e4c52c661: Pulling fs layer\n",
      "4f05f5570c7a: Pulling fs layer\n",
      "ab264e292103: Pulling fs layer\n",
      "527d1d5ab821: Pulling fs layer\n",
      "fc46d4e99009: Pulling fs layer\n",
      "9ccf692754fa: Pulling fs layer\n",
      "c6611ece70c4: Pulling fs layer\n",
      "fe6ff819b45f: Pulling fs layer\n",
      "c942e9416e07: Pulling fs layer\n",
      "139c62f03172: Pulling fs layer\n",
      "4f4fb700ef54: Pulling fs layer\n",
      "ac84182603c1: Pulling fs layer\n",
      "3860e93641d8: Pulling fs layer\n",
      "085441b54e3f: Pulling fs layer\n",
      "33b479b9d085: Pulling fs layer\n",
      "5ca4c4a165b6: Pulling fs layer\n",
      "86bfd6c69c42: Pulling fs layer\n",
      "a98f2a818740: Pulling fs layer\n",
      "e170e0655ccf: Pulling fs layer\n",
      "bd4c2b6fe3ff: Pulling fs layer\n",
      "dda86b68803d: Pulling fs layer\n",
      "7764511c6da3: Pulling fs layer\n",
      "56a69db5aee5: Pulling fs layer\n",
      "53bc18ba1883: Pulling fs layer\n",
      "ee4e8980b6be: Pulling fs layer\n",
      "50a6a9c26a84: Pulling fs layer\n",
      "4f0d87a457ad: Pulling fs layer\n",
      "6aa8aa286e6a: Pulling fs layer\n",
      "f37201cfff33: Pulling fs layer\n",
      "aee1ac3070bc: Pulling fs layer\n",
      "b78a76760f85: Pulling fs layer\n",
      "cbed8fa1c874: Pulling fs layer\n",
      "4ac3af8b3692: Pulling fs layer\n",
      "cdb012978bbb: Pulling fs layer\n",
      "527d1d5ab821: Waiting\n",
      "fc46d4e99009: Waiting\n",
      "9ccf692754fa: Waiting\n",
      "c6611ece70c4: Waiting\n",
      "fe6ff819b45f: Waiting\n",
      "c942e9416e07: Waiting\n",
      "139c62f03172: Waiting\n",
      "4f4fb700ef54: Waiting\n",
      "ac84182603c1: Waiting\n",
      "3860e93641d8: Waiting\n",
      "085441b54e3f: Waiting\n",
      "33b479b9d085: Waiting\n",
      "5ca4c4a165b6: Waiting\n",
      "86bfd6c69c42: Waiting\n",
      "a98f2a818740: Waiting\n",
      "e170e0655ccf: Waiting\n",
      "bd4c2b6fe3ff: Waiting\n",
      "dda86b68803d: Waiting\n",
      "7764511c6da3: Waiting\n",
      "56a69db5aee5: Waiting\n",
      "53bc18ba1883: Waiting\n",
      "ee4e8980b6be: Waiting\n",
      "50a6a9c26a84: Waiting\n",
      "4f0d87a457ad: Waiting\n",
      "6aa8aa286e6a: Waiting\n",
      "326d76058f67: Waiting\n",
      "ed7e4c52c661: Waiting\n",
      "f37201cfff33: Waiting\n",
      "aee1ac3070bc: Waiting\n",
      "b78a76760f85: Waiting\n",
      "cbed8fa1c874: Waiting\n",
      "4ac3af8b3692: Waiting\n",
      "cdb012978bbb: Waiting\n",
      "4f05f5570c7a: Waiting\n",
      "ab264e292103: Waiting\n",
      "0b311d7060d0: Download complete\n",
      "6fc9dd88827c: Verifying Checksum\n",
      "6fc9dd88827c: Download complete\n",
      "326d76058f67: Verifying Checksum\n",
      "326d76058f67: Download complete\n",
      "ed7e4c52c661: Verifying Checksum\n",
      "ed7e4c52c661: Download complete\n",
      "ab264e292103: Verifying Checksum\n",
      "ab264e292103: Download complete\n",
      "846c0b181fff: Verifying Checksum\n",
      "846c0b181fff: Download complete\n",
      "fc46d4e99009: Verifying Checksum\n",
      "fc46d4e99009: Download complete\n",
      "527d1d5ab821: Download complete\n",
      "c6611ece70c4: Verifying Checksum\n",
      "c6611ece70c4: Download complete\n",
      "846c0b181fff: Pull complete\n",
      "6fc9dd88827c: Pull complete\n",
      "0b311d7060d0: Pull complete\n",
      "326d76058f67: Pull complete\n",
      "ed7e4c52c661: Pull complete\n",
      "4f05f5570c7a: Download complete\n",
      "c942e9416e07: Verifying Checksum\n",
      "c942e9416e07: Download complete\n",
      "139c62f03172: Download complete\n",
      "4f4fb700ef54: Verifying Checksum\n",
      "4f4fb700ef54: Download complete\n",
      "9ccf692754fa: Verifying Checksum\n",
      "9ccf692754fa: Download complete\n",
      "ac84182603c1: Verifying Checksum\n",
      "ac84182603c1: Download complete\n",
      "085441b54e3f: Verifying Checksum\n",
      "085441b54e3f: Download complete\n",
      "33b479b9d085: Verifying Checksum\n",
      "33b479b9d085: Download complete\n",
      "5ca4c4a165b6: Verifying Checksum\n",
      "5ca4c4a165b6: Download complete\n",
      "86bfd6c69c42: Download complete\n",
      "3860e93641d8: Verifying Checksum\n",
      "3860e93641d8: Download complete\n",
      "e170e0655ccf: Verifying Checksum\n",
      "e170e0655ccf: Download complete\n",
      "bd4c2b6fe3ff: Verifying Checksum\n",
      "bd4c2b6fe3ff: Download complete\n",
      "dda86b68803d: Verifying Checksum\n",
      "dda86b68803d: Download complete\n",
      "7764511c6da3: Verifying Checksum\n",
      "7764511c6da3: Download complete\n",
      "56a69db5aee5: Verifying Checksum\n",
      "56a69db5aee5: Download complete\n",
      "53bc18ba1883: Verifying Checksum\n",
      "53bc18ba1883: Download complete\n",
      "ee4e8980b6be: Verifying Checksum\n",
      "ee4e8980b6be: Download complete\n",
      "50a6a9c26a84: Verifying Checksum\n",
      "50a6a9c26a84: Download complete\n",
      "4f0d87a457ad: Verifying Checksum\n",
      "4f0d87a457ad: Download complete\n",
      "6aa8aa286e6a: Verifying Checksum\n",
      "6aa8aa286e6a: Download complete\n",
      "f37201cfff33: Verifying Checksum\n",
      "f37201cfff33: Download complete\n",
      "a98f2a818740: Verifying Checksum\n",
      "a98f2a818740: Download complete\n",
      "fe6ff819b45f: Verifying Checksum\n",
      "fe6ff819b45f: Download complete\n",
      "cbed8fa1c874: Verifying Checksum\n",
      "cbed8fa1c874: Download complete\n",
      "4ac3af8b3692: Verifying Checksum\n",
      "4ac3af8b3692: Download complete\n",
      "cdb012978bbb: Verifying Checksum\n",
      "cdb012978bbb: Download complete\n",
      "aee1ac3070bc: Verifying Checksum\n",
      "aee1ac3070bc: Download complete\n",
      "4f05f5570c7a: Pull complete\n",
      "ab264e292103: Pull complete\n",
      "b78a76760f85: Verifying Checksum\n",
      "b78a76760f85: Download complete\n",
      "527d1d5ab821: Pull complete\n",
      "fc46d4e99009: Pull complete\n",
      "9ccf692754fa: Pull complete\n",
      "c6611ece70c4: Pull complete\n",
      "fe6ff819b45f: Pull complete\n",
      "c942e9416e07: Pull complete\n",
      "139c62f03172: Pull complete\n",
      "4f4fb700ef54: Pull complete\n",
      "ac84182603c1: Pull complete\n",
      "3860e93641d8: Pull complete\n",
      "085441b54e3f: Pull complete\n",
      "33b479b9d085: Pull complete\n",
      "5ca4c4a165b6: Pull complete\n",
      "86bfd6c69c42: Pull complete\n",
      "a98f2a818740: Pull complete\n",
      "e170e0655ccf: Pull complete\n",
      "bd4c2b6fe3ff: Pull complete\n",
      "dda86b68803d: Pull complete\n",
      "7764511c6da3: Pull complete\n",
      "56a69db5aee5: Pull complete\n",
      "53bc18ba1883: Pull complete\n",
      "ee4e8980b6be: Pull complete\n",
      "50a6a9c26a84: Pull complete\n",
      "4f0d87a457ad: Pull complete\n",
      "6aa8aa286e6a: Pull complete\n",
      "f37201cfff33: Pull complete\n",
      "aee1ac3070bc: Pull complete\n",
      "b78a76760f85: Pull complete\n",
      "cbed8fa1c874: Pull complete\n",
      "4ac3af8b3692: Pull complete\n",
      "cdb012978bbb: Pull complete\n",
      "Digest: sha256:29efd2853c37262f10d57e69228a6bed170a4ed78227c96f33586b31c3878c11\n",
      "Status: Downloaded newer image for gcr.io/deeplearning-platform-release/tf-gpu.2-10:latest\n",
      " ---> 9d3c2d44f624\n",
      "Step 2/5 : WORKDIR /src\n",
      " ---> Running in c6ad45e0a390\n",
      "Removing intermediate container c6ad45e0a390\n",
      " ---> b9e54ed3b7ff\n",
      "Step 3/5 : COPY two_tower_jt/* two_tower_jt/\n",
      " ---> 6abcc27c22e4\n",
      "Step 4/5 : RUN pip install -r two_tower_jt/requirements.txt\n",
      " ---> Running in abb3864af4bb\n",
      "Requirement already satisfied: google-cloud-aiplatform>=1.21.0 in /opt/conda/lib/python3.7/site-packages (from -r two_tower_jt/requirements.txt (line 1)) (1.21.0)\n",
      "Collecting tensorflow-recommenders==0.7.2\n",
      "  Downloading tensorflow_recommenders-0.7.2-py3-none-any.whl (89 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 89.3/89.3 kB 3.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: tensorboard==2.10.1 in /opt/conda/lib/python3.7/site-packages (from -r two_tower_jt/requirements.txt (line 3)) (2.10.1)\n",
      "Requirement already satisfied: tensorboard-data-server==0.6.1 in /opt/conda/lib/python3.7/site-packages (from -r two_tower_jt/requirements.txt (line 4)) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-profile==2.11.1 in /opt/conda/lib/python3.7/site-packages (from -r two_tower_jt/requirements.txt (line 5)) (2.11.1)\n",
      "Requirement already satisfied: tensorflow-io==0.27.0 in /opt/conda/lib/python3.7/site-packages (from -r two_tower_jt/requirements.txt (line 6)) (0.27.0)\n",
      "Requirement already satisfied: tensorflow>=2.9.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow-recommenders==0.7.2->-r two_tower_jt/requirements.txt (line 2)) (2.10.1)\n",
      "Requirement already satisfied: absl-py>=0.1.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow-recommenders==0.7.2->-r two_tower_jt/requirements.txt (line 2)) (1.4.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard==2.10.1->-r two_tower_jt/requirements.txt (line 3)) (1.8.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard==2.10.1->-r two_tower_jt/requirements.txt (line 3)) (2.16.0)\n",
      "Collecting protobuf<3.20,>=3.9.2\n",
      "  Downloading protobuf-3.19.6-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.1/1.1 MB 23.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard==2.10.1->-r two_tower_jt/requirements.txt (line 3)) (2.28.2)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard==2.10.1->-r two_tower_jt/requirements.txt (line 3)) (66.1.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard==2.10.1->-r two_tower_jt/requirements.txt (line 3)) (2.1.2)\n",
      "Requirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.7/site-packages (from tensorboard==2.10.1->-r two_tower_jt/requirements.txt (line 3)) (0.38.4)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard==2.10.1->-r two_tower_jt/requirements.txt (line 3)) (1.21.6)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard==2.10.1->-r two_tower_jt/requirements.txt (line 3)) (3.4.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard==2.10.1->-r two_tower_jt/requirements.txt (line 3)) (0.4.6)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard==2.10.1->-r two_tower_jt/requirements.txt (line 3)) (1.51.1)\n",
      "Requirement already satisfied: gviz-api>=1.9.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard-plugin-profile==2.11.1->-r two_tower_jt/requirements.txt (line 5)) (1.10.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard-plugin-profile==2.11.1->-r two_tower_jt/requirements.txt (line 5)) (1.16.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem==0.27.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow-io==0.27.0->-r two_tower_jt/requirements.txt (line 6)) (0.27.0)\n",
      "Requirement already satisfied: google-cloud-bigquery<4.0.0dev,>=1.15.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform>=1.21.0->-r two_tower_jt/requirements.txt (line 1)) (3.4.2)\n",
      "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform>=1.21.0->-r two_tower_jt/requirements.txt (line 1)) (1.8.1)\n",
      "Requirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform>=1.21.0->-r two_tower_jt/requirements.txt (line 1)) (2.10.1)\n",
      "Collecting packaging<22.0.0dev,>=14.3\n",
      "  Downloading packaging-21.3-py3-none-any.whl (40 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 40.8/40.8 kB 7.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: google-cloud-storage<3.0.0dev,>=1.32.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform>=1.21.0->-r two_tower_jt/requirements.txt (line 1)) (2.7.0)\n",
      "Requirement already satisfied: shapely<2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform>=1.21.0->-r two_tower_jt/requirements.txt (line 1)) (1.8.5.post1)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform>=1.21.0->-r two_tower_jt/requirements.txt (line 1)) (1.22.2)\n",
      "Collecting werkzeug>=1.0.1\n",
      "  Downloading Werkzeug-2.0.3-py3-none-any.whl (289 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 289.2/289.2 kB 35.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform>=1.21.0->-r two_tower_jt/requirements.txt (line 1)) (1.58.0)\n",
      "Collecting google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0\n",
      "  Downloading google_api_core-2.11.0-py3-none-any.whl (120 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 120.3/120.3 kB 19.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: grpcio-status<2.0dev,>=1.33.2 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform>=1.21.0->-r two_tower_jt/requirements.txt (line 1)) (1.48.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard==2.10.1->-r two_tower_jt/requirements.txt (line 3)) (4.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard==2.10.1->-r two_tower_jt/requirements.txt (line 3)) (0.2.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard==2.10.1->-r two_tower_jt/requirements.txt (line 3)) (5.3.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard==2.10.1->-r two_tower_jt/requirements.txt (line 3)) (1.3.1)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform>=1.21.0->-r two_tower_jt/requirements.txt (line 1)) (2.4.1)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform>=1.21.0->-r two_tower_jt/requirements.txt (line 1)) (2.3.2)\n",
      "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform>=1.21.0->-r two_tower_jt/requirements.txt (line 1)) (2.8.2)\n",
      "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /opt/conda/lib/python3.7/site-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform>=1.21.0->-r two_tower_jt/requirements.txt (line 1)) (0.12.6)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard==2.10.1->-r two_tower_jt/requirements.txt (line 3)) (6.0.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging<22.0.0dev,>=14.3->google-cloud-aiplatform>=1.21.0->-r two_tower_jt/requirements.txt (line 1)) (3.0.9)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard==2.10.1->-r two_tower_jt/requirements.txt (line 3)) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard==2.10.1->-r two_tower_jt/requirements.txt (line 3)) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard==2.10.1->-r two_tower_jt/requirements.txt (line 3)) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard==2.10.1->-r two_tower_jt/requirements.txt (line 3)) (2022.12.7)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.9.0->tensorflow-recommenders==0.7.2->-r two_tower_jt/requirements.txt (line 2)) (0.4.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.9.0->tensorflow-recommenders==0.7.2->-r two_tower_jt/requirements.txt (line 2)) (1.1.2)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.9.0->tensorflow-recommenders==0.7.2->-r two_tower_jt/requirements.txt (line 2)) (2.2.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.9.0->tensorflow-recommenders==0.7.2->-r two_tower_jt/requirements.txt (line 2)) (1.6.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.9.0->tensorflow-recommenders==0.7.2->-r two_tower_jt/requirements.txt (line 2)) (3.3.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.9.0->tensorflow-recommenders==0.7.2->-r two_tower_jt/requirements.txt (line 2)) (23.1.21)\n",
      "Requirement already satisfied: keras<2.11,>=2.10.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.9.0->tensorflow-recommenders==0.7.2->-r two_tower_jt/requirements.txt (line 2)) (2.10.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.9.0->tensorflow-recommenders==0.7.2->-r two_tower_jt/requirements.txt (line 2)) (3.8.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.9.0->tensorflow-recommenders==0.7.2->-r two_tower_jt/requirements.txt (line 2)) (15.0.6.1)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.9.0->tensorflow-recommenders==0.7.2->-r two_tower_jt/requirements.txt (line 2)) (1.14.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.9.0->tensorflow-recommenders==0.7.2->-r two_tower_jt/requirements.txt (line 2)) (4.4.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.11,>=2.10.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.9.0->tensorflow-recommenders==0.7.2->-r two_tower_jt/requirements.txt (line 2)) (2.10.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.9.0->tensorflow-recommenders==0.7.2->-r two_tower_jt/requirements.txt (line 2)) (0.2.0)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.7/site-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform>=1.21.0->-r two_tower_jt/requirements.txt (line 1)) (1.5.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard==2.10.1->-r two_tower_jt/requirements.txt (line 3)) (3.11.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard==2.10.1->-r two_tower_jt/requirements.txt (line 3)) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard==2.10.1->-r two_tower_jt/requirements.txt (line 3)) (3.2.2)\n",
      "Installing collected packages: werkzeug, protobuf, packaging, google-api-core, tensorflow-recommenders\n",
      "  Attempting uninstall: werkzeug\n",
      "    Found existing installation: Werkzeug 2.1.2\n",
      "    Uninstalling Werkzeug-2.1.2:\n",
      "      Successfully uninstalled Werkzeug-2.1.2\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.20.1\n",
      "    Uninstalling protobuf-3.20.1:\n",
      "      Successfully uninstalled protobuf-3.20.1\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 23.0\n",
      "    Uninstalling packaging-23.0:\n",
      "      Successfully uninstalled packaging-23.0\n",
      "  Attempting uninstall: google-api-core\n",
      "    Found existing installation: google-api-core 2.10.1\n",
      "    Uninstalling google-api-core-2.10.1:\n",
      "      Successfully uninstalled google-api-core-2.10.1\n",
      "\u001b[91mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "google-api-python-client 1.8.0 requires google-api-core<2dev,>=1.13.0, but you have google-api-core 2.11.0 which is incompatible.\n",
      "\u001b[0mSuccessfully installed google-api-core-2.11.0 packaging-21.3 protobuf-3.19.6 tensorflow-recommenders-0.7.2 werkzeug-2.0.3\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mRemoving intermediate container abb3864af4bb\n",
      " ---> 2ac0ff97cdb9\n",
      "Step 5/5 : RUN apt update && apt -y install nvtop\n",
      " ---> Running in eae83f0cf3fd\n",
      "\u001b[91m\n",
      "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
      "\n",
      "\u001b[0mGet:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease [1581 B]\n",
      "Get:2 http://packages.cloud.google.com/apt gcsfuse-focal InRelease [5002 B]\n",
      "Get:3 https://packages.cloud.google.com/apt cloud-sdk InRelease [6361 B]\n",
      "Get:4 http://archive.ubuntu.com/ubuntu focal InRelease [265 kB]\n",
      "Get:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  Packages [871 kB]\n",
      "Get:6 https://packages.cloud.google.com/apt google-fast-socket InRelease [5015 B]\n",
      "Get:7 http://packages.cloud.google.com/apt gcsfuse-focal/main amd64 Packages [1871 B]\n",
      "Get:8 http://security.ubuntu.com/ubuntu focal-security InRelease [114 kB]\n",
      "Get:9 http://archive.ubuntu.com/ubuntu focal-updates InRelease [114 kB]\n",
      "Get:10 https://packages.cloud.google.com/apt cloud-sdk/main amd64 Packages [390 kB]\n",
      "Get:11 http://archive.ubuntu.com/ubuntu focal-backports InRelease [108 kB]\n",
      "Get:12 https://packages.cloud.google.com/apt google-fast-socket/main amd64 Packages [415 B]\n",
      "Get:13 http://archive.ubuntu.com/ubuntu focal/universe amd64 Packages [11.3 MB]\n",
      "Get:14 http://archive.ubuntu.com/ubuntu focal/main amd64 Packages [1275 kB]\n",
      "Get:15 http://archive.ubuntu.com/ubuntu focal/multiverse amd64 Packages [177 kB]\n",
      "Get:16 http://archive.ubuntu.com/ubuntu focal/restricted amd64 Packages [33.4 kB]\n",
      "Get:17 http://archive.ubuntu.com/ubuntu focal-updates/restricted amd64 Packages [2062 kB]\n",
      "Get:18 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [1295 kB]\n",
      "Get:19 http://archive.ubuntu.com/ubuntu focal-updates/multiverse amd64 Packages [31.2 kB]\n",
      "Get:20 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [2965 kB]\n",
      "Get:21 http://security.ubuntu.com/ubuntu focal-security/main amd64 Packages [2479 kB]\n",
      "Get:22 http://archive.ubuntu.com/ubuntu focal-backports/main amd64 Packages [55.2 kB]\n",
      "Get:23 http://archive.ubuntu.com/ubuntu focal-backports/universe amd64 Packages [28.6 kB]\n",
      "Get:24 http://security.ubuntu.com/ubuntu focal-security/universe amd64 Packages [994 kB]\n",
      "Get:25 http://security.ubuntu.com/ubuntu focal-security/restricted amd64 Packages [1920 kB]\n",
      "Get:26 http://security.ubuntu.com/ubuntu focal-security/multiverse amd64 Packages [28.5 kB]\n",
      "Fetched 26.6 MB in 3s (10.4 MB/s)\n",
      "Reading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "39 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
      "\u001b[91m\n",
      "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
      "\u001b[0mReading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "The following additional packages will be installed:\n",
      "  libnvidia-compute-418 libnvidia-compute-430 libnvidia-compute-525\n",
      "The following NEW packages will be installed:\n",
      "  libnvidia-compute-418 libnvidia-compute-430 libnvidia-compute-525 nvtop\n",
      "0 upgraded, 4 newly installed, 0 to remove and 39 not upgraded.\n",
      "Need to get 50.4 MB of archives.\n",
      "After this operation, 235 MB of additional disk space will be used.\n",
      "Get:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  libnvidia-compute-525 525.85.12-0ubuntu1 [50.4 MB]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu focal/restricted amd64 libnvidia-compute-418 amd64 430.50-0ubuntu3 [6936 B]\n",
      "Get:3 http://archive.ubuntu.com/ubuntu focal/multiverse amd64 nvtop amd64 1.0.0-1ubuntu2 [26.8 kB]\n",
      "Get:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  libnvidia-compute-430 525.85.12-0ubuntu1 [6908 B]\n",
      "\u001b[91mdebconf: unable to initialize frontend: Dialog\n",
      "debconf: (TERM is not set, so the dialog frontend is not usable.)\n",
      "debconf: falling back to frontend: Readline\n",
      "\u001b[0m\u001b[91mdebconf: unable to initialize frontend: Readline\n",
      "debconf: (This frontend requires a controlling tty.)\n",
      "debconf: falling back to frontend: Teletype\n",
      "\u001b[0m\u001b[91mdpkg-preconfigure: unable to re-open stdin: \n",
      "\u001b[0mFetched 50.4 MB in 1s (64.1 MB/s)\n",
      "Selecting previously unselected package libnvidia-compute-525:amd64.\n",
      "(Reading database ... 91209 files and directories currently installed.)\n",
      "Preparing to unpack .../libnvidia-compute-525_525.85.12-0ubuntu1_amd64.deb ...\n",
      "Unpacking libnvidia-compute-525:amd64 (525.85.12-0ubuntu1) ...\n",
      "Selecting previously unselected package libnvidia-compute-430:amd64.\n",
      "Preparing to unpack .../libnvidia-compute-430_525.85.12-0ubuntu1_amd64.deb ...\n",
      "Unpacking libnvidia-compute-430:amd64 (525.85.12-0ubuntu1) ...\n",
      "Selecting previously unselected package libnvidia-compute-418:amd64.\n",
      "Preparing to unpack .../libnvidia-compute-418_430.50-0ubuntu3_amd64.deb ...\n",
      "Unpacking libnvidia-compute-418:amd64 (430.50-0ubuntu3) ...\n",
      "Selecting previously unselected package nvtop.\n",
      "Preparing to unpack .../nvtop_1.0.0-1ubuntu2_amd64.deb ...\n",
      "Unpacking nvtop (1.0.0-1ubuntu2) ...\n",
      "Setting up libnvidia-compute-525:amd64 (525.85.12-0ubuntu1) ...\n",
      "Setting up libnvidia-compute-430:amd64 (525.85.12-0ubuntu1) ...\n",
      "Setting up libnvidia-compute-418:amd64 (430.50-0ubuntu3) ...\n",
      "Setting up nvtop (1.0.0-1ubuntu2) ...\n",
      "Processing triggers for man-db (2.9.1-1) ...\n",
      "Processing triggers for libc-bin (2.31-0ubuntu9.9) ...\n",
      "Removing intermediate container eae83f0cf3fd\n",
      " ---> df01d75923dc\n",
      "Successfully built df01d75923dc\n",
      "Successfully tagged gcr.io/hybrid-vertex/sp-2tower-tfrs-jtv5-pipev3-training:latest\n",
      "PUSH\n",
      "Pushing gcr.io/hybrid-vertex/sp-2tower-tfrs-jtv5-pipev3-training\n",
      "The push refers to repository [gcr.io/hybrid-vertex/sp-2tower-tfrs-jtv5-pipev3-training]\n",
      "150153631104: Preparing\n",
      "cca6a66c174f: Preparing\n",
      "3ff86dbcdf65: Preparing\n",
      "3271b1c893c8: Preparing\n",
      "361b970e2e5d: Preparing\n",
      "35a78a3c4be2: Preparing\n",
      "d8e0010a550b: Preparing\n",
      "619508e8ee99: Preparing\n",
      "bb988af2b984: Preparing\n",
      "4636fe01e0b5: Preparing\n",
      "f857cc25e56d: Preparing\n",
      "82471d6461d7: Preparing\n",
      "5ac9d337aafc: Preparing\n",
      "a735f6dad37e: Preparing\n",
      "bc6af4d104d8: Preparing\n",
      "aa25d86089a5: Preparing\n",
      "4cbfec24b842: Preparing\n",
      "8d2278f0bee2: Preparing\n",
      "2814e52cd888: Preparing\n",
      "ee9a177ae05b: Preparing\n",
      "339bec97e3ba: Preparing\n",
      "55df74fa2018: Preparing\n",
      "89c3a55cc66f: Preparing\n",
      "532c075aae0f: Preparing\n",
      "af18a29a82ba: Preparing\n",
      "d8f3347e3ae5: Preparing\n",
      "3663689faddd: Preparing\n",
      "5f70bf18a086: Preparing\n",
      "3bec1a9c1f4d: Preparing\n",
      "60766204fa42: Preparing\n",
      "b3936e4c67d2: Preparing\n",
      "07d37209b7a9: Preparing\n",
      "fed0e8aa65b5: Preparing\n",
      "ad8fec0b36f1: Preparing\n",
      "3a217af3edf9: Preparing\n",
      "3297f5de02be: Preparing\n",
      "f3717d7fdfb7: Preparing\n",
      "e1eace4c0976: Preparing\n",
      "959a7375cb04: Preparing\n",
      "d79c672e1e8b: Preparing\n",
      "7b7c9e761223: Preparing\n",
      "0002c93bdb37: Preparing\n",
      "89c3a55cc66f: Waiting\n",
      "532c075aae0f: Waiting\n",
      "af18a29a82ba: Waiting\n",
      "d8f3347e3ae5: Waiting\n",
      "3663689faddd: Waiting\n",
      "5f70bf18a086: Waiting\n",
      "5ac9d337aafc: Waiting\n",
      "60766204fa42: Waiting\n",
      "ad8fec0b36f1: Waiting\n",
      "a735f6dad37e: Waiting\n",
      "3a217af3edf9: Waiting\n",
      "f3717d7fdfb7: Waiting\n",
      "bc6af4d104d8: Waiting\n",
      "3297f5de02be: Waiting\n",
      "aa25d86089a5: Waiting\n",
      "ee9a177ae05b: Waiting\n",
      "b3936e4c67d2: Waiting\n",
      "07d37209b7a9: Waiting\n",
      "fed0e8aa65b5: Waiting\n",
      "bb988af2b984: Waiting\n",
      "0002c93bdb37: Waiting\n",
      "4cbfec24b842: Waiting\n",
      "8d2278f0bee2: Waiting\n",
      "e1eace4c0976: Waiting\n",
      "959a7375cb04: Waiting\n",
      "339bec97e3ba: Waiting\n",
      "d79c672e1e8b: Waiting\n",
      "4636fe01e0b5: Waiting\n",
      "2814e52cd888: Waiting\n",
      "d8e0010a550b: Waiting\n",
      "619508e8ee99: Waiting\n",
      "35a78a3c4be2: Waiting\n",
      "f857cc25e56d: Waiting\n",
      "55df74fa2018: Waiting\n",
      "361b970e2e5d: Layer already exists\n",
      "35a78a3c4be2: Layer already exists\n",
      "d8e0010a550b: Layer already exists\n",
      "619508e8ee99: Layer already exists\n",
      "bb988af2b984: Layer already exists\n",
      "4636fe01e0b5: Layer already exists\n",
      "f857cc25e56d: Layer already exists\n",
      "82471d6461d7: Layer already exists\n",
      "5ac9d337aafc: Layer already exists\n",
      "3ff86dbcdf65: Pushed\n",
      "3271b1c893c8: Pushed\n",
      "a735f6dad37e: Layer already exists\n",
      "bc6af4d104d8: Layer already exists\n",
      "aa25d86089a5: Layer already exists\n",
      "4cbfec24b842: Layer already exists\n",
      "8d2278f0bee2: Layer already exists\n",
      "2814e52cd888: Layer already exists\n",
      "ee9a177ae05b: Layer already exists\n",
      "339bec97e3ba: Layer already exists\n",
      "55df74fa2018: Layer already exists\n",
      "89c3a55cc66f: Layer already exists\n",
      "532c075aae0f: Layer already exists\n",
      "af18a29a82ba: Layer already exists\n",
      "d8f3347e3ae5: Layer already exists\n",
      "5f70bf18a086: Layer already exists\n",
      "3663689faddd: Layer already exists\n",
      "3bec1a9c1f4d: Layer already exists\n",
      "b3936e4c67d2: Layer already exists\n",
      "60766204fa42: Layer already exists\n",
      "07d37209b7a9: Layer already exists\n",
      "fed0e8aa65b5: Layer already exists\n",
      "ad8fec0b36f1: Layer already exists\n",
      "3a217af3edf9: Layer already exists\n",
      "3297f5de02be: Layer already exists\n",
      "f3717d7fdfb7: Layer already exists\n",
      "e1eace4c0976: Layer already exists\n",
      "959a7375cb04: Layer already exists\n",
      "d79c672e1e8b: Layer already exists\n",
      "7b7c9e761223: Layer already exists\n",
      "0002c93bdb37: Layer already exists\n",
      "cca6a66c174f: Pushed\n",
      "150153631104: Pushed\n",
      "latest: digest: sha256:ec4861bcd04223b26a9ed1cb8b1c9fa5f5bc5d57468a1ef6896873fff45e85b5 size: 9133\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                       IMAGES                                                              STATUS\n",
      "6d6b4738-4edf-4fcf-aff8-5309a9860b0e  2023-02-14T05:47:48+00:00  3M46S     gs://hybrid-vertex_cloudbuild/source/1676353668.064571-9739bcfbd4ff4ace83214bef350f82a7.tgz  gcr.io/hybrid-vertex/sp-2tower-tfrs-jtv5-pipev3-training (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "! gcloud builds submit --config src/cloudbuild.yaml \\\n",
    "    --substitutions _DOCKERNAME=$DOCKERNAME,_IMAGE_URI=$IMAGE_URI,_FILE_LOCATION=$FILE_LOCATION \\\n",
    "    --timeout=2h \\\n",
    "    --machine-type=$MACHINE_TYPE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c889eb1-b663-4491-8bbf-4247d87b1762",
   "metadata": {},
   "source": [
    "# Pipeline Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3868029b-b1ce-45a9-aade-ebdfec8f110f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jupyter/jw-repo/spotify_mpd_two_tower'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aa1292ae-b51d-42bc-83c5-996e827c2633",
   "metadata": {},
   "outputs": [],
   "source": [
    "REPO_DOCKER_PATH_PREFIX = 'src'\n",
    "PIPELINES_SUB_DIR = 'train_pipes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6893f359-64de-458e-a964-d42ce581fd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -rf {REPO_DOCKER_PATH_PREFIX}/{PIPELINES_SUB_DIR}\n",
    "! mkdir {REPO_DOCKER_PATH_PREFIX}/{PIPELINES_SUB_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3fedd1-0897-4e32-bbd5-1dd6bbc12158",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Build Custom Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "335711ed-4425-409f-aebc-5fbdcbfedea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/train_pipes/build_custom_image.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/{PIPELINES_SUB_DIR}/build_custom_image.py\n",
    "\n",
    "import kfp\n",
    "from typing import Any, Callable, Dict, NamedTuple, Optional, List\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output,\n",
    "                        OutputPath, component, Metrics)\n",
    "@kfp.v2.dsl.component(\n",
    "    base_image=\"gcr.io/google.com/cloudsdktool/cloud-sdk:latest\",\n",
    "    packages_to_install=[\n",
    "        \"google-cloud-build\"\n",
    "    ],\n",
    ")\n",
    "def build_custom_image(\n",
    "    project: str,\n",
    "    artifact_gcs_path: str,\n",
    "    docker_name: str,\n",
    "    app_dir_name: str,\n",
    "    custom_image_uri: str,\n",
    ") -> NamedTuple('Outputs', [\n",
    "    ('custom_image_uri', str),\n",
    "]):\n",
    "    # TODO: make output Artifact for image_uri\n",
    "    \"\"\"\n",
    "    custom pipeline component to build custom image using\n",
    "    Cloud Build, the training/serving application code, and dependencies\n",
    "    defined in the Dockerfile\n",
    "    \"\"\"\n",
    "    \n",
    "    import logging\n",
    "    import os\n",
    "\n",
    "    from google.cloud.devtools import cloudbuild_v1 as cloudbuild\n",
    "    from google.protobuf.duration_pb2 import Duration\n",
    "\n",
    "    # initialize client for cloud build\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "    build_client = cloudbuild.services.cloud_build.CloudBuildClient()\n",
    "    \n",
    "    # parse step inputs to get path to Dockerfile and training application code\n",
    "    _gcs_dockerfile_path = os.path.join(artifact_gcs_path, f\"{docker_name}\") # Dockerfile.XXXXX\n",
    "    _gcs_script_dir_path = os.path.join(artifact_gcs_path, f\"{app_dir_name}/\") # \"trainer/\"\n",
    "    \n",
    "    logging.info(f\"_gcs_dockerfile_path: {_gcs_dockerfile_path}\")\n",
    "    logging.info(f\"_gcs_script_dir_path: {_gcs_script_dir_path}\")\n",
    "    \n",
    "    # define build steps to pull the training code and Dockerfile\n",
    "    # and build/push the custom training container image\n",
    "    build = cloudbuild.Build()\n",
    "    build.steps = [\n",
    "        {\n",
    "            \"name\": \"gcr.io/cloud-builders/gsutil\",\n",
    "            \"args\": [\"cp\", \"-r\", _gcs_script_dir_path, \".\"],\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"gcr.io/cloud-builders/gsutil\",\n",
    "            \"args\": [\"cp\", _gcs_dockerfile_path, \"Dockerfile\"],\n",
    "        },\n",
    "        # enabling Kaniko cache in a Docker build that caches intermediate\n",
    "        # layers and pushes image automatically to Container Registry\n",
    "        # https://cloud.google.com/build/docs/kaniko-cache\n",
    "        # {\n",
    "        #     \"name\": \"gcr.io/kaniko-project/executor:latest\",\n",
    "        #     # \"name\": \"gcr.io/kaniko-project/executor:v1.8.0\",        # TODO; downgraded to avoid error in build\n",
    "        #     # \"args\": [f\"--destination={training_image_uri}\", \"--cache=true\"],\n",
    "        #     \"args\": [f\"--destination={training_image_uri}\", \"--cache=false\"],\n",
    "        # },\n",
    "        {\n",
    "            \"name\": \"gcr.io/cloud-builders/docker\",\n",
    "            \"args\": ['build','-t', f'{custom_image_uri}', '.'],\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"gcr.io/cloud-builders/docker\",\n",
    "            \"args\": ['push', f'{custom_image_uri}'], \n",
    "        },\n",
    "    ]\n",
    "    # override default timeout of 10min\n",
    "    timeout = Duration()\n",
    "    timeout.seconds = 7200\n",
    "    build.timeout = timeout\n",
    "\n",
    "    # create build\n",
    "    operation = build_client.create_build(project_id=project, build=build)\n",
    "    logging.info(\"IN PROGRESS:\")\n",
    "    logging.info(operation.metadata)\n",
    "\n",
    "    # get build status\n",
    "    result = operation.result()\n",
    "    logging.info(\"RESULT:\", result.status)\n",
    "\n",
    "    # return step outputs\n",
    "    return (\n",
    "        custom_image_uri,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40fc5199-4cda-4d2f-8fb3-633618f405cc",
   "metadata": {},
   "source": [
    "## Create Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b8ab9fab-7a7e-4d7b-b241-19f4d39eb266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/train_pipes/create_tensorboard.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/{PIPELINES_SUB_DIR}/create_tensorboard.py\n",
    "\n",
    "import kfp\n",
    "from typing import Any, Callable, Dict, NamedTuple, Optional, List\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output,\n",
    "                        OutputPath, component, Metrics)\n",
    "@kfp.v2.dsl.component(\n",
    "    base_image='python:3.9',\n",
    "    packages_to_install=[\n",
    "        'google-cloud-aiplatform==1.20.0',\n",
    "        'numpy',\n",
    "        'google-cloud-storage',\n",
    "    ],\n",
    "    # output_component_file=\"./pipelines/train_custom_model.yaml\",\n",
    ")\n",
    "def create_tensorboard(\n",
    "    project: str,\n",
    "    location: str,\n",
    "    model_version: str,\n",
    "    pipeline_version: str,\n",
    "    model_name: str, \n",
    "    experiment_name: str,\n",
    "    experiment_run: str,\n",
    ") -> NamedTuple('Outputs', [\n",
    "    ('tensorboard_resource_name', str),\n",
    "    ('tensorboard_display_name', str),\n",
    "]):\n",
    "    \n",
    "    import logging\n",
    "    from google.cloud import aiplatform as vertex_ai\n",
    "    from google.cloud import storage\n",
    "    \n",
    "    vertex_ai.init(\n",
    "        project=project,\n",
    "        location=location,\n",
    "        # experiment=experiment_name,\n",
    "    )\n",
    "    \n",
    "    logging.info(f'experiment_name: {experiment_name}')\n",
    "    \n",
    "    # # create new TB instance\n",
    "    TENSORBOARD_DISPLAY_NAME=f\"{experiment_name}-v1\"\n",
    "    tensorboard = vertex_ai.Tensorboard.create(display_name=TENSORBOARD_DISPLAY_NAME, project=project, location=location)\n",
    "    TB_RESOURCE_NAME = tensorboard.resource_name\n",
    "    \n",
    "    logging.info(f'TENSORBOARD_DISPLAY_NAME: {TENSORBOARD_DISPLAY_NAME}')\n",
    "    logging.info(f'TB_RESOURCE_NAME: {TB_RESOURCE_NAME}')\n",
    "    \n",
    "    return (\n",
    "        f'{TB_RESOURCE_NAME}',\n",
    "        f'{TENSORBOARD_DISPLAY_NAME}',\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f317ba-9f05-422f-a8b7-dbfd85baed63",
   "metadata": {},
   "source": [
    "## Generate new train vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d319f76-f00b-437c-80ac-580b4c59e376",
   "metadata": {},
   "source": [
    "### fixed_text_layer adapts vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "695fbb3a-29dc-48a7-a3fb-f9ac082ad606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/train_pipes/adapt_fixed_text_layer_vocab.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/{PIPELINES_SUB_DIR}/adapt_fixed_text_layer_vocab.py\n",
    "\n",
    "import kfp\n",
    "from typing import Any, Callable, Dict, NamedTuple, Optional, List\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output,\n",
    "                        OutputPath, component, Metrics)\n",
    "\n",
    "@kfp.v2.dsl.component(\n",
    "    base_image=\"python:3.9\",\n",
    "    packages_to_install=[\n",
    "        # 'google-cloud-aiplatform==1.18.1',\n",
    "        'google-cloud-storage',\n",
    "        'tensorflow==2.10.1',\n",
    "    ],\n",
    ")\n",
    "def adapt_fixed_text_layer_vocab(\n",
    "    project: str,\n",
    "    location: str,\n",
    "    version: str,\n",
    "    data_dir_bucket_name: str,\n",
    "    data_dir_path_prefix: str,\n",
    "    train_output_gcs_bucket: str,\n",
    "    experiment_name: str,\n",
    "    experiment_run: str,\n",
    "    max_playlist_length: int,\n",
    "    max_tokens: int,\n",
    "    ngrams: int,\n",
    "    feature_name: str,\n",
    "    generate_new_vocab: bool,\n",
    "    # feat_type: str,\n",
    ") -> NamedTuple('Outputs', [\n",
    "    ('vocab_gcs_uri', str),\n",
    "    # ('feature_name', str),\n",
    "]):\n",
    "\n",
    "    \"\"\"\n",
    "    custom pipeline component to adapt the `pl_name_src` layer\n",
    "    writes vocab to pickled dict in GCS\n",
    "    dict combined with other layer vocabs and used in Two Tower training\n",
    "    \"\"\"\n",
    "    \n",
    "    # import packages\n",
    "    import os\n",
    "    import logging\n",
    "    import pickle as pkl\n",
    "    import time\n",
    "    \n",
    "    from google.cloud import storage\n",
    "    \n",
    "    import tensorflow as tf\n",
    "    \n",
    "    storage_client = storage.Client(project=project)\n",
    "    \n",
    "    logging.info(f\"feature_name: {feature_name}\")\n",
    "    \n",
    "    # ===================================================\n",
    "    # helper function\n",
    "    # ===================================================\n",
    "    \n",
    "    def download_blob(bucket_name, source_gcs_obj, local_filename):\n",
    "        \"\"\"Uploads a file to the bucket.\"\"\"\n",
    "        # storage_client = storage.Client(project=project_number)\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "        blob = bucket.blob(source_gcs_obj)\n",
    "        blob.download_to_filename(local_filename)\n",
    "\n",
    "        filehandler = open(f'{local_filename}', 'rb')\n",
    "        loaded_dict = pkl.load(filehandler)\n",
    "        filehandler.close()\n",
    "\n",
    "        logging.info(f\"File {local_filename} downloaded from gs://{bucket_name}/{source_gcs_obj}\")\n",
    "\n",
    "        return loaded_dict\n",
    "    \n",
    "    # ===================================================\n",
    "    # set feature vars\n",
    "    # ===================================================\n",
    "    MAX_PLAYLIST_LENGTH = max_playlist_length\n",
    "    logging.info(f\"MAX_PLAYLIST_LENGTH: {MAX_PLAYLIST_LENGTH}\")\n",
    "    \n",
    "    FEATURES_PREFIX = f'{experiment_name}/{experiment_run}/features'\n",
    "    logging.info(f\"FEATURES_PREFIX: {FEATURES_PREFIX}\")\n",
    "    \n",
    "    all_features_dict = {}\n",
    "    \n",
    "    # ===================================================\n",
    "    # load pickled Candidate features\n",
    "    # ===================================================\n",
    "    \n",
    "    # candidate features\n",
    "    CAND_FEAT_FILENAME = 'candidate_feats_dict.pkl'\n",
    "    CAND_FEAT_GCS_OBJ = f'{FEATURES_PREFIX}/{CAND_FEAT_FILENAME}'\n",
    "    LOADED_CANDIDATE_DICT = f'loaded_{CAND_FEAT_FILENAME}'\n",
    "    \n",
    "    loaded_candidate_features_dict = download_blob(\n",
    "        train_output_gcs_bucket,\n",
    "        CAND_FEAT_GCS_OBJ,\n",
    "        LOADED_CANDIDATE_DICT\n",
    "    )\n",
    "    # logging.info(f\"CAND_FEAT_FILENAME: {CAND_FEAT_FILENAME}; CAND_FEAT_GCS_OBJ:{CAND_FEAT_GCS_OBJ}; LOADED_CANDIDATE_DICT: {LOADED_CANDIDATE_DICT}\")\n",
    "    \n",
    "#     # os.system(f'gsutil cp gs://{train_output_gcs_bucket}/{CAND_FEAT_GCS_OBJ} {LOADED_CANDIDATE_DICT}')\n",
    "#     bucket = storage_client.bucket(train_output_gcs_bucket)\n",
    "#     blob = bucket.blob(CAND_FEAT_GCS_OBJ)\n",
    "#     blob.download_to_filename(LOADED_CANDIDATE_DICT)\n",
    "    \n",
    "#     filehandler = open(f'{LOADED_CANDIDATE_DICT}', 'rb')\n",
    "#     loaded_candidate_features_dict = pkl.load(filehandler)\n",
    "#     filehandler.close()\n",
    "#     logging.info(f\"loaded_candidate_features_dict: {loaded_candidate_features_dict}\")\n",
    "    \n",
    "    all_features_dict.update(loaded_candidate_features_dict)\n",
    "    logging.info(f\"all_features_dict: {all_features_dict}\")\n",
    "\n",
    "    # ===================================================\n",
    "    # load pickled Query features\n",
    "    # ===================================================\n",
    "\n",
    "    # query features\n",
    "    QUERY_FEAT_FILENAME = 'query_feats_dict.pkl'\n",
    "    QUERY_FEAT_GCS_OBJ = f'{FEATURES_PREFIX}/{QUERY_FEAT_FILENAME}'\n",
    "    LOADED_QUERY_DICT = f'loaded_{QUERY_FEAT_FILENAME}'\n",
    "    \n",
    "    loaded_query_features_dict = download_blob(\n",
    "        train_output_gcs_bucket,\n",
    "        QUERY_FEAT_GCS_OBJ,\n",
    "        LOADED_QUERY_DICT\n",
    "    )\n",
    "#     logging.info(f\"QUERY_FEAT_FILENAME: {QUERY_FEAT_FILENAME}; QUERY_FEAT_GCS_OBJ:{QUERY_FEAT_GCS_OBJ}; LOADED_QUERY_DICT: {LOADED_QUERY_DICT}\")\n",
    "    \n",
    "#     # os.system(f'gsutil cp gs://{train_output_gcs_bucket}/{QUERY_FEATURES_GCS_OBJ} {LOADED_QUERY_DICT}')\n",
    "#     bucket = storage_client.bucket(train_output_gcs_bucket)\n",
    "#     blob = bucket.blob(QUERY_FEAT_GCS_OBJ)\n",
    "#     blob.download_to_filename(LOADED_QUERY_DICT)\n",
    "    \n",
    "#     filehandler = open(f'{LOADED_QUERY_DICT}', 'rb')\n",
    "#     loaded_query_features_dict = pkl.load(filehandler)\n",
    "#     filehandler.close()\n",
    "#     logging.info(f\"loaded_query_features_dict: {loaded_query_features_dict}\")\n",
    "    \n",
    "    all_features_dict.update(loaded_query_features_dict)\n",
    "    logging.info(f\"all_features_dict: {all_features_dict}\")\n",
    "    \n",
    "    # ===================================================\n",
    "    # tfrecord parser\n",
    "    # ===================================================\n",
    "    \n",
    "#     candidate_features = {\n",
    "#         \"track_uri_can\":tf.io.FixedLenFeature(dtype=tf.string, shape=()),            \n",
    "#         \"track_name_can\":tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "#         \"artist_uri_can\":tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "#         \"artist_name_can\":tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "#         \"album_uri_can\":tf.io.FixedLenFeature(dtype=tf.string, shape=()),           \n",
    "#         \"album_name_can\":tf.io.FixedLenFeature(dtype=tf.string, shape=()), \n",
    "#         \"duration_ms_can\":tf.io.FixedLenFeature(dtype=tf.float32, shape=()),      \n",
    "#         \"track_pop_can\":tf.io.FixedLenFeature(dtype=tf.float32, shape=()),      \n",
    "#         \"artist_pop_can\":tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "#         \"artist_genres_can\":tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "#         \"artist_followers_can\":tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "#         # new\n",
    "#         # \"track_pl_titles_can\":tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "#         \"track_danceability_can\":tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "#         \"track_energy_can\":tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "#         \"track_key_can\":tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "#         \"track_loudness_can\":tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "#         \"track_mode_can\":tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "#         \"track_speechiness_can\":tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "#         \"track_acousticness_can\":tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "#         \"track_instrumentalness_can\":tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "#         \"track_liveness_can\":tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "#         \"track_valence_can\":tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "#         \"track_tempo_can\":tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "#         \"time_signature_can\":tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "#     }\n",
    "\n",
    "#     feats = {\n",
    "#         # ===================================================\n",
    "#         # candidate track features\n",
    "#         # ===================================================\n",
    "#         \"track_uri_can\":tf.io.FixedLenFeature(dtype=tf.string, shape=()),            \n",
    "#         \"track_name_can\":tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "#         \"artist_uri_can\":tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "#         \"artist_name_can\":tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "#         \"album_uri_can\":tf.io.FixedLenFeature(dtype=tf.string, shape=()),           \n",
    "#         \"album_name_can\":tf.io.FixedLenFeature(dtype=tf.string, shape=()), \n",
    "#         \"duration_ms_can\":tf.io.FixedLenFeature(dtype=tf.float32, shape=()),      \n",
    "#         \"track_pop_can\":tf.io.FixedLenFeature(dtype=tf.float32, shape=()),      \n",
    "#         \"artist_pop_can\":tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "#         \"artist_genres_can\":tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "#         \"artist_followers_can\":tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "#         # \"track_pl_titles_can\":tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "#         \"track_danceability_can\":tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "#         \"track_energy_can\":tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "#         \"track_key_can\":tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "#         \"track_loudness_can\":tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "#         \"track_mode_can\":tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "#         \"track_speechiness_can\":tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "#         \"track_acousticness_can\":tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "#         \"track_instrumentalness_can\":tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "#         \"track_liveness_can\":tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "#         \"track_valence_can\":tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "#         \"track_tempo_can\":tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "#         \"time_signature_can\": tf.io.FixedLenFeature(dtype=tf.string, shape=()), # track_time_signature_can\n",
    "\n",
    "#         # ===================================================\n",
    "#         # summary playlist features\n",
    "#         # ===================================================\n",
    "#         \"pl_name_src\" : tf.io.FixedLenFeature(dtype=tf.string, shape=()), \n",
    "#         'pl_collaborative_src' : tf.io.FixedLenFeature(dtype=tf.string, shape=()), \n",
    "#         # 'num_pl_followers_src' : tf.io.FixedLenFeature(dtype=tf.float32, shape=()), \n",
    "#         'pl_duration_ms_new' : tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "#         'num_pl_songs_new' : tf.io.FixedLenFeature(dtype=tf.float32, shape=()), # n_songs_pl_new | num_pl_songs_new\n",
    "#         'num_pl_artists_new' : tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "#         'num_pl_albums_new' : tf.io.FixedLenFeature(dtype=tf.float32, shape=()), \n",
    "#         # 'avg_track_pop_pl_new' : tf.io.FixedLenFeature(dtype=tf.float32, shape=()), \n",
    "#         # 'avg_artist_pop_pl_new' : tf.io.FixedLenFeature(dtype=tf.float32, shape=()), \n",
    "#         # 'avg_art_followers_pl_new' : tf.io.FixedLenFeature(dtype=tf.float32, shape=()), \n",
    "\n",
    "#         # ===================================================\n",
    "#         # ragged playlist features\n",
    "#         # ===================================================\n",
    "#         # bytes / string\n",
    "#         \"track_uri_pl\": tf.io.FixedLenFeature(dtype=tf.string, shape=(MAX_PLAYLIST_LENGTH,)),\n",
    "#         \"track_name_pl\": tf.io.FixedLenFeature(dtype=tf.string, shape=(MAX_PLAYLIST_LENGTH,)),\n",
    "#         \"artist_uri_pl\": tf.io.FixedLenFeature(dtype=tf.string, shape=(MAX_PLAYLIST_LENGTH,)),\n",
    "#         \"artist_name_pl\": tf.io.FixedLenFeature(dtype=tf.string, shape=(MAX_PLAYLIST_LENGTH,)),\n",
    "#         \"album_uri_pl\": tf.io.FixedLenFeature(dtype=tf.string, shape=(MAX_PLAYLIST_LENGTH,)),\n",
    "#         \"album_name_pl\": tf.io.FixedLenFeature(dtype=tf.string, shape=(MAX_PLAYLIST_LENGTH,)),\n",
    "#         \"artist_genres_pl\": tf.io.FixedLenFeature(dtype=tf.string, shape=(MAX_PLAYLIST_LENGTH,)),\n",
    "#         # \"tracks_playlist_titles_pl\": tf.io.FixedLenFeature(dtype=tf.string, shape=(MAX_PLAYLIST_LENGTH,)),\n",
    "#         \"track_key_pl\": tf.io.FixedLenFeature(dtype=tf.string, shape=(MAX_PLAYLIST_LENGTH,)),\n",
    "#         \"track_mode_pl\": tf.io.FixedLenFeature(dtype=tf.string, shape=(MAX_PLAYLIST_LENGTH,)),\n",
    "#         \"time_signature_pl\": tf.io.FixedLenFeature(dtype=tf.string, shape=(MAX_PLAYLIST_LENGTH,)), \n",
    "\n",
    "#         # Float List\n",
    "#         \"duration_ms_songs_pl\": tf.io.FixedLenFeature(dtype=tf.float32, shape=(MAX_PLAYLIST_LENGTH,)),\n",
    "#         \"track_pop_pl\": tf.io.FixedLenFeature(dtype=tf.float32, shape=(MAX_PLAYLIST_LENGTH,)),\n",
    "#         \"artist_pop_pl\": tf.io.FixedLenFeature(dtype=tf.float32, shape=(MAX_PLAYLIST_LENGTH,)),\n",
    "#         \"artists_followers_pl\": tf.io.FixedLenFeature(dtype=tf.float32, shape=(MAX_PLAYLIST_LENGTH,)),\n",
    "#         \"track_danceability_pl\": tf.io.FixedLenFeature(dtype=tf.float32, shape=(MAX_PLAYLIST_LENGTH,)),\n",
    "#         \"track_energy_pl\": tf.io.FixedLenFeature(dtype=tf.float32, shape=(MAX_PLAYLIST_LENGTH,)),\n",
    "#         \"track_loudness_pl\": tf.io.FixedLenFeature(dtype=tf.float32, shape=(MAX_PLAYLIST_LENGTH,)),\n",
    "#         \"track_speechiness_pl\": tf.io.FixedLenFeature(dtype=tf.float32, shape=(MAX_PLAYLIST_LENGTH,)),\n",
    "#         \"track_acousticness_pl\": tf.io.FixedLenFeature(dtype=tf.float32, shape=(MAX_PLAYLIST_LENGTH,)),\n",
    "#         \"track_instrumentalness_pl\": tf.io.FixedLenFeature(dtype=tf.float32, shape=(MAX_PLAYLIST_LENGTH,)),\n",
    "#         \"track_liveness_pl\": tf.io.FixedLenFeature(dtype=tf.float32, shape=(MAX_PLAYLIST_LENGTH,)),\n",
    "#         \"track_valence_pl\": tf.io.FixedLenFeature(dtype=tf.float32, shape=(MAX_PLAYLIST_LENGTH,)),\n",
    "#         \"track_tempo_pl\": tf.io.FixedLenFeature(dtype=tf.float32, shape=(MAX_PLAYLIST_LENGTH,)),\n",
    "#     }\n",
    "    \n",
    "    # parsing function\n",
    "    def parse_tfrecord(example):\n",
    "        \"\"\"\n",
    "        Reads a serialized example from GCS and converts to tfrecord\n",
    "        \"\"\"\n",
    "        # example = tf.io.parse_single_example(\n",
    "        example = tf.io.parse_example(\n",
    "            example,\n",
    "            # feats\n",
    "            features=all_features_dict\n",
    "        )\n",
    "        return example\n",
    "    \n",
    "    if generate_new_vocab:\n",
    "        logging.info(f\"Generating new vocab file...\")\n",
    "        \n",
    "        # list blobs (tfrecords)\n",
    "        train_files = []\n",
    "        for blob in storage_client.list_blobs(f'{data_dir_bucket_name}', prefix=f'{data_dir_path_prefix}'):\n",
    "            if '.tfrecords' in blob.name:\n",
    "                train_files.append(blob.public_url.replace(\"https://storage.googleapis.com/\", \"gs://\"))\n",
    "\n",
    "        logging.info(f\"TFRecord file count: {len(train_files)}\")\n",
    "\n",
    "        # ===================================================\n",
    "        # create TF dataset\n",
    "        # ===================================================\n",
    "        logging.info(f\"Creating TFRecordDataset...\")\n",
    "        train_dataset = tf.data.TFRecordDataset(train_files)\n",
    "        train_parsed = train_dataset.map(parse_tfrecord)\n",
    "\n",
    "        # ===================================================\n",
    "        # adapt layer for feature\n",
    "        # ===================================================\n",
    "        start = time.time()\n",
    "        text_layer = tf.keras.layers.TextVectorization(\n",
    "            max_tokens=max_tokens,\n",
    "            ngrams=ngrams\n",
    "        )\n",
    "        text_layer.adapt(train_parsed.map(lambda x: x[f'{feature_name}']))\n",
    "        end = time.time()\n",
    "\n",
    "        logging.info(f'Layer adapt elapsed time: {round((end - start), 2)} seconds')\n",
    "\n",
    "        # ===================================================\n",
    "        # write vocab to pickled dict --> gcs\n",
    "        # ===================================================\n",
    "        logging.info(f\"Writting pickled dict to GCS...\")\n",
    "\n",
    "        VOCAB_LOCAL_FILE = f'{feature_name}_vocab_dict.pkl'\n",
    "        VOCAB_GCS_OBJ = f'{experiment_name}/{experiment_run}/vocab-staging/{VOCAB_LOCAL_FILE}' # destination folder prefix and blob name\n",
    "        VOCAB_DICT = {f'{feature_name}' : text_layer.get_vocabulary(),}\n",
    "\n",
    "        logging.info(f\"VOCAB_LOCAL_FILE: {VOCAB_LOCAL_FILE}\")\n",
    "        logging.info(f\"VOCAB_GCS_OBJ: {VOCAB_GCS_OBJ}\")\n",
    "\n",
    "        # pickle\n",
    "        filehandler = open(f'{VOCAB_LOCAL_FILE}', 'wb')\n",
    "        pkl.dump(VOCAB_DICT, filehandler)\n",
    "        filehandler.close()\n",
    "\n",
    "        # upload to GCS\n",
    "        bucket_client = storage_client.bucket(train_output_gcs_bucket)\n",
    "        blob = bucket_client.blob(VOCAB_GCS_OBJ)\n",
    "        blob.upload_from_filename(VOCAB_LOCAL_FILE)\n",
    "\n",
    "        vocab_uri = f'gs://{train_output_gcs_bucket}/{VOCAB_GCS_OBJ}'\n",
    "\n",
    "        logging.info(f\"File {VOCAB_LOCAL_FILE} uploaded to {vocab_uri}\")\n",
    "        \n",
    "    else:\n",
    "        logging.info(f\"Using existing vocab file...\")\n",
    "        \n",
    "        vocab_uri = 'gs://two-tower-models/vocabs/vocab_dict.pkl'\n",
    "        logging.info(f\"Using vocab file: {vocab_uri}\")\n",
    "    \n",
    "    return(\n",
    "        vocab_uri,\n",
    "        # feature_name,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f541fef-844d-4b31-b642-c3d2e06615de",
   "metadata": {},
   "source": [
    "### ragged adapts vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6f946930-d271-4e88-b99f-8fde161527fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/train_pipes/adapt_ragged_text_layer_vocab.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/{PIPELINES_SUB_DIR}/adapt_ragged_text_layer_vocab.py\n",
    "\n",
    "import kfp\n",
    "from typing import Any, Callable, Dict, NamedTuple, Optional, List\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output,\n",
    "                        OutputPath, component, Metrics)\n",
    "\n",
    "@kfp.v2.dsl.component(\n",
    "    base_image=\"python:3.9\",\n",
    "    packages_to_install=[\n",
    "        # 'google-cloud-aiplatform==1.18.1',\n",
    "        'google-cloud-storage',\n",
    "        'tensorflow==2.10.1',\n",
    "    ],\n",
    ")\n",
    "def adapt_ragged_text_layer_vocab(\n",
    "    project: str,\n",
    "    location: str,\n",
    "    version: str,\n",
    "    data_dir_bucket_name: str,\n",
    "    data_dir_path_prefix: str,\n",
    "    train_output_gcs_bucket: str,\n",
    "    experiment_name: str,\n",
    "    experiment_run: str,\n",
    "    max_playlist_length: int,\n",
    "    max_tokens: int,\n",
    "    ngrams: int,\n",
    "    feature_name: str,\n",
    "    generate_new_vocab: bool,\n",
    "    # feat_type: str,\n",
    ") -> NamedTuple('Outputs', [\n",
    "    ('vocab_gcs_uri', str),\n",
    "    # ('feature_name', str),\n",
    "]):\n",
    "\n",
    "    \"\"\"\n",
    "    custom pipeline component to adapt the `pl_name_src` layer\n",
    "    writes vocab to pickled dict in GCS\n",
    "    dict combined with other layer vocabs and used in Two Tower training\n",
    "    \"\"\"\n",
    "    \n",
    "    # import packages\n",
    "    import os\n",
    "    import logging\n",
    "    import pickle as pkl\n",
    "    import time\n",
    "    \n",
    "    from google.cloud import storage\n",
    "    \n",
    "    import tensorflow as tf\n",
    "    \n",
    "    storage_client = storage.Client(project=project)\n",
    "    \n",
    "    logging.info(f\"feature_name: {feature_name}\")\n",
    "    # logging.info(f\"feat_type: {feat_type}\")\n",
    "    \n",
    "    # ===================================================\n",
    "    # set feature vars\n",
    "    # ===================================================\n",
    "    MAX_PLAYLIST_LENGTH = max_playlist_length\n",
    "    logging.info(f\"MAX_PLAYLIST_LENGTH: {MAX_PLAYLIST_LENGTH}\")\n",
    "    \n",
    "    FEATURES_PREFIX = f'{experiment_name}/{experiment_run}/features'\n",
    "    logging.info(f\"FEATURES_PREFIX: {FEATURES_PREFIX}\")\n",
    "    \n",
    "    all_features_dict = {}\n",
    "    \n",
    "    # ===================================================\n",
    "    # load pickled Candidate features\n",
    "    # ===================================================\n",
    "    \n",
    "    # candidate features\n",
    "    CAND_FEAT_FILENAME = 'candidate_feats_dict.pkl'\n",
    "    CAND_FEAT_GCS_OBJ = f'{FEATURES_PREFIX}/{CAND_FEAT_FILENAME}'\n",
    "    LOADED_CANDIDATE_DICT = f'loaded_{CAND_FEAT_FILENAME}'\n",
    "    logging.info(f\"CAND_FEAT_FILENAME: {CAND_FEAT_FILENAME}; CAND_FEAT_GCS_OBJ:{CAND_FEAT_GCS_OBJ}; LOADED_CANDIDATE_DICT: {LOADED_CANDIDATE_DICT}\")\n",
    "    \n",
    "    # os.system(f'gsutil cp gs://{train_output_gcs_bucket}/{CAND_FEAT_GCS_OBJ} {LOADED_CANDIDATE_DICT}')\n",
    "    bucket = storage_client.bucket(train_output_gcs_bucket)\n",
    "    blob = bucket.blob(CAND_FEAT_GCS_OBJ)\n",
    "    blob.download_to_filename(LOADED_CANDIDATE_DICT)\n",
    "    \n",
    "    filehandler = open(f'{LOADED_CANDIDATE_DICT}', 'rb')\n",
    "    loaded_candidate_features_dict = pkl.load(filehandler)\n",
    "    filehandler.close()\n",
    "    logging.info(f\"loaded_candidate_features_dict: {loaded_candidate_features_dict}\")\n",
    "    \n",
    "    all_features_dict.update(loaded_candidate_features_dict)\n",
    "    logging.info(f\"all_features_dict: {all_features_dict}\")\n",
    "\n",
    "    # ===================================================\n",
    "    # load pickled Query features\n",
    "    # ===================================================\n",
    "\n",
    "    # query features\n",
    "    QUERY_FEAT_FILENAME = 'query_feats_dict.pkl'\n",
    "    QUERY_FEAT_GCS_OBJ = f'{FEATURES_PREFIX}/{QUERY_FEAT_FILENAME}'\n",
    "    LOADED_QUERY_DICT = f'loaded_{QUERY_FEAT_FILENAME}'\n",
    "    logging.info(f\"QUERY_FEAT_FILENAME: {QUERY_FEAT_FILENAME}; QUERY_FEAT_GCS_OBJ:{QUERY_FEAT_GCS_OBJ}; LOADED_QUERY_DICT: {LOADED_QUERY_DICT}\")\n",
    "    \n",
    "    # os.system(f'gsutil cp gs://{train_output_gcs_bucket}/{QUERY_FEATURES_GCS_OBJ} {LOADED_QUERY_DICT}')\n",
    "    bucket = storage_client.bucket(train_output_gcs_bucket)\n",
    "    blob = bucket.blob(QUERY_FEAT_GCS_OBJ)\n",
    "    blob.download_to_filename(LOADED_QUERY_DICT)\n",
    "    \n",
    "    filehandler = open(f'{LOADED_QUERY_DICT}', 'rb')\n",
    "    loaded_query_features_dict = pkl.load(filehandler)\n",
    "    filehandler.close()\n",
    "    logging.info(f\"loaded_query_features_dict: {loaded_query_features_dict}\")\n",
    "    \n",
    "    all_features_dict.update(loaded_query_features_dict)\n",
    "    logging.info(f\"all_features_dict: {all_features_dict}\")\n",
    "    \n",
    "    # ===================================================\n",
    "    # tfrecord parser\n",
    "    # ===================================================\n",
    "    \n",
    "    # candidate_features = {\n",
    "    #     \"track_uri_can\":tf.io.FixedLenFeature(dtype=tf.string, shape=()),            \n",
    "    #     \"track_name_can\":tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    #     \"artist_uri_can\":tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    #     \"artist_name_can\":tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    #     \"album_uri_can\":tf.io.FixedLenFeature(dtype=tf.string, shape=()),           \n",
    "    #     \"album_name_can\":tf.io.FixedLenFeature(dtype=tf.string, shape=()), \n",
    "    #     \"duration_ms_can\":tf.io.FixedLenFeature(dtype=tf.float32, shape=()),      \n",
    "    #     \"track_pop_can\":tf.io.FixedLenFeature(dtype=tf.float32, shape=()),      \n",
    "    #     \"artist_pop_can\":tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "    #     \"artist_genres_can\":tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    #     \"artist_followers_can\":tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "    #     # new\n",
    "    #     # \"track_pl_titles_can\":tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    #     \"track_danceability_can\":tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "    #     \"track_energy_can\":tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "    #     \"track_key_can\":tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    #     \"track_loudness_can\":tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "    #     \"track_mode_can\":tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    #     \"track_speechiness_can\":tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "    #     \"track_acousticness_can\":tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "    #     \"track_instrumentalness_can\":tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "    #     \"track_liveness_can\":tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "    #     \"track_valence_can\":tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "    #     \"track_tempo_can\":tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "    #     \"time_signature_can\":tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    # }\n",
    "\n",
    "#     feats = {\n",
    "#         # ===================================================\n",
    "#         # candidate track features\n",
    "#         # ===================================================\n",
    "#         \"track_uri_can\":tf.io.FixedLenFeature(dtype=tf.string, shape=()),            \n",
    "#         \"track_name_can\":tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "#         \"artist_uri_can\":tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "#         \"artist_name_can\":tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "#         \"album_uri_can\":tf.io.FixedLenFeature(dtype=tf.string, shape=()),           \n",
    "#         \"album_name_can\":tf.io.FixedLenFeature(dtype=tf.string, shape=()), \n",
    "#         \"duration_ms_can\":tf.io.FixedLenFeature(dtype=tf.float32, shape=()),      \n",
    "#         \"track_pop_can\":tf.io.FixedLenFeature(dtype=tf.float32, shape=()),      \n",
    "#         \"artist_pop_can\":tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "#         \"artist_genres_can\":tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "#         \"artist_followers_can\":tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "#         # \"track_pl_titles_can\":tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "#         \"track_danceability_can\":tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "#         \"track_energy_can\":tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "#         \"track_key_can\":tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "#         \"track_loudness_can\":tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "#         \"track_mode_can\":tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "#         \"track_speechiness_can\":tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "#         \"track_acousticness_can\":tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "#         \"track_instrumentalness_can\":tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "#         \"track_liveness_can\":tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "#         \"track_valence_can\":tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "#         \"track_tempo_can\":tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "#         \"time_signature_can\": tf.io.FixedLenFeature(dtype=tf.string, shape=()), # track_time_signature_can\n",
    "\n",
    "#         # ===================================================\n",
    "#         # summary playlist features\n",
    "#         # ===================================================\n",
    "#         \"pl_name_src\" : tf.io.FixedLenFeature(dtype=tf.string, shape=()), \n",
    "#         'pl_collaborative_src' : tf.io.FixedLenFeature(dtype=tf.string, shape=()), \n",
    "#         # 'num_pl_followers_src' : tf.io.FixedLenFeature(dtype=tf.float32, shape=()), \n",
    "#         'pl_duration_ms_new' : tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "#         'num_pl_songs_new' : tf.io.FixedLenFeature(dtype=tf.float32, shape=()), # n_songs_pl_new | num_pl_songs_new\n",
    "#         'num_pl_artists_new' : tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "#         'num_pl_albums_new' : tf.io.FixedLenFeature(dtype=tf.float32, shape=()), \n",
    "#         # 'avg_track_pop_pl_new' : tf.io.FixedLenFeature(dtype=tf.float32, shape=()), \n",
    "#         # 'avg_artist_pop_pl_new' : tf.io.FixedLenFeature(dtype=tf.float32, shape=()), \n",
    "#         # 'avg_art_followers_pl_new' : tf.io.FixedLenFeature(dtype=tf.float32, shape=()), \n",
    "\n",
    "#         # ===================================================\n",
    "#         # ragged playlist features\n",
    "#         # ===================================================\n",
    "#         # bytes / string\n",
    "#         \"track_uri_pl\": tf.io.FixedLenFeature(dtype=tf.string, shape=(MAX_PLAYLIST_LENGTH,)),\n",
    "#         \"track_name_pl\": tf.io.FixedLenFeature(dtype=tf.string, shape=(MAX_PLAYLIST_LENGTH,)),\n",
    "#         \"artist_uri_pl\": tf.io.FixedLenFeature(dtype=tf.string, shape=(MAX_PLAYLIST_LENGTH,)),\n",
    "#         \"artist_name_pl\": tf.io.FixedLenFeature(dtype=tf.string, shape=(MAX_PLAYLIST_LENGTH,)),\n",
    "#         \"album_uri_pl\": tf.io.FixedLenFeature(dtype=tf.string, shape=(MAX_PLAYLIST_LENGTH,)),\n",
    "#         \"album_name_pl\": tf.io.FixedLenFeature(dtype=tf.string, shape=(MAX_PLAYLIST_LENGTH,)),\n",
    "#         \"artist_genres_pl\": tf.io.FixedLenFeature(dtype=tf.string, shape=(MAX_PLAYLIST_LENGTH,)),\n",
    "#         # \"tracks_playlist_titles_pl\": tf.io.FixedLenFeature(dtype=tf.string, shape=(MAX_PLAYLIST_LENGTH,)),\n",
    "#         \"track_key_pl\": tf.io.FixedLenFeature(dtype=tf.string, shape=(MAX_PLAYLIST_LENGTH,)),\n",
    "#         \"track_mode_pl\": tf.io.FixedLenFeature(dtype=tf.string, shape=(MAX_PLAYLIST_LENGTH,)),\n",
    "#         \"time_signature_pl\": tf.io.FixedLenFeature(dtype=tf.string, shape=(MAX_PLAYLIST_LENGTH,)), \n",
    "\n",
    "#         # Float List\n",
    "#         \"duration_ms_songs_pl\": tf.io.FixedLenFeature(dtype=tf.float32, shape=(MAX_PLAYLIST_LENGTH,)),\n",
    "#         \"track_pop_pl\": tf.io.FixedLenFeature(dtype=tf.float32, shape=(MAX_PLAYLIST_LENGTH,)),\n",
    "#         \"artist_pop_pl\": tf.io.FixedLenFeature(dtype=tf.float32, shape=(MAX_PLAYLIST_LENGTH,)),\n",
    "#         \"artists_followers_pl\": tf.io.FixedLenFeature(dtype=tf.float32, shape=(MAX_PLAYLIST_LENGTH,)),\n",
    "#         \"track_danceability_pl\": tf.io.FixedLenFeature(dtype=tf.float32, shape=(MAX_PLAYLIST_LENGTH,)),\n",
    "#         \"track_energy_pl\": tf.io.FixedLenFeature(dtype=tf.float32, shape=(MAX_PLAYLIST_LENGTH,)),\n",
    "#         \"track_loudness_pl\": tf.io.FixedLenFeature(dtype=tf.float32, shape=(MAX_PLAYLIST_LENGTH,)),\n",
    "#         \"track_speechiness_pl\": tf.io.FixedLenFeature(dtype=tf.float32, shape=(MAX_PLAYLIST_LENGTH,)),\n",
    "#         \"track_acousticness_pl\": tf.io.FixedLenFeature(dtype=tf.float32, shape=(MAX_PLAYLIST_LENGTH,)),\n",
    "#         \"track_instrumentalness_pl\": tf.io.FixedLenFeature(dtype=tf.float32, shape=(MAX_PLAYLIST_LENGTH,)),\n",
    "#         \"track_liveness_pl\": tf.io.FixedLenFeature(dtype=tf.float32, shape=(MAX_PLAYLIST_LENGTH,)),\n",
    "#         \"track_valence_pl\": tf.io.FixedLenFeature(dtype=tf.float32, shape=(MAX_PLAYLIST_LENGTH,)),\n",
    "#         \"track_tempo_pl\": tf.io.FixedLenFeature(dtype=tf.float32, shape=(MAX_PLAYLIST_LENGTH,)),\n",
    "#     }\n",
    "    \n",
    "    # parsing function\n",
    "    def parse_tfrecord(example):\n",
    "        \"\"\"\n",
    "        Reads a serialized example from GCS and converts to tfrecord\n",
    "        \"\"\"\n",
    "        # example = tf.io.parse_single_example(\n",
    "        example = tf.io.parse_example(\n",
    "            example,\n",
    "            # feats\n",
    "            features=all_features_dict\n",
    "        )\n",
    "        return example\n",
    "    \n",
    "    \n",
    "    if generate_new_vocab:\n",
    "        logging.info(f\"Generating new vocab file...\")\n",
    "    \n",
    "        # list blobs (tfrecords)\n",
    "        train_files = []\n",
    "        for blob in storage_client.list_blobs(f'{data_dir_bucket_name}', prefix=f'{data_dir_path_prefix}'):\n",
    "            if '.tfrecords' in blob.name:\n",
    "                train_files.append(blob.public_url.replace(\"https://storage.googleapis.com/\", \"gs://\"))\n",
    "\n",
    "        logging.info(f\"TFRecord file count: {len(train_files)}\")\n",
    "\n",
    "        # ===================================================\n",
    "        # create TF dataset\n",
    "        # ===================================================\n",
    "        logging.info(f\"Creating TFRecordDataset...\")\n",
    "        train_dataset = tf.data.TFRecordDataset(train_files)\n",
    "        train_parsed = train_dataset.map(parse_tfrecord)\n",
    "\n",
    "        # ===================================================\n",
    "        # adapt layer for feature\n",
    "        # ===================================================\n",
    "\n",
    "        start = time.time()\n",
    "        text_layer = tf.keras.layers.TextVectorization(\n",
    "            max_tokens=max_tokens,\n",
    "            ngrams=ngrams\n",
    "        )\n",
    "        text_layer.adapt(train_parsed.map(lambda x: tf.reshape(x[f'{feature_name}'], [-1, MAX_PLAYLIST_LENGTH, 1])))\n",
    "        end = time.time()\n",
    "\n",
    "        logging.info(f'Layer adapt elapsed time: {round((end - start), 2)} seconds')\n",
    "\n",
    "        # ===================================================\n",
    "        # write vocab to pickled dict --> gcs\n",
    "        # ===================================================\n",
    "        logging.info(f\"Writting pickled dict to GCS...\")\n",
    "\n",
    "        VOCAB_LOCAL_FILE = f'{feature_name}_vocab_dict.pkl'\n",
    "        VOCAB_GCS_OBJ = f'{experiment_name}/{experiment_run}/vocab-staging/{VOCAB_LOCAL_FILE}' # destination folder prefix and blob name\n",
    "        VOCAB_DICT = {f'{feature_name}' : text_layer.get_vocabulary(),}\n",
    "\n",
    "        logging.info(f\"VOCAB_LOCAL_FILE: {VOCAB_LOCAL_FILE}\")\n",
    "        logging.info(f\"VOCAB_GCS_OBJ: {VOCAB_GCS_OBJ}\")\n",
    "\n",
    "        # pickle\n",
    "        filehandler = open(f'{VOCAB_LOCAL_FILE}', 'wb')\n",
    "        pkl.dump(VOCAB_DICT, filehandler)\n",
    "        filehandler.close()\n",
    "\n",
    "        # upload to GCS\n",
    "        bucket_client = storage_client.bucket(train_output_gcs_bucket)\n",
    "        blob = bucket_client.blob(VOCAB_GCS_OBJ)\n",
    "        blob.upload_from_filename(VOCAB_LOCAL_FILE)\n",
    "\n",
    "        vocab_uri = f'gs://{train_output_gcs_bucket}/{VOCAB_GCS_OBJ}'\n",
    "\n",
    "        logging.info(f\"File {VOCAB_LOCAL_FILE} uploaded to {vocab_uri}\")\n",
    "        \n",
    "    else:\n",
    "        logging.info(f\"Using existing vocab files...\")\n",
    "        vocab_uri = 'gs://two-tower-models/vocabs/vocab_dict.pkl'\n",
    "        logging.info(f\"Using vocab file: {vocab_uri}\")\n",
    "    \n",
    "    return(\n",
    "        vocab_uri,\n",
    "        # feature_name,\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b338b3b-ba3c-4a0e-afe7-2b0bc2f3f3df",
   "metadata": {},
   "source": [
    "### create master vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b937d600-c4d1-4f5f-a5f3-29d13c350f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/train_pipes/create_master_vocab.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/{PIPELINES_SUB_DIR}/create_master_vocab.py\n",
    "\n",
    "import kfp\n",
    "from typing import Any, Callable, Dict, NamedTuple, Optional, List\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output,\n",
    "                        OutputPath, component, Metrics)\n",
    "\n",
    "@kfp.v2.dsl.component(\n",
    "    base_image=\"python:3.9\",\n",
    "    packages_to_install=[\n",
    "        # 'google-cloud-aiplatform==1.18.1',\n",
    "        'google-cloud-storage',\n",
    "        'numpy',\n",
    "        # 'tensorflow==2.8.3',\n",
    "    ],\n",
    ")\n",
    "def create_master_vocab(\n",
    "    project: str,\n",
    "    location: str,\n",
    "    version: str,\n",
    "    train_output_gcs_bucket: str,\n",
    "    experiment_name: str,\n",
    "    experiment_run: str,\n",
    "    vocab_uri_1: str,\n",
    "    vocab_uri_2: str,\n",
    "    vocab_uri_3: str,\n",
    "    vocab_uri_4: str,\n",
    "    vocab_uri_5: str,\n",
    "    vocab_uri_6: str,\n",
    "    vocab_uri_7: str,\n",
    "    vocab_uri_8: str,\n",
    "    vocab_uri_9: str,\n",
    "    generate_new_vocab: bool,\n",
    ") -> NamedTuple('Outputs', [\n",
    "    ('master_vocab_gcs_uri', str),\n",
    "    ('experiment_name', str),\n",
    "    ('experiment_run', str),\n",
    "]):\n",
    "    \n",
    "    \"\"\"\n",
    "    combine layer dictionaires to master dictionary\n",
    "    master dictionary passed to train job for layer vocabs\n",
    "    \"\"\"\n",
    "    \n",
    "    # import packages\n",
    "    import os\n",
    "    import logging\n",
    "    import pickle as pkl\n",
    "    import time\n",
    "    import numpy as np\n",
    "    \n",
    "    from google.cloud import storage\n",
    "    \n",
    "    # setup clients\n",
    "    storage_client = storage.Client()\n",
    "    \n",
    "    if generate_new_vocab:\n",
    "        \n",
    "        logging.info(f\"Generating new vocab master file...\")\n",
    "        # ===================================================\n",
    "        # Create list of all layer vocab dict uris\n",
    "        # ===================================================\n",
    "\n",
    "        vocab_dict_uris = [\n",
    "            vocab_uri_1, vocab_uri_2, \n",
    "            vocab_uri_3, vocab_uri_4, \n",
    "            vocab_uri_5, vocab_uri_6, \n",
    "            vocab_uri_7, vocab_uri_8, \n",
    "            vocab_uri_9, \n",
    "        ]\n",
    "        logging.info(f\"count of vocab_dict_uris: {len(vocab_dict_uris)}\")\n",
    "        logging.info(f\"vocab_dict_uris: {vocab_dict_uris}\")\n",
    "\n",
    "        # ===================================================\n",
    "        # load pickled dicts\n",
    "        # ===================================================\n",
    "\n",
    "        loaded_pickle_list = []\n",
    "        for i, pickled_dict in enumerate(vocab_dict_uris):\n",
    "\n",
    "            with open(f\"v{i}_vocab_pre_load\", 'wb') as local_vocab_file:\n",
    "                storage_client.download_blob_to_file(pickled_dict, local_vocab_file)\n",
    "\n",
    "            with open(f\"v{i}_vocab_pre_load\", 'rb') as pickle_file:\n",
    "                loaded_vocab_dict = pkl.load(pickle_file)\n",
    "\n",
    "            loaded_pickle_list.append(loaded_vocab_dict)\n",
    "\n",
    "        # ===================================================\n",
    "        # create master vocab dict\n",
    "        # ===================================================\n",
    "        master_dict = {}\n",
    "        for loaded_dict in loaded_pickle_list:\n",
    "            master_dict.update(loaded_dict)\n",
    "\n",
    "        # ===================================================\n",
    "        # Upload master to GCS\n",
    "        # ===================================================\n",
    "        MASTER_VOCAB_LOCAL_FILE = f'vocab_dict.pkl'\n",
    "        MASTER_VOCAB_GCS_OBJ = f'{experiment_name}/{experiment_run}/{MASTER_VOCAB_LOCAL_FILE}' # destination folder prefix and blob name\n",
    "\n",
    "        # pickle\n",
    "        filehandler = open(f'{MASTER_VOCAB_LOCAL_FILE}', 'wb')\n",
    "        pkl.dump(master_dict, filehandler)\n",
    "        filehandler.close()\n",
    "\n",
    "        # upload to GCS\n",
    "        bucket_client = storage_client.bucket(train_output_gcs_bucket)\n",
    "        blob = bucket_client.blob(MASTER_VOCAB_GCS_OBJ)\n",
    "        blob.upload_from_filename(MASTER_VOCAB_LOCAL_FILE)\n",
    "\n",
    "        master_vocab_uri = f'gs://{train_output_gcs_bucket}/{MASTER_VOCAB_GCS_OBJ}'\n",
    "\n",
    "        logging.info(f\"File {MASTER_VOCAB_LOCAL_FILE} uploaded to {master_vocab_uri}\")\n",
    "        \n",
    "    else:\n",
    "        logging.info(f\"Using existing vocab file...\")\n",
    "        master_vocab_uri = 'gs://two-tower-models/vocabs/vocab_dict.pkl'\n",
    "        logging.info(f\"Using vocab file: {master_vocab_uri}\")\n",
    "    \n",
    "    return(\n",
    "        master_vocab_uri,\n",
    "        experiment_name,\n",
    "        experiment_run\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8843984-b680-4c46-a456-b55291643e14",
   "metadata": {},
   "source": [
    "## Custom train job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bc26d281-a72f-4825-a5f8-33ff35d2fb03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/train_pipes/train_custom_model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/{PIPELINES_SUB_DIR}/train_custom_model.py\n",
    "\n",
    "import kfp\n",
    "from typing import Any, Callable, Dict, NamedTuple, Optional, List\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output,\n",
    "                        OutputPath, component, Metrics)\n",
    "@kfp.v2.dsl.component(\n",
    "    base_image='python:3.9',\n",
    "    packages_to_install=[\n",
    "        'google-cloud-aiplatform==1.21.0',\n",
    "        # 'tensorflow==2.9.2',\n",
    "        # 'tensorflow-recommenders==0.7.0',\n",
    "        'numpy',\n",
    "        'google-cloud-storage',\n",
    "    ],\n",
    "    # output_component_file=\"./pipelines/train_custom_model.yaml\",\n",
    ")\n",
    "def train_custom_model(\n",
    "    project: str,\n",
    "    location: str,\n",
    "    model_version: str,\n",
    "    pipeline_version: str,\n",
    "    model_name: str, \n",
    "    worker_pool_specs: dict,\n",
    "    # vocab_dict_uri: str, \n",
    "    train_output_gcs_bucket: str,                         # change to workdir?\n",
    "    training_image_uri: str,\n",
    "    tensorboard_resource_name: str,\n",
    "    service_account: str,\n",
    "    experiment_name: str,\n",
    "    experiment_run: str,\n",
    "    generate_new_vocab: bool,\n",
    ") -> NamedTuple('Outputs', [\n",
    "    ('job_dict_uri', str),\n",
    "    ('query_tower_dir_uri', str),\n",
    "    ('candidate_tower_dir_uri', str),\n",
    "    ('experiment_run_dir', str),\n",
    "]):\n",
    "    \n",
    "    import logging\n",
    "    import numpy as np\n",
    "    import pickle as pkl\n",
    "    \n",
    "    from google.cloud import aiplatform as vertex_ai\n",
    "    # import google.cloud.aiplatform_v1beta1 as aip_beta\n",
    "    from google.cloud import storage\n",
    "    \n",
    "    vertex_ai.init(\n",
    "        project=project,\n",
    "        location=location,\n",
    "        experiment=experiment_name,\n",
    "    )\n",
    "    \n",
    "    storage_client = storage.Client(project=project)\n",
    "    \n",
    "    JOB_NAME = f'train-{model_name}'\n",
    "    logging.info(f'JOB_NAME: {JOB_NAME}')\n",
    "    \n",
    "    BASE_OUTPUT_DIR = f'gs://{train_output_gcs_bucket}/{experiment_name}/{experiment_run}'\n",
    "    logging.info(f'BASE_OUTPUT_DIR: {BASE_OUTPUT_DIR}')\n",
    "    \n",
    "    # logging.info(f'vocab_dict_uri: {vocab_dict_uri}')\n",
    "    \n",
    "    logging.info(f'tensorboard_resource_name: {tensorboard_resource_name}')\n",
    "    logging.info(f'service_account: {service_account}')\n",
    "    logging.info(f'worker_pool_specs: {worker_pool_specs}')\n",
    "    \n",
    "    # ====================================================\n",
    "    # Launch Vertex job\n",
    "    # ====================================================\n",
    "    \n",
    "    worker_pool_specs[0]['container_spec']['args'].append(f'--tb_resource_name={tensorboard_resource_name}')\n",
    "    \n",
    "    if generate_new_vocab == 'True':\n",
    "        worker_pool_specs[0]['container_spec']['args'].append(f'--new_vocab')\n",
    "  \n",
    "    job = vertex_ai.CustomJob(\n",
    "        display_name=JOB_NAME,\n",
    "        worker_pool_specs=worker_pool_specs,\n",
    "        base_output_dir=BASE_OUTPUT_DIR,\n",
    "        staging_bucket=f\"{BASE_OUTPUT_DIR}/staging\",\n",
    "    )\n",
    "    \n",
    "    logging.info(f'Submitting train job to Vertex AI...')\n",
    "    \n",
    "    job.run(\n",
    "        tensorboard=tensorboard_resource_name,\n",
    "        service_account=f'{service_account}',\n",
    "        restart_job_on_worker_restart=False,\n",
    "        enable_web_access=True,\n",
    "        sync=False,\n",
    "    )\n",
    "        \n",
    "    # wait for job to complete\n",
    "    job.wait()\n",
    "    \n",
    "    # ====================================================\n",
    "    # Save job details\n",
    "    # ====================================================\n",
    "    \n",
    "    train_job_dict = job.to_dict()\n",
    "    logging.info(f'train_job_dict: {train_job_dict}')\n",
    "    \n",
    "    # pkl dict to GCS\n",
    "    logging.info(f\"Write pickled dict to GCS...\")\n",
    "    TRAIN_DICT_LOCAL = f'train_job_dict.pkl'\n",
    "    TRAIN_DICT_GCS_OBJ = f'{experiment_name}/{experiment_run}/{TRAIN_DICT_LOCAL}' # destination folder prefix and blob name\n",
    "    \n",
    "    logging.info(f\"TRAIN_DICT_LOCAL: {TRAIN_DICT_LOCAL}\")\n",
    "    logging.info(f\"TRAIN_DICT_GCS_OBJ: {TRAIN_DICT_GCS_OBJ}\")\n",
    "\n",
    "    # pickle\n",
    "    filehandler = open(f'{TRAIN_DICT_LOCAL}', 'wb')\n",
    "    pkl.dump(train_job_dict, filehandler)\n",
    "    filehandler.close()\n",
    "    \n",
    "    # upload to GCS\n",
    "    bucket_client = storage_client.bucket(train_output_gcs_bucket)\n",
    "    blob = bucket_client.blob(TRAIN_DICT_GCS_OBJ)\n",
    "    blob.upload_from_filename(TRAIN_DICT_LOCAL)\n",
    "    \n",
    "    job_dict_uri = f'gs://{train_output_gcs_bucket}/{TRAIN_DICT_GCS_OBJ}'\n",
    "    logging.info(f\"{TRAIN_DICT_LOCAL} uploaded to {job_dict_uri}\")\n",
    "    \n",
    "    # ====================================================\n",
    "    # Model and index artifact uris\n",
    "    # ====================================================\n",
    "    EXPERIMENT_RUN_DIR = f\"gs://{train_output_gcs_bucket}/{experiment_name}/{experiment_run}\"\n",
    "    query_tower_dir_uri = f\"{EXPERIMENT_RUN_DIR}/model-dir/query_model\" \n",
    "    candidate_tower_dir_uri = f\"{EXPERIMENT_RUN_DIR}/model-dir/candidate_model\"\n",
    "    # candidate_index_dir_uri = f\"gs://{output_dir_gcs_bucket_name}/{experiment_name}/{experiment_run}/candidate_model\"\n",
    "    \n",
    "    logging.info(f'query_tower_dir_uri: {query_tower_dir_uri}')\n",
    "    logging.info(f'candidate_tower_dir_uri: {candidate_tower_dir_uri}')\n",
    "    # logging.info(f'candidate_index_dir_uri: {candidate_index_dir_uri}')\n",
    "    \n",
    "    return (\n",
    "        f'{job_dict_uri}',\n",
    "        f'{query_tower_dir_uri}',\n",
    "        f'{candidate_tower_dir_uri}',\n",
    "        f'{EXPERIMENT_RUN_DIR}',\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098b5708-4536-4d8a-a4d4-7aaa76112a47",
   "metadata": {},
   "source": [
    "## Generate Candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bf697751-337f-4757-a5b4-fedc2246cc66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/train_pipes/generate_candidates.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/{PIPELINES_SUB_DIR}/generate_candidates.py\n",
    "\n",
    "import kfp\n",
    "from typing import Any, Callable, Dict, NamedTuple, Optional, List\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output,\n",
    "                        OutputPath, component, Metrics)\n",
    "\n",
    "@kfp.v2.dsl.component(\n",
    "    base_image=\"python:3.9\",\n",
    "    packages_to_install=[\n",
    "        'google-cloud-aiplatform==1.21.0',\n",
    "        'tensorflow==2.10.1',\n",
    "        'tensorflow-recommenders==0.7.2',\n",
    "        'numpy',\n",
    "        # 'google-cloud-storage',\n",
    "    ],\n",
    ")\n",
    "def generate_candidates(\n",
    "    project: str,\n",
    "    location: str,\n",
    "    version: str, \n",
    "    # emb_index_gcs_uri: str,\n",
    "    candidate_tower_dir_uri: str,\n",
    "    candidate_file_dir_bucket: str,\n",
    "    candidate_file_dir_prefix: str,\n",
    "    train_output_gcs_bucket: str,\n",
    "    experiment_name: str,\n",
    "    experiment_run: str,\n",
    "    experiment_run_dir: str,\n",
    ") -> NamedTuple('Outputs', [\n",
    "    ('emb_index_gcs_uri', str),\n",
    "    # ('emb_index_artifact', Artifact),\n",
    "]):\n",
    "    import logging\n",
    "    import json\n",
    "    import pickle as pkl\n",
    "    from pprint import pprint\n",
    "    import time\n",
    "    import numpy as np\n",
    "\n",
    "    import os\n",
    "\n",
    "    # os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "\n",
    "    import tensorflow as tf\n",
    "    import tensorflow_recommenders as tfrs\n",
    "\n",
    "    from google.cloud import storage\n",
    "    from google.cloud.storage.bucket import Bucket\n",
    "    from google.cloud.storage.blob import Blob\n",
    "\n",
    "    import google.cloud.aiplatform as vertex_ai\n",
    "    \n",
    "    # set clients\n",
    "    vertex_ai.init(\n",
    "        project=project,\n",
    "        location=location,\n",
    "    )\n",
    "    storage_client = storage.Client(project=project)\n",
    "\n",
    "    # tf.Data confg\n",
    "    options = tf.data.Options()\n",
    "    options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.DATA\n",
    "    \n",
    "    # ====================================================\n",
    "    # Load trained candidate tower\n",
    "    # ====================================================\n",
    "    logging.info(f\"candidate_tower_dir_uri: {candidate_tower_dir_uri}\")\n",
    "    \n",
    "    loaded_candidate_model = tf.saved_model.load(candidate_tower_dir_uri)\n",
    "    logging.info(f\"loaded_candidate_model.signatures: {loaded_candidate_model.signatures}\")\n",
    "    \n",
    "    candidate_predictor = loaded_candidate_model.signatures[\"serving_default\"]\n",
    "    logging.info(f\"structured_outputs: {candidate_predictor.structured_outputs}\")\n",
    "    \n",
    "    # ===================================================\n",
    "    # set feature vars\n",
    "    # ===================================================\n",
    "    FEATURES_PREFIX = f'{experiment_name}/{experiment_run}/features'\n",
    "    logging.info(f\"FEATURES_PREFIX: {FEATURES_PREFIX}\")\n",
    "    \n",
    "    def download_blob(bucket_name, source_gcs_obj, local_filename):\n",
    "        \"\"\"Uploads a file to the bucket.\"\"\"\n",
    "        # storage_client = storage.Client(project=project_number)\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "        blob = bucket.blob(source_gcs_obj)\n",
    "        blob.download_to_filename(local_filename)\n",
    "        \n",
    "        filehandler = open(f'{local_filename}', 'rb')\n",
    "        loaded_dict = pkl.load(filehandler)\n",
    "        filehandler.close()\n",
    "        \n",
    "        logging.info(f\"File {local_filename} downloaded from gs://{bucket_name}/{source_gcs_obj}\")\n",
    "        \n",
    "        return loaded_dict\n",
    "    \n",
    "    # ===================================================\n",
    "    # load pickled Candidate features\n",
    "    # ===================================================\n",
    "    \n",
    "    # candidate features\n",
    "    CAND_FEAT_FILENAME = 'candidate_feats_dict.pkl'\n",
    "    CAND_FEAT_GCS_OBJ = f'{FEATURES_PREFIX}/{CAND_FEAT_FILENAME}'\n",
    "    LOADED_CANDIDATE_DICT = f'loaded_{CAND_FEAT_FILENAME}'\n",
    "    \n",
    "    loaded_candidate_features_dict = download_blob(\n",
    "        train_output_gcs_bucket,\n",
    "        CAND_FEAT_GCS_OBJ,\n",
    "        LOADED_CANDIDATE_DICT\n",
    "    )\n",
    "    \n",
    "    # ====================================================\n",
    "    # Features and Helper Functions\n",
    "    # ====================================================\n",
    "    \n",
    "    def parse_candidate_tfrecord_fn(example):\n",
    "        \"\"\"\n",
    "        Reads candidate serialized examples from gcs and converts to tfrecord\n",
    "        \"\"\"\n",
    "        # example = tf.io.parse_single_example(\n",
    "        example = tf.io.parse_example(\n",
    "            example, \n",
    "            features=loaded_candidate_features_dict\n",
    "        )\n",
    "        return example\n",
    "\n",
    "    def full_parse(data):\n",
    "        # used for interleave - takes tensors and returns a tf.dataset\n",
    "        data = tf.data.TFRecordDataset(data)\n",
    "        return data\n",
    "    \n",
    "    # ====================================================\n",
    "    # Create Candidate Dataset\n",
    "    # ====================================================\n",
    "\n",
    "    candidate_files = []\n",
    "    for blob in storage_client.list_blobs(f\"{candidate_file_dir_bucket}\", prefix=f'{candidate_file_dir_prefix}/'):\n",
    "        if '.tfrecords' in blob.name:\n",
    "            candidate_files.append(blob.public_url.replace(\"https://storage.googleapis.com/\", \"gs://\"))\n",
    "\n",
    "    candidate_dataset = tf.data.Dataset.from_tensor_slices(candidate_files)\n",
    "\n",
    "    parsed_candidate_dataset = candidate_dataset.interleave(\n",
    "        # lambda x: tf.data.TFRecordDataset(x),\n",
    "        full_parse,\n",
    "        cycle_length=tf.data.AUTOTUNE, \n",
    "        num_parallel_calls=tf.data.AUTOTUNE,\n",
    "        deterministic=False\n",
    "    ).map(parse_candidate_tfrecord_fn, num_parallel_calls=tf.data.AUTOTUNE).with_options(options)\n",
    "\n",
    "    parsed_candidate_dataset = parsed_candidate_dataset.cache() #400 MB on machine mem\n",
    "    \n",
    "    # ====================================================\n",
    "    # Generate embedding vectors for each candidate\n",
    "    # ====================================================\n",
    "    logging.info(\"Starting candidate dataset mapping...\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    embs_iter = parsed_candidate_dataset.batch(1000).map(\n",
    "        lambda data: candidate_predictor(\n",
    "            track_uri_can = data[\"track_uri_can\"],\n",
    "            track_name_can = data['track_name_can'],\n",
    "            artist_uri_can = data['artist_uri_can'],\n",
    "            artist_name_can = data['artist_name_can'],\n",
    "            album_uri_can = data['album_uri_can'],\n",
    "            album_name_can = data['album_name_can'],\n",
    "            duration_ms_can = data['duration_ms_can'],\n",
    "            track_pop_can = data['track_pop_can'],\n",
    "            artist_pop_can = data['artist_pop_can'],\n",
    "            artist_genres_can = data['artist_genres_can'],\n",
    "            artist_followers_can = data['artist_followers_can'],\n",
    "            track_danceability_can = data['track_danceability_can'],\n",
    "            track_energy_can = data['track_energy_can'],\n",
    "            track_key_can = data['track_key_can'],\n",
    "            track_loudness_can = data['track_loudness_can'],\n",
    "            track_mode_can = data['track_mode_can'],\n",
    "            track_speechiness_can = data['track_speechiness_can'],\n",
    "            track_acousticness_can = data['track_acousticness_can'],\n",
    "            track_instrumentalness_can = data['track_instrumentalness_can'],\n",
    "            track_liveness_can = data['track_liveness_can'],\n",
    "            track_valence_can = data['track_valence_can'],\n",
    "            track_tempo_can = data['track_tempo_can'],\n",
    "            time_signature_can = data['time_signature_can']\n",
    "        )\n",
    "    )\n",
    "\n",
    "    embs = []\n",
    "    for emb in embs_iter:\n",
    "        embs.append(emb)\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = int((end_time - start_time) / 60)\n",
    "    logging.info(f\"elapsed_time: {elapsed_time}\")\n",
    "    logging.info(f\"Length of embs: {len(embs)}\")\n",
    "    logging.info(f\"embeddings[0]: {embs[0]}\")\n",
    "    \n",
    "    # ====================================================\n",
    "    # prep Track IDs and Vectors for JSON\n",
    "    # ====================================================\n",
    "    logging.info(\"Cleaning embeddings and track IDs...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    cleaned_embs = [x['output_1'].numpy()[0] for x in embs] #clean up the output\n",
    "    \n",
    "    end_time = time.time()\n",
    "    elapsed_time = int((end_time - start_time) / 60)\n",
    "    logging.info(f\"elapsed_time: {elapsed_time}\")\n",
    "    logging.info(f\"Length of cleaned_embs: {len(cleaned_embs)}\")\n",
    "    \n",
    "    # clean track IDs\n",
    "    track_uris = [x['track_uri_can'].numpy() for x in parsed_candidate_dataset]\n",
    "    logging.info(f\"Length of track_uris: {len(track_uris)}\")\n",
    "    \n",
    "    track_uris_decoded = [z.decode(\"utf-8\") for z in track_uris]\n",
    "    logging.info(f\"Length of track_uris_decoded: {len(track_uris_decoded)}\")\n",
    "    \n",
    "    # check for bad records\n",
    "    bad_records = []\n",
    "\n",
    "    for i, emb in enumerate(cleaned_embs):\n",
    "        bool_emb = np.isnan(emb)\n",
    "        for val in bool_emb:\n",
    "            if val:\n",
    "                bad_records.append(i)\n",
    "\n",
    "    bad_record_filter = np.unique(bad_records)\n",
    "\n",
    "    logging.info(f\"bad_records: {len(bad_records)}\")\n",
    "    logging.info(f\"bad_record_filter: {len(bad_record_filter)}\")\n",
    "    \n",
    "    # ZIP together\n",
    "    logging.info(\"Zipping IDs and vectors ...\")\n",
    "    \n",
    "    track_uris_valid = []\n",
    "    emb_valid = []\n",
    "\n",
    "    for i, pair in enumerate(zip(track_uris_decoded, cleaned_embs)):\n",
    "        if i in bad_record_filter:\n",
    "            pass\n",
    "        else:\n",
    "            t_uri, embed = pair\n",
    "            track_uris_valid.append(t_uri)\n",
    "            emb_valid.append(embed)\n",
    "            \n",
    "    logging.info(f\"track_uris_valid[0]: {track_uris_valid[0]}\")\n",
    "    logging.info(f\"bad_records: {len(bad_records)}\")\n",
    "            \n",
    "    # ====================================================\n",
    "    # writting JSON file to GCS\n",
    "    # ====================================================\n",
    "    TIMESTAMP = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    embeddings_index_filename = f'candidate_embs_{version}_{TIMESTAMP}.json'\n",
    "\n",
    "    with open(f'{embeddings_index_filename}', 'w') as f:\n",
    "        for prod, emb in zip(track_uris_valid, emb_valid):\n",
    "            f.write('{\"id\":\"' + str(prod) + '\",')\n",
    "            f.write('\"embedding\":[' + \",\".join(str(x) for x in list(emb)) + \"]}\")\n",
    "            f.write(\"\\n\")\n",
    "            \n",
    "    # write to GCS\n",
    "    INDEX_GCS_URI = f'{experiment_run_dir}/candidate-embeddings-{version}'\n",
    "    logging.info(f\"INDEX_GCS_URI: {INDEX_GCS_URI}\")\n",
    "\n",
    "    DESTINATION_BLOB_NAME = embeddings_index_filename\n",
    "    SOURCE_FILE_NAME = embeddings_index_filename\n",
    "\n",
    "    logging.info(f\"DESTINATION_BLOB_NAME: {DESTINATION_BLOB_NAME}\")\n",
    "    logging.info(f\"SOURCE_FILE_NAME: {SOURCE_FILE_NAME}\")\n",
    "    \n",
    "    blob = Blob.from_string(os.path.join(INDEX_GCS_URI, DESTINATION_BLOB_NAME))\n",
    "    blob.bucket._client = storage_client\n",
    "    blob.upload_from_filename(SOURCE_FILE_NAME)\n",
    "    \n",
    "    return (\n",
    "        f'{INDEX_GCS_URI}',\n",
    "        # f'{INDEX_GCS_URI}',\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a839b3b-207f-4b65-a2f6-607a56dc7181",
   "metadata": {},
   "source": [
    "## Create ANN Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "b7a65b44-3ad4-4168-af40-67289000a434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/train_pipes/create_ann_index.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/{PIPELINES_SUB_DIR}/create_ann_index.py\n",
    "\n",
    "import kfp\n",
    "from typing import Any, Callable, Dict, NamedTuple, Optional, List\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output,\n",
    "                        OutputPath, component, Metrics)\n",
    "\n",
    "@kfp.v2.dsl.component(\n",
    "    base_image=\"python:3.9\",\n",
    "    packages_to_install=[\n",
    "        'google-cloud-aiplatform==1.21.0',\n",
    "        'google-api-core==2.10.0'\n",
    "        # 'google-cloud-storage',\n",
    "    ],\n",
    ")\n",
    "def create_ann_index(\n",
    "    project: str,\n",
    "    location: str,\n",
    "    version: str, \n",
    "    vpc_network_name: str,\n",
    "    emb_index_gcs_uri: str,\n",
    "    dimensions: int,\n",
    "    ann_index_display_name: str,\n",
    "    approximate_neighbors_count: int,\n",
    "    distance_measure_type: str,\n",
    "    leaf_node_embedding_count: int,\n",
    "    leaf_nodes_to_search_percent: int, \n",
    "    ann_index_description: str,\n",
    "    # ann_index_labels: Dict, \n",
    ") -> NamedTuple('Outputs', [\n",
    "    ('ann_index_resource_uri', str),\n",
    "    ('ann_index', Artifact),\n",
    "]):\n",
    "    import logging\n",
    "    from google.cloud import aiplatform as vertex_ai\n",
    "    from datetime import datetime\n",
    "    import time\n",
    "\n",
    "    vertex_ai.init(\n",
    "        project=project,\n",
    "        location=location,\n",
    "    )\n",
    "    \n",
    "    VERSION = version.replace('_', '-')\n",
    "    \n",
    "    TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    \n",
    "    ENDPOINT = \"{}-aiplatform.googleapis.com\".format(location)\n",
    "    NETWORK_NAME = vpc_network_name\n",
    "    INDEX_DIR_GCS = emb_index_gcs_uri\n",
    "    PARENT = \"projects/{}/locations/{}\".format(project, location)\n",
    "\n",
    "    logging.info(f\"ENDPOINT: {ENDPOINT}\")\n",
    "    logging.info(f\"project: {project}\")\n",
    "    logging.info(f\"location: {location}\")\n",
    "    logging.info(f\"INDEX_DIR_GCS: {INDEX_DIR_GCS}\")\n",
    "    \n",
    "    display_name = f'{ann_index_display_name}-{VERSION}'\n",
    "    \n",
    "    logging.info(f\"display_name: {display_name}\")\n",
    "    \n",
    "    # ==============================================================================\n",
    "    # Create Index \n",
    "    # ==============================================================================\n",
    "\n",
    "    start = time.time()\n",
    "        \n",
    "    tree_ah_index = vertex_ai.MatchingEngineIndex.create_tree_ah_index(\n",
    "        display_name=display_name,\n",
    "        contents_delta_uri=f'{emb_index_gcs_uri}', # emb_index_gcs_uri,\n",
    "        dimensions=dimensions,\n",
    "        approximate_neighbors_count=approximate_neighbors_count,\n",
    "        distance_measure_type=distance_measure_type,\n",
    "        leaf_node_embedding_count=leaf_node_embedding_count,\n",
    "        leaf_nodes_to_search_percent=leaf_nodes_to_search_percent,\n",
    "        description=ann_index_description,\n",
    "        # labels=ann_index_labels,\n",
    "        sync=False,\n",
    "    )\n",
    "\n",
    "    end = time.time()\n",
    "    elapsed_time = round((end - start), 2)\n",
    "    logging.info(f'Elapsed time creating index: {elapsed_time} seconds\\n')\n",
    "    \n",
    "    ann_index_resource_uri = tree_ah_index.resource_name\n",
    "    logging.info(\"ann_index_resource_uri:\", ann_index_resource_uri) \n",
    "\n",
    "    return (\n",
    "      f'{ann_index_resource_uri}',\n",
    "      tree_ah_index,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b163cf-9bd7-4814-b1d4-eadc7554cf30",
   "metadata": {},
   "source": [
    "## Create brute force index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "758b11c7-0595-4787-a238-11bcbf3378ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/train_pipes/create_brute_force_index.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/{PIPELINES_SUB_DIR}/create_brute_force_index.py\n",
    "\n",
    "import kfp\n",
    "from typing import Any, Callable, Dict, NamedTuple, Optional, List\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output,\n",
    "                        OutputPath, component, Metrics)\n",
    "\n",
    "@kfp.v2.dsl.component(\n",
    "    base_image=\"python:3.9\",\n",
    "    packages_to_install=[\n",
    "        'google-cloud-aiplatform==1.21.0',\n",
    "        'google-api-core==2.10.0',\n",
    "        # 'google-cloud-storage',\n",
    "    ],\n",
    ")\n",
    "def create_brute_force_index(\n",
    "    project: str,\n",
    "    location: str,\n",
    "    version: str,\n",
    "    vpc_network_name: str,\n",
    "    emb_index_gcs_uri: str,\n",
    "    dimensions: int,\n",
    "    brute_force_index_display_name: str,\n",
    "    approximate_neighbors_count: int,\n",
    "    distance_measure_type: str,\n",
    "    brute_force_index_description: str,\n",
    "    # brute_force_index_labels: Dict,\n",
    ") -> NamedTuple('Outputs', [\n",
    "    ('brute_force_index_resource_uri', str),\n",
    "    ('brute_force_index', Artifact),\n",
    "]):\n",
    "\n",
    "    import logging\n",
    "    from google.cloud import aiplatform as vertex_ai\n",
    "    from datetime import datetime\n",
    "    import time\n",
    "\n",
    "    vertex_ai.init(\n",
    "        project=project,\n",
    "        location=location,\n",
    "    )\n",
    "    \n",
    "    TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    VERSION = version.replace('_', '-')\n",
    "    \n",
    "    ENDPOINT = \"{}-aiplatform.googleapis.com\".format(location)\n",
    "    NETWORK_NAME = vpc_network_name\n",
    "    INDEX_DIR_GCS = emb_index_gcs_uri\n",
    "    PARENT = \"projects/{}/locations/{}\".format(project, location)\n",
    "\n",
    "    logging.info(\"ENDPOINT: {}\".format(ENDPOINT))\n",
    "    logging.info(\"PROJECT_ID: {}\".format(project))\n",
    "    logging.info(\"REGION: {}\".format(location))\n",
    "    \n",
    "    display_name = f'{brute_force_index_display_name}-{VERSION}'\n",
    "    \n",
    "    logging.info(f\"display_name: {display_name}\")\n",
    "    \n",
    "    # ==============================================================================\n",
    "    # Create Index \n",
    "    # ==============================================================================\n",
    "\n",
    "    start = time.time()\n",
    "    \n",
    "    brute_force_index = vertex_ai.MatchingEngineIndex.create_brute_force_index(\n",
    "        display_name=display_name,\n",
    "        contents_delta_uri=f'{emb_index_gcs_uri}', # emb_index_gcs_uri,\n",
    "        dimensions=dimensions,\n",
    "        # approximate_neighbors_count=approximate_neighbors_count,\n",
    "        distance_measure_type=distance_measure_type,\n",
    "        description=brute_force_index_description,\n",
    "        # labels=brute_force_index_labels,\n",
    "        sync=False,\n",
    "    )\n",
    "    brute_force_index_resource_uri = brute_force_index.resource_name\n",
    "    print(\"brute_force_index_resource_uri:\",brute_force_index_resource_uri) \n",
    "\n",
    "    return (\n",
    "      f'{brute_force_index_resource_uri}',\n",
    "      brute_force_index,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c28ae77-5ce7-465c-ab5b-57d8ec8b3880",
   "metadata": {},
   "source": [
    "## Create ANN index endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "fa1f7c39-6fd8-43c0-ab6a-21fabcd57da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/train_pipes/create_ann_index_endpoint_vpc.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/{PIPELINES_SUB_DIR}/create_ann_index_endpoint_vpc.py\n",
    "\n",
    "import kfp\n",
    "from typing import Any, Callable, Dict, NamedTuple, Optional, List\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output,\n",
    "                        OutputPath, component, Metrics)\n",
    "\n",
    "@kfp.v2.dsl.component(\n",
    "    base_image=\"python:3.9\",\n",
    "    packages_to_install=[\n",
    "        'google-cloud-aiplatform==1.21.0',\n",
    "        # 'google-cloud-storage',\n",
    "    ],\n",
    ")\n",
    "def create_ann_index_endpoint_vpc(\n",
    "    ann_index_artifact: Input[Artifact],\n",
    "    project: str,\n",
    "    project_number: str,\n",
    "    location: str,\n",
    "    version: str,\n",
    "    vpc_network_name: str,\n",
    "    ann_index_endpoint_display_name: str,\n",
    "    ann_index_endpoint_description: str,\n",
    "    ann_index_resource_uri: str,\n",
    ") -> NamedTuple('Outputs', [\n",
    "    ('vpc_network_resource_uri', str),\n",
    "    ('ann_index_endpoint_resource_uri', str),\n",
    "    ('ann_index_endpoint', Artifact),\n",
    "    ('ann_index_endpoint_display_name', str),\n",
    "    ('ann_index_resource_uri', str),\n",
    "]):\n",
    "\n",
    "    import logging\n",
    "    from google.cloud import aiplatform as vertex_ai\n",
    "    from datetime import datetime\n",
    "    import time\n",
    "\n",
    "    vertex_ai.init(\n",
    "        project=project,\n",
    "        location=location,\n",
    "    )\n",
    "    \n",
    "    TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "    vpc_network_resource_uri = f'projects/{project_number}/global/networks/{vpc_network_name}'\n",
    "    logging.info(f\"vpc_network_resource_uri: {vpc_network_resource_uri}\")\n",
    "\n",
    "    ann_index_endpoint = vertex_ai.MatchingEngineIndexEndpoint.create(\n",
    "        display_name=f'{ann_index_endpoint_display_name}',\n",
    "        description=ann_index_endpoint_description,\n",
    "        network=vpc_network_resource_uri,\n",
    "    )\n",
    "    ann_index_endpoint_resource_uri = ann_index_endpoint.resource_name\n",
    "    logging.info(f\"ann_index_endpoint_resource_uri: {ann_index_endpoint_resource_uri}\")\n",
    "\n",
    "    return (\n",
    "        f'{vpc_network_resource_uri}',\n",
    "        f'{ann_index_endpoint_resource_uri}',\n",
    "        ann_index_endpoint,\n",
    "        f'{ann_index_endpoint_display_name}',\n",
    "        f'{ann_index_resource_uri}',\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d429b90f-25dc-47b9-b75e-1175f0bee26b",
   "metadata": {},
   "source": [
    "## Create brute force index endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "e00bf2fa-bc5c-4289-b6a0-7cf3b6e525f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/train_pipes/create_brute_index_endpoint_vpc.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/{PIPELINES_SUB_DIR}/create_brute_index_endpoint_vpc.py\n",
    "\n",
    "import kfp\n",
    "from typing import Any, Callable, Dict, NamedTuple, Optional, List\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output,\n",
    "                        OutputPath, component, Metrics)\n",
    "\n",
    "@kfp.v2.dsl.component(\n",
    "    base_image=\"python:3.9\",\n",
    "    packages_to_install=[\n",
    "        'google-cloud-aiplatform==1.21.0',\n",
    "        # 'google-cloud-storage',\n",
    "    ],\n",
    ")\n",
    "def create_brute_index_endpoint_vpc(\n",
    "    bf_index_artifact: Input[Artifact],\n",
    "    project: str,\n",
    "    project_number: str,\n",
    "    location: str,\n",
    "    version: str,\n",
    "    vpc_network_name: str,\n",
    "    brute_index_endpoint_display_name: str,\n",
    "    brute_index_endpoint_description: str,\n",
    "    brute_force_index_resource_uri: str,\n",
    ") -> NamedTuple('Outputs', [\n",
    "    ('vpc_network_resource_uri', str),\n",
    "    ('brute_index_endpoint_resource_uri', str),\n",
    "    ('brute_index_endpoint', Artifact),\n",
    "    ('brute_index_endpoint_display_name', str),\n",
    "    ('brute_force_index_resource_uri', str),\n",
    "]):\n",
    "\n",
    "    import logging\n",
    "    from google.cloud import aiplatform as vertex_ai\n",
    "    from datetime import datetime\n",
    "    import time\n",
    "\n",
    "    vertex_ai.init(\n",
    "        project=project,\n",
    "        location=location,\n",
    "    )\n",
    "    \n",
    "    TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "    vpc_network_resource_uri = f'projects/{project_number}/global/networks/{vpc_network_name}'\n",
    "    logging.info(f\"vpc_network_resource_uri: {vpc_network_resource_uri}\")\n",
    "\n",
    "    brute_index_endpoint = vertex_ai.MatchingEngineIndexEndpoint.create(\n",
    "        display_name=f'{brute_index_endpoint_display_name}',\n",
    "        description=brute_index_endpoint_description,\n",
    "        network=vpc_network_resource_uri,\n",
    "    )\n",
    "    brute_index_endpoint_resource_uri = brute_index_endpoint.resource_name\n",
    "    logging.info(f\"brute_index_endpoint_resource_uri: {brute_index_endpoint_resource_uri}\")\n",
    "\n",
    "    return (\n",
    "      f'{vpc_network_resource_uri}',\n",
    "      f'{brute_index_endpoint_resource_uri}',\n",
    "      brute_index_endpoint,\n",
    "      f'{brute_index_endpoint_display_name}',\n",
    "      f'{brute_force_index_resource_uri}',\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24b4ef9-ca2f-4e3b-8802-dd2a1abb1ddd",
   "metadata": {},
   "source": [
    "## Deploy ANN Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "f2a60680-96c2-4aaa-bea9-579bc26f6edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/train_pipes/deploy_ann_index.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/{PIPELINES_SUB_DIR}/deploy_ann_index.py\n",
    "\n",
    "import kfp\n",
    "from typing import Any, Callable, Dict, NamedTuple, Optional, List\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output,\n",
    "                        OutputPath, component, Metrics)\n",
    "\n",
    "@kfp.v2.dsl.component(\n",
    "    base_image=\"python:3.9\",\n",
    "    packages_to_install=[\n",
    "        'google-cloud-aiplatform==1.21.0'\n",
    "    ]\n",
    ")\n",
    "def deploy_ann_index(\n",
    "    project: str,\n",
    "    location: str,\n",
    "    version: str,\n",
    "    deployed_ann_index_name: str,\n",
    "    ann_index_resource_uri: str,\n",
    "    index_endpoint_resource_uri: str,\n",
    ") -> NamedTuple('Outputs', [\n",
    "    ('index_endpoint_resource_uri', str),\n",
    "    ('ann_index_resource_uri', str),\n",
    "    ('deployed_ann_index_name', str),\n",
    "    ('deployed_ann_index', Artifact),\n",
    "]):\n",
    "  \n",
    "    import logging\n",
    "    from google.cloud import aiplatform as vertex_ai\n",
    "    from datetime import datetime\n",
    "    import time\n",
    "\n",
    "    vertex_ai.init(\n",
    "        project=project,\n",
    "        location=location,\n",
    "    )\n",
    "    TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    deployed_ann_index_name = deployed_ann_index_name.replace('_', '-')\n",
    "    logging.info(f\"deployed_ann_index_name: {deployed_ann_index_name}\")\n",
    "    \n",
    "    ann_index = vertex_ai.MatchingEngineIndex(\n",
    "      index_name=ann_index_resource_uri\n",
    "    )\n",
    "    ann_index_resource_uri = ann_index.resource_name\n",
    "\n",
    "    index_endpoint = vertex_ai.MatchingEngineIndexEndpoint(\n",
    "      index_endpoint_resource_uri\n",
    "    )\n",
    "\n",
    "    index_endpoint = index_endpoint.deploy_index(\n",
    "      index=ann_index, \n",
    "      deployed_index_id=f'{deployed_ann_index_name}' #-{TIMESTAMP}'\n",
    "    )\n",
    "\n",
    "    logging.info(f\"index_endpoint.deployed_indexes: {index_endpoint.deployed_indexes}\")\n",
    "\n",
    "    return (\n",
    "      f'{index_endpoint_resource_uri}',\n",
    "      f'{ann_index_resource_uri}',\n",
    "      f'{deployed_ann_index_name}',\n",
    "      ann_index,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2557b1-3b03-48fc-a658-4b6c1b5cc812",
   "metadata": {},
   "source": [
    "## Deploy brute force Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "f073f3ad-c410-4c08-bd50-1b3ffcbe728c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/train_pipes/deploy_brute_index.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/{PIPELINES_SUB_DIR}/deploy_brute_index.py\n",
    "\n",
    "import kfp\n",
    "from typing import Any, Callable, Dict, NamedTuple, Optional, List\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output,\n",
    "                        OutputPath, component, Metrics)\n",
    "\n",
    "@kfp.v2.dsl.component(\n",
    "    base_image=\"python:3.9\",\n",
    "    packages_to_install=[\n",
    "        'google-cloud-aiplatform==1.21.0',\n",
    "        # 'google-cloud-storage',\n",
    "    ],\n",
    ")\n",
    "def deploy_brute_index(\n",
    "    project: str,\n",
    "    location: str,\n",
    "    version: str,\n",
    "    deployed_brute_force_index_name: str,\n",
    "    brute_force_index_resource_uri: str,\n",
    "    index_endpoint_resource_uri: str,\n",
    ") -> NamedTuple('Outputs', [\n",
    "    ('index_endpoint_resource_uri', str),\n",
    "    ('brute_force_index_resource_uri', str),\n",
    "    ('deployed_brute_force_index_name', str),\n",
    "    ('deployed_brute_force_index', Artifact),\n",
    "]):\n",
    "  \n",
    "    import logging\n",
    "    from google.cloud import aiplatform as vertex_ai\n",
    "    from datetime import datetime\n",
    "    import time\n",
    "\n",
    "    vertex_ai.init(\n",
    "        project=project,\n",
    "        location=location,\n",
    "    )\n",
    "    TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    deployed_brute_force_index_name = deployed_brute_force_index_name.replace('_', '-')\n",
    "    logging.info(f\"deployed_brute_force_index_name: {deployed_brute_force_index_name}\")\n",
    "\n",
    "    brute_index = vertex_ai.MatchingEngineIndex(\n",
    "        index_name=brute_force_index_resource_uri\n",
    "    )\n",
    "    brute_force_index_resource_uri = brute_index.resource_name\n",
    "\n",
    "    index_endpoint = vertex_ai.MatchingEngineIndexEndpoint(index_endpoint_resource_uri)\n",
    "\n",
    "    index_endpoint = index_endpoint.deploy_index(\n",
    "        index=brute_index, \n",
    "        deployed_index_id=f'{deployed_brute_force_index_name}', #-{TIMESTAMP}'\n",
    "    )\n",
    "\n",
    "    logging.info(f\"index_endpoint.deployed_indexes: {index_endpoint.deployed_indexes}\")\n",
    "\n",
    "    return (\n",
    "      f'{index_endpoint_resource_uri}',\n",
    "      f'{brute_force_index_resource_uri}',\n",
    "      f'{deployed_brute_force_index_name}', #-{TIMESTAMP}',\n",
    "      brute_index,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8331413f-8639-4c2d-b948-1162d97bec13",
   "metadata": {},
   "source": [
    "## Test index recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "53209e5c-2050-4333-a994-4133b8ee58b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/train_pipes/test_index_recall.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/{PIPELINES_SUB_DIR}/test_index_recall.py\n",
    "\n",
    "import kfp\n",
    "from typing import Any, Callable, Dict, NamedTuple, Optional, List\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output,\n",
    "                        OutputPath, component, Metrics)\n",
    "\n",
    "@kfp.v2.dsl.component(\n",
    "    base_image=\"python:3.9\",\n",
    "    packages_to_install=[\n",
    "        'google-cloud-aiplatform==1.21.0',\n",
    "        'google-cloud-storage',\n",
    "        'tensorflow==2.10.1',\n",
    "        'numpy'\n",
    "    ],\n",
    ")\n",
    "def test_index_recall(\n",
    "    project: str,\n",
    "    location: str,\n",
    "    version: str,\n",
    "    # train_dir: str,\n",
    "    # train_dir_prefix: str,\n",
    "    # ann_index_resource_uri: str,\n",
    "    ann_index_endpoint_resource_uri: str,\n",
    "    brute_index_endpoint_resource_uri: str,\n",
    "    gcs_train_script_path: str,\n",
    "    endpoint: str, # Input[Artifact],\n",
    "    metrics: Output[Metrics],\n",
    "):\n",
    "    \n",
    "    import logging\n",
    "    from datetime import datetime\n",
    "    import time\n",
    "    import numpy as np\n",
    "    \n",
    "    import base64\n",
    "\n",
    "    from typing import Dict, List, Union\n",
    "\n",
    "    from google.cloud import aiplatform as vertex_ai\n",
    "    from google.protobuf import json_format\n",
    "    from google.protobuf.json_format import Parse\n",
    "    from google.protobuf.struct_pb2 import Value\n",
    "\n",
    "    from google_cloud_pipeline_components.proto.gcp_resources_pb2 import GcpResources\n",
    "\n",
    "    import tensorflow as tf\n",
    "\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "    vertex_ai.init(\n",
    "        project=project,\n",
    "        location=location,\n",
    "    )\n",
    "    storage_client = storage.Client(project=project)\n",
    "    \n",
    "    # ====================================================\n",
    "    # helper functions\n",
    "    # ====================================================\n",
    "    \n",
    "    def download_blob(bucket_name, source_gcs_obj, local_filename):\n",
    "        \"\"\"Uploads a file to the bucket.\"\"\"\n",
    "        # storage_client = storage.Client(project=project_number)\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "        blob = bucket.blob(source_gcs_obj)\n",
    "        blob.download_to_filename(local_filename)\n",
    "        \n",
    "        filehandler = open(f'{local_filename}', 'rb')\n",
    "        loaded_dict = pkl.load(filehandler)\n",
    "        filehandler.close()\n",
    "        \n",
    "        logging.info(f\"File {local_filename} downloaded from gs://{bucket_name}/{source_gcs_obj}\")\n",
    "        \n",
    "        return loaded_dict\n",
    "    \n",
    "    # ====================================================\n",
    "    # get deployed model endpoint\n",
    "    # ====================================================\n",
    "    logging.info(f\"Endpoint = {endpoint}\")\n",
    "    gcp_resources = Parse(endpoint, GcpResources())\n",
    "    logging.info(f\"gcp_resources = {gcp_resources}\")\n",
    "    \n",
    "    _endpoint_resource = gcp_resources.resources[0].resource_uri\n",
    "    logging.info(f\"_endpoint_resource = {_endpoint_resource}\")\n",
    "    \n",
    "    _endpoint_uri = \"/\".join(_endpoint_resource.split(\"/\")[-8:-2])\n",
    "    logging.info(f\"_endpoint_uri = {_endpoint_uri}\")\n",
    "    \n",
    "    # define endpoint resource in component\n",
    "    _endpoint = vertex_ai.Endpoint(_endpoint_uri)\n",
    "    logging.info(f\"_endpoint defined\")\n",
    "    \n",
    "    # ==============================================================\n",
    "    # helper function for returning endpoint predictions via json\n",
    "    # ==============================================================\n",
    "    \n",
    "    def predict_custom_trained_model_sample(\n",
    "        project: str,\n",
    "        endpoint_id: str,\n",
    "        instances: Dict,\n",
    "        location: str = \"us-central1\",\n",
    "        api_endpoint: str = \"us-central1-aiplatform.googleapis.com\",):\n",
    "        \"\"\"\n",
    "        `instances` can be either single instance of type dict or a list\n",
    "        of instances.\n",
    "        \"\"\"\n",
    "\n",
    "        # The AI Platform services require regional API endpoints.\n",
    "        client_options = {\"api_endpoint\": api_endpoint}\n",
    "        \n",
    "        # Initialize client that will be used to create and send requests.\n",
    "        # This client only needs to be created once, and can be reused for multiple requests.\n",
    "        client = vertex_ai.gapic.PredictionServiceClient(client_options=client_options)\n",
    "        \n",
    "        # The format of each instance should conform to the deployed model's prediction input schema.\n",
    "        instances = instances if type(instances) == list else [instances]\n",
    "        instances = [\n",
    "            json_format.ParseDict(instance_dict, Value()) for instance_dict in instances\n",
    "        ]\n",
    "        \n",
    "        parameters_dict = {}\n",
    "        parameters = json_format.ParseDict(parameters_dict, Value())\n",
    "        \n",
    "        endpoint = client.endpoint_path(\n",
    "            project=project, location=location, endpoint=endpoint_id\n",
    "        )\n",
    "        \n",
    "        response = client.predict(\n",
    "            endpoint=endpoint, instances=instances, parameters=parameters\n",
    "        )\n",
    "        logging.info(f'Response: {response}')\n",
    "        \n",
    "        logging.info(f'Deployed Model ID(s): {response.deployed_model_id}')\n",
    "\n",
    "        # The predictions are a google.protobuf.Value representation of the model's predictions.\n",
    "        _predictions = response.predictions\n",
    "        logging.info(f'Response Predictions: {_predictions}')\n",
    "        \n",
    "        return _predictions\n",
    "    \n",
    "    # ===================================================\n",
    "    # load test instance\n",
    "    # ===================================================\n",
    "    BUCKET_TEST = 'spotify-data-regimes'\n",
    "    LOCAL_TEST_INSTANCE = 'test_instance_15_dict.pkl'\n",
    "    PREFIX = 'jtv15-8m'\n",
    "    TEST_GCS_OBJ = f'{PREFIX}/{LOCAL_TEST_INSTANCE}'\n",
    "    LOADED_CANDIDATE_DICT = f'loaded_{CAND_FEAT_FILENAME}'\n",
    "    \n",
    "    loaded_test_instance = download_blob(\n",
    "        BUCKET_TEST,\n",
    "        TEST_GCS_OBJ,\n",
    "        LOADED_CANDIDATE_DICT\n",
    "    )\n",
    "    logging.info(f'loaded_test_instance: {loaded_test_instance}')\n",
    "    \n",
    "    # make prediction request\n",
    "    _endpoint_id = _endpoint_uri.split('/')[-1]    # \"633325234048\",\n",
    "    logging.info(f\"_endpoint_id created = {_endpoint_id}\")\n",
    "    prediction_test = predict_custom_trained_model_sample(\n",
    "        project=project,                     \n",
    "        endpoint_id=_endpoint_id,\n",
    "        location=\"us-central1\",\n",
    "        instances=loaded_test_instance\n",
    "    )\n",
    "    \n",
    "    ## Indexes\n",
    "    logging.info(f\"ann_index_endpoint_resource_uri: {ann_index_endpoint_resource_uri}\")\n",
    "    logging.info(f\"brute_index_endpoint_resource_uri: {brute_index_endpoint_resource_uri}\")\n",
    "\n",
    "    deployed_ann_index = vertex_ai.MatchingEngineIndexEndpoint(ann_index_endpoint_resource_uri)\n",
    "    deployed_bf_index = vertex_ai.MatchingEngineIndexEndpoint(brute_index_endpoint_resource_uri)\n",
    "\n",
    "    DEPLOYED_ANN_ID = deployed_ann_index.deployed_indexes[0].id\n",
    "    DEPLOYED_BF_ID = deployed_bf_index.deployed_indexes[0].id\n",
    "    logging.info(f\"DEPLOYED_ANN_ID: {DEPLOYED_ANN_ID}\")\n",
    "    logging.info(f\"DEPLOYED_BF_ID: {DEPLOYED_BF_ID}\")\n",
    "    \n",
    "    logging.info('Retreiving neighbors from ANN index...')\n",
    "    \n",
    "    ANN_response = deployed_ann_index.match(\n",
    "        deployed_index_id=DEPLOYED_ANN_ID,\n",
    "        queries=prediction_test.predictions,\n",
    "        num_neighbors=10\n",
    "    )\n",
    "    \n",
    "    logging.info('Retreiving neighbors from BF index...')\n",
    "    BF_response = deployed_bf_index.match(\n",
    "        deployed_index_id=DEPLOYED_BF_ID,\n",
    "        queries=prediction_test.predictions,\n",
    "        num_neighbors=10\n",
    "    )\n",
    "    \n",
    "    # Calculate recall by determining how many neighbors were correctly retrieved as compared to the brute-force option.\n",
    "    recalled_neighbors = 0\n",
    "    for tree_ah_neighbors, brute_force_neighbors in zip(\n",
    "        ANN_response, BF_response\n",
    "    ):\n",
    "        tree_ah_neighbor_ids = [neighbor.id for neighbor in tree_ah_neighbors]\n",
    "        brute_force_neighbor_ids = [neighbor.id for neighbor in brute_force_neighbors]\n",
    "\n",
    "        recalled_neighbors += len(\n",
    "            set(tree_ah_neighbor_ids).intersection(brute_force_neighbor_ids)\n",
    "        )\n",
    "\n",
    "    recall = recalled_neighbors / len(\n",
    "        [neighbor for neighbors in BF_response for neighbor in neighbors]\n",
    "    )\n",
    "\n",
    "    logging.info(\"Recall: {}\".format(recall))\n",
    "    \n",
    "    metrics.log_metric(\"Recall\", (recall * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7930a118-ea1e-46ff-9844-0c1d3641c617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile {REPO_DOCKER_PATH_PREFIX}/{PIPELINES_SUB_DIR}/test_index_recall.py\n",
    "\n",
    "# import kfp\n",
    "# from typing import Any, Callable, Dict, NamedTuple, Optional, List\n",
    "# from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output,\n",
    "#                         OutputPath, component, Metrics)\n",
    "\n",
    "# @kfp.v2.dsl.component(\n",
    "#     base_image=\"python:3.9\",\n",
    "#     packages_to_install=[\n",
    "#         'google-cloud-aiplatform==1.20.0',\n",
    "#         # 'google-cloud-storage',\n",
    "#     ],\n",
    "# )\n",
    "# def test_index_recall(\n",
    "#     project: str,\n",
    "#     location: str,\n",
    "#     version: str,\n",
    "#     ann_index_resource_uri: str,\n",
    "#     brute_force_index_resource_uri: str,\n",
    "#     gcs_train_script_path: str,\n",
    "#     endpoint: Input[Artifact],\n",
    "#     metrics: Output[Metrics],\n",
    "# ):\n",
    "#     # here\n",
    "    \n",
    "#     import base64\n",
    "#     import logging\n",
    "\n",
    "#     from typing import Dict, List, Union\n",
    "\n",
    "#     from google.cloud import aiplatform as vertex_ai\n",
    "#     from google.protobuf import json_format\n",
    "#     from google.protobuf.json_format import Parse\n",
    "#     from google.protobuf.struct_pb2 import Value\n",
    "    \n",
    "#     from google_cloud_pipeline_components.proto.gcp_resources_pb2 import GcpResources\n",
    "\n",
    "#     logging.getLogger().setLevel(logging.INFO)\n",
    "#     vertex_ai.init(\n",
    "#         project=project,\n",
    "#         location=location,\n",
    "#     )\n",
    "    \n",
    "#     endpoint_resource_path = endpoint.metadata[\"resourceName\"]\n",
    "\n",
    "#     # define endpoint resource in component\n",
    "#     logging.info(f\"endpoint_resource_path = {endpoint_resource_path}\")\n",
    "#     _endpoint = vertex_ai.Endpoint(endpoint_resource_path)\n",
    "    \n",
    "#     ################################################################################\n",
    "#     # Helper function for returning endpoint predictions via required json format\n",
    "#     ################################################################################\n",
    "\n",
    "#     def predict_custom_trained_model_sample(\n",
    "#         project: str,\n",
    "#         endpoint_id: str,\n",
    "#         instances: Dict,\n",
    "#         location: str = \"us-central1\",\n",
    "#         api_endpoint: str = \"us-central1-aiplatform.googleapis.com\",\n",
    "# ):\n",
    "#         \"\"\"\n",
    "#         `instances` can be either single instance of type dict or a list\n",
    "#         of instances.\n",
    "#         \"\"\"\n",
    "\n",
    "#         ########################################################################\n",
    "#         # Initialize Vertex Endpoint\n",
    "#         ########################################################################\n",
    "\n",
    "#         # The AI Platform services require regional API endpoints.\n",
    "#         client_options = {\"api_endpoint\": api_endpoint}\n",
    "        \n",
    "#         # Initialize client that will be used to create and send requests.\n",
    "#         # This client only needs to be created once, and can be reused for multiple requests.\n",
    "#         client = vertex_ai.gapic.PredictionServiceClient(client_options=client_options)\n",
    "        \n",
    "#         # The format of each instance should conform to the deployed model's prediction input schema.\n",
    "#         instances = instances if type(instances) == list else [instances]\n",
    "#         instances = [\n",
    "#             json_format.ParseDict(instance_dict, Value()) for instance_dict in instances\n",
    "#         ]\n",
    "        \n",
    "#         parameters_dict = {}\n",
    "#         parameters = json_format.ParseDict(parameters_dict, Value())\n",
    "        \n",
    "#         endpoint = client.endpoint_path(\n",
    "#             project=project, location=location, endpoint=endpoint_id\n",
    "#         )\n",
    "        \n",
    "#         response = client.predict(\n",
    "#             endpoint=endpoint, instances=instances, parameters=parameters\n",
    "#         )\n",
    "#         logging.info(f'Response: {response}')\n",
    "        \n",
    "#         logging.info(f'Deployed Model ID(s): {response.deployed_model_id}')\n",
    "\n",
    "#         # The predictions are a google.protobuf.Value representation of the model's predictions.\n",
    "#         _predictions = response.predictions\n",
    "#         logging.info(f'Response Predictions: {_predictions}')\n",
    "        \n",
    "#         return _predictions\n",
    "    \n",
    "#     ################################################################################\n",
    "#     # Request Prediction\n",
    "#     ################################################################################\n",
    "#     logging.info(f'Response gcs_train_script_path: {gcs_train_script_path}')\n",
    "    \n",
    "#     # jt-tfrs-central-v2/tfrs-e2e-pipe-test-v15-jtv4/run-20230214-021008/pipeline_root/trainer\n",
    "#     # gs://jt-tfrs-central-v2/tfrs-e2e-pipe-test-v15-jtv4/run-20230214-021008/pipeline_root/trainer\n",
    "#     # gcs_train_script_path\n",
    "\n",
    "#     TEST_INSTANCE_15 = {\n",
    "#         'album_name_can': 'Capoeira Electronica',\n",
    "#         'album_name_pl': [\n",
    "#             'Odilara', 'Capoeira Electronica', 'Capoeira Ultimate','Festa Popular', 'Capoeira Electronica',\n",
    "#             'Odilara', 'Capoeira Electronica', 'Capoeira Ultimate','Festa Popular', 'Capoeira Electronica',\n",
    "#             'Odilara', 'Capoeira Electronica', 'Capoeira Ultimate','Festa Popular', 'Capoeira Electronica'\n",
    "#         ],\n",
    "#         'album_uri_can': 'spotify:album:2FsSSHGt8JM0JgRy6ZX3kR',\n",
    "#         'album_uri_pl': [\n",
    "#             'spotify:album:4Y8RfvZzCiApBCIZswj9Ry',\n",
    "#             'spotify:album:2FsSSHGt8JM0JgRy6ZX3kR',\n",
    "#             'spotify:album:55HHBqZ2SefPeaENOgWxYK',\n",
    "#             'spotify:album:150L1V6UUT7fGUI3PbxpkE',\n",
    "#             'spotify:album:2FsSSHGt8JM0JgRy6ZX3kR',\n",
    "#             'spotify:album:4Y8RfvZzCiApBCIZswj9Ry',\n",
    "#             'spotify:album:2FsSSHGt8JM0JgRy6ZX3kR',\n",
    "#             'spotify:album:55HHBqZ2SefPeaENOgWxYK',\n",
    "#             'spotify:album:150L1V6UUT7fGUI3PbxpkE',\n",
    "#             'spotify:album:2FsSSHGt8JM0JgRy6ZX3kR',\n",
    "#             'spotify:album:4Y8RfvZzCiApBCIZswj9Ry',\n",
    "#             'spotify:album:2FsSSHGt8JM0JgRy6ZX3kR',\n",
    "#             'spotify:album:55HHBqZ2SefPeaENOgWxYK',\n",
    "#             'spotify:album:150L1V6UUT7fGUI3PbxpkE',\n",
    "#             'spotify:album:2FsSSHGt8JM0JgRy6ZX3kR'\n",
    "#         ],\n",
    "#         'artist_followers_can': 5170.0,\n",
    "#         'artist_genres_can': 'capoeira',\n",
    "#         'artist_genres_pl': [\n",
    "#             'samba moderno', 'capoeira', 'capoeira', 'NONE','capoeira',\n",
    "#             'samba moderno', 'capoeira', 'capoeira', 'NONE','capoeira',\n",
    "#             'samba moderno', 'capoeira', 'capoeira', 'NONE','capoeira'\n",
    "#         ],\n",
    "#         'artist_name_can': 'Capoeira Experience',\n",
    "#         'artist_name_pl': [\n",
    "#             'Odilara', 'Capoeira Experience', 'Denis Porto', 'Zambe','Capoeira Experience',\n",
    "#             'Odilara', 'Capoeira Experience', 'Denis Porto', 'Zambe','Capoeira Experience',\n",
    "#             'Odilara', 'Capoeira Experience', 'Denis Porto', 'Zambe','Capoeira Experience'\n",
    "#         ],\n",
    "#         'artist_pop_can': 24.0,\n",
    "#         'artist_pop_pl':[\n",
    "#             4., 24.,  2.,  0., 24.,\n",
    "#             4., 24.,  2.,  0., 24.,\n",
    "#             4., 24.,  2.,  0., 24.\n",
    "#         ],\n",
    "#         'artist_uri_can': 'spotify:artist:5SKEXbgzIdRl3gQJ23CnUP',\n",
    "#         'artist_uri_pl': [\n",
    "#             'spotify:artist:72oameojLOPWYB7nB8rl6c',\n",
    "#             'spotify:artist:5SKEXbgzIdRl3gQJ23CnUP',\n",
    "#             'spotify:artist:67p5GMYQZOgaAfx1YyttQk',\n",
    "#             'spotify:artist:4fH3OXCRcPsaHFE5KhgqZS',\n",
    "#             'spotify:artist:5SKEXbgzIdRl3gQJ23CnUP',\n",
    "#             'spotify:artist:72oameojLOPWYB7nB8rl6c',\n",
    "#             'spotify:artist:5SKEXbgzIdRl3gQJ23CnUP',\n",
    "#             'spotify:artist:67p5GMYQZOgaAfx1YyttQk',\n",
    "#             'spotify:artist:4fH3OXCRcPsaHFE5KhgqZS',\n",
    "#             'spotify:artist:5SKEXbgzIdRl3gQJ23CnUP',\n",
    "#             'spotify:artist:72oameojLOPWYB7nB8rl6c',\n",
    "#             'spotify:artist:5SKEXbgzIdRl3gQJ23CnUP',\n",
    "#             'spotify:artist:67p5GMYQZOgaAfx1YyttQk',\n",
    "#             'spotify:artist:4fH3OXCRcPsaHFE5KhgqZS',\n",
    "#             'spotify:artist:5SKEXbgzIdRl3gQJ23CnUP'\n",
    "#         ],\n",
    "#         'artists_followers_pl': [ \n",
    "#             316., 5170.,  448.,   19., 5170.,\n",
    "#             316., 5170.,  448.,   19., 5170.,\n",
    "#             316., 5170.,  448.,   19., 5170.\n",
    "#         ],\n",
    "#         'duration_ms_can': 192640.0,\n",
    "#         'duration_ms_songs_pl': [234612., 226826., 203480., 287946., 271920., 234612., 226826., 203480., 287946., 271920., 234612., 226826., 203480., 287946., 271920.],\n",
    "#         'num_pl_albums_new': 9.0,\n",
    "#         'num_pl_artists_new': 5.0,\n",
    "#         'num_pl_songs_new': 85.0,\n",
    "#         'pl_collaborative_src': 'false',\n",
    "#         'pl_duration_ms_new': 17971314.0,\n",
    "#         'pl_name_src': 'Capoeira',\n",
    "#         'time_signature_can': '4',\n",
    "#         'time_signature_pl': ['4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4'],\n",
    "#         'track_acousticness_can': 0.478,\n",
    "#         'track_acousticness_pl': [0.238 , 0.105 , 0.0242, 0.125 , 0.304, 0.238 , 0.105 , 0.0242, 0.125 , 0.304, 0.238 , 0.105 , 0.0242, 0.125 , 0.304 ],\n",
    "#         'track_danceability_can': 0.709,\n",
    "#         'track_danceability_pl': [0.703, 0.712, 0.806, 0.529, 0.821, 0.238 , 0.105 , 0.0242, 0.125 , 0.304, 0.238 , 0.105 , 0.0242, 0.125 , 0.304],\n",
    "#         'track_energy_can': 0.742,\n",
    "#         'track_energy_pl': [0.743, 0.41 , 0.794, 0.776, 0.947, 0.238 , 0.105 , 0.0242, 0.125 , 0.304, 0.238 , 0.105 , 0.0242, 0.125 , 0.304],\n",
    "#         'track_instrumentalness_can': 0.00297,\n",
    "#         'track_instrumentalness_pl': [4.84e-06, 4.30e-01, 7.42e-04, 4.01e-01, 5.07e-03, 4.84e-06, 4.30e-01, 7.42e-04, 4.01e-01, 5.07e-03, 4.84e-06, 4.30e-01, 7.42e-04, 4.01e-01, 5.07e-03],\n",
    "#         'track_key_can': '0',\n",
    "#         'track_key_pl': ['5', '0', '1', '10', '10', '5', '0', '1', '10', '10', '5', '0', '1', '10', '10'],\n",
    "#         'track_liveness_can': 0.0346,\n",
    "#         'track_liveness_pl': [0.128 , 0.0725, 0.191 , 0.105 , 0.0552,0.128 , 0.0725, 0.191 , 0.105 , 0.0552, 0.128 , 0.0725, 0.191 , 0.105 , 0.0552],\n",
    "#         'track_loudness_can': -7.295,\n",
    "#         'track_loudness_pl': [-8.638, -8.754, -9.084, -7.04 , -6.694, -8.638, -8.754, -9.084, -7.04 , -6.694, -8.638, -8.754, -9.084, -7.04 , -6.694],\n",
    "#         'track_mode_can': '1',\n",
    "#         'track_mode_pl': ['0', '1', '1', '0', '1', '0', '1', '1', '0', '1', '0', '1', '1', '0', '1'],\n",
    "#         'track_name_can': 'Bezouro Preto - Studio',\n",
    "#         'track_name_pl': [\n",
    "#             'O Telefone Tocou Novamente', 'Bem Devagar - Studio','Angola Dream', 'Janaina', 'Louco Berimbau - Studio',\n",
    "#             'O Telefone Tocou Novamente', 'Bem Devagar - Studio','Angola Dream', 'Janaina', 'Louco Berimbau - Studio',\n",
    "#             'O Telefone Tocou Novamente', 'Bem Devagar - Studio','Angola Dream', 'Janaina', 'Louco Berimbau - Studio'\n",
    "#         ],\n",
    "#         'track_pop_can': 3.0,\n",
    "#         'track_pop_pl': [5., 1., 0., 0., 1., 5., 1., 0., 0., 1., 5., 1., 0., 0., 1.],\n",
    "#         'track_speechiness_can': 0.0802,\n",
    "#         'track_speechiness_pl':[0.0367, 0.0272, 0.0407, 0.132 , 0.0734, 0.0367, 0.0272, 0.0407, 0.132 , 0.0734, 0.0367, 0.0272, 0.0407, 0.132 , 0.0734],\n",
    "#         'track_tempo_can': 172.238,\n",
    "#         'track_tempo_pl': [100.039,  89.089, 123.999, 119.963, 119.214, 100.039,  89.089, 123.999, 119.963, 119.214, 100.039,  89.089, 123.999, 119.963, 119.214],\n",
    "#         'track_uri_can': 'spotify:track:0tlhK4OvpHCYpReTABvKFb',\n",
    "#         'track_uri_pl': [\n",
    "#             'spotify:track:1pQkOdcTDfLr84TDCrmGy7',\n",
    "#             'spotify:track:39grEDsAHAjmo2QFo4G8D9',\n",
    "#             'spotify:track:5vxSLdJXqbKYH487YO8LSL',\n",
    "#             'spotify:track:6T9GbmZ6voDM4aTBsG5VDh',\n",
    "#             'spotify:track:7ELt9eslVvWo276pX2garN',\n",
    "#             'spotify:track:1pQkOdcTDfLr84TDCrmGy7',\n",
    "#             'spotify:track:39grEDsAHAjmo2QFo4G8D9',\n",
    "#             'spotify:track:5vxSLdJXqbKYH487YO8LSL',\n",
    "#             'spotify:track:6T9GbmZ6voDM4aTBsG5VDh',\n",
    "#             'spotify:track:7ELt9eslVvWo276pX2garN',\n",
    "#             'spotify:track:1pQkOdcTDfLr84TDCrmGy7',\n",
    "#             'spotify:track:39grEDsAHAjmo2QFo4G8D9',\n",
    "#             'spotify:track:5vxSLdJXqbKYH487YO8LSL',\n",
    "#             'spotify:track:6T9GbmZ6voDM4aTBsG5VDh',\n",
    "#             'spotify:track:7ELt9eslVvWo276pX2garN'\n",
    "#         ],\n",
    "#         'track_valence_can': 0.844,\n",
    "#         'track_valence_pl': [\n",
    "#             0.966, 0.667, 0.696, 0.876, 0.655,\n",
    "#             0.966, 0.667, 0.696, 0.876, 0.655,\n",
    "#             0.966, 0.667, 0.696, 0.876, 0.655\n",
    "#         ],\n",
    "#     }\n",
    "\n",
    "#     prediction_test = predict_custom_trained_model_sample(\n",
    "#         project=project,                     \n",
    "#         endpoint_id=endpoint_resource_path,\n",
    "#         location=\"us-central1\",\n",
    "#         instances=TEST_INSTANCE_15\n",
    "#     )\n",
    "#     logging.info(f\"prediction_test: {prediction_test}\")\n",
    "    \n",
    "#     ################################################################################\n",
    "#     # Init deployed indexes\n",
    "#     ################################################################################\n",
    "    \n",
    "#     logging.info(f\"ann_index_resource_uri: {ann_index_resource_uri}\")\n",
    "#     logging.info(f\"brute_force_index_resource_uri: {brute_force_index_resource_uri}\")\n",
    "\n",
    "#     tree_ah_index = vertex_ai.MatchingEngineIndexEndpoint(index_name=ann_index_resource_uri)\n",
    "#     brute_force_index = vertex_ai.MatchingEngineIndexEndpoint(index_name=brute_force_index_resource_uri)\n",
    "    \n",
    "#     DEPLOYED_ANN_INDEX_ID = tree_ah_index.deployed_indexes[0]\n",
    "#     DEPLOYED_BF_INDEX_ID = brute_force_index.deployed_indexes[0]\n",
    "    \n",
    "#     logging.info(f\"DEPLOYED_ANN_INDEX_ID: {DEPLOYED_ANN_INDEX_ID}\")\n",
    "#     logging.info(f\"DEPLOYED_BF_INDEX_ID: {DEPLOYED_BF_INDEX_ID}\")\n",
    "    \n",
    "#     ANN_response = deployed_ann_index.match(\n",
    "#         deployed_index_id=DEPLOYED_ANN_INDEX_ID,\n",
    "#         queries=prediction_test.predictions,\n",
    "#         num_neighbors=10\n",
    "#     )\n",
    "    \n",
    "#     BF_response = deployed_bf_index.match(\n",
    "#         deployed_index_id=DEPLOYED_BF_INDEX_ID,\n",
    "#         queries=prediction_test.predictions,\n",
    "#         num_neighbors=10\n",
    "#     )\n",
    "    \n",
    "#     # Calculate recall by determining how many neighbors were correctly retrieved as compared to the brute-force option.\n",
    "#     recalled_neighbors = 0\n",
    "#     for tree_ah_neighbors, brute_force_neighbors in zip(\n",
    "#         ANN_response, BF_response\n",
    "#     ):\n",
    "#         tree_ah_neighbor_ids = [neighbor.id for neighbor in tree_ah_neighbors]\n",
    "#         brute_force_neighbor_ids = [neighbor.id for neighbor in brute_force_neighbors]\n",
    "\n",
    "#         recalled_neighbors += len(\n",
    "#             set(tree_ah_neighbor_ids).intersection(brute_force_neighbor_ids)\n",
    "#         )\n",
    "\n",
    "#     recall = recalled_neighbors / len(\n",
    "#         [neighbor for neighbors in BF_response for neighbor in neighbors]\n",
    "#     )\n",
    "\n",
    "#     logging.info(\"Recall: {}\".format(recall))\n",
    "    \n",
    "#     metrics.log_metric(\"Recall\", (recall * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76367d45-43a9-43b1-a475-cc1237696e2e",
   "metadata": {},
   "source": [
    "## compute config for pipeline steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "18a86934-2700-4ce3-b0c9-673fdce5a430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/train_pipes/pipeline_config.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/{PIPELINES_SUB_DIR}/pipeline_config.py\n",
    "\n",
    "CPU_LIMIT='96'\n",
    "MEMORY_LIMIT='624G'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa3eea5-05b5-46a7-ab2e-6ef4b826fcae",
   "metadata": {},
   "source": [
    "# Prepare Job Specs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1cfa03-1db5-44ac-9e19-b855a5c63975",
   "metadata": {},
   "source": [
    "## Vertex Train: workerpool specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2286562e-ff21-46e2-bfa4-14e732ddadef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_worker_pool_specs(\n",
    "    image_uri,\n",
    "    args,\n",
    "    cmd,\n",
    "    replica_count=1,\n",
    "    machine_type=\"n1-standard-16\",\n",
    "    accelerator_count=1,\n",
    "    accelerator_type=\"ACCELERATOR_TYPE_UNSPECIFIED\",\n",
    "    reduction_server_count=0,\n",
    "    reduction_server_machine_type=\"n1-highcpu-16\",\n",
    "    reduction_server_image_uri=\"us-docker.pkg.dev/vertex-ai-restricted/training/reductionserver:latest\",\n",
    "):\n",
    "\n",
    "    if accelerator_count > 0:\n",
    "        machine_spec = {\n",
    "            \"machine_type\": machine_type,\n",
    "            \"accelerator_type\": accelerator_type,\n",
    "            \"accelerator_count\": accelerator_count,\n",
    "        }\n",
    "    else:\n",
    "        machine_spec = {\"machine_type\": machine_type}\n",
    "\n",
    "    container_spec = {\n",
    "        \"image_uri\": image_uri,\n",
    "        \"args\": args,\n",
    "        \"command\": cmd,\n",
    "    }\n",
    "\n",
    "    chief_spec = {\n",
    "        \"replica_count\": 1,\n",
    "        \"machine_spec\": machine_spec,\n",
    "        \"container_spec\": container_spec,\n",
    "    }\n",
    "\n",
    "    worker_pool_specs = [chief_spec]\n",
    "    if replica_count > 1:\n",
    "        workers_spec = {\n",
    "            \"replica_count\": replica_count - 1,\n",
    "            \"machine_spec\": machine_spec,\n",
    "            \"container_spec\": container_spec,\n",
    "        }\n",
    "        worker_pool_specs.append(workers_spec)\n",
    "    if reduction_server_count > 1:\n",
    "        workers_spec = {\n",
    "            \"replica_count\": reduction_server_count,\n",
    "            \"machine_spec\": {\n",
    "                \"machine_type\": reduction_server_machine_type,\n",
    "            },\n",
    "            \"container_spec\": {\"image_uri\": reduction_server_image_uri},\n",
    "        }\n",
    "        worker_pool_specs.append(workers_spec)\n",
    "\n",
    "    return worker_pool_specs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045adff3-74fc-4e86-ac19-8e2feee61766",
   "metadata": {},
   "source": [
    "## Accelerators and Device Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a8a47f70-c364-4122-a9c6-abda876a0068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # # Single machine, single GPU\n",
    "WORKER_MACHINE_TYPE = 'a2-highgpu-1g'\n",
    "REPLICA_COUNT = 1\n",
    "ACCELERATOR_TYPE = 'NVIDIA_TESLA_A100'\n",
    "PER_MACHINE_ACCELERATOR_COUNT = 1\n",
    "REDUCTION_SERVER_COUNT = 0                                                      \n",
    "REDUCTION_SERVER_MACHINE_TYPE = \"n1-highcpu-16\"\n",
    "DISTRIBUTE_STRATEGY = 'single'\n",
    "\n",
    "# Single machine, single GPU, 80 GB 'NVIDIA_A100_80GB'\n",
    "# WORKER_MACHINE_TYPE = 'a2-ultragpu-1g' # 80 GB\n",
    "# REPLICA_COUNT = 1\n",
    "# ACCELERATOR_TYPE = 'NVIDIA_A100_80GB'\n",
    "# PER_MACHINE_ACCELERATOR_COUNT = 1\n",
    "# REDUCTION_SERVER_COUNT = 0                                                      \n",
    "# REDUCTION_SERVER_MACHINE_TYPE = \"n1-highcpu-16\"\n",
    "# DISTRIBUTE_STRATEGY = 'single'\n",
    "\n",
    "# # # Single Machine; multiple GPU\n",
    "# WORKER_MACHINE_TYPE = 'a2-highgpu-4g' # a2-ultragpu-4g\n",
    "# REPLICA_COUNT = 1\n",
    "# ACCELERATOR_TYPE = 'NVIDIA_TESLA_A100'\n",
    "# PER_MACHINE_ACCELERATOR_COUNT = 4\n",
    "# REDUCTION_SERVER_COUNT = 0                                                      \n",
    "# REDUCTION_SERVER_MACHINE_TYPE = \"n1-highcpu-16\"\n",
    "# DISTRIBUTE_STRATEGY = 'mirrored'\n",
    "\n",
    "# # # # Multiple Machine; 1 GPU per machine\n",
    "# WORKER_MACHINE_TYPE = 'a2-highgpu-2g' # a2-ultragpu-4g\n",
    "# REPLICA_COUNT = 2\n",
    "# ACCELERATOR_TYPE = 'NVIDIA_TESLA_A100'\n",
    "# PER_MACHINE_ACCELERATOR_COUNT = 2\n",
    "# REDUCTION_SERVER_COUNT = 4                                                      \n",
    "# REDUCTION_SERVER_MACHINE_TYPE = \"n1-highcpu-16\"\n",
    "# DISTRIBUTE_STRATEGY = 'multiworker'\n",
    "\n",
    "# # # Multiple Machines, 1 GPU per Machine\n",
    "# WORKER_MACHINE_TYPE = 'n1-standard-16'\n",
    "# REPLICA_COUNT = 9\n",
    "# ACCELERATOR_TYPE = 'NVIDIA_TESLA_T4'\n",
    "# PER_MACHINE_ACCELERATOR_COUNT = 1\n",
    "# REDUCTION_SERVER_COUNT = 10                                                      \n",
    "# REDUCTION_SERVER_MACHINE_TYPE = \"n1-highcpu-16\"\n",
    "# DISTRIBUTE_STRATEGY = 'multiworker'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83f96f0-3b01-4622-88e9-cb646b3f873a",
   "metadata": {},
   "source": [
    "## Vertex AI Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "28d8b91d-9772-49da-b7a4-4ab33a12c0c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPERIMENT_NAME: tfrs-e2e-pipe-test-v18-jtv5\n",
      "RUN_NAME: run-20230214-144020\n"
     ]
    }
   ],
   "source": [
    "EXPERIMENT_PREFIX = 'tfrs-e2e-pipe-test-v18'                     # custom identifier for organizing experiments\n",
    "EXPERIMENT_NAME=f'{EXPERIMENT_PREFIX}-{VERSION}'\n",
    "RUN_NAME = f'run-{time.strftime(\"%Y%m%d-%H%M%S\")}'\n",
    "\n",
    "print(f\"EXPERIMENT_NAME: {EXPERIMENT_NAME}\")\n",
    "print(f\"RUN_NAME: {RUN_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f2fed1-9d86-41cf-add4-4e25f6d93030",
   "metadata": {},
   "source": [
    "## Managed Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e7a56de9-f390-4c3f-9aad-24c09145afff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use existing TB instance\n",
    "# TB_RESOURCE_NAME = 'projects/934903580331/locations/us-central1/tensorboards/6924469145035603968'\n",
    "\n",
    "# # # create new TB instance\n",
    "# TENSORBOARD_DISPLAY_NAME=f\"{EXPERIMENT_PREFIX}-v1\"\n",
    "# tensorboard = vertex_ai.Tensorboard.create(display_name=TENSORBOARD_DISPLAY_NAME, project=PROJECT_ID, location=REGION)\n",
    "# TB_RESOURCE_NAME = tensorboard.resource_name\n",
    "\n",
    "\n",
    "# print(f\"TB_RESOURCE_NAME: {TB_RESOURCE_NAME}\")\n",
    "# print(f\"TB display name: {tensorboard.display_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66142882-0f35-4e7d-ab5a-c0d6b49d3f44",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Training Config\n",
    "\n",
    "* see [src code](https://github.com/googleapis/python-aiplatform/blob/e7bf0d83d8bb0849a9bce886c958d13f5cbe5fab/google/cloud/aiplatform/utils/worker_spec_utils.py#L153) for worker_pool_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cc68e120-39c8-4974-92a2-228cd0173853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PIPELINE_ROOT_PATH: gs://jt-tfrs-central-v2/tfrs-e2e-pipe-test-v18-jtv5/run-20230214-144020/pipeline_root\n"
     ]
    }
   ],
   "source": [
    "# =================================================\n",
    "# trainconfig: gcs locations\n",
    "# =================================================\n",
    "OUTPUT_BUCKET = 'jt-tfrs-central-v2'\n",
    "OUTPUT_GCS_URI =f'gs://{OUTPUT_BUCKET}'\n",
    "\n",
    "# Stores pipeline executions for each run\n",
    "PIPELINE_ROOT_PATH = f'gs://{OUTPUT_BUCKET}/{EXPERIMENT_NAME}/{RUN_NAME}/pipeline_root'\n",
    "print('PIPELINE_ROOT_PATH: {}'.format(PIPELINE_ROOT_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7104ca-6877-4d25-881f-6a40a4a6be08",
   "metadata": {},
   "source": [
    "### feature lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b3beb06e-05a8-470a-9d54-ff02a03758a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FEATURES_PREFIX: tfrs-e2e-pipe-test-v18-jtv5/run-20230214-144020/features\n"
     ]
    }
   ],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "    \n",
    "import tensorflow as tf\n",
    "\n",
    "FEATURES_PREFIX = f'{EXPERIMENT_NAME}/{RUN_NAME}/features'\n",
    "print(f\"FEATURES_PREFIX: {FEATURES_PREFIX}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb31939-5672-4800-a98a-82484ad2c197",
   "metadata": {},
   "source": [
    "##### candidate features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ab61ac70-9da5-4ed6-b1f8-aecc167e8192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# candidate features\n",
    "CANDIDATE_FILENAME = 'candidate_feats_dict.pkl'\n",
    "CANDIDATE_FEATURES_GCS_OBJ = f'{FEATURES_PREFIX}/{CANDIDATE_FILENAME}'\n",
    "\n",
    "CANDIDATE_FEATURES_DICT = {\n",
    "    \"track_uri_can\":tf.io.FixedLenFeature(dtype=tf.string, shape=()),            \n",
    "    \"track_name_can\":tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    \"artist_uri_can\":tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    \"artist_name_can\":tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    \"album_uri_can\":tf.io.FixedLenFeature(dtype=tf.string, shape=()),           \n",
    "    \"album_name_can\":tf.io.FixedLenFeature(dtype=tf.string, shape=()), \n",
    "    \"duration_ms_can\":tf.io.FixedLenFeature(dtype=tf.float32, shape=()),      \n",
    "    \"track_pop_can\":tf.io.FixedLenFeature(dtype=tf.float32, shape=()),      \n",
    "    \"artist_pop_can\":tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "    \"artist_genres_can\":tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    \"artist_followers_can\":tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "    # new\n",
    "    # \"track_pl_titles_can\":tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    \"track_danceability_can\":tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "    \"track_energy_can\":tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "    \"track_key_can\":tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    \"track_loudness_can\":tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "    \"track_mode_can\":tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    \"track_speechiness_can\":tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "    \"track_acousticness_can\":tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "    \"track_instrumentalness_can\":tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "    \"track_liveness_can\":tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "    \"track_valence_can\":tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "    \"track_tempo_can\":tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "    \"time_signature_can\":tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "}\n",
    "\n",
    "# pickle\n",
    "filehandler = open(f'{CANDIDATE_FILENAME}', 'wb')\n",
    "pkl.dump(CANDIDATE_FEATURES_DICT, filehandler)\n",
    "filehandler.close()\n",
    "\n",
    "# upload to GCS\n",
    "bucket_client = storage_client.bucket(OUTPUT_BUCKET)\n",
    "blob = bucket_client.blob(CANDIDATE_FEATURES_GCS_OBJ)\n",
    "blob.upload_from_filename(CANDIDATE_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674a0108-00f1-4656-a3c6-9d93b8c6892b",
   "metadata": {},
   "source": [
    "##### query features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "75b11787-0c10-4f3b-b76f-0b10dbfd5868",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_PLAYLIST_LENGTH=15\n",
    "\n",
    "# query features\n",
    "QUERY_FILENAME = 'query_feats_dict.pkl'\n",
    "QUERY_FEATURES_GCS_OBJ = f'{FEATURES_PREFIX}/{QUERY_FILENAME}'\n",
    "\n",
    "QUERY_FEATURES_DICT = {\n",
    "    # ===================================================\n",
    "    # summary playlist features\n",
    "    # ===================================================\n",
    "    \"pl_name_src\" : tf.io.FixedLenFeature(dtype=tf.string, shape=()), \n",
    "    'pl_collaborative_src' : tf.io.FixedLenFeature(dtype=tf.string, shape=()), \n",
    "    # 'num_pl_followers_src' : tf.io.FixedLenFeature(dtype=tf.float32, shape=()), \n",
    "    'pl_duration_ms_new' : tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "    'num_pl_songs_new' : tf.io.FixedLenFeature(dtype=tf.float32, shape=()), # n_songs_pl_new | num_pl_songs_new\n",
    "    'num_pl_artists_new' : tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "    'num_pl_albums_new' : tf.io.FixedLenFeature(dtype=tf.float32, shape=()), \n",
    "    # 'avg_track_pop_pl_new' : tf.io.FixedLenFeature(dtype=tf.float32, shape=()), \n",
    "    # 'avg_artist_pop_pl_new' : tf.io.FixedLenFeature(dtype=tf.float32, shape=()), \n",
    "    # 'avg_art_followers_pl_new' : tf.io.FixedLenFeature(dtype=tf.float32, shape=()), \n",
    "\n",
    "    # ===================================================\n",
    "    # ragged playlist features\n",
    "    # ===================================================\n",
    "    # bytes / string\n",
    "    \"track_uri_pl\": tf.io.FixedLenFeature(dtype=tf.string, shape=(MAX_PLAYLIST_LENGTH,)),\n",
    "    \"track_name_pl\": tf.io.FixedLenFeature(dtype=tf.string, shape=(MAX_PLAYLIST_LENGTH,)),\n",
    "    \"artist_uri_pl\": tf.io.FixedLenFeature(dtype=tf.string, shape=(MAX_PLAYLIST_LENGTH,)),\n",
    "    \"artist_name_pl\": tf.io.FixedLenFeature(dtype=tf.string, shape=(MAX_PLAYLIST_LENGTH,)),\n",
    "    \"album_uri_pl\": tf.io.FixedLenFeature(dtype=tf.string, shape=(MAX_PLAYLIST_LENGTH,)),\n",
    "    \"album_name_pl\": tf.io.FixedLenFeature(dtype=tf.string, shape=(MAX_PLAYLIST_LENGTH,)),\n",
    "    \"artist_genres_pl\": tf.io.FixedLenFeature(dtype=tf.string, shape=(MAX_PLAYLIST_LENGTH,)),\n",
    "    # \"tracks_playlist_titles_pl\": tf.io.FixedLenFeature(dtype=tf.string, shape=(MAX_PLAYLIST_LENGTH,)),\n",
    "    \"track_key_pl\": tf.io.FixedLenFeature(dtype=tf.string, shape=(MAX_PLAYLIST_LENGTH,)),\n",
    "    \"track_mode_pl\": tf.io.FixedLenFeature(dtype=tf.string, shape=(MAX_PLAYLIST_LENGTH,)),\n",
    "    \"time_signature_pl\": tf.io.FixedLenFeature(dtype=tf.string, shape=(MAX_PLAYLIST_LENGTH,)), \n",
    "\n",
    "    # Float List\n",
    "    \"duration_ms_songs_pl\": tf.io.FixedLenFeature(dtype=tf.float32, shape=(MAX_PLAYLIST_LENGTH,)),\n",
    "    \"track_pop_pl\": tf.io.FixedLenFeature(dtype=tf.float32, shape=(MAX_PLAYLIST_LENGTH,)),\n",
    "    \"artist_pop_pl\": tf.io.FixedLenFeature(dtype=tf.float32, shape=(MAX_PLAYLIST_LENGTH,)),\n",
    "    \"artists_followers_pl\": tf.io.FixedLenFeature(dtype=tf.float32, shape=(MAX_PLAYLIST_LENGTH,)),\n",
    "    \"track_danceability_pl\": tf.io.FixedLenFeature(dtype=tf.float32, shape=(MAX_PLAYLIST_LENGTH,)),\n",
    "    \"track_energy_pl\": tf.io.FixedLenFeature(dtype=tf.float32, shape=(MAX_PLAYLIST_LENGTH,)),\n",
    "    \"track_loudness_pl\": tf.io.FixedLenFeature(dtype=tf.float32, shape=(MAX_PLAYLIST_LENGTH,)),\n",
    "    \"track_speechiness_pl\": tf.io.FixedLenFeature(dtype=tf.float32, shape=(MAX_PLAYLIST_LENGTH,)),\n",
    "    \"track_acousticness_pl\": tf.io.FixedLenFeature(dtype=tf.float32, shape=(MAX_PLAYLIST_LENGTH,)),\n",
    "    \"track_instrumentalness_pl\": tf.io.FixedLenFeature(dtype=tf.float32, shape=(MAX_PLAYLIST_LENGTH,)),\n",
    "    \"track_liveness_pl\": tf.io.FixedLenFeature(dtype=tf.float32, shape=(MAX_PLAYLIST_LENGTH,)),\n",
    "    \"track_valence_pl\": tf.io.FixedLenFeature(dtype=tf.float32, shape=(MAX_PLAYLIST_LENGTH,)),\n",
    "    \"track_tempo_pl\": tf.io.FixedLenFeature(dtype=tf.float32, shape=(MAX_PLAYLIST_LENGTH,)),\n",
    "}\n",
    "\n",
    "# pickle\n",
    "filehandler = open(f'{QUERY_FILENAME}', 'wb')\n",
    "pkl.dump(QUERY_FEATURES_DICT, filehandler)\n",
    "filehandler.close()\n",
    "\n",
    "# upload to GCS\n",
    "bucket_client = storage_client.bucket(OUTPUT_BUCKET)\n",
    "blob = bucket_client.blob(QUERY_FEATURES_GCS_OBJ)\n",
    "blob.upload_from_filename(QUERY_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95b92ba-10f2-4f8c-80fe-9446da2dbd6b",
   "metadata": {},
   "source": [
    "### train image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8bc940d1-aecb-480b-9b00-cb5c3afcda81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMAGE_URI: gcr.io/hybrid-vertex/sp-2tower-tfrs-jtv5-pipev3-training\n"
     ]
    }
   ],
   "source": [
    "# =================================================\n",
    "# train image\n",
    "# =================================================\n",
    "# Existing image URI or name for image to create\n",
    "IMAGE_URI = f\"{IMAGE_URI}\"\n",
    "print(f\"IMAGE_URI: {IMAGE_URI}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1c38a4-7207-43a2-92c4-33d821abc448",
   "metadata": {},
   "source": [
    "### train params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "aef27ee8-e094-4293-a14d-23e87f687a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "\n",
    "# =================================================\n",
    "# trainconfig: GPU related\n",
    "# =================================================\n",
    "TF_GPU_THREAD_COUNT='8'      # '1' | '4' | '8'\n",
    "\n",
    "# =================================================\n",
    "# trainconfig: data input pipeline\n",
    "# =================================================\n",
    "BLOCK_LENGTH = 64            # 1, 8, 16, 32, 64\n",
    "NUM_DATA_SHARDS = 4          # 2, 4, 8, 16, 32, 64\n",
    "# TRAIN_PREFETCH=3\n",
    "\n",
    "# =================================================\n",
    "# trainconfig: training hparams\n",
    "# =================================================\n",
    "NUM_EPOCHS = 3\n",
    "LEARNING_RATE = 0.01\n",
    "BATCH_SIZE = 8192           # 4096, 2048, 1024, 512 \n",
    "\n",
    "# dropout\n",
    "DROPOUT_RATE = 0.33\n",
    "\n",
    "# model size\n",
    "EMBEDDING_DIM = 128\n",
    "PROJECTION_DIM = 50\n",
    "LAYER_SIZES = '[512,256]'\n",
    "MAX_TOKENS = 20000     # vocab\n",
    "\n",
    "# =================================================\n",
    "# trainconfig: tensorboard\n",
    "# =================================================\n",
    "EMBED_FREQUENCY=0\n",
    "HISTOGRAM_FREQUENCY=0\n",
    "CHECKPOINT_FREQ='epoch'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016185c2-7285-414f-8a22-84bfa02fd91c",
   "metadata": {},
   "source": [
    "### train & valid epoch steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1d2ee483-5376-49d5-a743-303e7f90d587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALID_STEPS: 10\n",
      "EPOCH_STEPS: 10\n"
     ]
    }
   ],
   "source": [
    "# =================================================\n",
    "# trainconfig: train & valid steps\n",
    "# =================================================\n",
    "train_sample_cnt = 82_959 # 8_205_265\n",
    "valid_samples_cnt = 82_959\n",
    "\n",
    "# validation & evaluation\n",
    "VALID_FREQUENCY = 50\n",
    "VALID_STEPS = valid_samples_cnt // BATCH_SIZE # 100\n",
    "EPOCH_STEPS = train_sample_cnt // BATCH_SIZE\n",
    "\n",
    "print(f\"VALID_STEPS: {VALID_STEPS}\")\n",
    "print(f\"EPOCH_STEPS: {EPOCH_STEPS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85669e4e-da9c-438b-accd-6e8a64973efc",
   "metadata": {},
   "source": [
    "### data source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c29169fd-6a15-48a4-99f1-c4bdfb754f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================\n",
    "# trainconfig: Data sources\n",
    "# =================================================\n",
    "BUCKET_DATA_DIR = 'spotify-data-regimes' \n",
    "\n",
    "# data strategy: 65m\n",
    "# CANDIDATE_PREFIX = 'jtv10/candidates'\n",
    "# TRAIN_DIR_PREFIX = 'jtv10/train_v9'\n",
    "# VALID_DIR_PREFIX = 'jtv10/valid_v9'\n",
    "\n",
    "# data strategy: 08m\n",
    "CANDIDATE_PREFIX = 'jtv15-8m/candidates'\n",
    "TRAIN_DIR_PREFIX = 'jtv15-8m/valid'     # train | train_v14\n",
    "VALID_DIR_PREFIX = 'jtv15-8m/valid'     # valid_v14"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c653b1a-b103-4e65-92e2-05e40148b867",
   "metadata": {},
   "source": [
    "### train args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d8fa7a02-6d4d-43ba-bce4-5289f86db2ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'container_spec': {'args': ['--project=hybrid-vertex',\n",
      "                              '--train_output_gcs_bucket=jt-tfrs-central-v2',\n",
      "                              '--train_dir=spotify-data-regimes',\n",
      "                              '--train_dir_prefix=jtv15-8m/valid',\n",
      "                              '--valid_dir=spotify-data-regimes',\n",
      "                              '--valid_dir_prefix=jtv15-8m/valid',\n",
      "                              '--candidate_file_dir=spotify-data-regimes',\n",
      "                              '--candidate_files_prefix=jtv15-8m/candidates',\n",
      "                              '--experiment_name=tfrs-e2e-pipe-test-v18-jtv5',\n",
      "                              '--experiment_run=run-20230214-144020',\n",
      "                              '--num_epochs=3',\n",
      "                              '--batch_size=8192',\n",
      "                              '--embedding_dim=128',\n",
      "                              '--projection_dim=50',\n",
      "                              '--layer_sizes=[512,256]',\n",
      "                              '--learning_rate=0.01',\n",
      "                              '--valid_frequency=50',\n",
      "                              '--valid_steps=10',\n",
      "                              '--epoch_steps=10',\n",
      "                              '--distribute=single',\n",
      "                              '--model_version=jtv5',\n",
      "                              '--pipeline_version=pipev3',\n",
      "                              '--seed=1234',\n",
      "                              '--max_tokens=20000',\n",
      "                              '--embed_frequency=0',\n",
      "                              '--hist_frequency=0',\n",
      "                              '--tf_gpu_thread_count=8',\n",
      "                              '--block_length=64',\n",
      "                              '--num_data_shards=4',\n",
      "                              '--chkpt_freq=epoch',\n",
      "                              '--dropout_rate=0.33',\n",
      "                              '--cache_train',\n",
      "                              '--profiler',\n",
      "                              '--compute_batch_metrics',\n",
      "                              '--use_cross_layer',\n",
      "                              '--use_dropout'],\n",
      "                     'command': ['python', 'two_tower_jt/task.py'],\n",
      "                     'image_uri': 'gcr.io/hybrid-vertex/sp-2tower-tfrs-jtv5-pipev3-training'},\n",
      "  'machine_spec': {'accelerator_count': 1,\n",
      "                   'accelerator_type': 'NVIDIA_TESLA_A100',\n",
      "                   'machine_type': 'a2-highgpu-1g'},\n",
      "  'replica_count': 1}]\n"
     ]
    }
   ],
   "source": [
    "WORKER_CMD = [\"python\", \"two_tower_jt/task.py\"]\n",
    "# WORKER_CMD [\"python\", \"-m\", \"trainer.task\"]\n",
    "\n",
    "WORKER_ARGS = [\n",
    "    f'--project={PROJECT_ID}',\n",
    "    f'--train_output_gcs_bucket={OUTPUT_BUCKET}',\n",
    "    f'--train_dir={BUCKET_DATA_DIR}',\n",
    "    f'--train_dir_prefix={TRAIN_DIR_PREFIX}',\n",
    "    f'--valid_dir={BUCKET_DATA_DIR}',\n",
    "    f'--valid_dir_prefix={VALID_DIR_PREFIX}',\n",
    "    f'--candidate_file_dir={BUCKET_DATA_DIR}',\n",
    "    f'--candidate_files_prefix={CANDIDATE_PREFIX}',\n",
    "    f'--experiment_name={EXPERIMENT_NAME}',\n",
    "    f'--experiment_run={RUN_NAME}',\n",
    "    f'--num_epochs={NUM_EPOCHS}',\n",
    "    f'--batch_size={BATCH_SIZE}',\n",
    "    f'--embedding_dim={EMBEDDING_DIM}',\n",
    "    f'--projection_dim={PROJECTION_DIM}',\n",
    "    f'--layer_sizes={LAYER_SIZES}',\n",
    "    f'--learning_rate={LEARNING_RATE}',\n",
    "    f'--valid_frequency={VALID_FREQUENCY}',\n",
    "    f'--valid_steps={VALID_STEPS}',\n",
    "    f'--epoch_steps={EPOCH_STEPS}',\n",
    "    f'--distribute={DISTRIBUTE_STRATEGY}',\n",
    "    f'--model_version={VERSION}',\n",
    "    f'--pipeline_version={PIPELINE_VERSION}',\n",
    "    f'--seed={SEED}',\n",
    "    f'--max_tokens={MAX_TOKENS}',\n",
    "    # f'--tb_resource_name={TB_RESOURCE_NAME}',\n",
    "    f'--embed_frequency={EMBED_FREQUENCY}',\n",
    "    f'--hist_frequency={HISTOGRAM_FREQUENCY}',\n",
    "    f'--tf_gpu_thread_count={TF_GPU_THREAD_COUNT}',\n",
    "    f'--block_length={BLOCK_LENGTH}',\n",
    "    f'--num_data_shards={NUM_DATA_SHARDS}',\n",
    "    f'--chkpt_freq={CHECKPOINT_FREQ}',\n",
    "    f'--dropout_rate={DROPOUT_RATE}',\n",
    "    # uncomment these to pass value of True (bool)\n",
    "    f'--cache_train',                                # caches train_dataset\n",
    "    # f'--evaluate_model',                           # runs model.eval()\n",
    "    # f'--write_embeddings',                         # writes embeddings index in train job\n",
    "    f'--profiler',                                   # runs TB profiler\n",
    "    # f'--set_jit',                                  # enables XLA\n",
    "    f'--compute_batch_metrics',\n",
    "    f'--use_cross_layer',\n",
    "    f'--use_dropout',\n",
    "]\n",
    "\n",
    "WORKER_POOL_SPECS = prepare_worker_pool_specs(\n",
    "    image_uri=IMAGE_URI,\n",
    "    args=WORKER_ARGS,\n",
    "    cmd=WORKER_CMD,\n",
    "    replica_count=REPLICA_COUNT,\n",
    "    machine_type=WORKER_MACHINE_TYPE,\n",
    "    accelerator_count=PER_MACHINE_ACCELERATOR_COUNT,\n",
    "    accelerator_type=ACCELERATOR_TYPE,\n",
    "    reduction_server_count=REDUCTION_SERVER_COUNT,\n",
    "    reduction_server_machine_type=REDUCTION_SERVER_MACHINE_TYPE,\n",
    ")\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(WORKER_POOL_SPECS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1c2a4775-821e-4a16-b5fe-b5aa54d95b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WORKER_POOL_SPECS_v2=WORKER_POOL_SPECS\n",
    "# TEST_TB_NAME = 'this-is-a-test'\n",
    "# WORKER_POOL_SPECS_v2[0]['container_spec']['args'].append(f'--tb_resource_name={TEST_TB_NAME}')\n",
    "# WORKER_POOL_SPECS_v2[0]['container_spec']['args']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "e4d826dc-0dfc-44de-99f1-79d68ecd559d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/jw-repo/spotify_mpd_two_tower\n",
      "gs://jt-tfrs-central-v2/tfrs-e2e-pipe-test-v18-jtv5/run-20230214-144020/pipeline_root\n",
      "src\n"
     ]
    }
   ],
   "source": [
    "!export PWD=pwd\n",
    "!export PIPELINE_ROOT_PATH=PIPELINE_ROOT_PATH\n",
    "!export REPO_DOCKER_PATH_PREFIX=REPO_DOCKER_PATH_PREFIX\n",
    "\n",
    "! echo $PWD\n",
    "! echo $PIPELINE_ROOT_PATH\n",
    "! echo $REPO_DOCKER_PATH_PREFIX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caff7b07-e1ad-4dcf-845a-7d2661cc63cd",
   "metadata": {},
   "source": [
    "### copy train package to GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "913e0f5d-abc0-4d78-915e-7a28489e3d23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://src/cloudbuild.yaml [Content-Type=application/octet-stream]...\n",
      "/ [1 files][  178.0 B/  178.0 B]                                                \n",
      "Operation completed over 1 objects/178.0 B.                                      \n",
      "Copying file://src/Dockerfile.tfrs [Content-Type=application/octet-stream]...\n",
      "/ [1 files][  387.0 B/  387.0 B]                                                \n",
      "Operation completed over 1 objects/387.0 B.                                      \n",
      "Copying file://src/two_tower_jt/__init__.py [Content-Type=text/x-python]...\n",
      "Copying file://src/two_tower_jt/__pycache__/__init__.cpython-37.pyc [Content-Type=application/x-python-code]...\n",
      "Copying file://src/two_tower_jt/__pycache__/two_tower.cpython-37.pyc [Content-Type=application/x-python-code]...\n",
      "Copying file://src/two_tower_jt/__pycache__/test_instances.cpython-37.pyc [Content-Type=application/x-python-code]...\n",
      "Copying file://src/two_tower_jt/__pycache__/train_config.cpython-37.pyc [Content-Type=application/x-python-code]...\n",
      "Copying file://src/two_tower_jt/data-pipeline.py [Content-Type=text/x-python]...\n",
      "Copying file://src/two_tower_jt/interactive_train.py [Content-Type=text/x-python]...\n",
      "Copying file://src/two_tower_jt/requirements.txt [Content-Type=text/plain]...   \n",
      "Copying file://src/two_tower_jt/task.py [Content-Type=text/x-python]...         \n",
      "Copying file://src/two_tower_jt/test_instances.py [Content-Type=text/x-python]...\n",
      "Copying file://src/two_tower_jt/train_config.py [Content-Type=text/x-python]... \n",
      "Copying file://src/two_tower_jt/two_tower.py [Content-Type=text/x-python]...    \n",
      "Copying file://src/two_tower_jt/two_tower_lite.py [Content-Type=text/x-python]...\n",
      "/ [13/13 files][162.4 KiB/162.4 KiB] 100% Done                                  \n",
      "Operation completed over 13 objects/162.4 KiB.                                   \n",
      "\n",
      " Copied training package and Dockerfile to gs://jt-tfrs-central-v2/tfrs-e2e-pipe-test-v18-jtv5/run-20230214-144020/pipeline_root\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# copy training Dockerfile\n",
    "!gsutil cp $REPO_DOCKER_PATH_PREFIX/cloudbuild.yaml $PIPELINE_ROOT_PATH/cloudbuild.yaml\n",
    "!gsutil cp $REPO_DOCKER_PATH_PREFIX/Dockerfile.tfrs $PIPELINE_ROOT_PATH/Dockerfile.tfrs\n",
    "\n",
    "# # # copy training application code\n",
    "! gsutil -m cp -r $REPO_DOCKER_PATH_PREFIX/two_tower_jt/* $PIPELINE_ROOT_PATH/trainer\n",
    "\n",
    "print(f\"\\n Copied training package and Dockerfile to {PIPELINE_ROOT_PATH}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "f94fccbf-ca7f-4326-b22d-e119fcee168c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://jt-tfrs-central-v2/tfrs-e2e-pipe-test-v18-jtv5/run-20230214-144020/pipeline_root/trainer/:\n",
      "         0  2023-02-14T18:21:12Z  gs://jt-tfrs-central-v2/tfrs-e2e-pipe-test-v18-jtv5/run-20230214-144020/pipeline_root/trainer/__init__.py\n",
      "       835  2023-02-14T18:21:12Z  gs://jt-tfrs-central-v2/tfrs-e2e-pipe-test-v18-jtv5/run-20230214-144020/pipeline_root/trainer/data-pipeline.py\n",
      "        44  2023-02-14T18:21:12Z  gs://jt-tfrs-central-v2/tfrs-e2e-pipe-test-v18-jtv5/run-20230214-144020/pipeline_root/trainer/interactive_train.py\n",
      "       219  2023-02-14T18:21:12Z  gs://jt-tfrs-central-v2/tfrs-e2e-pipe-test-v18-jtv5/run-20230214-144020/pipeline_root/trainer/requirements.txt\n",
      "     27330  2023-02-14T18:21:12Z  gs://jt-tfrs-central-v2/tfrs-e2e-pipe-test-v18-jtv5/run-20230214-144020/pipeline_root/trainer/task.py\n",
      "     10224  2023-02-14T18:21:12Z  gs://jt-tfrs-central-v2/tfrs-e2e-pipe-test-v18-jtv5/run-20230214-144020/pipeline_root/trainer/test_instances.py\n",
      "       247  2023-02-14T18:21:12Z  gs://jt-tfrs-central-v2/tfrs-e2e-pipe-test-v18-jtv5/run-20230214-144020/pipeline_root/trainer/train_config.py\n",
      "     58003  2023-02-14T18:21:12Z  gs://jt-tfrs-central-v2/tfrs-e2e-pipe-test-v18-jtv5/run-20230214-144020/pipeline_root/trainer/two_tower.py\n",
      "     44861  2023-02-14T18:21:12Z  gs://jt-tfrs-central-v2/tfrs-e2e-pipe-test-v18-jtv5/run-20230214-144020/pipeline_root/trainer/two_tower_lite.py\n",
      "\n",
      "gs://jt-tfrs-central-v2/tfrs-e2e-pipe-test-v18-jtv5/run-20230214-144020/pipeline_root/trainer/__pycache__/:\n",
      "       159  2023-02-14T18:21:12Z  gs://jt-tfrs-central-v2/tfrs-e2e-pipe-test-v18-jtv5/run-20230214-144020/pipeline_root/trainer/__pycache__/__init__.cpython-37.pyc\n",
      "      4118  2023-02-14T18:21:12Z  gs://jt-tfrs-central-v2/tfrs-e2e-pipe-test-v18-jtv5/run-20230214-144020/pipeline_root/trainer/__pycache__/test_instances.cpython-37.pyc\n",
      "       385  2023-02-14T18:21:12Z  gs://jt-tfrs-central-v2/tfrs-e2e-pipe-test-v18-jtv5/run-20230214-144020/pipeline_root/trainer/__pycache__/train_config.cpython-37.pyc\n",
      "     19876  2023-02-14T18:21:12Z  gs://jt-tfrs-central-v2/tfrs-e2e-pipe-test-v18-jtv5/run-20230214-144020/pipeline_root/trainer/__pycache__/two_tower.cpython-37.pyc\n",
      "TOTAL: 13 objects, 166301 bytes (162.4 KiB)\n"
     ]
    }
   ],
   "source": [
    "! gsutil ls -Rl $PIPELINE_ROOT_PATH/trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d64bb5-3128-44f6-b652-468a7a325c15",
   "metadata": {},
   "source": [
    "# Build & Submit Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "f1eb197b-ea56-4b83-aa92-8198a75711ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PIPELINE_TAG: 2tower-pipev3\n",
      "PIPELINE_NAME: tfrs-jtv5-2tower-pipev3\n"
     ]
    }
   ],
   "source": [
    "PIPELINE_TAG = f'2tower-{PIPELINE_VERSION}'\n",
    "print(\"PIPELINE_TAG:\", PIPELINE_TAG)\n",
    "\n",
    "PIPELINE_NAME = f'tfrs-{VERSION}-{PIPELINE_TAG}'.replace('_', '-')\n",
    "print(\"PIPELINE_NAME:\", PIPELINE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89d7d94-7e15-42d6-b38c-f3eab2affb1a",
   "metadata": {},
   "source": [
    "## Create pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "5e6f5b61-ab53-4619-b854-8c5a4da210e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.train_pipes import build_custom_image, train_custom_model, create_tensorboard, generate_candidates, \\\n",
    "                            create_ann_index, create_brute_force_index, create_ann_index_endpoint_vpc, \\\n",
    "                            create_brute_index_endpoint_vpc, deploy_ann_index, deploy_brute_index, \\\n",
    "                            adapt_ragged_text_layer_vocab, adapt_fixed_text_layer_vocab, create_master_vocab, \\\n",
    "                            test_index_recall\n",
    "\n",
    "from src.train_pipes import pipeline_config as cfg\n",
    "\n",
    "@kfp.v2.dsl.pipeline(\n",
    "    name=f'{PIPELINE_NAME}'.replace('_', '-')\n",
    ")\n",
    "def pipeline(\n",
    "    project: str,\n",
    "    project_number: str,\n",
    "    location: str,\n",
    "    service_account: str,\n",
    "    model_version: str,\n",
    "    pipeline_version: str,\n",
    "    train_image_uri: str,\n",
    "    train_output_gcs_bucket: str,\n",
    "    gcs_train_script_path: str,\n",
    "    model_display_name: str,\n",
    "    train_dockerfile_name: str,\n",
    "    train_dir: str,\n",
    "    train_dir_prefix: str,\n",
    "    valid_dir: str,\n",
    "    valid_dir_prefix: str,\n",
    "    candidate_file_dir: str,\n",
    "    candidate_files_prefix: str,\n",
    "    # tensorboard_resource_name: str,\n",
    "    experiment_name: str,\n",
    "    experiment_run: str,\n",
    "    register_model_flag: str,\n",
    "    # deploy_indexes_flag: str,\n",
    "    vpc_network_name: str,\n",
    "    generate_new_vocab: bool,\n",
    "    max_playlist_length: int,\n",
    "    max_tokens: int,\n",
    "    ngrams: int,\n",
    "):\n",
    "    \n",
    "    from kfp.v2.components import importer_node\n",
    "    from google_cloud_pipeline_components.types import artifact_types\n",
    "    \n",
    "    # ========================================================================\n",
    "    # Build Custom Train Image\n",
    "    # ========================================================================\n",
    "    \n",
    "    # build_custom_train_image_op = (\n",
    "    #     build_custom_train_image.build_custom_train_image(\n",
    "    #         project=project,\n",
    "    #         gcs_train_script_path=gcs_train_script_path,\n",
    "    #         training_image_uri=train_image_uri,\n",
    "    #         train_dockerfile_name=train_dockerfile_name,\n",
    "    #     )\n",
    "    #     .set_display_name(\"Build custom train image\")\n",
    "    #     .set_caching_options(False)\n",
    "    # )\n",
    "    \n",
    "    # ========================================================================\n",
    "    # Conditional: Upload models to Vertex model registry\n",
    "    # ========================================================================\n",
    "    # with kfp.v2.dsl.Condition(generate_new_vocab == 'True', name=\"Generate New Vocab\"):\n",
    "        \n",
    "        # here\n",
    "    # pl_name_src\n",
    "    adapt_pl_name_src_op = (\n",
    "        adapt_fixed_text_layer_vocab.adapt_fixed_text_layer_vocab(\n",
    "            project=project,\n",
    "            location=location,\n",
    "            version=model_version,\n",
    "            data_dir_bucket_name=train_dir,\n",
    "            data_dir_path_prefix=train_dir_prefix,\n",
    "            train_output_gcs_bucket=train_output_gcs_bucket,\n",
    "            experiment_name=experiment_name,\n",
    "            experiment_run=experiment_run,\n",
    "            max_playlist_length=max_playlist_length,\n",
    "            max_tokens=max_tokens,\n",
    "            ngrams=ngrams,\n",
    "            feature_name='pl_name_src',\n",
    "            generate_new_vocab=generate_new_vocab,\n",
    "        )\n",
    "        .set_display_name(f\"adapt: pl_name_src\")\n",
    "        .set_caching_options(True)\n",
    "        .set_cpu_limit('96')\n",
    "        .set_memory_limit('624G')\n",
    "    )\n",
    "    # track_name_can\n",
    "    adapt_track_name_can_op = (\n",
    "        adapt_fixed_text_layer_vocab.adapt_fixed_text_layer_vocab(\n",
    "            project=project,\n",
    "            location=location,\n",
    "            version=model_version,\n",
    "            data_dir_bucket_name=train_dir,\n",
    "            data_dir_path_prefix=train_dir_prefix,\n",
    "            train_output_gcs_bucket=train_output_gcs_bucket,\n",
    "            experiment_name=experiment_name,\n",
    "            experiment_run=experiment_run,\n",
    "            max_playlist_length=max_playlist_length,\n",
    "            max_tokens=max_tokens,\n",
    "            ngrams=ngrams,\n",
    "            feature_name='track_name_can',\n",
    "            generate_new_vocab=generate_new_vocab,\n",
    "        )\n",
    "        .set_display_name(f\"adapt: track_name_can\")\n",
    "        .set_caching_options(True)\n",
    "        .set_cpu_limit(cfg.CPU_LIMIT)\n",
    "        .set_memory_limit(cfg.MEMORY_LIMIT)\n",
    "    )\n",
    "    # artist_name_can\n",
    "    adapt_artist_name_can_op = (\n",
    "        adapt_fixed_text_layer_vocab.adapt_fixed_text_layer_vocab(\n",
    "            project=project,\n",
    "            location=location,\n",
    "            version=model_version,\n",
    "            data_dir_bucket_name=train_dir,\n",
    "            data_dir_path_prefix=train_dir_prefix,\n",
    "            train_output_gcs_bucket=train_output_gcs_bucket,\n",
    "            experiment_name=experiment_name,\n",
    "            experiment_run=experiment_run,\n",
    "            max_playlist_length=max_playlist_length,\n",
    "            max_tokens=max_tokens,\n",
    "            ngrams=ngrams,\n",
    "            feature_name='artist_name_can',\n",
    "            generate_new_vocab=generate_new_vocab,\n",
    "        )\n",
    "        .set_display_name(f\"adapt: artist_name_can\")\n",
    "        .set_caching_options(True)\n",
    "        .set_cpu_limit(cfg.CPU_LIMIT)\n",
    "        .set_memory_limit(cfg.MEMORY_LIMIT)\n",
    "    )\n",
    "\n",
    "    # album_name_can\n",
    "    adapt_album_name_can_op = (\n",
    "        adapt_fixed_text_layer_vocab.adapt_fixed_text_layer_vocab(\n",
    "            project=project,\n",
    "            location=location,\n",
    "            version=model_version,\n",
    "            data_dir_bucket_name=train_dir,\n",
    "            data_dir_path_prefix=train_dir_prefix,\n",
    "            train_output_gcs_bucket=train_output_gcs_bucket,\n",
    "            experiment_name=experiment_name,\n",
    "            experiment_run=experiment_run,\n",
    "            max_playlist_length=max_playlist_length,\n",
    "            max_tokens=max_tokens,\n",
    "            ngrams=ngrams,\n",
    "            feature_name='album_name_can',\n",
    "            generate_new_vocab=generate_new_vocab,\n",
    "        )\n",
    "        .set_display_name(f\"adapt: album_name_can\")\n",
    "        .set_caching_options(True)\n",
    "        .set_cpu_limit(cfg.CPU_LIMIT)\n",
    "        .set_memory_limit(cfg.MEMORY_LIMIT)\n",
    "    )\n",
    "    # artist_genres_can\n",
    "    adapt_artist_genres_can_op = (\n",
    "        adapt_fixed_text_layer_vocab.adapt_fixed_text_layer_vocab(\n",
    "            project=project,\n",
    "            location=location,\n",
    "            version=model_version,\n",
    "            data_dir_bucket_name=train_dir,\n",
    "            data_dir_path_prefix=train_dir_prefix,\n",
    "            train_output_gcs_bucket=train_output_gcs_bucket,\n",
    "            experiment_name=experiment_name,\n",
    "            experiment_run=experiment_run,\n",
    "            max_playlist_length=max_playlist_length,\n",
    "            max_tokens=max_tokens,\n",
    "            ngrams=ngrams,\n",
    "            feature_name='artist_genres_can',\n",
    "            generate_new_vocab=generate_new_vocab,\n",
    "        )\n",
    "        .set_display_name(f\"adapt: artist_genres_can\")\n",
    "        .set_caching_options(True)\n",
    "        .set_cpu_limit(cfg.CPU_LIMIT)\n",
    "        .set_memory_limit(cfg.MEMORY_LIMIT)\n",
    "    )\n",
    "    # raggeds\n",
    "\n",
    "    # track_name_pl\n",
    "    adapt_track_name_pl_features_op = (\n",
    "        adapt_ragged_text_layer_vocab.adapt_ragged_text_layer_vocab(\n",
    "            project=project,\n",
    "            location=location,\n",
    "            version=model_version,\n",
    "            data_dir_bucket_name=train_dir,\n",
    "            data_dir_path_prefix=train_dir_prefix,\n",
    "            train_output_gcs_bucket=train_output_gcs_bucket,\n",
    "            experiment_name=experiment_name,\n",
    "            experiment_run=experiment_run,\n",
    "            max_playlist_length=max_playlist_length,\n",
    "            max_tokens=max_tokens,\n",
    "            ngrams=ngrams,\n",
    "            feature_name='track_name_pl',\n",
    "            generate_new_vocab=generate_new_vocab,\n",
    "        )\n",
    "        .set_display_name(f\"adapt: track_name_pl\")\n",
    "        .set_caching_options(True)\n",
    "        .set_cpu_limit(cfg.CPU_LIMIT)\n",
    "        .set_memory_limit(cfg.MEMORY_LIMIT)\n",
    "    )\n",
    "    # artist_name_pl\n",
    "    adapt_artist_name_pl_op = (\n",
    "        adapt_ragged_text_layer_vocab.adapt_ragged_text_layer_vocab(\n",
    "            project=project,\n",
    "            location=location,\n",
    "            version=model_version,\n",
    "            data_dir_bucket_name=train_dir,\n",
    "            data_dir_path_prefix=train_dir_prefix,\n",
    "            train_output_gcs_bucket=train_output_gcs_bucket,\n",
    "            experiment_name=experiment_name,\n",
    "            experiment_run=experiment_run,\n",
    "            max_playlist_length=max_playlist_length,\n",
    "            max_tokens=max_tokens,\n",
    "            ngrams=ngrams,\n",
    "            feature_name='artist_name_pl',\n",
    "            generate_new_vocab=generate_new_vocab,\n",
    "        )\n",
    "        .set_display_name(f\"adapt: artist_name_pl\")\n",
    "        .set_caching_options(True)\n",
    "        .set_cpu_limit(cfg.CPU_LIMIT)\n",
    "        .set_memory_limit(cfg.MEMORY_LIMIT)\n",
    "    )\n",
    "\n",
    "    # album_name_pl\n",
    "    adapt_album_name_pl_op = (\n",
    "        adapt_ragged_text_layer_vocab.adapt_ragged_text_layer_vocab(\n",
    "            project=project,\n",
    "            location=location,\n",
    "            version=model_version,\n",
    "            data_dir_bucket_name=train_dir,\n",
    "            data_dir_path_prefix=train_dir_prefix,\n",
    "            train_output_gcs_bucket=train_output_gcs_bucket,\n",
    "            experiment_name=experiment_name,\n",
    "            experiment_run=experiment_run,\n",
    "            max_playlist_length=max_playlist_length,\n",
    "            max_tokens=max_tokens,\n",
    "            ngrams=ngrams,\n",
    "            feature_name='album_name_pl',\n",
    "            generate_new_vocab=generate_new_vocab,\n",
    "        )\n",
    "        .set_display_name(f\"adapt: album_name_pl\")\n",
    "        .set_caching_options(True)\n",
    "        .set_cpu_limit(cfg.CPU_LIMIT)\n",
    "        .set_memory_limit(cfg.MEMORY_LIMIT)\n",
    "    )\n",
    "\n",
    "    # artist_genres_pl\n",
    "    adapt_artist_genres_pl_op = (\n",
    "        adapt_ragged_text_layer_vocab.adapt_ragged_text_layer_vocab(\n",
    "            project=project,\n",
    "            location=location,\n",
    "            version=model_version,\n",
    "            data_dir_bucket_name=train_dir,\n",
    "            data_dir_path_prefix=train_dir_prefix,\n",
    "            train_output_gcs_bucket=train_output_gcs_bucket,\n",
    "            experiment_name=experiment_name,\n",
    "            experiment_run=experiment_run,\n",
    "            max_playlist_length=max_playlist_length,\n",
    "            max_tokens=max_tokens,\n",
    "            ngrams=ngrams,\n",
    "            feature_name='artist_genres_pl',\n",
    "            generate_new_vocab=generate_new_vocab,\n",
    "        )\n",
    "        .set_display_name(f\"adapt: artist_genres_pl\")\n",
    "        .set_caching_options(True)\n",
    "        .set_cpu_limit(cfg.CPU_LIMIT)\n",
    "        .set_memory_limit(cfg.MEMORY_LIMIT)\n",
    "    )\n",
    "\n",
    "    # ====================================================\n",
    "    # Aggregate all Dicts\n",
    "    # ====================================================\n",
    "\n",
    "    create_master_vocab_op = (\n",
    "        create_master_vocab.create_master_vocab(\n",
    "            project=project,\n",
    "            location=location,\n",
    "            version=model_version,\n",
    "            train_output_gcs_bucket=train_output_gcs_bucket,\n",
    "            experiment_name=experiment_name,\n",
    "            experiment_run=experiment_run,\n",
    "            vocab_uri_1=adapt_pl_name_src_op.outputs['vocab_gcs_uri'], \n",
    "            vocab_uri_2=adapt_track_name_can_op.outputs['vocab_gcs_uri'], \n",
    "            vocab_uri_3=adapt_artist_name_can_op.outputs['vocab_gcs_uri'], \n",
    "            vocab_uri_4=adapt_album_name_can_op.outputs['vocab_gcs_uri'], \n",
    "            vocab_uri_5=adapt_artist_genres_can_op.outputs['vocab_gcs_uri'], \n",
    "            vocab_uri_6=adapt_track_name_pl_features_op.outputs['vocab_gcs_uri'], \n",
    "            vocab_uri_7=adapt_artist_name_pl_op.outputs['vocab_gcs_uri'], \n",
    "            vocab_uri_8=adapt_album_name_pl_op.outputs['vocab_gcs_uri'], \n",
    "            vocab_uri_9=adapt_artist_genres_pl_op.outputs['vocab_gcs_uri'],\n",
    "            generate_new_vocab=generate_new_vocab,\n",
    "        )\n",
    "        # .after(fixed_for_loop_op).after(ragged_for_loop_op)\n",
    "        .set_display_name(\"create master vocab\")\n",
    "        .set_caching_options(True)\n",
    "        .set_cpu_limit(cfg.CPU_LIMIT)\n",
    "        .set_memory_limit(cfg.MEMORY_LIMIT)\n",
    "    )\n",
    "            \n",
    "    # ========================================================================\n",
    "    # Managed TB\n",
    "    # ========================================================================\n",
    "    \n",
    "    create_managed_tensorboard_op = (\n",
    "        create_tensorboard.create_tensorboard(\n",
    "            # here\n",
    "            project=project,\n",
    "            location=location,\n",
    "            model_version=model_version,\n",
    "            pipeline_version=pipeline_version,\n",
    "            model_name=model_display_name, \n",
    "            experiment_name=create_master_vocab_op.outputs['experiment_name'],\n",
    "            experiment_run=create_master_vocab_op.outputs['experiment_run'],\n",
    "        )\n",
    "        .set_display_name(\"Managed TB\")\n",
    "        .set_caching_options(True)\n",
    "    )\n",
    "\n",
    "\n",
    "    run_train_task_op = (\n",
    "        train_custom_model.train_custom_model(\n",
    "            project=project,\n",
    "            location=location,\n",
    "            model_version=model_version,\n",
    "            pipeline_version=pipeline_version,\n",
    "            model_name=model_display_name,\n",
    "            worker_pool_specs=WORKER_POOL_SPECS, \n",
    "            train_output_gcs_bucket=train_output_gcs_bucket,\n",
    "            # vocab_dict_uri=create_master_vocab_op.outputs['master_vocab_gcs_uri'],\n",
    "            experiment_name=experiment_name,\n",
    "            experiment_run=experiment_run,\n",
    "            training_image_uri=train_image_uri,\n",
    "            tensorboard_resource_name=create_managed_tensorboard_op.outputs['tensorboard_resource_name'], #tensorboard_resource_name, \n",
    "            service_account=service_account,\n",
    "            generate_new_vocab=generate_new_vocab,\n",
    "        )\n",
    "        .set_display_name(\"2Tower Training\")\n",
    "        .set_caching_options(True)\n",
    "        # .after(build_custom_train_image_op)\n",
    "    )\n",
    "    \n",
    "    # ========================================================================\n",
    "    # Import trained Query and Candidate Towers to this DAG (metadata)\n",
    "    # ========================================================================\n",
    "    \n",
    "    import_unmanaged_query_model_task = (\n",
    "        importer_node.importer(\n",
    "            artifact_uri=run_train_task_op.outputs['query_tower_dir_uri'],\n",
    "            artifact_class=artifact_types.UnmanagedContainerModel,\n",
    "            metadata={\n",
    "                'containerSpec': {\n",
    "                    'imageUri': 'us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-10:latest',\n",
    "                },\n",
    "            },\n",
    "        )\n",
    "        .set_display_name(\"Import Query Tower\")\n",
    "        .after(run_train_task_op)\n",
    "        .set_caching_options(True)\n",
    "    )\n",
    "    \n",
    "    import_unmanaged_candidate_model_task = (\n",
    "        importer_node.importer(\n",
    "            artifact_uri=run_train_task_op.outputs['candidate_tower_dir_uri'],\n",
    "            artifact_class=artifact_types.UnmanagedContainerModel,\n",
    "            metadata={\n",
    "                'containerSpec': {\n",
    "                    'imageUri': 'us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-10:latest',\n",
    "                },\n",
    "            },\n",
    "        )\n",
    "        .set_display_name(\"Import Candidate Tower\")\n",
    "        .after(run_train_task_op)\n",
    "        .set_caching_options(True)\n",
    "    )\n",
    "    \n",
    "    # ========================================================================\n",
    "    # Conditional: Upload models to Vertex model registry\n",
    "    # ========================================================================\n",
    "    # with kfp.v2.dsl.Condition(register_model_flag == \"True\", name=\"Register towers\"):\n",
    "        \n",
    "    # here\n",
    "\n",
    "    query_model_upload_op = (\n",
    "        gcc_aip.ModelUploadOp(\n",
    "            project=project,\n",
    "            location=location,\n",
    "            display_name=f'query-tower-{model_display_name}',\n",
    "            unmanaged_container_model=import_unmanaged_query_model_task.outputs[\"artifact\"],\n",
    "            labels={\"tower\": \"query\"},\n",
    "        )\n",
    "        .set_display_name(\"Upload Query Tower\")\n",
    "        .set_caching_options(True)\n",
    "    )\n",
    "\n",
    "    candidate_model_upload_op = (\n",
    "        gcc_aip.ModelUploadOp(\n",
    "            project=project,\n",
    "            location=location,\n",
    "            display_name=f'candidate-tower-{model_display_name}',\n",
    "            unmanaged_container_model=import_unmanaged_candidate_model_task.outputs[\"artifact\"],\n",
    "            labels={\"tower\": \"candidate\"},\n",
    "        )\n",
    "        .set_display_name(\"Upload Query Tower to Vertex\")\n",
    "        .set_caching_options(True)\n",
    "    )\n",
    "\n",
    "    # ========================================================================\n",
    "    # Conditional: Precompute Candidate embeddings, Create & Deploy ME indexes\n",
    "    # ========================================================================\n",
    "    # with kfp.v2.dsl.Condition(deploy_indexes_flag == \"True\", name=\"Create and Deploy Indexes\"):\n",
    "\n",
    "    # ========================================================================\n",
    "    # Deploy Query Tower to Endpoint\n",
    "    # ========================================================================\n",
    "    endpoint_create_op = (\n",
    "        gcc_aip.EndpointCreateOp(\n",
    "            project=project,\n",
    "            display_name=f'query-tower-endpoint-{model_version}'\n",
    "        )\n",
    "        .after(query_model_upload_op)\n",
    "        .set_display_name(\"Create Query Endpoint\")\n",
    "        .set_caching_options(True)\n",
    "    )\n",
    "\n",
    "    model_deploy_op = (\n",
    "        gcc_aip.ModelDeployOp(\n",
    "            endpoint=endpoint_create_op.outputs['endpoint'],\n",
    "            model=query_model_upload_op.outputs['model'],\n",
    "            deployed_model_display_name=f'deployed-qtower-{model_version}',\n",
    "            # dedicated_resources_accelerator_type=\"NVIDIA_TESLA_T4\",\n",
    "            # dedicated_resources_accelerator_count=1,\n",
    "            # dedicated_resources_max_replica_count=1,\n",
    "            # dedicated_resources_min_replica_count=1,\n",
    "            dedicated_resources_machine_type=\"n1-standard-16\",\n",
    "            dedicated_resources_min_replica_count=1,\n",
    "            dedicated_resources_max_replica_count=1,\n",
    "            service_account=service_account,\n",
    "        )\n",
    "        .set_display_name(\"Deploy Query Tower\")\n",
    "        .set_caching_options(True)\n",
    "    )\n",
    "\n",
    "    generate_candidates_op = (\n",
    "        generate_candidates.generate_candidates(\n",
    "            project=project,\n",
    "            location=location,\n",
    "            version=model_version,\n",
    "            candidate_tower_dir_uri=run_train_task_op.outputs['candidate_tower_dir_uri'],\n",
    "            train_output_gcs_bucket=train_output_gcs_bucket,\n",
    "            experiment_name=experiment_name,\n",
    "            experiment_run=experiment_run,\n",
    "            candidate_file_dir_bucket=candidate_file_dir,\n",
    "            candidate_file_dir_prefix=candidate_files_prefix,\n",
    "            experiment_run_dir=run_train_task_op.outputs['experiment_run_dir']\n",
    "        )\n",
    "        .set_display_name(\"Generate Candidate emb vectors\")\n",
    "        .set_caching_options(True)\n",
    "        .set_cpu_limit(cfg.CPU_LIMIT)\n",
    "        .set_memory_limit(cfg.MEMORY_LIMIT)\n",
    "    )\n",
    "\n",
    "    # ========================================================================\n",
    "    # Create ME indexes\n",
    "    # ========================================================================\n",
    "\n",
    "    create_ann_index_op = (\n",
    "        create_ann_index.create_ann_index(\n",
    "            project=project,\n",
    "            location=location,\n",
    "            version=model_version,\n",
    "            vpc_network_name=vpc_network_name,\n",
    "            emb_index_gcs_uri=generate_candidates_op.outputs['emb_index_gcs_uri'],\n",
    "            dimensions=256,\n",
    "            ann_index_display_name=f'ann_index_pipe_test',\n",
    "            approximate_neighbors_count=50,\n",
    "            distance_measure_type=\"DOT_PRODUCT_DISTANCE\",\n",
    "            leaf_node_embedding_count=500,\n",
    "            leaf_nodes_to_search_percent=7, \n",
    "            ann_index_description=\"testing ann index for Merlin deployment\",\n",
    "            # ann_index_labels=ann_index_labels,\n",
    "        )\n",
    "        .set_display_name(\"Create ANN Index\")\n",
    "        # .after(XXXX)\n",
    "        .set_caching_options(True)\n",
    "    )\n",
    "\n",
    "    create_brute_force_index_op = (\n",
    "        create_brute_force_index.create_brute_force_index(\n",
    "            project=project,\n",
    "            location=location,\n",
    "            version=model_version,\n",
    "            vpc_network_name=vpc_network_name,\n",
    "            emb_index_gcs_uri=generate_candidates_op.outputs['emb_index_gcs_uri'],\n",
    "            dimensions=256,\n",
    "            brute_force_index_display_name=f'bf_index_pipe_test',\n",
    "            approximate_neighbors_count=50,\n",
    "            distance_measure_type=\"DOT_PRODUCT_DISTANCE\",\n",
    "            brute_force_index_description=\"testing bf index for Merlin deployment\",\n",
    "            # brute_force_index_labels=brute_force_index_labels,\n",
    "        )\n",
    "        .set_display_name(\"Create BF Index\")\n",
    "        # .after(XXX)\n",
    "        .set_caching_options(True)\n",
    "    )\n",
    "\n",
    "    # ========================================================================\n",
    "    # Create ME index endpoints\n",
    "    # ========================================================================\n",
    "\n",
    "    create_ann_index_endpoint_vpc_op = (\n",
    "        create_ann_index_endpoint_vpc.create_ann_index_endpoint_vpc(\n",
    "            ann_index_artifact=create_ann_index_op.outputs['ann_index'],\n",
    "            project=project,\n",
    "            project_number=project_number,\n",
    "            version=model_version,\n",
    "            location=location,\n",
    "            vpc_network_name=vpc_network_name,\n",
    "            ann_index_endpoint_display_name=f'ann-index-endpoint',\n",
    "            ann_index_endpoint_description='endpoint for ann index',\n",
    "            ann_index_resource_uri=create_ann_index_op.outputs['ann_index_resource_uri'],\n",
    "        )\n",
    "        .set_display_name(\"Create ANN Index Endpoint\")\n",
    "        # .after(XXX)\n",
    "    )\n",
    "\n",
    "    create_brute_index_endpoint_vpc_op = (\n",
    "        create_brute_index_endpoint_vpc.create_brute_index_endpoint_vpc(\n",
    "            bf_index_artifact=create_brute_force_index_op.outputs['brute_force_index'],\n",
    "            project=project,\n",
    "            project_number=project_number,\n",
    "            version=model_version,\n",
    "            location=location,\n",
    "            vpc_network_name=vpc_network_name,\n",
    "            brute_index_endpoint_display_name=f'bf-index-endpoint',\n",
    "            brute_index_endpoint_description='endpoint for brute force index',\n",
    "            brute_force_index_resource_uri=create_brute_force_index_op.outputs['brute_force_index_resource_uri'],\n",
    "        )\n",
    "        .set_display_name(\"Create BF Index Endpoint\")\n",
    "        # .after(XXX)\n",
    "    )\n",
    "\n",
    "    # ========================================================================\n",
    "    # Deploy Indexes\n",
    "    # ========================================================================\n",
    "\n",
    "    deploy_ann_index_op = (\n",
    "        deploy_ann_index.deploy_ann_index(\n",
    "            project=project,\n",
    "            location=location,\n",
    "            version=model_version,\n",
    "            deployed_ann_index_name=f'deployed_ann_index',\n",
    "            ann_index_resource_uri=create_ann_index_endpoint_vpc_op.outputs['ann_index_resource_uri'],\n",
    "            index_endpoint_resource_uri=create_ann_index_endpoint_vpc_op.outputs['ann_index_endpoint_resource_uri'],\n",
    "        )\n",
    "        .set_display_name(\"Deploy ANN Index\")\n",
    "        .set_caching_options(True)\n",
    "    )\n",
    "\n",
    "    deploy_brute_index_op = (\n",
    "        deploy_brute_index.deploy_brute_index(\n",
    "            project=project,\n",
    "            location=location,\n",
    "            version=model_version,\n",
    "            deployed_brute_force_index_name=f'deployed_bf_index',\n",
    "            brute_force_index_resource_uri=create_brute_index_endpoint_vpc_op.outputs['brute_force_index_resource_uri'],\n",
    "            index_endpoint_resource_uri=create_brute_index_endpoint_vpc_op.outputs['brute_index_endpoint_resource_uri'],\n",
    "        )\n",
    "        .set_display_name(\"Deploy BF Index\")\n",
    "        .set_caching_options(True)\n",
    "    )\n",
    "\n",
    "    test_index_recall_op = (\n",
    "        test_index_recall.test_index_recall(\n",
    "            project=project,\n",
    "            location=location,\n",
    "            version=model_version,\n",
    "            gcs_train_script_path=gcs_train_script_path,\n",
    "            ann_index_endpoint_resource_uri=deploy_ann_index_op.outputs['index_endpoint_resource_uri'],\n",
    "            brute_index_endpoint_resource_uri=deploy_brute_index_op.outputs['index_endpoint_resource_uri'],\n",
    "            endpoint=model_deploy_op.outputs['gcp_resources']\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "65c65526-e42c-459c-a3a2-db8c84b6cc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! rm -f custom_container_pipeline_spec.json\n",
    "\n",
    "PIPELINE_JSON_SPEC_LOCAL = \"custom_pipeline_spec.json\"\n",
    "\n",
    "! rm -f $PIPELINE_JSON_SPEC_LOCAL\n",
    "\n",
    "kfp.v2.compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline, package_path=PIPELINE_JSON_SPEC_LOCAL,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8d598e-9456-452f-bb41-6c80feb288a7",
   "metadata": {},
   "source": [
    "### save pipeline spec json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "b3c4df38-23c9-449c-bbd2-bb6432d6e6a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PIPELINES_FILEPATH: gs://jt-tfrs-central-v2/tfrs-e2e-pipe-test-v18-jtv5/run-20230214-144020/pipeline_root/pipeline_spec.json\n",
      "Copying file://custom_pipeline_spec.json [Content-Type=application/json]...\n",
      "/ [1 files][306.7 KiB/306.7 KiB]                                                \n",
      "Operation completed over 1 objects/306.7 KiB.                                    \n"
     ]
    }
   ],
   "source": [
    "# !gsutil cp custom_container_pipeline_spec.json $PIPELINE_ROOT_PATH/pipeline_spec.json\n",
    "\n",
    "PIPELINES_FILEPATH = f'{PIPELINE_ROOT_PATH}/pipeline_spec.json'\n",
    "print(\"PIPELINES_FILEPATH:\", PIPELINES_FILEPATH)\n",
    "\n",
    "!gsutil cp $PIPELINE_JSON_SPEC_LOCAL $PIPELINES_FILEPATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "c6dba00a-d42b-40f5-8135-777caaae79a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://jt-tfrs-central-v2/tfrs-e2e-pipe-test-v18-jtv5/run-20230214-144020/pipeline_root/Dockerfile.tfrs\n",
      "gs://jt-tfrs-central-v2/tfrs-e2e-pipe-test-v18-jtv5/run-20230214-144020/pipeline_root/cloudbuild.yaml\n",
      "gs://jt-tfrs-central-v2/tfrs-e2e-pipe-test-v18-jtv5/run-20230214-144020/pipeline_root/pipeline_spec.json\n",
      "gs://jt-tfrs-central-v2/tfrs-e2e-pipe-test-v18-jtv5/run-20230214-144020/pipeline_root/934903580331/\n",
      "gs://jt-tfrs-central-v2/tfrs-e2e-pipe-test-v18-jtv5/run-20230214-144020/pipeline_root/trainer/\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls $PIPELINE_ROOT_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa01119-431a-49ce-bd7d-0acc1ca37e6c",
   "metadata": {},
   "source": [
    "## Submit pipeline to Vertex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "9e110b33-7f68-4575-a1a6-e0515727505e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tfrs-jtv5-2tower-pipev3'"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PIPELINE_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "db63e2ee-ab3e-4e08-bc95-91e9b194d469",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_NUMBER='934903580331'\n",
    "vpc_network_name = 'ucaip-haystack-vpc-network'\n",
    "# SERVICE_ACCOUNT = '934903580331-compute@developer.gserviceaccount.com'\n",
    "SERVICE_ACCOUNT = 'notebooksa@hybrid-vertex.iam.gserviceaccount.com'\n",
    "\n",
    "TRAIN_APP_CODE_PATH = f'{PIPELINE_ROOT_PATH}/trainer'\n",
    "\n",
    "job = vertex_ai.PipelineJob(\n",
    "    display_name=PIPELINE_NAME,\n",
    "    template_path=PIPELINES_FILEPATH,\n",
    "    pipeline_root=f'{PIPELINE_ROOT_PATH}',\n",
    "    failure_policy='fast', # slow | fast\n",
    "    # enable_caching=False,\n",
    "    parameter_values={\n",
    "        'project': PROJECT_ID,\n",
    "        'project_number': PROJECT_NUMBER,\n",
    "        'location': REGION,\n",
    "        'model_version': VERSION,\n",
    "        'pipeline_version': PIPELINE_VERSION,\n",
    "        'model_display_name': MODEL_ROOT_NAME,\n",
    "        'vpc_network_name':vpc_network_name,\n",
    "        # 'pipeline_tag': PIPELINE_TAG,\n",
    "        'gcs_train_script_path': TRAIN_APP_CODE_PATH,\n",
    "        'train_image_uri': f\"{IMAGE_URI}\",\n",
    "        'train_output_gcs_bucket': OUTPUT_BUCKET,\n",
    "        'train_dir': BUCKET_DATA_DIR,\n",
    "        'train_dir_prefix': TRAIN_DIR_PREFIX,\n",
    "        'valid_dir': BUCKET_DATA_DIR,\n",
    "        'valid_dir_prefix': VALID_DIR_PREFIX,\n",
    "        'candidate_file_dir': BUCKET_DATA_DIR,\n",
    "        'candidate_files_prefix': CANDIDATE_PREFIX,\n",
    "        # 'tensorboard_resource_name': TB_RESOURCE_NAME,\n",
    "        'train_dockerfile_name': DOCKERNAME,\n",
    "        'experiment_name': EXPERIMENT_NAME,\n",
    "        'experiment_run': RUN_NAME,\n",
    "        'service_account': SERVICE_ACCOUNT,\n",
    "        'register_model_flag': 'True',\n",
    "        # 'deploy_indexes_flag': 'True',\n",
    "        'generate_new_vocab': False,\n",
    "        'max_playlist_length': 15,\n",
    "        'max_tokens': 20000,\n",
    "        'ngrams': 2,\n",
    "    },\n",
    ")\n",
    "\n",
    "job.run(\n",
    "    sync=False,\n",
    "    service_account=SERVICE_ACCOUNT,\n",
    "    network=f'projects/{PROJECT_NUMBER}/global/networks/{vpc_network_name}'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb75704-0393-48c1-8281-de18bd55fd71",
   "metadata": {},
   "source": [
    "#### clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "deea05b0-4a31-4b8c-941b-e0dd79979f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! rm -rf custom_pipeline_spec.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e9911b-1b74-4c32-a24f-cd943d446fa6",
   "metadata": {},
   "source": [
    "## local test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1a8c8953-cc08-44ab-b0c2-5d82e82eb11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ls\n",
    "from src.two_tower_jt import test_instances as test_instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "b9a81a04-22b1-4e43-8593-4b416027ca68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_instances.TEST_INSTANCE_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "483c839c-fe7d-4d37-bd6f-c9f805a5977b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_instances.TEST_INSTANCE_15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68cdd01d-5818-402b-a3bd-988a05b50071",
   "metadata": {},
   "source": [
    "### Feature defs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608e52a9-0e0a-462e-9eaf-e02d6ff94495",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dict_uris = [\n",
    "    vocab_uri_1, vocab_uri_2, \n",
    "    vocab_uri_3, vocab_uri_4, \n",
    "    vocab_uri_5, vocab_uri_6, \n",
    "    vocab_uri_7, vocab_uri_8, \n",
    "    vocab_uri_9, \n",
    "]\n",
    "logging.info(f\"count of vocab_dict_uris: {len(vocab_dict_uris)}\")\n",
    "logging.info(f\"vocab_dict_uris: {vocab_dict_uris}\")\n",
    "\n",
    "# ===================================================\n",
    "# load pickled dicts\n",
    "# ===================================================\n",
    "\n",
    "loaded_pickle_list = []\n",
    "for i, pickled_dict in enumerate(vocab_dict_uris):\n",
    "\n",
    "    with open(f\"v{i}_vocab_pre_load\", 'wb') as local_vocab_file:\n",
    "        storage_client.download_blob_to_file(pickled_dict, local_vocab_file)\n",
    "\n",
    "    with open(f\"v{i}_vocab_pre_load\", 'rb') as pickle_file:\n",
    "        loaded_vocab_dict = pkl.load(pickle_file)\n",
    "\n",
    "    loaded_pickle_list.append(loaded_vocab_dict)\n",
    "\n",
    "# ===================================================\n",
    "# create master vocab dict\n",
    "# ===================================================\n",
    "master_dict = {}\n",
    "for loaded_dict in loaded_pickle_list:\n",
    "    master_dict.update(loaded_dict)\n",
    "\n",
    "# ===================================================\n",
    "# Upload master to GCS\n",
    "# ===================================================\n",
    "MASTER_VOCAB_LOCAL_FILE = f'vocab_dict.pkl'\n",
    "MASTER_VOCAB_GCS_OBJ = f'{experiment_name}/{experiment_run}/{MASTER_VOCAB_LOCAL_FILE}' # destination folder prefix and blob name\n",
    "\n",
    "# pickle\n",
    "filehandler = open(f'{MASTER_VOCAB_LOCAL_FILE}', 'wb')\n",
    "pkl.dump(master_dict, filehandler)\n",
    "filehandler.close()\n",
    "\n",
    "# upload to GCS\n",
    "bucket_client = storage_client.bucket(train_output_gcs_bucket)\n",
    "blob = bucket_client.blob(MASTER_VOCAB_GCS_OBJ)\n",
    "blob.upload_from_filename(MASTER_VOCAB_LOCAL_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "5c381f5d-facd-499a-9072-7546847d2c3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-14 08:19:42.940902: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-14 08:19:45.429956: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-02-14 08:19:53.453171: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2023-02-14 08:19:53.453940: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2023-02-14 08:19:53.453958: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "candidate_features_dict = {\n",
    "    \"track_uri_can\":tf.io.FixedLenFeature(dtype=tf.string, shape=()),            \n",
    "    \"track_name_can\":tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    \"artist_uri_can\":tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    \"artist_name_can\":tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    \"album_uri_can\":tf.io.FixedLenFeature(dtype=tf.string, shape=()),           \n",
    "    \"album_name_can\":tf.io.FixedLenFeature(dtype=tf.string, shape=()), \n",
    "    \"duration_ms_can\":tf.io.FixedLenFeature(dtype=tf.float32, shape=()),      \n",
    "    \"track_pop_can\":tf.io.FixedLenFeature(dtype=tf.float32, shape=()),      \n",
    "    \"artist_pop_can\":tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "    \"artist_genres_can\":tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    \"artist_followers_can\":tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "    # new\n",
    "    # \"track_pl_titles_can\":tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    \"track_danceability_can\":tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "    \"track_energy_can\":tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "    \"track_key_can\":tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    \"track_loudness_can\":tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "    \"track_mode_can\":tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    \"track_speechiness_can\":tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "    \"track_acousticness_can\":tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "    \"track_instrumentalness_can\":tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "    \"track_liveness_can\":tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "    \"track_valence_can\":tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "    \"track_tempo_can\":tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "    \"time_signature_can\":tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b01e447-a386-4875-8e87-d6da4f304386",
   "metadata": {},
   "source": [
    "### dump to pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "08059dfc-f942-4989-a32c-cee46dd11d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "\n",
    "candidate_pickle = 'candidate_features.pkl'\n",
    "\n",
    "filehandler = open(f'{candidate_pickle}', 'wb')\n",
    "pkl.dump(candidate_features_dict, filehandler)\n",
    "filehandler.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8edf39-f601-4290-b2b7-f3feb4221e4b",
   "metadata": {},
   "source": [
    "### upload to GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "d204a02a-307c-493f-82f8-d201f1f784f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "DESTINATION_PATH_PREFIX = f'a-local-test/feature-dicts'\n",
    "\n",
    "storage_client = storage.Client(project=PROJECT_ID)\n",
    "bucket = storage_client.bucket(OUTPUT_BUCKET)\n",
    "blob = bucket.blob(f'{DESTINATION_PATH_PREFIX}/{candidate_pickle}')\n",
    "\n",
    "blob.upload_from_filename(candidate_pickle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99d1760-84d3-4e0c-a77f-0b6ef63d8b78",
   "metadata": {},
   "source": [
    "### load pickled dict & use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "65f8bd1d-46e1-4c52-96cf-aab5f413cbae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying gs://jt-tfrs-central-v2/a-local-test/feature-dicts/candidate_features.pkl...\n",
      "/ [1 files][  965.0 B/  965.0 B]                                                \n",
      "Operation completed over 1 objects/965.0 B.                                      \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.system(f'gsutil cp gs://{OUTPUT_BUCKET}/{DESTINATION_PATH_PREFIX}/{candidate_pickle} loaded_cand_feats.pkl')  # jw-repo/spotify_mpd_two_tower/loaded_cand_feats.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "21f4b5ee-e21e-4d66-8fbf-c64d7e968e8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'track_uri_can': FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n",
       " 'track_name_can': FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n",
       " 'artist_uri_can': FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n",
       " 'artist_name_can': FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n",
       " 'album_uri_can': FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n",
       " 'album_name_can': FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n",
       " 'duration_ms_can': FixedLenFeature(shape=(), dtype=tf.float32, default_value=None),\n",
       " 'track_pop_can': FixedLenFeature(shape=(), dtype=tf.float32, default_value=None),\n",
       " 'artist_pop_can': FixedLenFeature(shape=(), dtype=tf.float32, default_value=None),\n",
       " 'artist_genres_can': FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n",
       " 'artist_followers_can': FixedLenFeature(shape=(), dtype=tf.float32, default_value=None),\n",
       " 'track_danceability_can': FixedLenFeature(shape=(), dtype=tf.float32, default_value=None),\n",
       " 'track_energy_can': FixedLenFeature(shape=(), dtype=tf.float32, default_value=None),\n",
       " 'track_key_can': FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n",
       " 'track_loudness_can': FixedLenFeature(shape=(), dtype=tf.float32, default_value=None),\n",
       " 'track_mode_can': FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n",
       " 'track_speechiness_can': FixedLenFeature(shape=(), dtype=tf.float32, default_value=None),\n",
       " 'track_acousticness_can': FixedLenFeature(shape=(), dtype=tf.float32, default_value=None),\n",
       " 'track_instrumentalness_can': FixedLenFeature(shape=(), dtype=tf.float32, default_value=None),\n",
       " 'track_liveness_can': FixedLenFeature(shape=(), dtype=tf.float32, default_value=None),\n",
       " 'track_valence_can': FixedLenFeature(shape=(), dtype=tf.float32, default_value=None),\n",
       " 'track_tempo_can': FixedLenFeature(shape=(), dtype=tf.float32, default_value=None),\n",
       " 'time_signature_can': FixedLenFeature(shape=(), dtype=tf.string, default_value=None)}"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.system(f'gsutil cp gs://{OUTPUT_BUCKET}/{DESTINATION_PATH_PREFIX}/{candidate_pickle} loaded_cand_feats.pkl')\n",
    "\n",
    "filehandler = open('loaded_cand_feats.pkl', 'rb')\n",
    "loaded_candidate_feature_dict = pkl.load(filehandler)\n",
    "filehandler.close()\n",
    "\n",
    "loaded_candidate_feature_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e5327c-0f4c-499b-b115-9201b17ea3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, count):\n",
    "    instance = {}\n",
    "    for key, dict in loaded_candidate_feature_dict.items():\n",
    "        new_dict = dict\n",
    "        if key in candidate_features_dict.keys():\n",
    "            new_dict = dict + perturb_cat[key]\n",
    "        instance[key] = random.choices(new_dict)[0]\n",
    "\n",
    "\n",
    "  for i in range(0, count):\n",
    "    instance = {}\n",
    "    for key, dict in CATEGORICAL_FEATURES.items():\n",
    "      new_dict = dict\n",
    "      if key in perturb_cat.keys():\n",
    "        new_dict = dict + perturb_cat[key]\n",
    "      instance[key] = random.choices(new_dict)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf99efb-4a8c-4f54-8ebb-b11576f2c9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for feat_name, fixed_length_def in loaded_candidate_feature_dict:\n",
    "    \n",
    "    names = [x['output_1'].numpy()[0] for x in embs] #clean up the output\n",
    "    \n",
    "embs_iter = parsed_candidate_dataset.batch(1).map(\n",
    "    lambda data: candidate_predictor(\n",
    "        track_uri_can = data[\"track_uri_can\"],\n",
    "        track_name_can = data['track_name_can'],\n",
    "        artist_uri_can = data['artist_uri_can'],\n",
    "        artist_name_can = data['artist_name_can'],\n",
    "        album_uri_can = data['album_uri_can'],\n",
    "        album_name_can = data['album_name_can'],\n",
    "        duration_ms_can = data['duration_ms_can'],\n",
    "        track_pop_can = data['track_pop_can'],\n",
    "        artist_pop_can = data['artist_pop_can'],\n",
    "        artist_genres_can = data['artist_genres_can'],\n",
    "        artist_followers_can = data['artist_followers_can'],\n",
    "        track_danceability_can = data['track_danceability_can'],\n",
    "        track_energy_can = data['track_energy_can'],\n",
    "        track_key_can = data['track_key_can'],\n",
    "        track_loudness_can = data['track_loudness_can'],\n",
    "        track_mode_can = data['track_mode_can'],\n",
    "        track_speechiness_can = data['track_speechiness_can'],\n",
    "        track_acousticness_can = data['track_acousticness_can'],\n",
    "        track_instrumentalness_can = data['track_instrumentalness_can'],\n",
    "        track_liveness_can = data['track_liveness_can'],\n",
    "        track_valence_can = data['track_valence_can'],\n",
    "        track_tempo_can = data['track_tempo_can'],\n",
    "        time_signature_can = data['time_signature_can']\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3ca651ac-f958-4b5e-8bf9-c2a551ec2b4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'v2-loaded-candidate-gcs.pkl'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GCS_URI = 'gs://jt-tfrs-central-v2/tfrs-e2e-pipe-test-v16-jtv5/run-20230214-110916/features/candidate_feats_dict.pkl'\n",
    "LOCAL_FILENAME = 'v2-loaded-candidate-gcs.pkl'\n",
    "with open(f\"{LOCAL_FILENAME}\", 'wb') as loaded:\n",
    "        storage_client.download_blob_to_file(\n",
    "            f\"{GCS_URI}\", loaded\n",
    "        )\n",
    "LOCAL_FILENAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c0793cf2-4775-4291-b2e4-d434073bcc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_BUCKET = 'jt-tfrs-central-v2'\n",
    "TEST_BLOB_NAME = 'tfrs-e2e-pipe-test-v16-jtv5/run-20230214-110916/features/candidate_feats_dict.pkl'\n",
    "TEST_LOCAL_FILENAME = 'v3-loaded_candidate_feats_dict.pkl'\n",
    "\n",
    "# blob = bucket.get_blob(source_blob_name)\n",
    "\n",
    "bucket = storage_client.bucket(TEST_BUCKET)\n",
    "blob = bucket.blob(TEST_BLOB_NAME)\n",
    "blob.download_to_filename(TEST_LOCAL_FILENAME)\n",
    "\n",
    "filehandler = open(f'{TEST_LOCAL_FILENAME}', 'rb')\n",
    "v3_loaded_query_features_dict = pkl.load(filehandler)\n",
    "filehandler.close()\n",
    "\n",
    "v3_loaded_query_features_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "794e4ba2-8cde-4b16-a5c7-ec612056fb4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'track_uri_can': FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n",
       " 'track_name_can': FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n",
       " 'artist_uri_can': FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n",
       " 'artist_name_can': FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n",
       " 'album_uri_can': FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n",
       " 'album_name_can': FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n",
       " 'duration_ms_can': FixedLenFeature(shape=(), dtype=tf.float32, default_value=None),\n",
       " 'track_pop_can': FixedLenFeature(shape=(), dtype=tf.float32, default_value=None),\n",
       " 'artist_pop_can': FixedLenFeature(shape=(), dtype=tf.float32, default_value=None),\n",
       " 'artist_genres_can': FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n",
       " 'artist_followers_can': FixedLenFeature(shape=(), dtype=tf.float32, default_value=None),\n",
       " 'track_danceability_can': FixedLenFeature(shape=(), dtype=tf.float32, default_value=None),\n",
       " 'track_energy_can': FixedLenFeature(shape=(), dtype=tf.float32, default_value=None),\n",
       " 'track_key_can': FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n",
       " 'track_loudness_can': FixedLenFeature(shape=(), dtype=tf.float32, default_value=None),\n",
       " 'track_mode_can': FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n",
       " 'track_speechiness_can': FixedLenFeature(shape=(), dtype=tf.float32, default_value=None),\n",
       " 'track_acousticness_can': FixedLenFeature(shape=(), dtype=tf.float32, default_value=None),\n",
       " 'track_instrumentalness_can': FixedLenFeature(shape=(), dtype=tf.float32, default_value=None),\n",
       " 'track_liveness_can': FixedLenFeature(shape=(), dtype=tf.float32, default_value=None),\n",
       " 'track_valence_can': FixedLenFeature(shape=(), dtype=tf.float32, default_value=None),\n",
       " 'track_tempo_can': FixedLenFeature(shape=(), dtype=tf.float32, default_value=None),\n",
       " 'time_signature_can': FixedLenFeature(shape=(), dtype=tf.string, default_value=None)}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filehandler = open(f'{TEST_LOCAL_FILENAME}', 'rb')\n",
    "v3_loaded_query_features_dict = pkl.load(filehandler)\n",
    "filehandler.close()\n",
    "\n",
    "v3_loaded_query_features_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17436f9a-ab92-4ce8-a45c-28fa1a23c0c9",
   "metadata": {},
   "source": [
    "### query features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1fe611-4a9f-4e0f-a6a6-386b67238e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl_query_features = {\n",
    "    # ===================================================\n",
    "    # summary playlist features\n",
    "    # ===================================================\n",
    "    \"pl_name_src\" : tf.io.FixedLenFeature(dtype=tf.string, shape=()), \n",
    "    'pl_collaborative_src' : tf.io.FixedLenFeature(dtype=tf.string, shape=()), \n",
    "    # 'num_pl_followers_src' : tf.io.FixedLenFeature(dtype=tf.float32, shape=()), \n",
    "    'pl_duration_ms_new' : tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "    'num_pl_songs_new' : tf.io.FixedLenFeature(dtype=tf.float32, shape=()), # n_songs_pl_new | num_pl_songs_new\n",
    "    'num_pl_artists_new' : tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "    'num_pl_albums_new' : tf.io.FixedLenFeature(dtype=tf.float32, shape=()), \n",
    "    # 'avg_track_pop_pl_new' : tf.io.FixedLenFeature(dtype=tf.float32, shape=()), \n",
    "    # 'avg_artist_pop_pl_new' : tf.io.FixedLenFeature(dtype=tf.float32, shape=()), \n",
    "    # 'avg_art_followers_pl_new' : tf.io.FixedLenFeature(dtype=tf.float32, shape=()), \n",
    "\n",
    "    # ===================================================\n",
    "    # ragged playlist features\n",
    "    # ===================================================\n",
    "    # bytes / string\n",
    "    \"track_uri_pl\": tf.io.FixedLenFeature(dtype=tf.string, shape=(MAX_PLAYLIST_LENGTH,)),\n",
    "    \"track_name_pl\": tf.io.FixedLenFeature(dtype=tf.string, shape=(MAX_PLAYLIST_LENGTH,)),\n",
    "    \"artist_uri_pl\": tf.io.FixedLenFeature(dtype=tf.string, shape=(MAX_PLAYLIST_LENGTH,)),\n",
    "    \"artist_name_pl\": tf.io.FixedLenFeature(dtype=tf.string, shape=(MAX_PLAYLIST_LENGTH,)),\n",
    "    \"album_uri_pl\": tf.io.FixedLenFeature(dtype=tf.string, shape=(MAX_PLAYLIST_LENGTH,)),\n",
    "    \"album_name_pl\": tf.io.FixedLenFeature(dtype=tf.string, shape=(MAX_PLAYLIST_LENGTH,)),\n",
    "    \"artist_genres_pl\": tf.io.FixedLenFeature(dtype=tf.string, shape=(MAX_PLAYLIST_LENGTH,)),\n",
    "    # \"tracks_playlist_titles_pl\": tf.io.FixedLenFeature(dtype=tf.string, shape=(MAX_PLAYLIST_LENGTH,)),\n",
    "    \"track_key_pl\": tf.io.FixedLenFeature(dtype=tf.string, shape=(MAX_PLAYLIST_LENGTH,)),\n",
    "    \"track_mode_pl\": tf.io.FixedLenFeature(dtype=tf.string, shape=(MAX_PLAYLIST_LENGTH,)),\n",
    "    \"time_signature_pl\": tf.io.FixedLenFeature(dtype=tf.string, shape=(MAX_PLAYLIST_LENGTH,)), \n",
    "\n",
    "    # Float List\n",
    "    \"duration_ms_songs_pl\": tf.io.FixedLenFeature(dtype=tf.float32, shape=(MAX_PLAYLIST_LENGTH,)),\n",
    "    \"track_pop_pl\": tf.io.FixedLenFeature(dtype=tf.float32, shape=(MAX_PLAYLIST_LENGTH,)),\n",
    "    \"artist_pop_pl\": tf.io.FixedLenFeature(dtype=tf.float32, shape=(MAX_PLAYLIST_LENGTH,)),\n",
    "    \"artists_followers_pl\": tf.io.FixedLenFeature(dtype=tf.float32, shape=(MAX_PLAYLIST_LENGTH,)),\n",
    "    \"track_danceability_pl\": tf.io.FixedLenFeature(dtype=tf.float32, shape=(MAX_PLAYLIST_LENGTH,)),\n",
    "    \"track_energy_pl\": tf.io.FixedLenFeature(dtype=tf.float32, shape=(MAX_PLAYLIST_LENGTH,)),\n",
    "    \"track_loudness_pl\": tf.io.FixedLenFeature(dtype=tf.float32, shape=(MAX_PLAYLIST_LENGTH,)),\n",
    "    \"track_speechiness_pl\": tf.io.FixedLenFeature(dtype=tf.float32, shape=(MAX_PLAYLIST_LENGTH,)),\n",
    "    \"track_acousticness_pl\": tf.io.FixedLenFeature(dtype=tf.float32, shape=(MAX_PLAYLIST_LENGTH,)),\n",
    "    \"track_instrumentalness_pl\": tf.io.FixedLenFeature(dtype=tf.float32, shape=(MAX_PLAYLIST_LENGTH,)),\n",
    "    \"track_liveness_pl\": tf.io.FixedLenFeature(dtype=tf.float32, shape=(MAX_PLAYLIST_LENGTH,)),\n",
    "    \"track_valence_pl\": tf.io.FixedLenFeature(dtype=tf.float32, shape=(MAX_PLAYLIST_LENGTH,)),\n",
    "    \"track_tempo_pl\": tf.io.FixedLenFeature(dtype=tf.float32, shape=(MAX_PLAYLIST_LENGTH,)),\n",
    "}\n",
    "\n",
    "# feats = {\n",
    "    # # ===================================================\n",
    "    # # candidate track features\n",
    "    # # ===================================================\n",
    "    # \"track_uri_can\":tf.io.FixedLenFeature(dtype=tf.string, shape=()),            \n",
    "    # \"track_name_can\":tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    # \"artist_uri_can\":tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    # \"artist_name_can\":tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    # \"album_uri_can\":tf.io.FixedLenFeature(dtype=tf.string, shape=()),           \n",
    "    # \"album_name_can\":tf.io.FixedLenFeature(dtype=tf.string, shape=()), \n",
    "    # \"duration_ms_can\":tf.io.FixedLenFeature(dtype=tf.float32, shape=()),      \n",
    "    # \"track_pop_can\":tf.io.FixedLenFeature(dtype=tf.float32, shape=()),      \n",
    "    # \"artist_pop_can\":tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "    # \"artist_genres_can\":tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    # \"artist_followers_can\":tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "    # # \"track_pl_titles_can\":tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    # \"track_danceability_can\":tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "    # \"track_energy_can\":tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "    # \"track_key_can\":tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    # \"track_loudness_can\":tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "    # \"track_mode_can\":tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    # \"track_speechiness_can\":tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "    # \"track_acousticness_can\":tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "    # \"track_instrumentalness_can\":tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "    # \"track_liveness_can\":tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "    # \"track_valence_can\":tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "    # \"track_tempo_can\":tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "    # \"time_signature_can\": tf.io.FixedLenFeature(dtype=tf.string, shape=()), # track_time_signature_can"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6521c198-12bc-4c15-a236-89ef2afb3bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_blob(bucket_name, source_gcs_obj, local_filename):\n",
    "    \"\"\"Uploads a file to the bucket.\"\"\"\n",
    "    # storage_client = storage.Client(project=project_number)\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(source_gcs_obj)\n",
    "    blob.download_to_filename(local_filename)\n",
    "\n",
    "    filehandler = open(f'{local_filename}', 'rb')\n",
    "    loaded_dict = pkl.load(filehandler)\n",
    "    filehandler.close()\n",
    "\n",
    "    logging.info(f\"File {local_filename} downloaded from gs://{bucket_name}/{source_gcs_obj}\")\n",
    "\n",
    "    return loaded_dict\n",
    "\n",
    "# candidate features\n",
    "train_output_gcs_bucket = 'jt-tfrs-central-v2'\n",
    "CAND_FEAT_FILENAME = 'candidate_feats_dict.pkl'\n",
    "CAND_FEAT_GCS_OBJ = f'tfrs-e2e-pipe-test-v16-jtv5/run-20230214-110916/features/{CAND_FEAT_FILENAME}'\n",
    "LOADED_CANDIDATE_DICT = f'loaded_{CAND_FEAT_FILENAME}'\n",
    "\n",
    "loaded_candidate_features_dict = download_blob(\n",
    "    train_output_gcs_bucket,\n",
    "    CAND_FEAT_GCS_OBJ,\n",
    "    LOADED_CANDIDATE_DICT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fa76e4e6-d9df-474d-80ba-eebd4783e534",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'track_uri_can': FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n",
       " 'track_name_can': FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n",
       " 'artist_uri_can': FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n",
       " 'artist_name_can': FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n",
       " 'album_uri_can': FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n",
       " 'album_name_can': FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n",
       " 'duration_ms_can': FixedLenFeature(shape=(), dtype=tf.float32, default_value=None),\n",
       " 'track_pop_can': FixedLenFeature(shape=(), dtype=tf.float32, default_value=None),\n",
       " 'artist_pop_can': FixedLenFeature(shape=(), dtype=tf.float32, default_value=None),\n",
       " 'artist_genres_can': FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n",
       " 'artist_followers_can': FixedLenFeature(shape=(), dtype=tf.float32, default_value=None),\n",
       " 'track_danceability_can': FixedLenFeature(shape=(), dtype=tf.float32, default_value=None),\n",
       " 'track_energy_can': FixedLenFeature(shape=(), dtype=tf.float32, default_value=None),\n",
       " 'track_key_can': FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n",
       " 'track_loudness_can': FixedLenFeature(shape=(), dtype=tf.float32, default_value=None),\n",
       " 'track_mode_can': FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n",
       " 'track_speechiness_can': FixedLenFeature(shape=(), dtype=tf.float32, default_value=None),\n",
       " 'track_acousticness_can': FixedLenFeature(shape=(), dtype=tf.float32, default_value=None),\n",
       " 'track_instrumentalness_can': FixedLenFeature(shape=(), dtype=tf.float32, default_value=None),\n",
       " 'track_liveness_can': FixedLenFeature(shape=(), dtype=tf.float32, default_value=None),\n",
       " 'track_valence_can': FixedLenFeature(shape=(), dtype=tf.float32, default_value=None),\n",
       " 'track_tempo_can': FixedLenFeature(shape=(), dtype=tf.float32, default_value=None),\n",
       " 'time_signature_can': FixedLenFeature(shape=(), dtype=tf.string, default_value=None)}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# candidate features\n",
    "train_output_gcs_bucket = 'jt-tfrs-central-v2'\n",
    "CAND_FEAT_FILENAME = 'candidate_feats_dict.pkl'\n",
    "CAND_FEAT_GCS_OBJ = f'tfrs-e2e-pipe-test-v16-jtv5/run-20230214-110916/features/{CAND_FEAT_FILENAME}'\n",
    "LOADED_CANDIDATE_DICT = f'loaded_{CAND_FEAT_FILENAME}'\n",
    "\n",
    "loaded_candidate_features_dict = download_blob(\n",
    "    train_output_gcs_bucket,\n",
    "    CAND_FEAT_GCS_OBJ,\n",
    "    LOADED_CANDIDATE_DICT\n",
    ")\n",
    "# logg\n",
    "loaded_candidate_features_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bbd5fe72-b0a7-4973-934c-0d068208f6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_INSTANCE_15 = {\n",
    "        'album_name_can': 'Capoeira Electronica',\n",
    "        'album_name_pl': [\n",
    "            'Odilara', 'Capoeira Electronica', 'Capoeira Ultimate','Festa Popular', 'Capoeira Electronica',\n",
    "            'Odilara', 'Capoeira Electronica', 'Capoeira Ultimate','Festa Popular', 'Capoeira Electronica',\n",
    "            'Odilara', 'Capoeira Electronica', 'Capoeira Ultimate','Festa Popular', 'Capoeira Electronica'\n",
    "        ],\n",
    "        'album_uri_can': 'spotify:album:2FsSSHGt8JM0JgRy6ZX3kR',\n",
    "        'album_uri_pl': [\n",
    "            'spotify:album:4Y8RfvZzCiApBCIZswj9Ry',\n",
    "            'spotify:album:2FsSSHGt8JM0JgRy6ZX3kR',\n",
    "            'spotify:album:55HHBqZ2SefPeaENOgWxYK',\n",
    "            'spotify:album:150L1V6UUT7fGUI3PbxpkE',\n",
    "            'spotify:album:2FsSSHGt8JM0JgRy6ZX3kR',\n",
    "            'spotify:album:4Y8RfvZzCiApBCIZswj9Ry',\n",
    "            'spotify:album:2FsSSHGt8JM0JgRy6ZX3kR',\n",
    "            'spotify:album:55HHBqZ2SefPeaENOgWxYK',\n",
    "            'spotify:album:150L1V6UUT7fGUI3PbxpkE',\n",
    "            'spotify:album:2FsSSHGt8JM0JgRy6ZX3kR',\n",
    "            'spotify:album:4Y8RfvZzCiApBCIZswj9Ry',\n",
    "            'spotify:album:2FsSSHGt8JM0JgRy6ZX3kR',\n",
    "            'spotify:album:55HHBqZ2SefPeaENOgWxYK',\n",
    "            'spotify:album:150L1V6UUT7fGUI3PbxpkE',\n",
    "            'spotify:album:2FsSSHGt8JM0JgRy6ZX3kR'\n",
    "        ],\n",
    "        'artist_followers_can': 5170.0,\n",
    "        'artist_genres_can': 'capoeira',\n",
    "        'artist_genres_pl': [\n",
    "            'samba moderno', 'capoeira', 'capoeira', 'NONE','capoeira',\n",
    "            'samba moderno', 'capoeira', 'capoeira', 'NONE','capoeira',\n",
    "            'samba moderno', 'capoeira', 'capoeira', 'NONE','capoeira'\n",
    "        ],\n",
    "        'artist_name_can': 'Capoeira Experience',\n",
    "        'artist_name_pl': [\n",
    "            'Odilara', 'Capoeira Experience', 'Denis Porto', 'Zambe','Capoeira Experience',\n",
    "            'Odilara', 'Capoeira Experience', 'Denis Porto', 'Zambe','Capoeira Experience',\n",
    "            'Odilara', 'Capoeira Experience', 'Denis Porto', 'Zambe','Capoeira Experience'\n",
    "        ],\n",
    "        'artist_pop_can': 24.0,\n",
    "        'artist_pop_pl':[\n",
    "            4., 24.,  2.,  0., 24.,\n",
    "            4., 24.,  2.,  0., 24.,\n",
    "            4., 24.,  2.,  0., 24.\n",
    "        ],\n",
    "        'artist_uri_can': 'spotify:artist:5SKEXbgzIdRl3gQJ23CnUP',\n",
    "        'artist_uri_pl': [\n",
    "            'spotify:artist:72oameojLOPWYB7nB8rl6c',\n",
    "            'spotify:artist:5SKEXbgzIdRl3gQJ23CnUP',\n",
    "            'spotify:artist:67p5GMYQZOgaAfx1YyttQk',\n",
    "            'spotify:artist:4fH3OXCRcPsaHFE5KhgqZS',\n",
    "            'spotify:artist:5SKEXbgzIdRl3gQJ23CnUP',\n",
    "            'spotify:artist:72oameojLOPWYB7nB8rl6c',\n",
    "            'spotify:artist:5SKEXbgzIdRl3gQJ23CnUP',\n",
    "            'spotify:artist:67p5GMYQZOgaAfx1YyttQk',\n",
    "            'spotify:artist:4fH3OXCRcPsaHFE5KhgqZS',\n",
    "            'spotify:artist:5SKEXbgzIdRl3gQJ23CnUP',\n",
    "            'spotify:artist:72oameojLOPWYB7nB8rl6c',\n",
    "            'spotify:artist:5SKEXbgzIdRl3gQJ23CnUP',\n",
    "            'spotify:artist:67p5GMYQZOgaAfx1YyttQk',\n",
    "            'spotify:artist:4fH3OXCRcPsaHFE5KhgqZS',\n",
    "            'spotify:artist:5SKEXbgzIdRl3gQJ23CnUP'\n",
    "        ],\n",
    "        'artists_followers_pl': [ \n",
    "            316., 5170.,  448.,   19., 5170.,\n",
    "            316., 5170.,  448.,   19., 5170.,\n",
    "            316., 5170.,  448.,   19., 5170.\n",
    "        ],\n",
    "        'duration_ms_can': 192640.0,\n",
    "        'duration_ms_songs_pl': [234612., 226826., 203480., 287946., 271920., 234612., 226826., 203480., 287946., 271920., 234612., 226826., 203480., 287946., 271920.],\n",
    "        'num_pl_albums_new': 9.0,\n",
    "        'num_pl_artists_new': 5.0,\n",
    "        'num_pl_songs_new': 85.0,\n",
    "        'pl_collaborative_src': 'false',\n",
    "        'pl_duration_ms_new': 17971314.0,\n",
    "        'pl_name_src': 'Capoeira',\n",
    "        'time_signature_can': '4',\n",
    "        'time_signature_pl': ['4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4'],\n",
    "        'track_acousticness_can': 0.478,\n",
    "        'track_acousticness_pl': [0.238 , 0.105 , 0.0242, 0.125 , 0.304, 0.238 , 0.105 , 0.0242, 0.125 , 0.304, 0.238 , 0.105 , 0.0242, 0.125 , 0.304 ],\n",
    "        'track_danceability_can': 0.709,\n",
    "        'track_danceability_pl': [0.703, 0.712, 0.806, 0.529, 0.821, 0.238 , 0.105 , 0.0242, 0.125 , 0.304, 0.238 , 0.105 , 0.0242, 0.125 , 0.304],\n",
    "        'track_energy_can': 0.742,\n",
    "        'track_energy_pl': [0.743, 0.41 , 0.794, 0.776, 0.947, 0.238 , 0.105 , 0.0242, 0.125 , 0.304, 0.238 , 0.105 , 0.0242, 0.125 , 0.304],\n",
    "        'track_instrumentalness_can': 0.00297,\n",
    "        'track_instrumentalness_pl': [4.84e-06, 4.30e-01, 7.42e-04, 4.01e-01, 5.07e-03, 4.84e-06, 4.30e-01, 7.42e-04, 4.01e-01, 5.07e-03, 4.84e-06, 4.30e-01, 7.42e-04, 4.01e-01, 5.07e-03],\n",
    "        'track_key_can': '0',\n",
    "        'track_key_pl': ['5', '0', '1', '10', '10', '5', '0', '1', '10', '10', '5', '0', '1', '10', '10'],\n",
    "        'track_liveness_can': 0.0346,\n",
    "        'track_liveness_pl': [0.128 , 0.0725, 0.191 , 0.105 , 0.0552,0.128 , 0.0725, 0.191 , 0.105 , 0.0552, 0.128 , 0.0725, 0.191 , 0.105 , 0.0552],\n",
    "        'track_loudness_can': -7.295,\n",
    "        'track_loudness_pl': [-8.638, -8.754, -9.084, -7.04 , -6.694, -8.638, -8.754, -9.084, -7.04 , -6.694, -8.638, -8.754, -9.084, -7.04 , -6.694],\n",
    "        'track_mode_can': '1',\n",
    "        'track_mode_pl': ['0', '1', '1', '0', '1', '0', '1', '1', '0', '1', '0', '1', '1', '0', '1'],\n",
    "        'track_name_can': 'Bezouro Preto - Studio',\n",
    "        'track_name_pl': [\n",
    "            'O Telefone Tocou Novamente', 'Bem Devagar - Studio','Angola Dream', 'Janaina', 'Louco Berimbau - Studio',\n",
    "            'O Telefone Tocou Novamente', 'Bem Devagar - Studio','Angola Dream', 'Janaina', 'Louco Berimbau - Studio',\n",
    "            'O Telefone Tocou Novamente', 'Bem Devagar - Studio','Angola Dream', 'Janaina', 'Louco Berimbau - Studio'\n",
    "        ],\n",
    "        'track_pop_can': 3.0,\n",
    "        'track_pop_pl': [5., 1., 0., 0., 1., 5., 1., 0., 0., 1., 5., 1., 0., 0., 1.],\n",
    "        'track_speechiness_can': 0.0802,\n",
    "        'track_speechiness_pl':[0.0367, 0.0272, 0.0407, 0.132 , 0.0734, 0.0367, 0.0272, 0.0407, 0.132 , 0.0734, 0.0367, 0.0272, 0.0407, 0.132 , 0.0734],\n",
    "        'track_tempo_can': 172.238,\n",
    "        'track_tempo_pl': [100.039,  89.089, 123.999, 119.963, 119.214, 100.039,  89.089, 123.999, 119.963, 119.214, 100.039,  89.089, 123.999, 119.963, 119.214],\n",
    "        'track_uri_can': 'spotify:track:0tlhK4OvpHCYpReTABvKFb',\n",
    "        'track_uri_pl': [\n",
    "            'spotify:track:1pQkOdcTDfLr84TDCrmGy7',\n",
    "            'spotify:track:39grEDsAHAjmo2QFo4G8D9',\n",
    "            'spotify:track:5vxSLdJXqbKYH487YO8LSL',\n",
    "            'spotify:track:6T9GbmZ6voDM4aTBsG5VDh',\n",
    "            'spotify:track:7ELt9eslVvWo276pX2garN',\n",
    "            'spotify:track:1pQkOdcTDfLr84TDCrmGy7',\n",
    "            'spotify:track:39grEDsAHAjmo2QFo4G8D9',\n",
    "            'spotify:track:5vxSLdJXqbKYH487YO8LSL',\n",
    "            'spotify:track:6T9GbmZ6voDM4aTBsG5VDh',\n",
    "            'spotify:track:7ELt9eslVvWo276pX2garN',\n",
    "            'spotify:track:1pQkOdcTDfLr84TDCrmGy7',\n",
    "            'spotify:track:39grEDsAHAjmo2QFo4G8D9',\n",
    "            'spotify:track:5vxSLdJXqbKYH487YO8LSL',\n",
    "            'spotify:track:6T9GbmZ6voDM4aTBsG5VDh',\n",
    "            'spotify:track:7ELt9eslVvWo276pX2garN'\n",
    "        ],\n",
    "        'track_valence_can': 0.844,\n",
    "        'track_valence_pl': [\n",
    "            0.966, 0.667, 0.696, 0.876, 0.655,\n",
    "            0.966, 0.667, 0.696, 0.876, 0.655,\n",
    "            0.966, 0.667, 0.696, 0.876, 0.655\n",
    "        ],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3f12dcd5-5367-4ab1-949a-a7c207814c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query features\n",
    "LOCAL_TEST_INSTANCE = 'test_instance_15_dict.pkl'\n",
    "\n",
    "BUCKET_TEST = 'spotify-data-regimes'\n",
    "PREFIX = 'jtv15-8m'\n",
    "TEST_GCS_OBJ = f'{PREFIX}/{LOCAL_TEST_INSTANCE}'\n",
    "\n",
    "# pickle\n",
    "filehandler = open(f'{LOCAL_TEST_INSTANCE}', 'wb')\n",
    "pkl.dump(TEST_INSTANCE_15, filehandler)\n",
    "filehandler.close()\n",
    "\n",
    "# upload to GCS\n",
    "bucket_client = storage_client.bucket(BUCKET_TEST)\n",
    "blob = bucket_client.blob(TEST_GCS_OBJ)\n",
    "blob.upload_from_filename(LOCAL_TEST_INSTANCE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "778b4992-0aca-4ad0-980b-437e5d387802",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'album_name_can': 'Capoeira Electronica',\n",
       " 'album_name_pl': ['Odilara',\n",
       "  'Capoeira Electronica',\n",
       "  'Capoeira Ultimate',\n",
       "  'Festa Popular',\n",
       "  'Capoeira Electronica',\n",
       "  'Odilara',\n",
       "  'Capoeira Electronica',\n",
       "  'Capoeira Ultimate',\n",
       "  'Festa Popular',\n",
       "  'Capoeira Electronica',\n",
       "  'Odilara',\n",
       "  'Capoeira Electronica',\n",
       "  'Capoeira Ultimate',\n",
       "  'Festa Popular',\n",
       "  'Capoeira Electronica'],\n",
       " 'album_uri_can': 'spotify:album:2FsSSHGt8JM0JgRy6ZX3kR',\n",
       " 'album_uri_pl': ['spotify:album:4Y8RfvZzCiApBCIZswj9Ry',\n",
       "  'spotify:album:2FsSSHGt8JM0JgRy6ZX3kR',\n",
       "  'spotify:album:55HHBqZ2SefPeaENOgWxYK',\n",
       "  'spotify:album:150L1V6UUT7fGUI3PbxpkE',\n",
       "  'spotify:album:2FsSSHGt8JM0JgRy6ZX3kR',\n",
       "  'spotify:album:4Y8RfvZzCiApBCIZswj9Ry',\n",
       "  'spotify:album:2FsSSHGt8JM0JgRy6ZX3kR',\n",
       "  'spotify:album:55HHBqZ2SefPeaENOgWxYK',\n",
       "  'spotify:album:150L1V6UUT7fGUI3PbxpkE',\n",
       "  'spotify:album:2FsSSHGt8JM0JgRy6ZX3kR',\n",
       "  'spotify:album:4Y8RfvZzCiApBCIZswj9Ry',\n",
       "  'spotify:album:2FsSSHGt8JM0JgRy6ZX3kR',\n",
       "  'spotify:album:55HHBqZ2SefPeaENOgWxYK',\n",
       "  'spotify:album:150L1V6UUT7fGUI3PbxpkE',\n",
       "  'spotify:album:2FsSSHGt8JM0JgRy6ZX3kR'],\n",
       " 'artist_followers_can': 5170.0,\n",
       " 'artist_genres_can': 'capoeira',\n",
       " 'artist_genres_pl': ['samba moderno',\n",
       "  'capoeira',\n",
       "  'capoeira',\n",
       "  'NONE',\n",
       "  'capoeira',\n",
       "  'samba moderno',\n",
       "  'capoeira',\n",
       "  'capoeira',\n",
       "  'NONE',\n",
       "  'capoeira',\n",
       "  'samba moderno',\n",
       "  'capoeira',\n",
       "  'capoeira',\n",
       "  'NONE',\n",
       "  'capoeira'],\n",
       " 'artist_name_can': 'Capoeira Experience',\n",
       " 'artist_name_pl': ['Odilara',\n",
       "  'Capoeira Experience',\n",
       "  'Denis Porto',\n",
       "  'Zambe',\n",
       "  'Capoeira Experience',\n",
       "  'Odilara',\n",
       "  'Capoeira Experience',\n",
       "  'Denis Porto',\n",
       "  'Zambe',\n",
       "  'Capoeira Experience',\n",
       "  'Odilara',\n",
       "  'Capoeira Experience',\n",
       "  'Denis Porto',\n",
       "  'Zambe',\n",
       "  'Capoeira Experience'],\n",
       " 'artist_pop_can': 24.0,\n",
       " 'artist_pop_pl': [4.0,\n",
       "  24.0,\n",
       "  2.0,\n",
       "  0.0,\n",
       "  24.0,\n",
       "  4.0,\n",
       "  24.0,\n",
       "  2.0,\n",
       "  0.0,\n",
       "  24.0,\n",
       "  4.0,\n",
       "  24.0,\n",
       "  2.0,\n",
       "  0.0,\n",
       "  24.0],\n",
       " 'artist_uri_can': 'spotify:artist:5SKEXbgzIdRl3gQJ23CnUP',\n",
       " 'artist_uri_pl': ['spotify:artist:72oameojLOPWYB7nB8rl6c',\n",
       "  'spotify:artist:5SKEXbgzIdRl3gQJ23CnUP',\n",
       "  'spotify:artist:67p5GMYQZOgaAfx1YyttQk',\n",
       "  'spotify:artist:4fH3OXCRcPsaHFE5KhgqZS',\n",
       "  'spotify:artist:5SKEXbgzIdRl3gQJ23CnUP',\n",
       "  'spotify:artist:72oameojLOPWYB7nB8rl6c',\n",
       "  'spotify:artist:5SKEXbgzIdRl3gQJ23CnUP',\n",
       "  'spotify:artist:67p5GMYQZOgaAfx1YyttQk',\n",
       "  'spotify:artist:4fH3OXCRcPsaHFE5KhgqZS',\n",
       "  'spotify:artist:5SKEXbgzIdRl3gQJ23CnUP',\n",
       "  'spotify:artist:72oameojLOPWYB7nB8rl6c',\n",
       "  'spotify:artist:5SKEXbgzIdRl3gQJ23CnUP',\n",
       "  'spotify:artist:67p5GMYQZOgaAfx1YyttQk',\n",
       "  'spotify:artist:4fH3OXCRcPsaHFE5KhgqZS',\n",
       "  'spotify:artist:5SKEXbgzIdRl3gQJ23CnUP'],\n",
       " 'artists_followers_pl': [316.0,\n",
       "  5170.0,\n",
       "  448.0,\n",
       "  19.0,\n",
       "  5170.0,\n",
       "  316.0,\n",
       "  5170.0,\n",
       "  448.0,\n",
       "  19.0,\n",
       "  5170.0,\n",
       "  316.0,\n",
       "  5170.0,\n",
       "  448.0,\n",
       "  19.0,\n",
       "  5170.0],\n",
       " 'duration_ms_can': 192640.0,\n",
       " 'duration_ms_songs_pl': [234612.0,\n",
       "  226826.0,\n",
       "  203480.0,\n",
       "  287946.0,\n",
       "  271920.0,\n",
       "  234612.0,\n",
       "  226826.0,\n",
       "  203480.0,\n",
       "  287946.0,\n",
       "  271920.0,\n",
       "  234612.0,\n",
       "  226826.0,\n",
       "  203480.0,\n",
       "  287946.0,\n",
       "  271920.0],\n",
       " 'num_pl_albums_new': 9.0,\n",
       " 'num_pl_artists_new': 5.0,\n",
       " 'num_pl_songs_new': 85.0,\n",
       " 'pl_collaborative_src': 'false',\n",
       " 'pl_duration_ms_new': 17971314.0,\n",
       " 'pl_name_src': 'Capoeira',\n",
       " 'time_signature_can': '4',\n",
       " 'time_signature_pl': ['4',\n",
       "  '4',\n",
       "  '4',\n",
       "  '4',\n",
       "  '4',\n",
       "  '4',\n",
       "  '4',\n",
       "  '4',\n",
       "  '4',\n",
       "  '4',\n",
       "  '4',\n",
       "  '4',\n",
       "  '4',\n",
       "  '4',\n",
       "  '4'],\n",
       " 'track_acousticness_can': 0.478,\n",
       " 'track_acousticness_pl': [0.238,\n",
       "  0.105,\n",
       "  0.0242,\n",
       "  0.125,\n",
       "  0.304,\n",
       "  0.238,\n",
       "  0.105,\n",
       "  0.0242,\n",
       "  0.125,\n",
       "  0.304,\n",
       "  0.238,\n",
       "  0.105,\n",
       "  0.0242,\n",
       "  0.125,\n",
       "  0.304],\n",
       " 'track_danceability_can': 0.709,\n",
       " 'track_danceability_pl': [0.703,\n",
       "  0.712,\n",
       "  0.806,\n",
       "  0.529,\n",
       "  0.821,\n",
       "  0.238,\n",
       "  0.105,\n",
       "  0.0242,\n",
       "  0.125,\n",
       "  0.304,\n",
       "  0.238,\n",
       "  0.105,\n",
       "  0.0242,\n",
       "  0.125,\n",
       "  0.304],\n",
       " 'track_energy_can': 0.742,\n",
       " 'track_energy_pl': [0.743,\n",
       "  0.41,\n",
       "  0.794,\n",
       "  0.776,\n",
       "  0.947,\n",
       "  0.238,\n",
       "  0.105,\n",
       "  0.0242,\n",
       "  0.125,\n",
       "  0.304,\n",
       "  0.238,\n",
       "  0.105,\n",
       "  0.0242,\n",
       "  0.125,\n",
       "  0.304],\n",
       " 'track_instrumentalness_can': 0.00297,\n",
       " 'track_instrumentalness_pl': [4.84e-06,\n",
       "  0.43,\n",
       "  0.000742,\n",
       "  0.401,\n",
       "  0.00507,\n",
       "  4.84e-06,\n",
       "  0.43,\n",
       "  0.000742,\n",
       "  0.401,\n",
       "  0.00507,\n",
       "  4.84e-06,\n",
       "  0.43,\n",
       "  0.000742,\n",
       "  0.401,\n",
       "  0.00507],\n",
       " 'track_key_can': '0',\n",
       " 'track_key_pl': ['5',\n",
       "  '0',\n",
       "  '1',\n",
       "  '10',\n",
       "  '10',\n",
       "  '5',\n",
       "  '0',\n",
       "  '1',\n",
       "  '10',\n",
       "  '10',\n",
       "  '5',\n",
       "  '0',\n",
       "  '1',\n",
       "  '10',\n",
       "  '10'],\n",
       " 'track_liveness_can': 0.0346,\n",
       " 'track_liveness_pl': [0.128,\n",
       "  0.0725,\n",
       "  0.191,\n",
       "  0.105,\n",
       "  0.0552,\n",
       "  0.128,\n",
       "  0.0725,\n",
       "  0.191,\n",
       "  0.105,\n",
       "  0.0552,\n",
       "  0.128,\n",
       "  0.0725,\n",
       "  0.191,\n",
       "  0.105,\n",
       "  0.0552],\n",
       " 'track_loudness_can': -7.295,\n",
       " 'track_loudness_pl': [-8.638,\n",
       "  -8.754,\n",
       "  -9.084,\n",
       "  -7.04,\n",
       "  -6.694,\n",
       "  -8.638,\n",
       "  -8.754,\n",
       "  -9.084,\n",
       "  -7.04,\n",
       "  -6.694,\n",
       "  -8.638,\n",
       "  -8.754,\n",
       "  -9.084,\n",
       "  -7.04,\n",
       "  -6.694],\n",
       " 'track_mode_can': '1',\n",
       " 'track_mode_pl': ['0',\n",
       "  '1',\n",
       "  '1',\n",
       "  '0',\n",
       "  '1',\n",
       "  '0',\n",
       "  '1',\n",
       "  '1',\n",
       "  '0',\n",
       "  '1',\n",
       "  '0',\n",
       "  '1',\n",
       "  '1',\n",
       "  '0',\n",
       "  '1'],\n",
       " 'track_name_can': 'Bezouro Preto - Studio',\n",
       " 'track_name_pl': ['O Telefone Tocou Novamente',\n",
       "  'Bem Devagar - Studio',\n",
       "  'Angola Dream',\n",
       "  'Janaina',\n",
       "  'Louco Berimbau - Studio',\n",
       "  'O Telefone Tocou Novamente',\n",
       "  'Bem Devagar - Studio',\n",
       "  'Angola Dream',\n",
       "  'Janaina',\n",
       "  'Louco Berimbau - Studio',\n",
       "  'O Telefone Tocou Novamente',\n",
       "  'Bem Devagar - Studio',\n",
       "  'Angola Dream',\n",
       "  'Janaina',\n",
       "  'Louco Berimbau - Studio'],\n",
       " 'track_pop_can': 3.0,\n",
       " 'track_pop_pl': [5.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  5.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  5.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.0],\n",
       " 'track_speechiness_can': 0.0802,\n",
       " 'track_speechiness_pl': [0.0367,\n",
       "  0.0272,\n",
       "  0.0407,\n",
       "  0.132,\n",
       "  0.0734,\n",
       "  0.0367,\n",
       "  0.0272,\n",
       "  0.0407,\n",
       "  0.132,\n",
       "  0.0734,\n",
       "  0.0367,\n",
       "  0.0272,\n",
       "  0.0407,\n",
       "  0.132,\n",
       "  0.0734],\n",
       " 'track_tempo_can': 172.238,\n",
       " 'track_tempo_pl': [100.039,\n",
       "  89.089,\n",
       "  123.999,\n",
       "  119.963,\n",
       "  119.214,\n",
       "  100.039,\n",
       "  89.089,\n",
       "  123.999,\n",
       "  119.963,\n",
       "  119.214,\n",
       "  100.039,\n",
       "  89.089,\n",
       "  123.999,\n",
       "  119.963,\n",
       "  119.214],\n",
       " 'track_uri_can': 'spotify:track:0tlhK4OvpHCYpReTABvKFb',\n",
       " 'track_uri_pl': ['spotify:track:1pQkOdcTDfLr84TDCrmGy7',\n",
       "  'spotify:track:39grEDsAHAjmo2QFo4G8D9',\n",
       "  'spotify:track:5vxSLdJXqbKYH487YO8LSL',\n",
       "  'spotify:track:6T9GbmZ6voDM4aTBsG5VDh',\n",
       "  'spotify:track:7ELt9eslVvWo276pX2garN',\n",
       "  'spotify:track:1pQkOdcTDfLr84TDCrmGy7',\n",
       "  'spotify:track:39grEDsAHAjmo2QFo4G8D9',\n",
       "  'spotify:track:5vxSLdJXqbKYH487YO8LSL',\n",
       "  'spotify:track:6T9GbmZ6voDM4aTBsG5VDh',\n",
       "  'spotify:track:7ELt9eslVvWo276pX2garN',\n",
       "  'spotify:track:1pQkOdcTDfLr84TDCrmGy7',\n",
       "  'spotify:track:39grEDsAHAjmo2QFo4G8D9',\n",
       "  'spotify:track:5vxSLdJXqbKYH487YO8LSL',\n",
       "  'spotify:track:6T9GbmZ6voDM4aTBsG5VDh',\n",
       "  'spotify:track:7ELt9eslVvWo276pX2garN'],\n",
       " 'track_valence_can': 0.844,\n",
       " 'track_valence_pl': [0.966,\n",
       "  0.667,\n",
       "  0.696,\n",
       "  0.876,\n",
       "  0.655,\n",
       "  0.966,\n",
       "  0.667,\n",
       "  0.696,\n",
       "  0.876,\n",
       "  0.655,\n",
       "  0.966,\n",
       "  0.667,\n",
       "  0.696,\n",
       "  0.876,\n",
       "  0.655]}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_name = \"local-jt-test-instance.pkl\"\n",
    "BUCKET_TEST = 'spotify-data-regimes'\n",
    "PREFIX = 'jtv15-8m'\n",
    "TEST_GCS_OBJ = f'{PREFIX}/{LOCAL_TEST_INSTANCE}'\n",
    "\n",
    "bucket = storage_client.bucket(BUCKET_TEST)\n",
    "blob = bucket.blob(TEST_GCS_OBJ)\n",
    "blob.download_to_filename(local_name)\n",
    "\n",
    "filehandler = open(f'{local_name}', 'rb')\n",
    "loaded_test_dict = pkl.load(filehandler)\n",
    "filehandler.close()\n",
    "\n",
    "loaded_test_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60eb5dfe-23a4-4c04-b2e3-cfba63e7c490",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-10.m103",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-10:m103"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
