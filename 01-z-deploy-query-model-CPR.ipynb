{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d8757f6-8f36-4b0d-8c37-ba31a1773f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install google-cloud-aiplatform[prediction]>=1.16.0 fastapi --user"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201e92f9-980b-43e9-8c76-b2aa02260e96",
   "metadata": {},
   "source": [
    "# Sklearn with Pandas - Custom Prediction Routine to get Merlin Model predictions\n",
    "\n",
    "Your output should look like this - you are going to use the query model endpoint to create a CPR Endpoing\n",
    "\n",
    "![](img/merlin-bucket.png)\n",
    "\n",
    "This is similar to [the other notebook](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/ml_ops/stage6/get_started_with_cpr.ipynb) except we will be using pandas and bigquery\n",
    "\n",
    "Topics covered\n",
    "* Training sklearn locally, deploying to endpoint\n",
    "* Saving data as CSV and doing batch predict from GCS\n",
    "* Loading data to BQ, using BQ magics\n",
    "* Running a batch prediction from BQ to BQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b65e26b-31d7-459e-8f80-92e2c206fc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !gsutil mb -l us-central1 gs://wortz-project-bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cbe8a06-ed15-40ac-bc2f-d387bb406301",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "\n",
    "PROJECT = 'hybrid-vertex'  # <--- TODO: CHANGE THIS\n",
    "REGION = 'us-central1' \n",
    "BUCKET = 'gs://spotify-beam-v3'\n",
    "REPOSITORY = 'merlin-spotify-cpr'\n",
    "ARTIFACT_URI = f'{BUCKET}/merlin-processed'\n",
    "MODEL_DIR = f'{BUCKET}/merlin-processed/query_model_merlin'\n",
    "PREFIX = 'merlin-spotify'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2831cdd6-8d18-40aa-88ba-50889c3624a0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# New section - preprocessor creation.\n",
    "\n",
    "In this section we will create a pipeline object that stores a standard scaler \n",
    "using the `PipeLine` class is important as it provides a lot of flexibility and conforms to sklearn's framework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c749d4-18ca-46a0-91f9-432dbf1220e3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create a generic sklearn container that returns instances\n",
    "\n",
    "https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/ml_ops/stage6/get_started_with_cpr.ipynb\n",
    "\n",
    "**highly recommend reviewing this notebook first as it breaks down the custom predictor interface**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bf429ed-e23c-47ac-93da-8a70c4e3e92a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! rm -rf container_code\n",
    "! mkdir container_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4af962a7-3196-4b9a-9c36-3f35fd525b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing container_code/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile container_code/requirements.txt\n",
    "fastapi\n",
    "uvicorn==0.17.6\n",
    "google-cloud-aiplatform[prediction]>=1.16.0\n",
    "typing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0c1627-edf8-4879-b1eb-10f28e19cb7a",
   "metadata": {},
   "source": [
    "### CPR Template from here https://cloud.google.com/vertex-ai/docs/predictions/custom-prediction-routines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d80c477-d403-4d39-9220-98d90511a180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting container_code/predictor.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile container_code/predictor.py\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Any\n",
    "from google.cloud.aiplatform.prediction.predictor import Predictor\n",
    "#libs from base image (link to build in notebook 01 top)\n",
    "import nvtabular as nvt\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import merlin.models.tf as mm\n",
    "from nvtabular.loader.tf_utils import configure_tensorflow\n",
    "configure_tensorflow()\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class Predictor(Predictor):\n",
    "    \"\"\"Interface of the Predictor class for Custom Prediction Routines.\n",
    "    The Predictor is responsible for the ML logic for processing a prediction request.\n",
    "    Specifically, the Predictor must define:\n",
    "    (1) How to load all model artifacts used during prediction into memory.\n",
    "    (2) The logic that should be executed at predict time.\n",
    "    When using the default PredictionHandler, the Predictor will be invoked as follows:\n",
    "      predictor.postprocess(predictor.predict(predictor.preprocess(prediction_input)))\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        return\n",
    "    \n",
    "    def load(self, artifacts_uri: str) -> None:\n",
    "        \"\"\"Loads the model artifact.\n",
    "        Args:\n",
    "            artifacts_uri (str):\n",
    "                Required. The value of the environment variable AIP_STORAGE_URI.\n",
    "        \"\"\"\n",
    "        self._model = tf.keras.models.load_model(os.path.join(artifacts_uri, \"query_model_merlin\" ))\n",
    "        self._workflow = nvt.Workflow.load(os.path.join(artifacts_uri, \"workflow/2t-spotify-workflow\"))\n",
    "        self._workflow.remove_inputs(['track_pop_can', 'track_uri_can', \n",
    "                            'duration_ms_can', 'track_name_can', \n",
    "                            'artist_name_can','album_name_can',\n",
    "                            'album_uri_can','artist_followers_can',\n",
    "                            'artist_genres_can','artist_name_can',\n",
    "                            'artist_pop_can','artist_pop_pl','artist_uri_can',\n",
    "                            'artists_followers_pl',])            \n",
    "        \n",
    "    def preprocess(self, prediction_input: Any) -> Any:\n",
    "        \"\"\"Preprocesses the prediction input before doing the prediction.\n",
    "        Args:\n",
    "            prediction_input (Any):\n",
    "                Required. The prediction input that needs to be preprocessed.\n",
    "        Returns:\n",
    "            The preprocessed prediction input.\n",
    "        \"\"\"\n",
    "        dict_input = json.loads(prediction_input)\n",
    "        #handle different input types, can take a dict or list of dicts\n",
    "        if type(dict_input) == list:\n",
    "            pandas_instance = pd.DataFrame.from_dict(dict_input[0], orient='index').T\n",
    "            if len(dict_input) > 1:\n",
    "                for ti in dict_input[0:]:\n",
    "                    pandas_instance = pandas_instance.append(pd.DataFrame.from_dict(ti, orient='index').T)\n",
    "        if type(dict_input) == dict:\n",
    "            pandas_instance = pd.DataFrame.from_dict(dict_input, orient='index').T\n",
    "        else:\n",
    "            raise Exception(\"Data must be provided as a dict or list of dicts\")\n",
    "\n",
    "        transformed_inputs = nvt.Dataset(pandas_instance)\n",
    "        transformed_instance = self._workflow.transform(transformed_inputs)\n",
    "        batch = mm.sample_batch(transformed_instance, batch_size=128, include_targets=False)\n",
    "        return batch\n",
    "\n",
    "    def predict(self, instances: Any) -> Any:\n",
    "        \"\"\"Performs prediction.\n",
    "        Args:\n",
    "            instances (Any):\n",
    "                Required. The instance(s) used for performing prediction.\n",
    "        Returns:\n",
    "            Prediction results.\n",
    "        \"\"\"\n",
    "        predictions = self._model(instances)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a17737-58ad-4819-9b2d-72de6fd6e827",
   "metadata": {},
   "source": [
    "### Build and push container to Artifact Registry\n",
    "#### Build your container\n",
    "To build a custom container, we also need to write an entrypoint of the image that starts the model server. However, with the Custom Prediction Routine feature, you don't need to write the entrypoint anymore. Vertex AI SDK will populate the entrypoint with the custom predictor you provide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e72acda9-ae76-4ce9-8600-71238d545f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mERROR:\u001b[0m (gcloud.beta.artifacts.repositories.create) ALREADY_EXISTS: the repository already exists\n"
     ]
    }
   ],
   "source": [
    "# Create the repo if needed for the artifacts\n",
    "\n",
    "! gcloud beta artifacts repositories create {REPOSITORY} \\\n",
    "    --repository-format=docker \\\n",
    "    --location=$REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1858a130-739b-4752-bf02-d0e8356a0829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;33mWARNING:\u001b[0m Your config file at [/root/.docker/config.json] contains these credential helper entries:\n",
      "\n",
      "{\n",
      "  \"credHelpers\": {\n",
      "    \"us-central1-docker.pkg.dev\": \"gcloud\"\n",
      "  }\n",
      "}\n",
      "Adding credentials for: us-central1-docker.pkg.dev\n",
      "gcloud credential helpers already registered correctly.\n"
     ]
    }
   ],
   "source": [
    "! gcloud auth configure-docker {REGION}-docker.pkg.dev --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9af44323-4e94-43d0-9dc8-649879be77dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install google-cloud-aiplatform[prediction]>=1.16.0 --user\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "05bf27d9-03c8-46e7-9dd3-45b6586f1316",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.8/subprocess.py:842: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used\n",
      "  self.stdin = io.open(p2cwrite, 'wb', bufsize)\n",
      "/usr/lib/python3.8/subprocess.py:848: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used\n",
      "  self.stdout = io.open(c2pread, 'rb', bufsize)\n"
     ]
    },
    {
     "ename": "DockerError",
     "evalue": "('\\nDocker failed with error code 1.\\nCommand: docker build -t us-central1-docker.pkg.dev/hybrid-vertex/merlin-spotify-cpr/merlin-prediction-cpr --rm -f- container_code\\n', ['docker', 'build', '-t', 'us-central1-docker.pkg.dev/hybrid-vertex/merlin-spotify-cpr/merlin-prediction-cpr', '--rm', '-f-', 'container_code'], 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDockerError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [21], line 9\u001b[0m\n\u001b[1;32m      5\u001b[0m base_image \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mus-central1-docker.pkg.dev/hybrid-vertex/workbench/merlin-tensorflow-22.09:latest\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      7\u001b[0m SERVER_IMAGE \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmerlin-prediction-cpr\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# @param {type:\"string\"} \u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m local_model \u001b[38;5;241m=\u001b[39m LocalModel\u001b[38;5;241m.\u001b[39mbuild_cpr_model(\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontainer_code\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mREGION\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-docker.pkg.dev/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mPROJECT\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mREPOSITORY\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mSERVER_IMAGE\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     12\u001b[0m     predictor\u001b[38;5;241m=\u001b[39mPredictor,\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# base_image=base_image,\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     requirements_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontainer_code/requirements.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     15\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/google/cloud/aiplatform/prediction/local_model.py:289\u001b[0m, in \u001b[0;36mLocalModel.build_cpr_model\u001b[0;34m(cls, src_dir, output_image_uri, predictor, handler, base_image, requirements_path, extra_packages, no_cache)\u001b[0m\n\u001b[1;32m    284\u001b[0m     environment_variables[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPREDICTOR_CLASS\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m predictor_class\n\u001b[1;32m    286\u001b[0m is_prebuilt_prediction_image \u001b[38;5;241m=\u001b[39m helpers\u001b[38;5;241m.\u001b[39mis_prebuilt_prediction_container_uri(\n\u001b[1;32m    287\u001b[0m     base_image\n\u001b[1;32m    288\u001b[0m )\n\u001b[0;32m--> 289\u001b[0m _ \u001b[38;5;241m=\u001b[39m \u001b[43mbuild\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_image\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_image\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43msrc_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_image_uri\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpython_module\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_DEFAULT_PYTHON_MODULE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequirements_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequirements_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mextra_requirements\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_DEFAULT_SDK_REQUIREMENTS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43mextra_packages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_packages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexposed_ports\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mDEFAULT_HTTP_PORT\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[43menvironment_variables\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menvironment_variables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpip_command\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpip3\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_prebuilt_prediction_image\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpip\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpython_command\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpython3\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_prebuilt_prediction_image\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpython\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[43m    \u001b[49m\u001b[43mno_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mno_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    304\u001b[0m container_spec \u001b[38;5;241m=\u001b[39m gca_model_compat\u001b[38;5;241m.\u001b[39mModelContainerSpec(\n\u001b[1;32m    305\u001b[0m     image_uri\u001b[38;5;241m=\u001b[39moutput_image_uri,\n\u001b[1;32m    306\u001b[0m     predict_route\u001b[38;5;241m=\u001b[39mDEFAULT_PREDICT_ROUTE,\n\u001b[1;32m    307\u001b[0m     health_route\u001b[38;5;241m=\u001b[39mDEFAULT_HEALTH_ROUTE,\n\u001b[1;32m    308\u001b[0m )\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(serving_container_spec\u001b[38;5;241m=\u001b[39mcontainer_spec)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/google/cloud/aiplatform/docker_utils/build.py:548\u001b[0m, in \u001b[0;36mbuild_image\u001b[0;34m(base_image, host_workdir, output_image_name, python_module, requirements_path, extra_requirements, setup_path, extra_packages, container_workdir, container_home, extra_dirs, exposed_ports, pip_command, python_command, no_cache, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    540\u001b[0m     error_msg \u001b[38;5;241m=\u001b[39m textwrap\u001b[38;5;241m.\u001b[39mdedent(\n\u001b[1;32m    541\u001b[0m         \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    542\u001b[0m \u001b[38;5;124;03m        Docker failed with error code {code}.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    546\u001b[0m         )\n\u001b[1;32m    547\u001b[0m     )\n\u001b[0;32m--> 548\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DockerError(error_msg, command, return_code)\n",
      "\u001b[0;31mDockerError\u001b[0m: ('\\nDocker failed with error code 1.\\nCommand: docker build -t us-central1-docker.pkg.dev/hybrid-vertex/merlin-spotify-cpr/merlin-prediction-cpr --rm -f- container_code\\n', ['docker', 'build', '-t', 'us-central1-docker.pkg.dev/hybrid-vertex/merlin-spotify-cpr/merlin-prediction-cpr', '--rm', '-f-', 'container_code'], 1)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from container_code.predictor import Predictor\n",
    "\n",
    "from google.cloud.aiplatform.prediction import LocalModel\n",
    "base_image = 'us-central1-docker.pkg.dev/hybrid-vertex/workbench/merlin-tensorflow-22.09:latest'\n",
    "\n",
    "SERVER_IMAGE = \"merlin-prediction-cpr\"  # @param {type:\"string\"} \n",
    "\n",
    "local_model = LocalModel.build_cpr_model(\n",
    "    \"container_code\",\n",
    "    f\"{REGION}-docker.pkg.dev/{PROJECT}/{REPOSITORY}/{SERVER_IMAGE}\",\n",
    "    predictor=Predictor,\n",
    "    # base_image=base_image,\n",
    "    requirements_path=\"container_code/requirements.txt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80330616-933a-4ec9-9322-2dd5815c4947",
   "metadata": {},
   "source": [
    "### Test it out with a locally deployed endpoint\n",
    "Need to generate credentials to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "a01981a9-6972-4eb9-b7ba-18689a31b90f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "image_uri: \"us-central1-docker.pkg.dev/hybrid-vertex/merlin-spotify-cpr/merlin-prediction-cpr\"\n",
       "predict_route: \"/predict\"\n",
       "health_route: \"/health\""
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_model.get_serving_container_spec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "fae134d5-92da-48b5-afdb-7a330c88ace0",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_INSTANCE = {'collaborative': 'false',\n",
    "                 'album_name_pl': [\"There's Really A Wolf\", 'Late Nights: The Album',\n",
    "                       'American Teen', 'Crazy In Love', 'Pony'], \n",
    "                 # 'album_uri_can': 'spotify:album:5l83t3mbVgCrIe1VU9uJZR', \n",
    "                 # 'artist_followers_can': 4339757.0, \n",
    "                 # 'artist_genres_can': \"'hawaiian hip hop', 'rap'\", \n",
    "                 'artist_genres_pl': [\"'hawaiian hip hop', 'rap'\",\n",
    "                       \"'chicago rap', 'dance pop', 'pop', 'pop rap', 'r&b', 'southern hip hop', 'trap', 'urban contemporary'\",\n",
    "                       \"'pop', 'pop r&b'\", \"'dance pop', 'pop', 'r&b'\",\n",
    "                       \"'chill r&b', 'pop', 'pop r&b', 'r&b', 'urban contemporary'\"], \n",
    "                 # 'artist_name_can': 'Russ', \n",
    "                 'artist_name_pl': ['Russ', 'Jeremih', 'Khalid', 'Beyonc\\xc3\\xa9',\n",
    "                       'William Singe'], \n",
    "                 # 'artist_pop_can': 82.0, \n",
    "                 # 'artist_pop_pl': [82., 80., 90., 87., 65.], \n",
    "                 # 'artist_uri_can': 'spotify:artist:1z7b1Pr1rSlvWRzsW3HOrS', \n",
    "                 # 'artists_followers_pl': [ 4339757.,  5611842., 15046756., 30713126.,   603837.],  \n",
    "                 'description_pl': '', \n",
    "                 # 'duration_ms_can': 237322.0, \n",
    "                 #'duration_ms_songs_pl': [237506., 217200., 219080., 226400., 121739.], \n",
    "                 'n_songs_pl': 8.0, \n",
    "                 'name': 'Lit Tunes ', \n",
    "                 'num_albums_pl': 8.0, \n",
    "                 'num_artists_pl': 8.0, \n",
    "                 # 'track_name_can': 'We Just Havent Met Yet', \n",
    "                 'track_name_pl': ['Losin Control', 'Paradise', 'Location',\n",
    "                       'Crazy In Love - Remix', 'Pony'], \n",
    "                 # 'track_pop_can': 57.0, \n",
    "                 #'track_pop_pl': [79., 58., 83., 71., 57.],\n",
    "                 'duration_ms_seed_pl': 51023.1,\n",
    "                 'pid': 1,\n",
    "                 # 'track_uri_can': 'spotify:track:0VzDv4wiuZsLsNOmfaUy2W', \n",
    "                 'track_uri_pl': ['spotify:track:4cxMGhkinTocPSVVKWIw0d',\n",
    "                       'spotify:track:1wNEBPo3nsbGCZRryI832I',\n",
    "                       'spotify:track:152lZdxL1OR0ZMW6KquMif',\n",
    "                       'spotify:track:2f4IuijXLxYOeBncS60GUD',\n",
    "                       'spotify:track:4Lj8paMFwyKTGfILLELVxt']\n",
    "                     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "90b03877-2451-4dae-af1d-94e1cb4b4350",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"collaborative\": \"false\", \"album_name_pl\": [\"There\\'s Really A Wolf\", \"Late Nights: The Album\", \"American Teen\", \"Crazy In Love\", \"Pony\"], \"artist_genres_pl\": [\"\\'hawaiian hip hop\\', \\'rap\\'\", \"\\'chicago rap\\', \\'dance pop\\', \\'pop\\', \\'pop rap\\', \\'r&b\\', \\'southern hip hop\\', \\'trap\\', \\'urban contemporary\\'\", \"\\'pop\\', \\'pop r&b\\'\", \"\\'dance pop\\', \\'pop\\', \\'r&b\\'\", \"\\'chill r&b\\', \\'pop\\', \\'pop r&b\\', \\'r&b\\', \\'urban contemporary\\'\"], \"artist_name_pl\": [\"Russ\", \"Jeremih\", \"Khalid\", \"Beyonc\\\\u00c3\\\\u00a9\", \"William Singe\"], \"description_pl\": \"\", \"n_songs_pl\": 8.0, \"name\": \"Lit Tunes \", \"num_albums_pl\": 8.0, \"num_artists_pl\": 8.0, \"track_name_pl\": [\"Losin Control\", \"Paradise\", \"Location\", \"Crazy In Love - Remix\", \"Pony\"], \"duration_ms_seed_pl\": 51023.1, \"pid\": 1, \"track_uri_pl\": [\"spotify:track:4cxMGhkinTocPSVVKWIw0d\", \"spotify:track:1wNEBPo3nsbGCZRryI832I\", \"spotify:track:152lZdxL1OR0ZMW6KquMif\", \"spotify:track:2f4IuijXLxYOeBncS60GUD\", \"spotify:track:4Lj8paMFwyKTGfILLELVxt\"]}'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "json_instance = json.dumps(TEST_INSTANCE)\n",
    "json_instance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c322126-7fca-4e71-ab14-96bd3facee99",
   "metadata": {},
   "source": [
    "### Generate credentials - use your \n",
    "\n",
    "Go to the console and search \"Service Accounts\" from there - select your compute account\n",
    "\n",
    "Then add a json key and upload back to this notebook, then cange the filename for `CREDENTIALS_FILE` below\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "a657cad4-c000-4539-b4c9-b853af79846b",
   "metadata": {},
   "outputs": [],
   "source": [
    "CREDENTIALS_FILE = \"hybrid-vertex-983c0966dff8.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "9968407e-f579-44ab-ab1f-5b47db30c7eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:google.cloud.aiplatform.prediction.local_endpoint:Exception during starting serving: ('The health check never succeeded.', '', 1).\n",
      "ERROR:google.cloud.aiplatform.prediction.local_endpoint:Exception during entering a context: ('The health check never succeeded.', '', 1).\n"
     ]
    },
    {
     "ename": "DockerError",
     "evalue": "('The health check never succeeded.', '', 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDockerError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_19691/2697002770.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m with local_model.deploy_to_local_endpoint(\n\u001b[1;32m      2\u001b[0m     \u001b[0martifact_uri\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mARTIFACT_URI\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     credential_path=CREDENTIALS_FILE) as local_endpoint:\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mhealth_check_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlocal_endpoint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_health_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlocal_endpoint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_instance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/google/cloud/aiplatform/prediction/local_endpoint.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0;34m\"\"\"Enters the runtime context related to this object.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m             \u001b[0m_logger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Exception during entering a context: {exception}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/google/cloud/aiplatform/prediction/local_endpoint.py\u001b[0m in \u001b[0;36mserve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    274\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontainer_is_running\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0;31m# Waits until the model server starts.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_until_health_check_succeeds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m             \u001b[0m_logger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Exception during starting serving: {exception}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/google/cloud/aiplatform/prediction/local_endpoint.py\u001b[0m in \u001b[0;36m_wait_until_health_check_succeeds\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    339\u001b[0m                 \u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Health check never succeeds, all container logs:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m             )\n\u001b[0;32m--> 341\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mDockerError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The health check never succeeded.\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_stop_container_if_exists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDockerError\u001b[0m: ('The health check never succeeded.', '', 1)"
     ]
    }
   ],
   "source": [
    "with local_model.deploy_to_local_endpoint(\n",
    "    artifact_uri=ARTIFACT_URI,\n",
    "    credential_path=CREDENTIALS_FILE) as local_endpoint:\n",
    "    health_check_response = local_endpoint.run_health_check()\n",
    "    prediction = local_endpoint.predict(json_instance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efa05b7-4303-4655-9cc0-069be8bcf969",
   "metadata": {},
   "source": [
    "#### Only run once to generate creds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a979a4-48a8-4453-90a3-3629957bd878",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Upload the model to Vertex using new Prediction Route Serving Container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59eb3650-759a-40a1-bd2f-704daad82410",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "local_model.push_image() #push to container registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d8e71d-9702-4c7d-bd15-925278f759d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "model = local_model.upload(\n",
    "        display_name='merlin spotify query model',\n",
    "        artifact_uri=ARTIFACT_URI,\n",
    "        description='two tower model using merlin models with spotify data',\n",
    "        labels= {'version': 'v1_00'}, \n",
    "              \n",
    "        sync=True, #false will not bind up your notebook instance with the creation operation\n",
    "    ) \n",
    "# model = aiplatform.Model('projects/679926387543/locations/us-central1/models/5966834099661307904')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "01bee664-c1bd-438a-b8e3-6bb2f09129a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Endpoint\n",
      "Create Endpoint backing LRO: projects/679926387543/locations/us-central1/endpoints/7051678242322776064/operations/1548668243755925504\n",
      "Endpoint created. Resource name: projects/679926387543/locations/us-central1/endpoints/7051678242322776064\n",
      "To use this Endpoint in another session:\n",
      "endpoint = aiplatform.Endpoint('projects/679926387543/locations/us-central1/endpoints/7051678242322776064')\n",
      "Deploying model to Endpoint : projects/679926387543/locations/us-central1/endpoints/7051678242322776064\n",
      "Deploy Endpoint model backing LRO: projects/679926387543/locations/us-central1/endpoints/7051678242322776064/operations/5585582359740153856\n",
      "Endpoint model deployed. Resource name: projects/679926387543/locations/us-central1/endpoints/7051678242322776064\n"
     ]
    }
   ],
   "source": [
    "endpoint = model.deploy(machine_type=\"n1-standard-4\")\n",
    "# endpoint = aiplatform.Endpoint('projects/679926387543/locations/us-central1/endpoints/8555880517864521728')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2ca04949-78d9-4885-8b11-069bb12f9e2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(predictions=[[0.79, 0.21], [0.24, 0.76]], deployed_model_id='2882294965424095232', explanations=None)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "endpoint.predict(instances=[[47.7, 83.1, 38.7], [53.6, 76.1, 24.]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ecbb6a-059e-4dee-bcb7-9974a518965c",
   "metadata": {},
   "source": [
    "# You should be able to see the logging ops by searching for `aiplatform.googleapis.com`\n",
    "+ Make sure you click `show query` slider in case there are other limitations\n",
    "![](images/log_example.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f8328221-1c7c-4c65-bf71-59b59b75a7b3",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Model' object has no attribute 'predict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_25239/4292456183.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0minstances_formatted_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m predict_response = model.predict(\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mrequest_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minstances_formatted_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"Content-Type\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"application/json\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Model' object has no attribute 'predict'"
     ]
    }
   ],
   "source": [
    "df2 = pd.DataFrame(np.random.randint(0.0,100.0,size=(10,3)), # we will do batch predictions based on this\n",
    "              index=range(10,20),\n",
    "              columns=['col1','col2','col3'],\n",
    "              dtype='float64')\n",
    "\n",
    "instances_formatted_data = df2.to_numpy().tolist()\n",
    "\n",
    "predict_response = model.predict(\n",
    "        request_file=instances_formatted_data,\n",
    "        headers={\"Content-Type\": \"application/json\"},\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c98f67d-b47e-43de-a41e-0e927bff6e43",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Expected output\n",
    "From documentation:\n",
    "```\n",
    "array([[0.8 , 0.2 ],\n",
    "       [0.38, 0.62],\n",
    "       [0.61, 0.39],\n",
    "       [0.65, 0.35],\n",
    "       [0.56, 0.44],\n",
    "       [0.63, 0.37],\n",
    "       [0.55, 0.45],\n",
    "       [0.43, 0.57],\n",
    "       [0.43, 0.57],\n",
    "       [0.38, 0.62]])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4994f47d-37e8-4992-9c38-762a713818b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "import csv\n",
    "\n",
    "# save the csv with the header, no index\n",
    "df2.to_csv('df2.csv', index=False)\n",
    "\n",
    "data_directory = BUCKET + \"/data\"\n",
    "storage_path = os.path.join(data_directory, 'df2.csv')\n",
    "blob = storage.blob.Blob.from_string(storage_path, client=storage.Client())\n",
    "blob.upload_from_filename(\"df2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a10c6c-18c1-4bf8-9641-61e4e1fe4602",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_prediction_job = model.batch_predict(\n",
    "        job_display_name='pandas batch predict job sklearn - VALUES JSON',\n",
    "        gcs_source=storage_path,\n",
    "        gcs_destination_prefix=BUCKET+\"/predictions\",\n",
    "        machine_type='n1-standard-2',\n",
    "        instances_format='csv', #This is key to parsing CSV input\n",
    "        # accelerator_count=accelerator_count,\n",
    "        # accelerator_type=accelerator_type, #if you want gpus\n",
    "        starting_replica_count=1,\n",
    "        max_replica_count=2,\n",
    "        sync=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903dc3b1-3458-447d-ab53-a25774d6c2d6",
   "metadata": {},
   "source": [
    "### When successful you should see this\n",
    "```\n",
    "{\"instance\": [16.0, 64.0, 61.0], \"prediction\": [0.63, 0.37]}\n",
    "{\"instance\": [83.0, 27.0, 87.0], \"prediction\": [0.35, 0.65]}\n",
    "{\"instance\": [96.0, 83.0, 57.0], \"prediction\": [0.68, 0.32]}\n",
    "{\"instance\": [11.0, 62.0, 17.0], \"prediction\": [0.89, 0.11]}\n",
    "{\"instance\": [61.0, 28.0, 1.0], \"prediction\": [0.36, 0.64]}\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m98",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m98"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
