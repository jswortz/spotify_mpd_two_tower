{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1df6c870-747a-44c2-854e-bcd5ed14f320",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3148ce99-d7d0-4298-a3d3-4ea5a1aee30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import argparse\n",
    "import gcsfs\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import apache_beam as beam\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "from apache_beam.options.pipeline_options import SetupOptions\n",
    "\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1176b3bf-c98f-4d44-a410-31f816b1998d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! gsutil mb -l us-central1 gs://spotify-beam-v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95d4dc78-ea0b-48ee-a712-673429bac525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup\n",
    "PROJECT_ID = 'hybrid-vertex'\n",
    "BUCKET_NAME = 'spotify-beam-v3' # 'spotify-tfrecords-blog' # Set your Bucket name\n",
    "REGION = 'us-central1' # Set the region for Dataflow jobs\n",
    "VERSION = 'v3'\n",
    "\n",
    "# storage\n",
    "ROOT = f'gs://{BUCKET_NAME}/{VERSION}'\n",
    "\n",
    "DATA_DIR = ROOT + '/data/' # Location to store data\n",
    "STATS_DIR = ROOT +'/stats/' # Location to store stats \n",
    "STAGING_DIR = ROOT + '/job/staging/' # Dataflow staging directory on GCP\n",
    "TEMP_DIR =  ROOT + '/job/temp/' # Dataflow temporary directory on GCP\n",
    "TF_RECORD_DIR = ROOT + '/tf-records/'\n",
    "CANDIDATE_DIR = ROOT + \"/candidates/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a8d1e27-2d7f-4e0a-96c3-0d6d91a64690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Expected TFRecords: 8\n"
     ]
    }
   ],
   "source": [
    "# estimate TF-Record shard count needed\n",
    "# TF-Records\n",
    "total_samples = 2262292  \n",
    "samples_per_file = 300_000 \n",
    "NUM_TF_RECORDS = total_samples // samples_per_file\n",
    "\n",
    "if NUM_TF_RECORDS % total_samples:\n",
    "    NUM_TF_RECORDS += 1\n",
    "    \n",
    "print(\"Number of Expected TFRecords: {}\".format(NUM_TF_RECORDS)) # 5343"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1852644-52f4-4fd2-82e8-901c1ebb1e30",
   "metadata": {},
   "source": [
    "#### Defining Custom DoFnâ€™s\n",
    "\n",
    "> `process()` function receives the yielded output value from the prior block as the input argument value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953460d2-ca00-4687-88bb-ee597532b16f",
   "metadata": {},
   "source": [
    "## Candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8173c149-161d-4ce2-96f1-c36b2123fe70",
   "metadata": {},
   "source": [
    "`apache_beam.io.gcp.bigquery.ReadFromBigQuery` ?\n",
    "* [docs](https://beam.apache.org/releases/pydoc/2.24.0/apache_beam.io.gcp.bigquery.html?highlight=readfrombigquery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d9edf669-67b4-4e6c-bb14-49ed8e6d24d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_pipe(args):\n",
    "    \n",
    "    BQ_TABLE = args['bq_source_table']\n",
    "    CANDIDATE_SINK = args['candidate_sink']\n",
    "    RUNNER = args['runner']\n",
    "    NUM_TF_RECORDS = args['num_candidate_tfrecords']\n",
    "    \n",
    "    # Convert rows to tf-example\n",
    "    _to_tf_example = candidates_to_tfexample(mode='candidates')\n",
    "    \n",
    "    # Write serialized example to tfrecords\n",
    "    write_to_tf_record = beam.io.WriteToTFRecord(\n",
    "        file_path_prefix = f'{CANDIDATE_SINK}/candidate-tracks', \n",
    "        file_name_suffix=\".tfrecords\",\n",
    "        num_shards=NUM_TF_RECORDS\n",
    "    )\n",
    "    \n",
    "    pipeline_options = beam.options.pipeline_options.GoogleCloudOptions(**args)\n",
    "    pipeline_options.view_as(SetupOptions).save_main_session = True\n",
    "    print(pipeline_options)\n",
    "\n",
    "    with beam.Pipeline(RUNNER, options=pipeline_options) as pipeline:\n",
    "        (pipeline \n",
    "         | \"Read from BigQuery\">> beam.io.Read(beam.io.BigQuerySource(query=args['source_query'], flatten_results=True))\n",
    "         | 'Convert to tf Example' >> beam.ParDo(_to_tf_example)\n",
    "         | 'Serialize to String' >> beam.Map(lambda example: example.SerializeToString(deterministic=True))\n",
    "         | \"Write as TFRecords to GCS\" >> write_to_tf_record\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "880e9bb8-c550-414b-9b65-b2124837e937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline args are set to:\n",
      "{'bq_source_table': 'hybrid-vertex:mdp_eda_test.candidates',\n",
      " 'candidate_sink': 'gs://spotify-beam-v3/v3/candidates/',\n",
      " 'job_name': 'spotify-bq-tfrecords-v3-220920-163929',\n",
      " 'network': 'ucaip-haystack-vpc-network',\n",
      " 'num_candidate_tfrecords': 8,\n",
      " 'project': 'hybrid-vertex',\n",
      " 'region': 'us-central1',\n",
      " 'runner': 'DataflowRunner',\n",
      " 'save_main_session': True,\n",
      " 'setup_file': 'beam-candidates/setup.py',\n",
      " 'source_query': 'SELECT * FROM hybrid-vertex.mdp_eda_test.candidates',\n",
      " 'staging_location': 'gs://spotify-beam-v3/v3/job/staging/',\n",
      " 'temp_location': 'gs://spotify-beam-v3/v3/job/temp/'}\n"
     ]
    }
   ],
   "source": [
    "# setup\n",
    "PROJECT_ID = 'hybrid-vertex'\n",
    "BUCKET_NAME = 'spotify-beam-v3' # 'spotify-tfrecords-blog' # Set your Bucket name\n",
    "REGION = 'us-central1' # Set the region for Dataflow jobs\n",
    "VERSION = 'v3'\n",
    "\n",
    "# Pipeline Params\n",
    "TIMESTAMP = datetime.utcnow().strftime('%y%m%d-%H%M%S')\n",
    "JOB_NAME = f'spotify-bq-tfrecords-{VERSION}-{TIMESTAMP}'\n",
    "MAX_WORKERS = '20'\n",
    "RUNNER = 'DataflowRunner'\n",
    "NETWORK = 'ucaip-haystack-vpc-network'\n",
    "\n",
    "# Source data\n",
    "BQ_TABLE = 'candidates'\n",
    "BQ_DATASET = 'mdp_eda_test'\n",
    "TABLE_SPEC = f'{PROJECT_ID}:{BQ_DATASET}.{BQ_TABLE}' # need \" : \" between project and ds\n",
    "\n",
    "# storage\n",
    "ROOT = f'gs://{BUCKET_NAME}/{VERSION}'\n",
    "\n",
    "DATA_DIR = ROOT + '/data/' # Location to store data\n",
    "STATS_DIR = ROOT +'/stats/' # Location to store stats \n",
    "STAGING_DIR = ROOT + '/job/staging/' # Dataflow staging directory on GCP\n",
    "TEMP_DIR =  ROOT + '/job/temp/' # Dataflow temporary directory on GCP\n",
    "TF_RECORD_DIR = ROOT + '/tf-records/'\n",
    "CANDIDATE_DIR = ROOT + \"/candidates/\"\n",
    "\n",
    "QUERY = f\"SELECT * FROM {PROJECT_ID}.{BQ_DATASET}.{BQ_TABLE}\"\n",
    "\n",
    "NUM_TF_RECORDS = 8\n",
    "\n",
    "args = {\n",
    "    'job_name': JOB_NAME,\n",
    "    'runner': RUNNER,\n",
    "    'source_query': QUERY,\n",
    "    'bq_source_table': TABLE_SPEC,\n",
    "    'network': NETWORK,\n",
    "    'candidate_sink': CANDIDATE_DIR,\n",
    "    'num_candidate_tfrecords': NUM_TF_RECORDS,\n",
    "    'project': PROJECT_ID,\n",
    "    'region': REGION,\n",
    "    'staging_location': STAGING_DIR,\n",
    "    'temp_location': TEMP_DIR,\n",
    "    'save_main_session': True,\n",
    "    'setup_file': 'beam-candidates/setup.py',\n",
    "}\n",
    "print(\"Pipeline args are set to:\")\n",
    "pprint(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e1f4b63c-f8ab-4cc1-b0a5-17b110d102e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/spotify_mpd_two_tower\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7b51fcc5-b073-4419-aef0-44a9a5d9b7af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GoogleCloudOptions(create_from_snapshot=None, dataflow_endpoint=https://dataflow.googleapis.com, dataflow_kms_key=None, dataflow_service_options=None, enable_artifact_caching=False, enable_hot_key_logging=False, enable_streaming_engine=False, flexrs_goal=None, impersonate_service_account=None, job_name=spotify-bq-tfrecords-v3-220920-163929, labels=None, no_auth=False, project=hybrid-vertex, region=us-central1, service_account_email=None, staging_location=gs://spotify-beam-v3/v3/job/staging/, temp_location=gs://spotify-beam-v3/v3/job/temp/, template_location=None, transform_name_mapping=None, update=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: BeamDeprecationWarning: BigQuerySource is deprecated since 2.25.0. Use ReadFromBigQuery instead.\n",
      "warning: sdist: standard file not found: should have one of README, README.rst, README.txt, README.md\n",
      "\n",
      "warning: check: missing required meta-data: url\n",
      "\n",
      "warning: check: missing meta-data: either (author and author_email) or (maintainer and maintainer_email) must be supplied\n",
      "\n",
      "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['-f', '/home/jupyter/.local/share/jupyter/runtime/kernel-f8963804-a2c2-4048-80bc-87ef64b3f9fb.json']\n",
      "WARNING:apache_beam.options.pipeline_options:Discarding invalid overrides: {'source_query': 'SELECT * FROM hybrid-vertex.mdp_eda_test.candidates', 'bq_source_table': 'hybrid-vertex:mdp_eda_test.candidates', 'candidate_sink': 'gs://spotify-beam-v3/v3/candidates/', 'num_candidate_tfrecords': 8}\n",
      "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['-f', '/home/jupyter/.local/share/jupyter/runtime/kernel-f8963804-a2c2-4048-80bc-87ef64b3f9fb.json']\n",
      "WARNING:apache_beam.options.pipeline_options:Discarding invalid overrides: {'source_query': 'SELECT * FROM hybrid-vertex.mdp_eda_test.candidates', 'bq_source_table': 'hybrid-vertex:mdp_eda_test.candidates', 'candidate_sink': 'gs://spotify-beam-v3/v3/candidates/', 'num_candidate_tfrecords': 8}\n",
      "ERROR:apache_beam.runners.dataflow.dataflow_runner:Console URL: https://console.cloud.google.com/dataflow/jobs/<RegionId>/2022-09-20_09_39_43-3352881796352466712?project=<ProjectId>\n"
     ]
    },
    {
     "ename": "DataflowRuntimeException",
     "evalue": "Dataflow pipeline failed. State: FAILED, Error:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.7/site-packages/apache_beam/internal/dill_pickler.py\", line 285, in loads\n    return dill.loads(s)\n  File \"/usr/local/lib/python3.7/site-packages/dill/_dill.py\", line 275, in loads\n    return load(file, ignore, **kwds)\n  File \"/usr/local/lib/python3.7/site-packages/dill/_dill.py\", line 270, in load\n    return Unpickler(file, ignore=ignore, **kwds).load()\n  File \"/usr/local/lib/python3.7/site-packages/dill/_dill.py\", line 472, in load\n    obj = StockUnpickler.load(self)\n  File \"/usr/local/lib/python3.7/site-packages/dill/_dill.py\", line 462, in find_class\n    return StockUnpickler.find_class(self, module, name)\nAttributeError: Can't get attribute '_create_code' on <module 'dill._dill' from '/usr/local/lib/python3.7/site-packages/dill/_dill.py'>\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py\", line 284, in _execute\n    response = task()\n  File \"/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py\", line 357, in <lambda>\n    lambda: self.create_worker().do_instruction(request), request)\n  File \"/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py\", line 598, in do_instruction\n    getattr(request, request_type), request.instruction_id)\n  File \"/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py\", line 629, in process_bundle\n    instruction_id, request.process_bundle_descriptor_id)\n  File \"/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py\", line 462, in get\n    self.data_channel_factory)\n  File \"/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/bundle_processor.py\", line 871, in __init__\n    self.ops = self.create_execution_tree(self.process_bundle_descriptor)\n  File \"/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/bundle_processor.py\", line 928, in create_execution_tree\n    descriptor.transforms, key=topological_height, reverse=True)])\n  File \"/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/bundle_processor.py\", line 927, in <listcomp>\n    get_operation(transform_id))) for transform_id in sorted(\n  File \"/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/bundle_processor.py\", line 814, in wrapper\n    result = cache[args] = func(*args)\n  File \"/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/bundle_processor.py\", line 909, in get_operation\n    pcoll_id in descriptor.transforms[transform_id].outputs.items()\n  File \"/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/bundle_processor.py\", line 909, in <dictcomp>\n    pcoll_id in descriptor.transforms[transform_id].outputs.items()\n  File \"/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/bundle_processor.py\", line 907, in <listcomp>\n    tag: [get_operation(op) for op in pcoll_consumers[pcoll_id]]\n  File \"/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/bundle_processor.py\", line 814, in wrapper\n    result = cache[args] = func(*args)\n  File \"/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/bundle_processor.py\", line 909, in get_operation\n    pcoll_id in descriptor.transforms[transform_id].outputs.items()\n  File \"/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/bundle_processor.py\", line 909, in <dictcomp>\n    pcoll_id in descriptor.transforms[transform_id].outputs.items()\n  File \"/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/bundle_processor.py\", line 907, in <listcomp>\n    tag: [get_operation(op) for op in pcoll_consumers[pcoll_id]]\n  File \"/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/bundle_processor.py\", line 814, in wrapper\n    result = cache[args] = func(*args)\n  File \"/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/bundle_processor.py\", line 909, in get_operation\n    pcoll_id in descriptor.transforms[transform_id].outputs.items()\n  File \"/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/bundle_processor.py\", line 909, in <dictcomp>\n    pcoll_id in descriptor.transforms[transform_id].outputs.items()\n  File \"/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/bundle_processor.py\", line 907, in <listcomp>\n    tag: [get_operation(op) for op in pcoll_consumers[pcoll_id]]\n  File \"/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/bundle_processor.py\", line 814, in wrapper\n    result = cache[args] = func(*args)\n  File \"/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/bundle_processor.py\", line 912, in get_operation\n    transform_id, transform_consumers)\n  File \"/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/bundle_processor.py\", line 1206, in create_operation\n    return creator(self, transform_id, transform_proto, payload, consumers)\n  File \"/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/bundle_processor.py\", line 1560, in create_par_do\n    parameter)\n  File \"/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/bundle_processor.py\", line 1596, in _create_pardo_operation\n    dofn_data = pickler.loads(serialized_fn)\n  File \"/usr/local/lib/python3.7/site-packages/apache_beam/internal/pickler.py\", line 52, in loads\n    encoded, enable_trace=enable_trace, use_zlib=use_zlib)\n  File \"/usr/local/lib/python3.7/site-packages/apache_beam/internal/dill_pickler.py\", line 289, in loads\n    return dill.loads(s)\n  File \"/usr/local/lib/python3.7/site-packages/dill/_dill.py\", line 275, in loads\n    return load(file, ignore, **kwds)\n  File \"/usr/local/lib/python3.7/site-packages/dill/_dill.py\", line 270, in load\n    return Unpickler(file, ignore=ignore, **kwds).load()\n  File \"/usr/local/lib/python3.7/site-packages/dill/_dill.py\", line 472, in load\n    obj = StockUnpickler.load(self)\n  File \"/usr/local/lib/python3.7/site-packages/dill/_dill.py\", line 462, in find_class\n    return StockUnpickler.find_class(self, module, name)\nAttributeError: Can't get attribute '_create_code' on <module 'dill._dill' from '/usr/local/lib/python3.7/site-packages/dill/_dill.py'>\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDataflowRuntimeException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_24253/3839680678.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbeam_pipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_24253/1165543750.py\u001b[0m in \u001b[0;36mbeam_pipe\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     25\u001b[0m          \u001b[0;34m|\u001b[0m \u001b[0;34m'Convert to tf Example'\u001b[0m \u001b[0;34m>>\u001b[0m \u001b[0mbeam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParDo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_tf_example\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m          \u001b[0;34m|\u001b[0m \u001b[0;34m'Serialize to String'\u001b[0m \u001b[0;34m>>\u001b[0m \u001b[0mbeam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mexample\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mexample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerializeToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeterministic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m          \u001b[0;34m|\u001b[0m \u001b[0;34m\"Write as TFRecords to GCS\"\u001b[0m \u001b[0;34m>>\u001b[0m \u001b[0mwrite_to_tf_record\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         )\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/apache_beam/pipeline.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    596\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mexc_type\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_until_finish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    599\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extra_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_tb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/apache_beam/runners/dataflow/dataflow_runner.py\u001b[0m in \u001b[0;36mwait_until_finish\u001b[0;34m(self, duration)\u001b[0m\n\u001b[1;32m   1674\u001b[0m             \u001b[0;34m'Dataflow pipeline failed. State: %s, Error:\\n%s'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1675\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_runner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'last_error_msg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1676\u001b[0;31m             self)\n\u001b[0m\u001b[1;32m   1677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1678\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDataflowRuntimeException\u001b[0m: Dataflow pipeline failed. State: FAILED, Error:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.7/site-packages/apache_beam/internal/dill_pickler.py\", line 285, in loads\n    return dill.loads(s)\n  File \"/usr/local/lib/python3.7/site-packages/dill/_dill.py\", line 275, in loads\n    return load(file, ignore, **kwds)\n  File \"/usr/local/lib/python3.7/site-packages/dill/_dill.py\", line 270, in load\n    return Unpickler(file, ignore=ignore, **kwds).load()\n  File \"/usr/local/lib/python3.7/site-packages/dill/_dill.py\", line 472, in load\n    obj = StockUnpickler.load(self)\n  File \"/usr/local/lib/python3.7/site-packages/dill/_dill.py\", line 462, in find_class\n    return StockUnpickler.find_class(self, module, name)\nAttributeError: Can't get attribute '_create_code' on <module 'dill._dill' from '/usr/local/lib/python3.7/site-packages/dill/_dill.py'>\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py\", line 284, in _execute\n    response = task()\n  File \"/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py\", line 357, in <lambda>\n    lambda: self.create_worker().do_instruction(request), request)\n  File \"/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py\", line 598, in do_instruction\n    getattr(request, request_type), request.instruction_id)\n  File \"/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py\", line 629, in process_bundle\n    instruction_id, request.process_bundle_descriptor_id)\n  File \"/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py\", line 462, in get\n    self.data_channel_factory)\n  File \"/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/bundle_processor.py\", line 871, in __init__\n    self.ops = self.create_execution_tree(self.process_bundle_descriptor)\n  File \"/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/bundle_processor.py\", line 928, in create_execution_tree\n    descriptor.transforms, key=topological_height, reverse=True)])\n  File \"/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/bundle_processor.py\", line 927, in <listcomp>\n    get_operation(transform_id))) for transform_id in sorted(\n  File \"/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/bundle_processor.py\", line 814, in wrapper\n    result = cache[args] = func(*args)\n  File \"/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/bundle_processor.py\", line 909, in get_operation\n    pcoll_id in descriptor.transforms[transform_id].outputs.items()\n  File \"/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/bundle_processor.py\", line 909, in <dictcomp>\n    pcoll_id in descriptor.transforms[transform_id].outputs.items()\n  File \"/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/bundle_processor.py\", line 907, in <listcomp>\n    tag: [get_operation(op) for op in pcoll_consumers[pcoll_id]]\n  File \"/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/bundle_processor.py\", line 814, in wrapper\n    result = cache[args] = func(*args)\n  File \"/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/bundle_processor.py\", line 909, in get_operation\n    pcoll_id in descriptor.transforms[transform_id].outputs.items()\n  File \"/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/bundle_processor.py\", line 909, in <dictcomp>\n    pcoll_id in descriptor.transforms[transform_id].outputs.items()\n  File \"/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/bundle_processor.py\", line 907, in <listcomp>\n    tag: [get_operation(op) for op in pcoll_consumers[pcoll_id]]\n  File \"/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/bundle_processor.py\", line 814, in wrapper\n    result = cache[args] = func(*args)\n  File \"/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/bundle_processor.py\", line 909, in get_operation\n    pcoll_id in descriptor.transforms[transform_id].outputs.items()\n  File \"/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/bundle_processor.py\", line 909, in <dictcomp>\n    pcoll_id in descriptor.transforms[transform_id].outputs.items()\n  File \"/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/bundle_processor.py\", line 907, in <listcomp>\n    tag: [get_operation(op) for op in pcoll_consumers[pcoll_id]]\n  File \"/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/bundle_processor.py\", line 814, in wrapper\n    result = cache[args] = func(*args)\n  File \"/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/bundle_processor.py\", line 912, in get_operation\n    transform_id, transform_consumers)\n  File \"/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/bundle_processor.py\", line 1206, in create_operation\n    return creator(self, transform_id, transform_proto, payload, consumers)\n  File \"/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/bundle_processor.py\", line 1560, in create_par_do\n    parameter)\n  File \"/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/bundle_processor.py\", line 1596, in _create_pardo_operation\n    dofn_data = pickler.loads(serialized_fn)\n  File \"/usr/local/lib/python3.7/site-packages/apache_beam/internal/pickler.py\", line 52, in loads\n    encoded, enable_trace=enable_trace, use_zlib=use_zlib)\n  File \"/usr/local/lib/python3.7/site-packages/apache_beam/internal/dill_pickler.py\", line 289, in loads\n    return dill.loads(s)\n  File \"/usr/local/lib/python3.7/site-packages/dill/_dill.py\", line 275, in loads\n    return load(file, ignore, **kwds)\n  File \"/usr/local/lib/python3.7/site-packages/dill/_dill.py\", line 270, in load\n    return Unpickler(file, ignore=ignore, **kwds).load()\n  File \"/usr/local/lib/python3.7/site-packages/dill/_dill.py\", line 472, in load\n    obj = StockUnpickler.load(self)\n  File \"/usr/local/lib/python3.7/site-packages/dill/_dill.py\", line 462, in find_class\n    return StockUnpickler.find_class(self, module, name)\nAttributeError: Can't get attribute '_create_code' on <module 'dill._dill' from '/usr/local/lib/python3.7/site-packages/dill/_dill.py'>\n"
     ]
    }
   ],
   "source": [
    "beam_pipe(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a33f54-c717-4130-b375-3108836e9e04",
   "metadata": {},
   "source": [
    "## Full Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ee7c5cbe-24d1-4bb6-a773-21f44fa1873e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Expected TFRecords: 5106\n"
     ]
    }
   ],
   "source": [
    "# setup\n",
    "PROJECT_ID = 'hybrid-vertex'\n",
    "BUCKET_NAME = 'spotify-beam-v1' # 'spotify-tfrecords-blog' # Set your Bucket name\n",
    "REGION = 'us-central1' # Set the region for Dataflow jobs\n",
    "VERSION = 'v3'\n",
    "\n",
    "# storage\n",
    "ROOT = f'gs://{BUCKET_NAME}/{VERSION}'\n",
    "\n",
    "DATA_DIR = ROOT + '/data/' # Location to store data\n",
    "STATS_DIR = ROOT +'/stats/' # Location to store stats \n",
    "STAGING_DIR = ROOT + '/job/staging/' # Dataflow staging directory on GCP\n",
    "TEMP_DIR =  ROOT + '/job/temp/' # Dataflow temporary directory on GCP\n",
    "TF_RECORD_DIR = ROOT + '/tf-records/'\n",
    "CANDIDATE_DIR = ROOT + \"/candidates/\"\n",
    "\n",
    "# estimate TF-Record shard count needed\n",
    "# TF-Records\n",
    "total_samples = 65_346_428  \n",
    "samples_per_file = 12_800 \n",
    "NUM_TF_RECORDS = total_samples // samples_per_file\n",
    "\n",
    "if NUM_TF_RECORDS % total_samples:\n",
    "    NUM_TF_RECORDS += 1\n",
    "    \n",
    "print(\"Number of Expected TFRecords: {}\".format(NUM_TF_RECORDS)) # 5343"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cebaca1f-00f8-4a81-869a-ee3699388689",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainTfSeqExampleDoFn(beam.DoFn):\n",
    "    \"\"\"\n",
    "    Convert training sample into TFExample\n",
    "    \"\"\"\n",
    "    def __init__(self, task):\n",
    "        \"\"\"\n",
    "        Initialization\n",
    "        \"\"\"\n",
    "        self.task = task\n",
    "\n",
    "    @staticmethod\n",
    "    def _bytes_feature(value):\n",
    "        \"\"\"\n",
    "        Get byte features\n",
    "        \"\"\"\n",
    "        return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "    @staticmethod\n",
    "    def _int64_feature(value):\n",
    "        \"\"\"\n",
    "        Get int64 feature\n",
    "        \"\"\"\n",
    "        return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "    \n",
    "    @staticmethod\n",
    "    def _string_array(value):\n",
    "        \"\"\"\n",
    "        Returns a bytes_list from a string / byte.\n",
    "        \"\"\"\n",
    "        return tf.train.Feature(bytes_list=tf.train.BytesList(value=[v.encode('utf-8') for v in value]))\n",
    "    \n",
    "    @staticmethod\n",
    "    def _float_feature(value):\n",
    "        \"\"\"Returns a float_list from a float / double.\"\"\"\n",
    "        return tf.train.Feature(float_list=tf.train.FloatList(value=[float(v) for v in value]))\n",
    "    \n",
    "    def process(self, data):\n",
    "        \"\"\"\n",
    "        Convert BQ row to tf-example\n",
    "        \"\"\"\n",
    "    \n",
    "        # ===============================\n",
    "        # Ragged Features - Query\n",
    "        # ===============================\n",
    "        ragged_key_list = [\n",
    "            'track_name_pl',\n",
    "            'artist_name_pl',\n",
    "            'album_name_pl',\n",
    "            # 'track_uri_pl',\n",
    "            'duration_ms_songs_pl',\n",
    "            'artist_pop_pl',\n",
    "            'artists_followers_pl',\n",
    "            'track_pop_pl',\n",
    "            'artist_genres_pl',\n",
    "        ]\n",
    "\n",
    "        ragged_dict = {}\n",
    "\n",
    "        for _ in ragged_key_list:\n",
    "            ragged_dict[_] = []\n",
    "\n",
    "        for x in data['track_name_pl']:\n",
    "            ragged_dict['track_name_pl'].append(x.encode('utf8'))\n",
    "\n",
    "        for x in data['artist_name_pl']:\n",
    "            ragged_dict['artist_name_pl'].append(x.encode('utf8'))\n",
    "\n",
    "        for x in data['album_name_pl']:\n",
    "            ragged_dict['album_name_pl'].append(x.encode('utf8'))\n",
    "\n",
    "        # for x in data['track_uri_pl']:\n",
    "        #     ragged_dict['track_uri_pl'].append(x.encode('utf8'))\n",
    "\n",
    "        for x in data['duration_ms_songs_pl']:\n",
    "            ragged_dict['duration_ms_songs_pl'].append(x)\n",
    "\n",
    "        for x in data['artist_pop_pl']:\n",
    "            ragged_dict['artist_pop_pl'].append(x)\n",
    "\n",
    "        for x in data['artists_followers_pl']:\n",
    "            ragged_dict['artists_followers_pl'].append(x)\n",
    "\n",
    "        for x in data['track_pop_pl']:\n",
    "            ragged_dict['track_pop_pl'].append(x)\n",
    "\n",
    "        for x in data['artist_genres_pl']:\n",
    "            ragged_dict['artist_genres_pl'].append(x.encode('utf8'))\n",
    "\n",
    "        # Set List Types\n",
    "        # Bytes\n",
    "        track_name_pl = BytesList(value=ragged_dict['track_name_pl'])\n",
    "        artist_name_pl = BytesList(value=ragged_dict['artist_name_pl'])\n",
    "        album_name_pl = BytesList(value=ragged_dict['album_name_pl'])\n",
    "        # track_uri_pl = BytesList(value=ragged_dict['track_uri_pl'])\n",
    "        artist_genres_pl = BytesList(value=ragged_dict['artist_genres_pl'])\n",
    "\n",
    "        # Float List\n",
    "        duration_ms_songs_pl = FloatList(value=ragged_dict['duration_ms_songs_pl'])\n",
    "        artist_pop_pl = FloatList(value=ragged_dict['artist_pop_pl'])\n",
    "        artists_followers_pl = FloatList(value=ragged_dict['artists_followers_pl'])\n",
    "        track_pop_pl = FloatList(value=ragged_dict['track_pop_pl'])\n",
    "\n",
    "        # Set FeatureLists\n",
    "        # Bytes\n",
    "        track_name_pl = FeatureList(feature=[Feature(bytes_list=track_name_pl)])\n",
    "        artist_name_pl = FeatureList(feature=[Feature(bytes_list=artist_name_pl)])\n",
    "        album_name_pl = FeatureList(feature=[Feature(bytes_list=album_name_pl)])\n",
    "        # track_uri_pl = FeatureList(feature=[Feature(bytes_list=track_uri_pl)])\n",
    "        artist_genres_pl = FeatureList(feature=[Feature(bytes_list=artist_genres_pl)])\n",
    "\n",
    "        # Float Lists\n",
    "        duration_ms_songs_pl = FeatureList(feature=[Feature(float_list=duration_ms_songs_pl)])\n",
    "        artist_pop_pl = FeatureList(feature=[Feature(float_list=artist_pop_pl)])\n",
    "        artists_followers_pl = FeatureList(feature=[Feature(float_list=artists_followers_pl)])\n",
    "        track_pop_pl = FeatureList(feature=[Feature(float_list=track_pop_pl)])\n",
    "        \n",
    "        # ===============================\n",
    "        # Create Context Features\n",
    "        # ===============================\n",
    "        context_features = {\n",
    "            # playlist - context features\n",
    "            \"name\": _string_array(data['name']),\n",
    "            'collaborative' : _string_array(data['collaborative']),\n",
    "            # 'duration_ms_seed_pl' : _float_feature(data['duration_ms_seed_pl']),\n",
    "            'n_songs_pl' : _float_feature(data['n_songs_pl']),\n",
    "            'num_artists_pl' : _float_feature(data['num_artists_pl']),\n",
    "            'num_albums_pl' : _float_feature(data['num_albums_pl']),\n",
    "            'description_pl' : _string_array(data['description_pl']),\n",
    "\n",
    "            # seed track - context features\n",
    "            'track_name_seed_track' : _string_array(data['track_name_seed_track']),\n",
    "            'artist_name_seed_track' : _string_array(data['artist_name_seed_track']),\n",
    "            'album_name_seed_track' : _string_array(data['album_name_seed_track']),\n",
    "            # 'track_uri_seed_track' : _string_array(data['track_uri_seed_track']),\n",
    "            # 'artist_uri_seed_track' : _string_array(data['artist_uri_seed_track']),\n",
    "            # 'album_uri_seed_track' : _string_array(data['album_uri_seed_track']),\n",
    "            'duration_seed_track' : _float_feature(data['duration_seed_track']),\n",
    "            'track_pop_seed_track' : _float_feature(data['track_pop_seed_track']),\n",
    "            'artist_pop_seed_track' : _float_feature(data['artist_pop_seed_track']),\n",
    "            'artist_genres_seed_track' : _string_array(data['artist_genres_seed_track']),\n",
    "            'artist_followers_seed_track' : _float_feature(data['artist_followers_seed_track']),\n",
    "\n",
    "            #candidate features\n",
    "            \"track_name_can\": _string_array(data['track_name_can']), \n",
    "            \"artist_name_can\": _string_array(data['artist_name_can']),\n",
    "            \"album_name_can\": _string_array(data['album_name_can']),\n",
    "            \"track_uri_can\": _string_array(data['track_uri_can']),\n",
    "            # \"artist_uri_can\": _string_array(data['artist_uri_can']),\n",
    "            # \"album_uri_can\": _string_array(data['album_uri_can']),\n",
    "            \"duration_ms_can\": _float_feature(data['duration_ms_can']),\n",
    "            \"track_pop_can\": _float_feature(data['track_pop_can']), \n",
    "            \"artist_pop_can\": _float_feature(data['artist_pop_can']),\n",
    "            \"artist_genres_can\": _string_array(data['artist_genres_can']),\n",
    "            \"artist_followers_can\": _float_feature(data['artist_followers_can']),\n",
    "        }\n",
    "        \n",
    "        # ===============================\n",
    "        # Create Sequence\n",
    "        # ===============================\n",
    "        seq = SequenceExample(\n",
    "            context=tf.train.Features(\n",
    "                feature=context_features\n",
    "            ),\n",
    "            feature_lists=FeatureLists(\n",
    "                feature_list={\n",
    "                    \"track_name_pl\": track_name_pl,\n",
    "                    \"artist_name_pl\": artist_name_pl,\n",
    "                    \"album_name_pl\": album_name_pl,\n",
    "                    # \"track_uri_pl\": track_uri_pl,\n",
    "                    \"duration_ms_songs_pl\": duration_ms_songs_pl,\n",
    "                    \"artist_pop_pl\": artist_pop_pl,\n",
    "                    \"artists_followers_pl\": artists_followers_pl,\n",
    "                    \"track_pop_pl\": track_pop_pl,\n",
    "                    \"artist_genres_pl\": artist_genres_pl\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "\n",
    "        yield seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8635baa5-8fcd-4fc3-b0e1-3d5f9adf8f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_pipe(args):\n",
    "    \n",
    "    BQ_TABLE = args['bq_source_table']\n",
    "    CANDIDATE_SINK = args['candidate_sink']\n",
    "    RUNNER = args['runner']\n",
    "    NUM_TF_RECORDS = args['num_candidate_tfrecords']\n",
    "    \n",
    "    # Convert rows to tf-example\n",
    "    _to_tf_example = TrainTfSeqExampleDoFn(task=\"train\")\n",
    "    \n",
    "    # Write serialized example to tfrecords\n",
    "    write_to_tf_record = beam.io.WriteToTFRecord(\n",
    "        file_path_prefix = f'{TF_RECORD_DIR}/{args[\"folder\"]}', \n",
    "        file_name_suffix=\".tfrecords\",\n",
    "        num_shards=NUM_TF_RECORDS\n",
    "    )\n",
    "    \n",
    "    pipeline_options = beam.options.pipeline_options.GoogleCloudOptions(**args)\n",
    "    pipeline_options.view_as(SetupOptions).save_main_session = True\n",
    "    print(pipeline_options)\n",
    "\n",
    "    with beam.Pipeline(RUNNER, options=pipeline_options) as pipeline:\n",
    "        (pipeline \n",
    "         | \"Read from BigQuery\">> beam.io.Read(beam.io.BigQuerySource(query=args['source_query'], flatten_results=True))\n",
    "         | 'Convert to tf Example' >> beam.ParDo(_to_tf_example)\n",
    "         | 'Serialize to String' >> beam.Map(lambda example: example.SerializeToString(deterministic=True))\n",
    "         | \"Write as TFRecords to GCS\" >> write_to_tf_record\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "85ff92b7-c4d9-4e0b-a222-8576ab790f01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline args are set to:\n",
      "{'bq_source_table': 'hybrid-vertex:mdp_eda_test.train_flatten',\n",
      " 'candidate_sink': 'gs://spotify-beam-v3/v3/candidates/',\n",
      " 'folder': 'train',\n",
      " 'job_name': 'spotify-bq-tfrecords-v3-220920-163847',\n",
      " 'network': 'ucaip-haystack-vpc-network',\n",
      " 'num_candidate_tfrecords': 8,\n",
      " 'project': 'hybrid-vertex',\n",
      " 'region': 'us-central1',\n",
      " 'runner': 'DataflowRunner',\n",
      " 'save_main_session': True,\n",
      " 'setup_file': 'beam-training/setup.py',\n",
      " 'source_query': 'SELECT * FROM hybrid-vertex.mdp_eda_test.train_flatten',\n",
      " 'staging_location': 'gs://spotify-beam-v3/v3/job/staging/',\n",
      " 'temp_location': 'gs://spotify-beam-v3/v3/job/temp/'}\n"
     ]
    }
   ],
   "source": [
    "# setup\n",
    "PROJECT_ID = 'hybrid-vertex'\n",
    "BUCKET_NAME = 'spotify-beam-v3' # 'spotify-tfrecords-blog' # Set your Bucket name\n",
    "REGION = 'us-central1' # Set the region for Dataflow jobs\n",
    "VERSION = 'v3'\n",
    "\n",
    "# Pipeline Params\n",
    "TIMESTAMP = datetime.utcnow().strftime('%y%m%d-%H%M%S')\n",
    "JOB_NAME = f'spotify-bq-tfrecords-{VERSION}-{TIMESTAMP}'\n",
    "MAX_WORKERS = '20'\n",
    "RUNNER = 'DataflowRunner'\n",
    "NETWORK = 'ucaip-haystack-vpc-network'\n",
    "\n",
    "# Source data\n",
    "BQ_TABLE = 'train_flatten'\n",
    "BQ_DATASET = 'mdp_eda_test'\n",
    "TABLE_SPEC = f'{PROJECT_ID}:{BQ_DATASET}.{BQ_TABLE}' # need \" : \" between project and ds\n",
    "\n",
    "# storage\n",
    "ROOT = f'gs://{BUCKET_NAME}/{VERSION}'\n",
    "\n",
    "DATA_DIR = ROOT + '/data/' # Location to store data\n",
    "STATS_DIR = ROOT +'/stats/' # Location to store stats \n",
    "STAGING_DIR = ROOT + '/job/staging/' # Dataflow staging directory on GCP\n",
    "TEMP_DIR =  ROOT + '/job/temp/' # Dataflow temporary directory on GCP\n",
    "TF_RECORD_DIR = ROOT + '/tf-records/'\n",
    "CANDIDATE_DIR = ROOT + \"/candidates/\"\n",
    "\n",
    "QUERY = f\"SELECT * FROM {PROJECT_ID}.{BQ_DATASET}.{BQ_TABLE}\"\n",
    "\n",
    "NUM_TF_RECORDS = 8\n",
    "\n",
    "\n",
    "args = {\n",
    "    'job_name': JOB_NAME,\n",
    "    'runner': RUNNER,\n",
    "    'source_query': QUERY,\n",
    "    'bq_source_table': TABLE_SPEC,\n",
    "    'network': NETWORK,\n",
    "    'candidate_sink': CANDIDATE_DIR,\n",
    "    'num_candidate_tfrecords': NUM_TF_RECORDS,\n",
    "    'project': PROJECT_ID,\n",
    "    'region': REGION,\n",
    "    'staging_location': STAGING_DIR,\n",
    "    'temp_location': TEMP_DIR,\n",
    "    'save_main_session': True,\n",
    "    'setup_file': 'beam-training/setup.py',\n",
    "    'folder': 'train'\n",
    "}\n",
    "print(\"Pipeline args are set to:\")\n",
    "pprint(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4c0dfce5-3d6b-4af8-8e67-453a3ba6808e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GoogleCloudOptions(create_from_snapshot=None, dataflow_endpoint=https://dataflow.googleapis.com, dataflow_kms_key=None, dataflow_service_options=None, enable_artifact_caching=False, enable_hot_key_logging=False, enable_streaming_engine=False, flexrs_goal=None, impersonate_service_account=None, job_name=spotify-bq-tfrecords-v3-220920-163539, labels=None, no_auth=False, project=hybrid-vertex, region=us-central1, service_account_email=None, staging_location=gs://spotify-beam-v3/v3/job/staging/, temp_location=gs://spotify-beam-v3/v3/job/temp/, template_location=None, transform_name_mapping=None, update=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: BeamDeprecationWarning: BigQuerySource is deprecated since 2.25.0. Use ReadFromBigQuery instead.\n",
      "warning: sdist: standard file not found: should have one of README, README.rst, README.txt, README.md\n",
      "\n",
      "warning: check: missing required meta-data: url\n",
      "\n",
      "warning: check: missing meta-data: either (author and author_email) or (maintainer and maintainer_email) must be supplied\n",
      "\n",
      "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['-f', '/home/jupyter/.local/share/jupyter/runtime/kernel-f8963804-a2c2-4048-80bc-87ef64b3f9fb.json']\n",
      "WARNING:apache_beam.options.pipeline_options:Discarding invalid overrides: {'source_query': 'SELECT * FROM hybrid-vertex.mdp_eda_test.train_flatten', 'bq_source_table': 'hybrid-vertex:mdp_eda_test.train_flatten', 'candidate_sink': 'gs://spotify-beam-v3/v3/candidates/', 'num_candidate_tfrecords': 8, 'folder': 'train'}\n",
      "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['-f', '/home/jupyter/.local/share/jupyter/runtime/kernel-f8963804-a2c2-4048-80bc-87ef64b3f9fb.json']\n",
      "WARNING:apache_beam.options.pipeline_options:Discarding invalid overrides: {'source_query': 'SELECT * FROM hybrid-vertex.mdp_eda_test.train_flatten', 'bq_source_table': 'hybrid-vertex:mdp_eda_test.train_flatten', 'candidate_sink': 'gs://spotify-beam-v3/v3/candidates/', 'num_candidate_tfrecords': 8, 'folder': 'train'}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_24253/3839680678.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbeam_pipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_24253/2211418990.py\u001b[0m in \u001b[0;36mbeam_pipe\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     24\u001b[0m          \u001b[0;34m|\u001b[0m \u001b[0;34m'Convert to tf Example'\u001b[0m \u001b[0;34m>>\u001b[0m \u001b[0mbeam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParDo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_tf_example\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m          \u001b[0;34m|\u001b[0m \u001b[0;34m'Serialize to String'\u001b[0m \u001b[0;34m>>\u001b[0m \u001b[0mbeam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mexample\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mexample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerializeToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeterministic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m          \u001b[0;34m|\u001b[0m \u001b[0;34m\"Write as TFRecords to GCS\"\u001b[0m \u001b[0;34m>>\u001b[0m \u001b[0mwrite_to_tf_record\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         )\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/apache_beam/pipeline.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    596\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mexc_type\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_until_finish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    599\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extra_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_tb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/apache_beam/runners/dataflow/dataflow_runner.py\u001b[0m in \u001b[0;36mwait_until_finish\u001b[0;34m(self, duration)\u001b[0m\n\u001b[1;32m   1656\u001b[0m       \u001b[0mthread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1657\u001b[0m       \u001b[0;32mwhile\u001b[0m \u001b[0mthread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1658\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1659\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m       \u001b[0;31m# TODO: Merge the termination code in poll_for_job_completion and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "beam_pipe(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81329196-7e7c-4915-b49c-458cc15b381e",
   "metadata": {},
   "outputs": [],
   "source": [
    "BQ_TABLE = 'train_flatten_valid'\n",
    "\n",
    "QUERY = f\"SELECT * FROM {PROJECT_ID}.{BQ_DATASET}.{BQ_TABLE}\"\n",
    "\n",
    "args.update({'source_query': QUERY, 'folder': 'valid'}\n",
    "            print(\"Pipeline args are set to:\")\n",
    "pprint(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fd9798-6db7-43d3-a5a9-7afa75b47d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_pipe(args)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-9.m95",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-9:m95"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
