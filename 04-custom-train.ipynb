{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a7ef552-9756-457e-8734-fcb8899d035f",
   "metadata": {},
   "source": [
    "# Scaling Two-Tower training with Vertex AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7956da31-55a5-41de-a9c6-cb29c7da766c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ID: hybrid-vertex\n",
      "PROJECT_NUM: 934903580331\n",
      "REGION: us-central1\n"
     ]
    }
   ],
   "source": [
    "GCP_PROJECTS = !gcloud config get-value project\n",
    "PROJECT_ID = GCP_PROJECTS[0]\n",
    "PROJECT_NUM = !gcloud projects list --filter=\"$PROJECT_ID\" --format=\"value(PROJECT_NUMBER)\"\n",
    "PROJECT_NUM = PROJECT_NUM[0]\n",
    "REGION = 'us-central1'\n",
    "\n",
    "print(f\"PROJECT_ID: {PROJECT_ID}\")\n",
    "print(f\"PROJECT_NUM: {PROJECT_NUM}\")\n",
    "print(f\"REGION: {REGION}\")\n",
    "\n",
    "VERTEX_SA = '934903580331-compute@developer.gserviceaccount.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "319b0b73-d1bc-4a90-a4aa-864d2215d646",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "from time import time\n",
    "import pandas as pd\n",
    "# disable INFO and DEBUG logging everywhere\n",
    "import logging\n",
    "import time\n",
    "from pprint import pprint\n",
    "import pickle as pkl\n",
    "\n",
    "logging.disable(logging.WARNING)\n",
    "\n",
    "from google.cloud import aiplatform as vertex_ai\n",
    "from google.cloud import storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e4b6da8-8246-410c-8718-6a0dea4ea269",
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_client = storage.Client(project=PROJECT_ID)\n",
    "\n",
    "vertex_ai.init(project=PROJECT_ID,location=REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bf1920-60e3-409b-af57-19aef0c492c5",
   "metadata": {},
   "source": [
    "### update vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e79c199-7f18-4f12-b6f1-c759caa56930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL_ROOT_NAME: sp-2tower-tfrs-trainerv1\n"
     ]
    }
   ],
   "source": [
    "VERSION= \"trainerv1\"\n",
    "APP='sp'\n",
    "MODEL_TYPE='2tower'\n",
    "FRAMEWORK = 'tfrs'\n",
    "MODEL_ROOT_NAME = f'{APP}-{MODEL_TYPE}-{FRAMEWORK}-{VERSION}'\n",
    "\n",
    "print(f\"MODEL_ROOT_NAME: {MODEL_ROOT_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349297e0-8d40-4ca2-bf63-b1b4a8c3af12",
   "metadata": {},
   "source": [
    "## Create training package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60d17843-5aec-4f58-b8c9-2734ccc69eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "REPO_DOCKER_PATH_PREFIX = 'src'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c1ad2cd-bb52-42e7-8693-f271f26fb014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMAGE_URI: gcr.io/hybrid-vertex/sp-2tower-tfrs-trainerv1-tr\n"
     ]
    }
   ],
   "source": [
    "# Docker definitions for training\n",
    "IMAGE_NAME = f'{MODEL_ROOT_NAME}-tr'\n",
    "IMAGE_URI = f'gcr.io/{PROJECT_ID}/{IMAGE_NAME}'\n",
    "\n",
    "DOCKERNAME = 'tfrs'\n",
    "REPO_DOCKER_PATH_PREFIX = 'src'\n",
    "MACHINE_TYPE ='e2-highcpu-32'\n",
    "FILE_LOCATION = './src'\n",
    "\n",
    "print(f\"IMAGE_URI: {IMAGE_URI}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "719ee528-33b3-429d-a1ab-898bf069d157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/cloudbuild.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/cloudbuild.yaml\n",
    "\n",
    "steps:\n",
    "- name: 'gcr.io/cloud-builders/docker'\n",
    "  args: ['build', '-t', '$_IMAGE_URI', '$_FILE_LOCATION', '-f', '$_FILE_LOCATION/Dockerfile.$_DOCKERNAME']\n",
    "images:\n",
    "- '$_IMAGE_URI'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aae79238-4e73-4b33-92b6-9d784c4a02cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/two_tower_jt/train_config.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/two_tower_jt/train_config.py\n",
    "MAX_PLAYLIST_LENGTH = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f24c3c05-e92c-417e-a4c0-901b91886c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/two_tower_jt/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/two_tower_jt/requirements.txt\n",
    "google-cloud-aiplatform>=1.21.0\n",
    "tensorflow-recommenders==0.7.2\n",
    "tensorboard==2.10.1\n",
    "tensorboard-data-server==0.6.1\n",
    "tensorboard-plugin-profile==2.11.1\n",
    "tensorflow-io==0.27.0\n",
    "google-cloud-aiplatform[cloud_profiler]>=1.20.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc5822fd-0cef-41da-a2bb-3af326a94f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/Dockerfile.tfrs\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/Dockerfile.{DOCKERNAME}\n",
    "\n",
    "# FROM tensorflow/tensorflow:2.10.1-gpu\n",
    "FROM gcr.io/deeplearning-platform-release/tf-gpu.2-10\n",
    "\n",
    "WORKDIR /src\n",
    "\n",
    "# Copies the trainer code to the docker image.\n",
    "COPY two_tower_jt/* two_tower_jt/ \n",
    "\n",
    "RUN pip install -r two_tower_jt/requirements.txt\n",
    "\n",
    "RUN apt update && apt -y install nvtop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3856eb73-d4dc-4ca2-b2be-2224cc51ba2a",
   "metadata": {},
   "source": [
    "### Build Training image with Cloud Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9469cdf9-e9d5-4d7a-a850-a436ff8383ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOCKERNAME: tfrs\n",
      "IMAGE_URI: gcr.io/hybrid-vertex/sp-2tower-tfrs-trainerv1-tr\n",
      "FILE_LOCATION: ./src\n",
      "MACHINE_TYPE: e2-highcpu-32\n"
     ]
    }
   ],
   "source": [
    "print(f\"DOCKERNAME: {DOCKERNAME}\")\n",
    "print(f\"IMAGE_URI: {IMAGE_URI}\")\n",
    "print(f\"FILE_LOCATION: {FILE_LOCATION}\")\n",
    "print(f\"MACHINE_TYPE: {MACHINE_TYPE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eebb9ac8-5c5a-44f6-b95f-b0c392099a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [gcloudignore/enabled].\n"
     ]
    }
   ],
   "source": [
    "! gcloud config set gcloudignore/enabled true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0e434002-cea0-43a6-bb63-f6d1a3c7e2d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting .gcloudignore\n"
     ]
    }
   ],
   "source": [
    "%%writefile .gcloudignore\n",
    ".gcloudignore\n",
    "/local_files/\n",
    "/img/\n",
    "*.pkl\n",
    "*.png\n",
    "*.ipynb\n",
    ".git\n",
    ".github\n",
    ".ipynb_checkpoints/*\n",
    "*__pycache__\n",
    "*cpython-37.pyc\n",
    "spotipy_secret_creds.py\n",
    "custom_pipeline_spec.json\n",
    "/WIP/*\n",
    "beam_candidates/*\n",
    "beam_training/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1e54839e-207c-4c89-bcf6-c5a104d227ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !gcloud meta list-files-for-upload\n",
    "# !ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f74d1342-796e-4fff-af1f-eb9196cdb1ec",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 38 file(s) totalling 290.7 KiB before compression.\n",
      "Uploading tarball of [.] to [gs://hybrid-vertex_cloudbuild/source/1677095746.824004-c07bf07b8ac7430383aab53aedaf2526.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/hybrid-vertex/locations/global/builds/2ba1aaaa-161b-4d76-9053-531b3a91ee58].\n",
      "Logs are available at [ https://console.cloud.google.com/cloud-build/builds/2ba1aaaa-161b-4d76-9053-531b3a91ee58?project=934903580331 ].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"2ba1aaaa-161b-4d76-9053-531b3a91ee58\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://hybrid-vertex_cloudbuild/source/1677095746.824004-c07bf07b8ac7430383aab53aedaf2526.tgz#1677095747155509\n",
      "Copying gs://hybrid-vertex_cloudbuild/source/1677095746.824004-c07bf07b8ac7430383aab53aedaf2526.tgz#1677095747155509...\n",
      "/ [1 files][ 49.1 KiB/ 49.1 KiB]                                                \n",
      "Operation completed over 1 objects/49.1 KiB.\n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  254.5kB\n",
      "Step 1/5 : FROM gcr.io/deeplearning-platform-release/tf-gpu.2-10\n",
      "latest: Pulling from deeplearning-platform-release/tf-gpu.2-10\n",
      "846c0b181fff: Pulling fs layer\n",
      "6fc9dd88827c: Pulling fs layer\n",
      "0b311d7060d0: Pulling fs layer\n",
      "326d76058f67: Pulling fs layer\n",
      "ed7e4c52c661: Pulling fs layer\n",
      "4f05f5570c7a: Pulling fs layer\n",
      "ab264e292103: Pulling fs layer\n",
      "527d1d5ab821: Pulling fs layer\n",
      "fc46d4e99009: Pulling fs layer\n",
      "9ccf692754fa: Pulling fs layer\n",
      "c6611ece70c4: Pulling fs layer\n",
      "fe6ff819b45f: Pulling fs layer\n",
      "c942e9416e07: Pulling fs layer\n",
      "139c62f03172: Pulling fs layer\n",
      "4f4fb700ef54: Pulling fs layer\n",
      "ac84182603c1: Pulling fs layer\n",
      "3860e93641d8: Pulling fs layer\n",
      "085441b54e3f: Pulling fs layer\n",
      "33b479b9d085: Pulling fs layer\n",
      "5ca4c4a165b6: Pulling fs layer\n",
      "86bfd6c69c42: Pulling fs layer\n",
      "a98f2a818740: Pulling fs layer\n",
      "e170e0655ccf: Pulling fs layer\n",
      "bd4c2b6fe3ff: Pulling fs layer\n",
      "dda86b68803d: Pulling fs layer\n",
      "7764511c6da3: Pulling fs layer\n",
      "56a69db5aee5: Pulling fs layer\n",
      "53bc18ba1883: Pulling fs layer\n",
      "ee4e8980b6be: Pulling fs layer\n",
      "50a6a9c26a84: Pulling fs layer\n",
      "4f0d87a457ad: Pulling fs layer\n",
      "6aa8aa286e6a: Pulling fs layer\n",
      "f37201cfff33: Pulling fs layer\n",
      "aee1ac3070bc: Pulling fs layer\n",
      "b78a76760f85: Pulling fs layer\n",
      "cbed8fa1c874: Pulling fs layer\n",
      "4ac3af8b3692: Pulling fs layer\n",
      "cdb012978bbb: Pulling fs layer\n",
      "ab264e292103: Waiting\n",
      "527d1d5ab821: Waiting\n",
      "fc46d4e99009: Waiting\n",
      "9ccf692754fa: Waiting\n",
      "c6611ece70c4: Waiting\n",
      "fe6ff819b45f: Waiting\n",
      "c942e9416e07: Waiting\n",
      "326d76058f67: Waiting\n",
      "ed7e4c52c661: Waiting\n",
      "4f05f5570c7a: Waiting\n",
      "139c62f03172: Waiting\n",
      "86bfd6c69c42: Waiting\n",
      "56a69db5aee5: Waiting\n",
      "085441b54e3f: Waiting\n",
      "a98f2a818740: Waiting\n",
      "33b479b9d085: Waiting\n",
      "53bc18ba1883: Waiting\n",
      "e170e0655ccf: Waiting\n",
      "5ca4c4a165b6: Waiting\n",
      "ee4e8980b6be: Waiting\n",
      "bd4c2b6fe3ff: Waiting\n",
      "50a6a9c26a84: Waiting\n",
      "cbed8fa1c874: Waiting\n",
      "dda86b68803d: Waiting\n",
      "4f0d87a457ad: Waiting\n",
      "7764511c6da3: Waiting\n",
      "4ac3af8b3692: Waiting\n",
      "6aa8aa286e6a: Waiting\n",
      "aee1ac3070bc: Waiting\n",
      "cdb012978bbb: Waiting\n",
      "b78a76760f85: Waiting\n",
      "f37201cfff33: Waiting\n",
      "4f4fb700ef54: Waiting\n",
      "3860e93641d8: Waiting\n",
      "846c0b181fff: Verifying Checksum\n",
      "846c0b181fff: Download complete\n",
      "6fc9dd88827c: Verifying Checksum\n",
      "6fc9dd88827c: Download complete\n",
      "0b311d7060d0: Verifying Checksum\n",
      "0b311d7060d0: Download complete\n",
      "326d76058f67: Verifying Checksum\n",
      "326d76058f67: Download complete\n",
      "ed7e4c52c661: Download complete\n",
      "ab264e292103: Verifying Checksum\n",
      "ab264e292103: Download complete\n",
      "527d1d5ab821: Verifying Checksum\n",
      "527d1d5ab821: Download complete\n",
      "fc46d4e99009: Verifying Checksum\n",
      "fc46d4e99009: Download complete\n",
      "c6611ece70c4: Verifying Checksum\n",
      "c6611ece70c4: Download complete\n",
      "846c0b181fff: Pull complete\n",
      "6fc9dd88827c: Pull complete\n",
      "0b311d7060d0: Pull complete\n",
      "326d76058f67: Pull complete\n",
      "ed7e4c52c661: Pull complete\n",
      "4f05f5570c7a: Verifying Checksum\n",
      "4f05f5570c7a: Download complete\n",
      "c942e9416e07: Verifying Checksum\n",
      "c942e9416e07: Download complete\n",
      "139c62f03172: Verifying Checksum\n",
      "139c62f03172: Download complete\n",
      "4f4fb700ef54: Verifying Checksum\n",
      "4f4fb700ef54: Download complete\n",
      "ac84182603c1: Verifying Checksum\n",
      "ac84182603c1: Download complete\n",
      "9ccf692754fa: Verifying Checksum\n",
      "9ccf692754fa: Download complete\n",
      "3860e93641d8: Verifying Checksum\n",
      "3860e93641d8: Download complete\n",
      "085441b54e3f: Verifying Checksum\n",
      "085441b54e3f: Download complete\n",
      "33b479b9d085: Verifying Checksum\n",
      "33b479b9d085: Download complete\n",
      "86bfd6c69c42: Verifying Checksum\n",
      "86bfd6c69c42: Download complete\n",
      "5ca4c4a165b6: Verifying Checksum\n",
      "5ca4c4a165b6: Download complete\n",
      "e170e0655ccf: Verifying Checksum\n",
      "e170e0655ccf: Download complete\n",
      "bd4c2b6fe3ff: Verifying Checksum\n",
      "bd4c2b6fe3ff: Download complete\n",
      "dda86b68803d: Verifying Checksum\n",
      "dda86b68803d: Download complete\n",
      "7764511c6da3: Verifying Checksum\n",
      "7764511c6da3: Download complete\n",
      "56a69db5aee5: Verifying Checksum\n",
      "56a69db5aee5: Download complete\n",
      "53bc18ba1883: Download complete\n",
      "ee4e8980b6be: Verifying Checksum\n",
      "ee4e8980b6be: Download complete\n",
      "50a6a9c26a84: Download complete\n",
      "4f0d87a457ad: Verifying Checksum\n",
      "4f0d87a457ad: Download complete\n",
      "6aa8aa286e6a: Verifying Checksum\n",
      "6aa8aa286e6a: Download complete\n",
      "f37201cfff33: Verifying Checksum\n",
      "f37201cfff33: Download complete\n",
      "a98f2a818740: Verifying Checksum\n",
      "a98f2a818740: Download complete\n",
      "fe6ff819b45f: Verifying Checksum\n",
      "fe6ff819b45f: Download complete\n",
      "cbed8fa1c874: Download complete\n",
      "4ac3af8b3692: Verifying Checksum\n",
      "4ac3af8b3692: Download complete\n",
      "cdb012978bbb: Verifying Checksum\n",
      "cdb012978bbb: Download complete\n",
      "aee1ac3070bc: Verifying Checksum\n",
      "aee1ac3070bc: Download complete\n",
      "4f05f5570c7a: Pull complete\n",
      "ab264e292103: Pull complete\n",
      "527d1d5ab821: Pull complete\n",
      "fc46d4e99009: Pull complete\n",
      "b78a76760f85: Verifying Checksum\n",
      "b78a76760f85: Download complete\n",
      "9ccf692754fa: Pull complete\n",
      "c6611ece70c4: Pull complete\n",
      "fe6ff819b45f: Pull complete\n",
      "c942e9416e07: Pull complete\n",
      "139c62f03172: Pull complete\n",
      "4f4fb700ef54: Pull complete\n",
      "ac84182603c1: Pull complete\n",
      "3860e93641d8: Pull complete\n",
      "085441b54e3f: Pull complete\n",
      "33b479b9d085: Pull complete\n",
      "5ca4c4a165b6: Pull complete\n",
      "86bfd6c69c42: Pull complete\n",
      "a98f2a818740: Pull complete\n",
      "e170e0655ccf: Pull complete\n",
      "bd4c2b6fe3ff: Pull complete\n",
      "dda86b68803d: Pull complete\n",
      "7764511c6da3: Pull complete\n",
      "56a69db5aee5: Pull complete\n",
      "53bc18ba1883: Pull complete\n",
      "ee4e8980b6be: Pull complete\n",
      "50a6a9c26a84: Pull complete\n",
      "4f0d87a457ad: Pull complete\n",
      "6aa8aa286e6a: Pull complete\n",
      "f37201cfff33: Pull complete\n",
      "aee1ac3070bc: Pull complete\n",
      "b78a76760f85: Pull complete\n",
      "cbed8fa1c874: Pull complete\n",
      "4ac3af8b3692: Pull complete\n",
      "cdb012978bbb: Pull complete\n",
      "Digest: sha256:29efd2853c37262f10d57e69228a6bed170a4ed78227c96f33586b31c3878c11\n",
      "Status: Downloaded newer image for gcr.io/deeplearning-platform-release/tf-gpu.2-10:latest\n",
      " ---> 9d3c2d44f624\n",
      "Step 2/5 : WORKDIR /src\n",
      " ---> Running in f6531fd9fa40\n",
      "Removing intermediate container f6531fd9fa40\n",
      " ---> fbbfb2c022a1\n",
      "Step 3/5 : COPY two_tower_jt/* two_tower_jt/\n",
      " ---> c55e19a304ed\n",
      "Step 4/5 : RUN pip install -r two_tower_jt/requirements.txt\n",
      " ---> Running in 3d56a9255843\n",
      "Requirement already satisfied: google-cloud-aiplatform>=1.21.0 in /opt/conda/lib/python3.7/site-packages (from -r two_tower_jt/requirements.txt (line 1)) (1.21.0)\n",
      "Collecting tensorflow-recommenders==0.7.2\n",
      "  Downloading tensorflow_recommenders-0.7.2-py3-none-any.whl (89 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 89.3/89.3 kB 4.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: tensorboard==2.10.1 in /opt/conda/lib/python3.7/site-packages (from -r two_tower_jt/requirements.txt (line 3)) (2.10.1)\n",
      "Requirement already satisfied: tensorboard-data-server==0.6.1 in /opt/conda/lib/python3.7/site-packages (from -r two_tower_jt/requirements.txt (line 4)) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-profile==2.11.1 in /opt/conda/lib/python3.7/site-packages (from -r two_tower_jt/requirements.txt (line 5)) (2.11.1)\n",
      "Requirement already satisfied: tensorflow-io==0.27.0 in /opt/conda/lib/python3.7/site-packages (from -r two_tower_jt/requirements.txt (line 6)) (0.27.0)\n",
      "Requirement already satisfied: absl-py>=0.1.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow-recommenders==0.7.2->-r two_tower_jt/requirements.txt (line 2)) (1.4.0)\n",
      "Requirement already satisfied: tensorflow>=2.9.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow-recommenders==0.7.2->-r two_tower_jt/requirements.txt (line 2)) (2.10.1)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard==2.10.1->-r two_tower_jt/requirements.txt (line 3)) (1.21.6)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard==2.10.1->-r two_tower_jt/requirements.txt (line 3)) (3.4.1)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard==2.10.1->-r two_tower_jt/requirements.txt (line 3)) (1.51.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard==2.10.1->-r two_tower_jt/requirements.txt (line 3)) (2.28.2)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard==2.10.1->-r two_tower_jt/requirements.txt (line 3)) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard==2.10.1->-r two_tower_jt/requirements.txt (line 3)) (2.1.2)\n",
      "Requirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.7/site-packages (from tensorboard==2.10.1->-r two_tower_jt/requirements.txt (line 3)) (0.38.4)\n",
      "Collecting protobuf<3.20,>=3.9.2\n",
      "  Downloading protobuf-3.19.6-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.1/1.1 MB 23.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard==2.10.1->-r two_tower_jt/requirements.txt (line 3)) (66.1.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard==2.10.1->-r two_tower_jt/requirements.txt (line 3)) (0.4.6)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard==2.10.1->-r two_tower_jt/requirements.txt (line 3)) (2.16.0)\n",
      "Requirement already satisfied: gviz-api>=1.9.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard-plugin-profile==2.11.1->-r two_tower_jt/requirements.txt (line 5)) (1.10.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard-plugin-profile==2.11.1->-r two_tower_jt/requirements.txt (line 5)) (1.16.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem==0.27.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow-io==0.27.0->-r two_tower_jt/requirements.txt (line 6)) (0.27.0)\n",
      "Collecting packaging<22.0.0dev,>=14.3\n",
      "  Downloading packaging-21.3-py3-none-any.whl (40 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 40.8/40.8 kB 6.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: google-cloud-bigquery<4.0.0dev,>=1.15.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform>=1.21.0->-r two_tower_jt/requirements.txt (line 1)) (3.4.2)\n",
      "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform>=1.21.0->-r two_tower_jt/requirements.txt (line 1)) (1.8.1)\n",
      "Requirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform>=1.21.0->-r two_tower_jt/requirements.txt (line 1)) (2.10.1)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform>=1.21.0->-r two_tower_jt/requirements.txt (line 1)) (1.22.2)\n",
      "Requirement already satisfied: google-cloud-storage<3.0.0dev,>=1.32.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform>=1.21.0->-r two_tower_jt/requirements.txt (line 1)) (2.7.0)\n",
      "Requirement already satisfied: shapely<2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform>=1.21.0->-r two_tower_jt/requirements.txt (line 1)) (1.8.5.post1)\n",
      "Collecting werkzeug>=1.0.1\n",
      "  Downloading Werkzeug-2.0.3-py3-none-any.whl (289 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 289.2/289.2 kB 23.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform>=1.21.0->-r two_tower_jt/requirements.txt (line 1)) (1.58.0)\n",
      "Collecting google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0\n",
      "  Downloading google_api_core-2.11.0-py3-none-any.whl (120 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 120.3/120.3 kB 18.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: grpcio-status<2.0dev,>=1.33.2 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform>=1.21.0->-r two_tower_jt/requirements.txt (line 1)) (1.48.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard==2.10.1->-r two_tower_jt/requirements.txt (line 3)) (5.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard==2.10.1->-r two_tower_jt/requirements.txt (line 3)) (4.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard==2.10.1->-r two_tower_jt/requirements.txt (line 3)) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard==2.10.1->-r two_tower_jt/requirements.txt (line 3)) (1.3.1)\n",
      "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform>=1.21.0->-r two_tower_jt/requirements.txt (line 1)) (2.8.2)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform>=1.21.0->-r two_tower_jt/requirements.txt (line 1)) (2.3.2)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform>=1.21.0->-r two_tower_jt/requirements.txt (line 1)) (2.4.1)\n",
      "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /opt/conda/lib/python3.7/site-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform>=1.21.0->-r two_tower_jt/requirements.txt (line 1)) (0.12.6)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard==2.10.1->-r two_tower_jt/requirements.txt (line 3)) (6.0.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging<22.0.0dev,>=14.3->google-cloud-aiplatform>=1.21.0->-r two_tower_jt/requirements.txt (line 1)) (3.0.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard==2.10.1->-r two_tower_jt/requirements.txt (line 3)) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard==2.10.1->-r two_tower_jt/requirements.txt (line 3)) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard==2.10.1->-r two_tower_jt/requirements.txt (line 3)) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard==2.10.1->-r two_tower_jt/requirements.txt (line 3)) (1.26.14)\n",
      "Requirement already satisfied: keras<2.11,>=2.10.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.9.0->tensorflow-recommenders==0.7.2->-r two_tower_jt/requirements.txt (line 2)) (2.10.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.9.0->tensorflow-recommenders==0.7.2->-r two_tower_jt/requirements.txt (line 2)) (2.2.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.9.0->tensorflow-recommenders==0.7.2->-r two_tower_jt/requirements.txt (line 2)) (1.6.3)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.9.0->tensorflow-recommenders==0.7.2->-r two_tower_jt/requirements.txt (line 2)) (15.0.6.1)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.9.0->tensorflow-recommenders==0.7.2->-r two_tower_jt/requirements.txt (line 2)) (23.1.21)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.9.0->tensorflow-recommenders==0.7.2->-r two_tower_jt/requirements.txt (line 2)) (3.8.0)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.9.0->tensorflow-recommenders==0.7.2->-r two_tower_jt/requirements.txt (line 2)) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.9.0->tensorflow-recommenders==0.7.2->-r two_tower_jt/requirements.txt (line 2)) (0.2.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.9.0->tensorflow-recommenders==0.7.2->-r two_tower_jt/requirements.txt (line 2)) (1.14.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.9.0->tensorflow-recommenders==0.7.2->-r two_tower_jt/requirements.txt (line 2)) (3.3.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.9.0->tensorflow-recommenders==0.7.2->-r two_tower_jt/requirements.txt (line 2)) (1.1.2)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.9.0->tensorflow-recommenders==0.7.2->-r two_tower_jt/requirements.txt (line 2)) (4.4.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.11,>=2.10.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.9.0->tensorflow-recommenders==0.7.2->-r two_tower_jt/requirements.txt (line 2)) (2.10.0)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.7/site-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform>=1.21.0->-r two_tower_jt/requirements.txt (line 1)) (1.5.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard==2.10.1->-r two_tower_jt/requirements.txt (line 3)) (3.11.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard==2.10.1->-r two_tower_jt/requirements.txt (line 3)) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard==2.10.1->-r two_tower_jt/requirements.txt (line 3)) (3.2.2)\n",
      "Installing collected packages: werkzeug, protobuf, packaging, google-api-core, tensorflow-recommenders\n",
      "  Attempting uninstall: werkzeug\n",
      "    Found existing installation: Werkzeug 2.1.2\n",
      "    Uninstalling Werkzeug-2.1.2:\n",
      "      Successfully uninstalled Werkzeug-2.1.2\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.20.1\n",
      "    Uninstalling protobuf-3.20.1:\n",
      "      Successfully uninstalled protobuf-3.20.1\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 23.0\n",
      "    Uninstalling packaging-23.0:\n",
      "      Successfully uninstalled packaging-23.0\n",
      "  Attempting uninstall: google-api-core\n",
      "    Found existing installation: google-api-core 2.10.1\n",
      "    Uninstalling google-api-core-2.10.1:\n",
      "      Successfully uninstalled google-api-core-2.10.1\n",
      "\u001b[91mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "google-api-python-client 1.8.0 requires google-api-core<2dev,>=1.13.0, but you have google-api-core 2.11.0 which is incompatible.\n",
      "\u001b[0mSuccessfully installed google-api-core-2.11.0 packaging-21.3 protobuf-3.19.6 tensorflow-recommenders-0.7.2 werkzeug-2.0.3\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mRemoving intermediate container 3d56a9255843\n",
      " ---> f6280dc78486\n",
      "Step 5/5 : RUN apt update && apt -y install nvtop\n",
      " ---> Running in f36e684bf7b9\n",
      "\u001b[91m\n",
      "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
      "\n",
      "\u001b[0mGet:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease [1581 B]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu focal InRelease [265 kB]\n",
      "Get:3 http://security.ubuntu.com/ubuntu focal-security InRelease [114 kB]\n",
      "Get:4 https://packages.cloud.google.com/apt cloud-sdk InRelease [6361 B]\n",
      "Get:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  Packages [871 kB]\n",
      "Get:6 https://packages.cloud.google.com/apt google-fast-socket InRelease [5015 B]\n",
      "Get:7 https://packages.cloud.google.com/apt cloud-sdk/main amd64 Packages [397 kB]\n",
      "Get:8 https://packages.cloud.google.com/apt google-fast-socket/main amd64 Packages [415 B]\n",
      "Get:9 http://archive.ubuntu.com/ubuntu focal-updates InRelease [114 kB]\n",
      "Get:10 http://security.ubuntu.com/ubuntu focal-security/universe amd64 Packages [998 kB]\n",
      "Get:11 http://archive.ubuntu.com/ubuntu focal-backports InRelease [108 kB]\n",
      "Get:12 http://archive.ubuntu.com/ubuntu focal/restricted amd64 Packages [33.4 kB]\n",
      "Get:13 http://archive.ubuntu.com/ubuntu focal/multiverse amd64 Packages [177 kB]\n",
      "Get:14 http://archive.ubuntu.com/ubuntu focal/main amd64 Packages [1275 kB]\n",
      "Get:15 http://security.ubuntu.com/ubuntu focal-security/multiverse amd64 Packages [28.5 kB]\n",
      "Get:16 http://security.ubuntu.com/ubuntu focal-security/restricted amd64 Packages [1937 kB]\n",
      "Get:17 http://packages.cloud.google.com/apt gcsfuse-focal InRelease [5002 B]\n",
      "Get:18 http://archive.ubuntu.com/ubuntu focal/universe amd64 Packages [11.3 MB]\n",
      "Get:19 http://security.ubuntu.com/ubuntu focal-security/main amd64 Packages [2496 kB]\n",
      "Get:20 http://packages.cloud.google.com/apt gcsfuse-focal/main amd64 Packages [1871 B]\n",
      "Get:21 http://archive.ubuntu.com/ubuntu focal-updates/restricted amd64 Packages [2066 kB]\n",
      "Get:22 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [1297 kB]\n",
      "Get:23 http://archive.ubuntu.com/ubuntu focal-updates/multiverse amd64 Packages [31.2 kB]\n",
      "Get:24 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [2970 kB]\n",
      "Get:25 http://archive.ubuntu.com/ubuntu focal-backports/universe amd64 Packages [28.6 kB]\n",
      "Get:26 http://archive.ubuntu.com/ubuntu focal-backports/main amd64 Packages [55.2 kB]\n",
      "Fetched 26.6 MB in 3s (9985 kB/s)\n",
      "Reading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "41 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
      "\u001b[91m\n",
      "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
      "\u001b[0mReading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "The following additional packages will be installed:\n",
      "  libnvidia-compute-418 libnvidia-compute-430 libnvidia-compute-525\n",
      "The following NEW packages will be installed:\n",
      "  libnvidia-compute-418 libnvidia-compute-430 libnvidia-compute-525 nvtop\n",
      "0 upgraded, 4 newly installed, 0 to remove and 41 not upgraded.\n",
      "Need to get 50.4 MB of archives.\n",
      "After this operation, 235 MB of additional disk space will be used.\n",
      "Get:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  libnvidia-compute-525 525.85.12-0ubuntu1 [50.4 MB]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu focal/restricted amd64 libnvidia-compute-418 amd64 430.50-0ubuntu3 [6936 B]\n",
      "Get:3 http://archive.ubuntu.com/ubuntu focal/multiverse amd64 nvtop amd64 1.0.0-1ubuntu2 [26.8 kB]\n",
      "Get:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  libnvidia-compute-430 525.85.12-0ubuntu1 [6908 B]\n",
      "\u001b[91mdebconf: unable to initialize frontend: Dialog\n",
      "debconf: (TERM is not set, so the dialog frontend is not usable.)\n",
      "debconf: falling back to frontend: Readline\n",
      "\u001b[0m\u001b[91mdebconf: unable to initialize frontend: Readline\n",
      "debconf: (This frontend requires a controlling tty.)\n",
      "debconf: falling back to frontend: Teletype\n",
      "\u001b[0m\u001b[91mdpkg-preconfigure: unable to re-open stdin: \n",
      "\u001b[0mFetched 50.4 MB in 1s (65.6 MB/s)\n",
      "Selecting previously unselected package libnvidia-compute-525:amd64.\n",
      "(Reading database ... 91209 files and directories currently installed.)\n",
      "Preparing to unpack .../libnvidia-compute-525_525.85.12-0ubuntu1_amd64.deb ...\n",
      "Unpacking libnvidia-compute-525:amd64 (525.85.12-0ubuntu1) ...\n",
      "Selecting previously unselected package libnvidia-compute-430:amd64.\n",
      "Preparing to unpack .../libnvidia-compute-430_525.85.12-0ubuntu1_amd64.deb ...\n",
      "Unpacking libnvidia-compute-430:amd64 (525.85.12-0ubuntu1) ...\n",
      "Selecting previously unselected package libnvidia-compute-418:amd64.\n",
      "Preparing to unpack .../libnvidia-compute-418_430.50-0ubuntu3_amd64.deb ...\n",
      "Unpacking libnvidia-compute-418:amd64 (430.50-0ubuntu3) ...\n",
      "Selecting previously unselected package nvtop.\n",
      "Preparing to unpack .../nvtop_1.0.0-1ubuntu2_amd64.deb ...\n",
      "Unpacking nvtop (1.0.0-1ubuntu2) ...\n",
      "Setting up libnvidia-compute-525:amd64 (525.85.12-0ubuntu1) ...\n",
      "Setting up libnvidia-compute-430:amd64 (525.85.12-0ubuntu1) ...\n",
      "Setting up libnvidia-compute-418:amd64 (430.50-0ubuntu3) ...\n",
      "Setting up nvtop (1.0.0-1ubuntu2) ...\n",
      "Processing triggers for man-db (2.9.1-1) ...\n",
      "Processing triggers for libc-bin (2.31-0ubuntu9.9) ...\n",
      "Removing intermediate container f36e684bf7b9\n",
      " ---> 65d9df440d4f\n",
      "Successfully built 65d9df440d4f\n",
      "Successfully tagged gcr.io/hybrid-vertex/sp-2tower-tfrs-trainerv1-tr:latest\n",
      "PUSH\n",
      "Pushing gcr.io/hybrid-vertex/sp-2tower-tfrs-trainerv1-tr\n",
      "The push refers to repository [gcr.io/hybrid-vertex/sp-2tower-tfrs-trainerv1-tr]\n",
      "626c61ad05c4: Preparing\n",
      "d420b794975d: Preparing\n",
      "d68289a24d67: Preparing\n",
      "bdc6fd38c1a0: Preparing\n",
      "361b970e2e5d: Preparing\n",
      "35a78a3c4be2: Preparing\n",
      "d8e0010a550b: Preparing\n",
      "619508e8ee99: Preparing\n",
      "bb988af2b984: Preparing\n",
      "4636fe01e0b5: Preparing\n",
      "f857cc25e56d: Preparing\n",
      "82471d6461d7: Preparing\n",
      "5ac9d337aafc: Preparing\n",
      "a735f6dad37e: Preparing\n",
      "bc6af4d104d8: Preparing\n",
      "aa25d86089a5: Preparing\n",
      "4cbfec24b842: Preparing\n",
      "8d2278f0bee2: Preparing\n",
      "2814e52cd888: Preparing\n",
      "ee9a177ae05b: Preparing\n",
      "339bec97e3ba: Preparing\n",
      "55df74fa2018: Preparing\n",
      "89c3a55cc66f: Preparing\n",
      "532c075aae0f: Preparing\n",
      "af18a29a82ba: Preparing\n",
      "d8f3347e3ae5: Preparing\n",
      "3663689faddd: Preparing\n",
      "5f70bf18a086: Preparing\n",
      "3bec1a9c1f4d: Preparing\n",
      "60766204fa42: Preparing\n",
      "b3936e4c67d2: Preparing\n",
      "07d37209b7a9: Preparing\n",
      "fed0e8aa65b5: Preparing\n",
      "ad8fec0b36f1: Preparing\n",
      "3a217af3edf9: Preparing\n",
      "3297f5de02be: Preparing\n",
      "f3717d7fdfb7: Preparing\n",
      "e1eace4c0976: Preparing\n",
      "959a7375cb04: Preparing\n",
      "d79c672e1e8b: Preparing\n",
      "7b7c9e761223: Preparing\n",
      "0002c93bdb37: Preparing\n",
      "35a78a3c4be2: Waiting\n",
      "af18a29a82ba: Waiting\n",
      "d8e0010a550b: Waiting\n",
      "ad8fec0b36f1: Waiting\n",
      "07d37209b7a9: Waiting\n",
      "959a7375cb04: Waiting\n",
      "fed0e8aa65b5: Waiting\n",
      "b3936e4c67d2: Waiting\n",
      "d79c672e1e8b: Waiting\n",
      "3a217af3edf9: Waiting\n",
      "7b7c9e761223: Waiting\n",
      "3297f5de02be: Waiting\n",
      "0002c93bdb37: Waiting\n",
      "f3717d7fdfb7: Waiting\n",
      "e1eace4c0976: Waiting\n",
      "3bec1a9c1f4d: Waiting\n",
      "619508e8ee99: Waiting\n",
      "aa25d86089a5: Waiting\n",
      "8d2278f0bee2: Waiting\n",
      "bb988af2b984: Waiting\n",
      "ee9a177ae05b: Waiting\n",
      "4636fe01e0b5: Waiting\n",
      "55df74fa2018: Waiting\n",
      "339bec97e3ba: Waiting\n",
      "f857cc25e56d: Waiting\n",
      "2814e52cd888: Waiting\n",
      "a735f6dad37e: Waiting\n",
      "82471d6461d7: Waiting\n",
      "5ac9d337aafc: Waiting\n",
      "bc6af4d104d8: Waiting\n",
      "3663689faddd: Waiting\n",
      "60766204fa42: Waiting\n",
      "5f70bf18a086: Waiting\n",
      "361b970e2e5d: Layer already exists\n",
      "35a78a3c4be2: Layer already exists\n",
      "d8e0010a550b: Layer already exists\n",
      "619508e8ee99: Layer already exists\n",
      "bb988af2b984: Layer already exists\n",
      "4636fe01e0b5: Layer already exists\n",
      "d68289a24d67: Pushed\n",
      "f857cc25e56d: Layer already exists\n",
      "bdc6fd38c1a0: Pushed\n",
      "5ac9d337aafc: Layer already exists\n",
      "82471d6461d7: Layer already exists\n",
      "a735f6dad37e: Layer already exists\n",
      "bc6af4d104d8: Layer already exists\n",
      "4cbfec24b842: Layer already exists\n",
      "aa25d86089a5: Layer already exists\n",
      "8d2278f0bee2: Layer already exists\n",
      "2814e52cd888: Layer already exists\n",
      "d420b794975d: Pushed\n",
      "ee9a177ae05b: Layer already exists\n",
      "339bec97e3ba: Layer already exists\n",
      "55df74fa2018: Layer already exists\n",
      "89c3a55cc66f: Layer already exists\n",
      "532c075aae0f: Layer already exists\n",
      "d8f3347e3ae5: Layer already exists\n",
      "af18a29a82ba: Layer already exists\n",
      "3663689faddd: Layer already exists\n",
      "5f70bf18a086: Layer already exists\n",
      "3bec1a9c1f4d: Layer already exists\n",
      "60766204fa42: Layer already exists\n",
      "b3936e4c67d2: Layer already exists\n",
      "07d37209b7a9: Layer already exists\n",
      "fed0e8aa65b5: Layer already exists\n",
      "ad8fec0b36f1: Layer already exists\n",
      "3a217af3edf9: Layer already exists\n",
      "3297f5de02be: Layer already exists\n",
      "f3717d7fdfb7: Layer already exists\n",
      "e1eace4c0976: Layer already exists\n",
      "959a7375cb04: Layer already exists\n",
      "d79c672e1e8b: Layer already exists\n",
      "0002c93bdb37: Layer already exists\n",
      "7b7c9e761223: Layer already exists\n",
      "626c61ad05c4: Pushed\n",
      "latest: digest: sha256:f84299a2781aa9df83106856b907c24c757a88a906133d620ba04352c31d78c2 size: 9133\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                       IMAGES                                                      STATUS\n",
      "2ba1aaaa-161b-4d76-9053-531b3a91ee58  2023-02-22T19:55:47+00:00  3M49S     gs://hybrid-vertex_cloudbuild/source/1677095746.824004-c07bf07b8ac7430383aab53aedaf2526.tgz  gcr.io/hybrid-vertex/sp-2tower-tfrs-trainerv1-tr (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "! gcloud builds submit --config src/cloudbuild.yaml \\\n",
    "    --substitutions _DOCKERNAME=$DOCKERNAME,_IMAGE_URI=$IMAGE_URI,_FILE_LOCATION=$FILE_LOCATION \\\n",
    "    --timeout=2h \\\n",
    "    --machine-type=$MACHINE_TYPE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c45c7a0-c35e-4d60-9b70-e020e82402ab",
   "metadata": {},
   "source": [
    "## Prepare Train Job Specs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28730bb-4171-43d0-9cbf-9492f1e31526",
   "metadata": {},
   "source": [
    "### workerpool specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1e979861-41c9-47f0-bba0-3eb0133cf40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_worker_pool_specs(\n",
    "    image_uri,\n",
    "    args,\n",
    "    cmd,\n",
    "    replica_count=1,\n",
    "    machine_type=\"n1-standard-16\",\n",
    "    accelerator_count=1,\n",
    "    accelerator_type=\"ACCELERATOR_TYPE_UNSPECIFIED\",\n",
    "    reduction_server_count=0,\n",
    "    reduction_server_machine_type=\"n1-highcpu-16\",\n",
    "    reduction_server_image_uri=\"us-docker.pkg.dev/vertex-ai-restricted/training/reductionserver:latest\",\n",
    "):\n",
    "\n",
    "    if accelerator_count > 0:\n",
    "        machine_spec = {\n",
    "            \"machine_type\": machine_type,\n",
    "            \"accelerator_type\": accelerator_type,\n",
    "            \"accelerator_count\": accelerator_count,\n",
    "        }\n",
    "    else:\n",
    "        machine_spec = {\"machine_type\": machine_type}\n",
    "\n",
    "    container_spec = {\n",
    "        \"image_uri\": image_uri,\n",
    "        \"args\": args,\n",
    "        \"command\": cmd,\n",
    "    }\n",
    "\n",
    "    chief_spec = {\n",
    "        \"replica_count\": 1,\n",
    "        \"machine_spec\": machine_spec,\n",
    "        \"container_spec\": container_spec,\n",
    "    }\n",
    "\n",
    "    worker_pool_specs = [chief_spec]\n",
    "    if replica_count > 1:\n",
    "        workers_spec = {\n",
    "            \"replica_count\": replica_count - 1,\n",
    "            \"machine_spec\": machine_spec,\n",
    "            \"container_spec\": container_spec,\n",
    "        }\n",
    "        worker_pool_specs.append(workers_spec)\n",
    "    if reduction_server_count > 1:\n",
    "        workers_spec = {\n",
    "            \"replica_count\": reduction_server_count,\n",
    "            \"machine_spec\": {\n",
    "                \"machine_type\": reduction_server_machine_type,\n",
    "            },\n",
    "            \"container_spec\": {\"image_uri\": reduction_server_image_uri},\n",
    "        }\n",
    "        worker_pool_specs.append(workers_spec)\n",
    "\n",
    "    return worker_pool_specs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa482a72-54a8-4ef0-9fbc-de576b18443e",
   "metadata": {},
   "source": [
    "### Training Accelerators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7e0ce8e9-6f25-49ec-a1ae-a287dfd0b58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKER_MACHINE_TYPE = 'a2-highgpu-1g'\n",
    "REPLICA_COUNT = 1\n",
    "ACCELERATOR_TYPE = 'NVIDIA_TESLA_A100'\n",
    "PER_MACHINE_ACCELERATOR_COUNT = 1\n",
    "REDUCTION_SERVER_COUNT = 0                                                      \n",
    "REDUCTION_SERVER_MACHINE_TYPE = \"n1-highcpu-16\"\n",
    "DISTRIBUTE_STRATEGY = 'single'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d132c8-c958-4b3c-a4df-ae56f24ecefc",
   "metadata": {},
   "source": [
    "## Vertex Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fa718a-1d12-4921-b1ca-9dae7b7cb661",
   "metadata": {},
   "source": [
    "### Create an experiemnt and experiment run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b87a2c89-08c9-4b52-8f25-bfa60a92b82e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPERIMENT_NAME: customjob-test-trainerv1\n",
      "RUN_NAME: run-20230222-200110\n"
     ]
    }
   ],
   "source": [
    "EXPERIMENT_PREFIX = 'customjob-test'                     # custom identifier for organizing experiments\n",
    "EXPERIMENT_NAME=f'{EXPERIMENT_PREFIX}-{VERSION}'\n",
    "RUN_NAME = f'run-{time.strftime(\"%Y%m%d-%H%M%S\")}'\n",
    "\n",
    "print(f\"EXPERIMENT_NAME: {EXPERIMENT_NAME}\")\n",
    "print(f\"RUN_NAME: {RUN_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e934b4be-dd2a-48ff-84de-be9525b741ac",
   "metadata": {},
   "source": [
    "### Create Managed TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e6952007-ed86-4e41-bffb-59b27107dcc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TB_RESOURCE_NAME: projects/934903580331/locations/us-central1/tensorboards/3550358230460792832\n",
      "TB display name: customjob-test-v1\n"
     ]
    }
   ],
   "source": [
    "# use existing TB instance\n",
    "# TB_RESOURCE_NAME = 'projects/934903580331/locations/us-central1/tensorboards/6924469145035603968'\n",
    "\n",
    "# # create new TB instance\n",
    "TENSORBOARD_DISPLAY_NAME=f\"{EXPERIMENT_PREFIX}-v1\"\n",
    "tensorboard = vertex_ai.Tensorboard.create(display_name=TENSORBOARD_DISPLAY_NAME, project=PROJECT_ID, location=REGION)\n",
    "TB_RESOURCE_NAME = tensorboard.resource_name\n",
    "\n",
    "\n",
    "print(f\"TB_RESOURCE_NAME: {TB_RESOURCE_NAME}\")\n",
    "print(f\"TB display name: {tensorboard.display_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a4cf3b-0e59-4604-96fa-8b658057bc04",
   "metadata": {},
   "source": [
    "## Training config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3cb06e80-175e-45df-8f33-89f85987e766",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "\n",
    "# =================================================\n",
    "# trainconfig: GPU related\n",
    "# =================================================\n",
    "TF_GPU_THREAD_COUNT='8'      # '1' | '4' | '8'\n",
    "\n",
    "# =================================================\n",
    "# trainconfig: data input pipeline\n",
    "# =================================================\n",
    "BLOCK_LENGTH = 64            # 1, 8, 16, 32, 64\n",
    "NUM_DATA_SHARDS = 4          # 2, 4, 8, 16, 32, 64\n",
    "# TRAIN_PREFETCH=3\n",
    "\n",
    "# =================================================\n",
    "# trainconfig: training hparams\n",
    "# =================================================\n",
    "NUM_EPOCHS = 2\n",
    "LEARNING_RATE = 0.01\n",
    "BATCH_SIZE = 8192           # 4096, 2048, 1024, 512 \n",
    "\n",
    "# dropout\n",
    "DROPOUT_RATE = 0.33\n",
    "\n",
    "# model size\n",
    "EMBEDDING_DIM = 128\n",
    "PROJECTION_DIM = 50\n",
    "LAYER_SIZES = '[512,256,128]'\n",
    "MAX_TOKENS = 20000     # vocab\n",
    "\n",
    "# =================================================\n",
    "# trainconfig: tensorboard\n",
    "# =================================================\n",
    "EMBED_FREQUENCY=0\n",
    "HISTOGRAM_FREQUENCY=0\n",
    "CHECKPOINT_FREQ='epoch'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9e0a3d78-0570-49fd-a2ae-018375f82ce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALID_STEPS: 10\n",
      "EPOCH_STEPS: 1001\n"
     ]
    }
   ],
   "source": [
    "# =================================================\n",
    "# trainconfig: train & valid steps\n",
    "# =================================================\n",
    "train_sample_cnt = 8_205_265 # 8_205_265\n",
    "valid_samples_cnt = 82_959\n",
    "\n",
    "# validation & evaluation\n",
    "VALID_FREQUENCY = 20\n",
    "VALID_STEPS = valid_samples_cnt // BATCH_SIZE # 100\n",
    "EPOCH_STEPS = train_sample_cnt // BATCH_SIZE\n",
    "\n",
    "print(f\"VALID_STEPS: {VALID_STEPS}\")\n",
    "print(f\"EPOCH_STEPS: {EPOCH_STEPS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01dd24f-3005-4fa2-ac03-65d97cb3d001",
   "metadata": {},
   "source": [
    "### data sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f39edd30-6c16-4dc6-9381-9a2cdbf14c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================\n",
    "# trainconfig: gcs locations\n",
    "# =================================================\n",
    "OUTPUT_BUCKET = 'jt-tfrs-central-v3' # TODO: change this\n",
    "OUTPUT_GCS_URI =f'gs://{OUTPUT_BUCKET}'\n",
    "\n",
    "# =================================================\n",
    "# trainconfig: Data sources\n",
    "# =================================================\n",
    "BUCKET_DATA_DIR = 'spotify-data-regimes' \n",
    "\n",
    "# data strategy: 08m\n",
    "CANDIDATE_PREFIX = 'jtv15-8m/candidates'\n",
    "TRAIN_DIR_PREFIX = 'jtv15-8m/train'     # train | train_v14\n",
    "VALID_DIR_PREFIX = 'jtv15-8m/valid'     # valid_v14"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2862f6a6-11f5-44be-ac64-a3cd0e8a6ed1",
   "metadata": {},
   "source": [
    "### training args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "651cc211-4660-406d-a586-5e60ebfeb805",
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKER_CMD = [\"python\", \"two_tower_jt/task.py\"]\n",
    "# WORKER_CMD [\"python\", \"-m\", \"trainer.task\"]\n",
    "\n",
    "WORKER_ARGS = [\n",
    "    f'--project={PROJECT_ID}',\n",
    "    f'--train_output_gcs_bucket={OUTPUT_BUCKET}',\n",
    "    f'--train_dir={BUCKET_DATA_DIR}',\n",
    "    f'--train_dir_prefix={TRAIN_DIR_PREFIX}',\n",
    "    f'--valid_dir={BUCKET_DATA_DIR}',\n",
    "    f'--valid_dir_prefix={VALID_DIR_PREFIX}',\n",
    "    f'--candidate_file_dir={BUCKET_DATA_DIR}',\n",
    "    f'--candidate_files_prefix={CANDIDATE_PREFIX}',\n",
    "    f'--experiment_name={EXPERIMENT_NAME}',\n",
    "    f'--experiment_run={RUN_NAME}',\n",
    "    f'--num_epochs={NUM_EPOCHS}',\n",
    "    f'--batch_size={BATCH_SIZE}',\n",
    "    f'--embedding_dim={EMBEDDING_DIM}',\n",
    "    f'--projection_dim={PROJECTION_DIM}',\n",
    "    f'--layer_sizes={LAYER_SIZES}',\n",
    "    f'--learning_rate={LEARNING_RATE}',\n",
    "    f'--valid_frequency={VALID_FREQUENCY}',\n",
    "    f'--valid_steps={VALID_STEPS}',\n",
    "    f'--epoch_steps={EPOCH_STEPS}',\n",
    "    f'--distribute={DISTRIBUTE_STRATEGY}',\n",
    "    f'--model_version={VERSION}',\n",
    "    f'--pipeline_version={VERSION}',\n",
    "    f'--seed={SEED}',\n",
    "    f'--max_tokens={MAX_TOKENS}',\n",
    "    f'--tb_resource_name={TB_RESOURCE_NAME}',\n",
    "    f'--embed_frequency={EMBED_FREQUENCY}',\n",
    "    f'--hist_frequency={HISTOGRAM_FREQUENCY}',\n",
    "    f'--tf_gpu_thread_count={TF_GPU_THREAD_COUNT}',\n",
    "    f'--block_length={BLOCK_LENGTH}',\n",
    "    f'--num_data_shards={NUM_DATA_SHARDS}',\n",
    "    f'--chkpt_freq={CHECKPOINT_FREQ}',\n",
    "    f'--dropout_rate={DROPOUT_RATE}',\n",
    "    # uncomment these to pass value of True (bool)\n",
    "    f'--cache_train',                                # caches train_dataset\n",
    "    # f'--evaluate_model',                           # runs model.eval()\n",
    "    # f'--write_embeddings',                         # writes embeddings index in train job\n",
    "    f'--profiler',                                   # runs TB profiler\n",
    "    # f'--set_jit',                                  # enables XLA\n",
    "    f'--compute_batch_metrics',\n",
    "    f'--use_cross_layer',\n",
    "    f'--use_dropout',\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "354feb4e-43c7-4238-9d4c-8a37b6278a62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'container_spec': {'args': ['--project=hybrid-vertex',\n",
      "                              '--train_output_gcs_bucket=jt-tfrs-central-v3',\n",
      "                              '--train_dir=spotify-data-regimes',\n",
      "                              '--train_dir_prefix=jtv15-8m/train',\n",
      "                              '--valid_dir=spotify-data-regimes',\n",
      "                              '--valid_dir_prefix=jtv15-8m/valid',\n",
      "                              '--candidate_file_dir=spotify-data-regimes',\n",
      "                              '--candidate_files_prefix=jtv15-8m/candidates',\n",
      "                              '--experiment_name=customjob-test-trainerv1',\n",
      "                              '--experiment_run=run-20230222-200110',\n",
      "                              '--num_epochs=2',\n",
      "                              '--batch_size=8192',\n",
      "                              '--embedding_dim=128',\n",
      "                              '--projection_dim=50',\n",
      "                              '--layer_sizes=[512,256,128]',\n",
      "                              '--learning_rate=0.01',\n",
      "                              '--valid_frequency=20',\n",
      "                              '--valid_steps=10',\n",
      "                              '--epoch_steps=1001',\n",
      "                              '--distribute=single',\n",
      "                              '--model_version=trainerv1',\n",
      "                              '--pipeline_version=trainerv1',\n",
      "                              '--seed=1234',\n",
      "                              '--max_tokens=20000',\n",
      "                              '--tb_resource_name=projects/934903580331/locations/us-central1/tensorboards/3550358230460792832',\n",
      "                              '--embed_frequency=0',\n",
      "                              '--hist_frequency=0',\n",
      "                              '--tf_gpu_thread_count=8',\n",
      "                              '--block_length=64',\n",
      "                              '--num_data_shards=4',\n",
      "                              '--chkpt_freq=epoch',\n",
      "                              '--dropout_rate=0.33',\n",
      "                              '--cache_train',\n",
      "                              '--profiler',\n",
      "                              '--compute_batch_metrics',\n",
      "                              '--use_cross_layer',\n",
      "                              '--use_dropout'],\n",
      "                     'command': ['python', 'two_tower_jt/task.py'],\n",
      "                     'image_uri': 'gcr.io/hybrid-vertex/sp-2tower-tfrs-trainerv1-tr'},\n",
      "  'machine_spec': {'accelerator_count': 1,\n",
      "                   'accelerator_type': 'NVIDIA_TESLA_A100',\n",
      "                   'machine_type': 'a2-highgpu-1g'},\n",
      "  'replica_count': 1}]\n"
     ]
    }
   ],
   "source": [
    "WORKER_POOL_SPECS = prepare_worker_pool_specs(\n",
    "    image_uri=IMAGE_URI,\n",
    "    args=WORKER_ARGS,\n",
    "    cmd=WORKER_CMD,\n",
    "    replica_count=REPLICA_COUNT,\n",
    "    machine_type=WORKER_MACHINE_TYPE,\n",
    "    accelerator_count=PER_MACHINE_ACCELERATOR_COUNT,\n",
    "    accelerator_type=ACCELERATOR_TYPE,\n",
    "    reduction_server_count=REDUCTION_SERVER_COUNT,\n",
    "    reduction_server_machine_type=REDUCTION_SERVER_MACHINE_TYPE,\n",
    ")\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(WORKER_POOL_SPECS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79957d8-b9f4-4c64-86df-908ad5a03bbd",
   "metadata": {},
   "source": [
    "### copy training package to GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5f034f37-89cc-41aa-ba3d-f648f924db07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://src/cloudbuild.yaml [Content-Type=application/octet-stream]...\n",
      "/ [1 files][  178.0 B/  178.0 B]                                                \n",
      "Operation completed over 1 objects/178.0 B.                                      \n",
      "Copying file://src/Dockerfile.tfrs [Content-Type=application/octet-stream]...\n",
      "/ [1 files][  282.0 B/  282.0 B]                                                \n",
      "Operation completed over 1 objects/282.0 B.                                      \n",
      "Copying file://src/two_tower_jt/__init__.py [Content-Type=text/x-python]...\n",
      "Copying file://src/two_tower_jt/__pycache__/__init__.cpython-37.pyc [Content-Type=application/x-python-code]...\n",
      "Copying file://src/two_tower_jt/data-pipeline.py [Content-Type=text/x-python]...\n",
      "Copying file://src/two_tower_jt/__pycache__/two_tower.cpython-37.pyc [Content-Type=application/x-python-code]...\n",
      "Copying file://src/two_tower_jt/__pycache__/test_instances.cpython-37.pyc [Content-Type=application/x-python-code]...\n",
      "Copying file://src/two_tower_jt/interactive_train.py [Content-Type=text/x-python]...\n",
      "Copying file://src/two_tower_jt/requirements.txt [Content-Type=text/plain]...   \n",
      "Copying file://src/two_tower_jt/task.py [Content-Type=text/x-python]...         \n",
      "Copying file://src/two_tower_jt/__pycache__/train_config.cpython-37.pyc [Content-Type=application/x-python-code]...\n",
      "Copying file://src/two_tower_jt/test_instances.py [Content-Type=text/x-python]...\n",
      "Copying file://src/two_tower_jt/train_config.py [Content-Type=text/x-python]... \n",
      "Copying file://src/two_tower_jt/two_tower.py [Content-Type=text/x-python]...    \n",
      "Copying file://src/two_tower_jt/two_tower_lite.py [Content-Type=text/x-python]...\n",
      "/ [13/13 files][162.2 KiB/162.2 KiB] 100% Done                                  \n",
      "Operation completed over 13 objects/162.2 KiB.                                   \n",
      "\n",
      " Copied training package and Dockerfile to gs://jt-tfrs-central-v3/customjob-test-trainerv1/run-20230222-200110\n",
      "\n"
     ]
    }
   ],
   "source": [
    "BASE_OUTPUT_DIR = f'gs://{OUTPUT_BUCKET}/{EXPERIMENT_NAME}/{RUN_NAME}'\n",
    "\n",
    "# copy training Dockerfile\n",
    "!gsutil cp $REPO_DOCKER_PATH_PREFIX/cloudbuild.yaml $BASE_OUTPUT_DIR/cloudbuild.yaml\n",
    "!gsutil cp $REPO_DOCKER_PATH_PREFIX/Dockerfile.tfrs $BASE_OUTPUT_DIR/Dockerfile.tfrs\n",
    "\n",
    "# # # copy training application code\n",
    "! gsutil -m cp -r $REPO_DOCKER_PATH_PREFIX/two_tower_jt/* $BASE_OUTPUT_DIR/trainer\n",
    "\n",
    "print(f\"\\n Copied training package and Dockerfile to {BASE_OUTPUT_DIR}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556feb75-b1df-4a8d-a852-9e9be89f15fa",
   "metadata": {},
   "source": [
    "# submit Training job to Vertex AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "93b8a038-15b4-4a11-a458-e854f246d0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_ai.init(\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    "    experiment=EXPERIMENT_NAME,\n",
    ")\n",
    "\n",
    "JOB_NAME = f'train-{VERSION}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "36104f63-a11c-44f5-8f4c-6ff87b6784e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "job = vertex_ai.CustomJob(\n",
    "    display_name=JOB_NAME,\n",
    "    worker_pool_specs=WORKER_POOL_SPECS,\n",
    "    base_output_dir=BASE_OUTPUT_DIR,\n",
    "    staging_bucket=f\"{BASE_OUTPUT_DIR}/staging\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1132d226-6862-4458-87a1-972c8b6deeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "job.run(\n",
    "    tensorboard=TB_RESOURCE_NAME,\n",
    "    service_account=VERTEX_SA,\n",
    "    restart_job_on_worker_restart=False,\n",
    "    enable_web_access=True,\n",
    "    sync=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56675ff9-158e-486b-ad94-76a9c0fc0b0a",
   "metadata": {},
   "source": [
    "## TensorBoard Profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fd5533-d364-45a6-b1de-2db034d6df6f",
   "metadata": {},
   "source": [
    "Once the profiler has uploaded trace logs to `BASE_OUTPUT_DIR/logs`, we can use the in-notebook tensoborad extension to view the profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "33a58184-e0c4-4403-ad80-da5fdb7dbaed",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "TB_LOGS_PATH = f'{BASE_OUTPUT_DIR}/logs' # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d432d552-4416-4f4c-964f-bcca57b60a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "# %reload_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "171227db-d5f9-47e4-8b67-a528950233ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-535181dc4fb6663b\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-535181dc4fb6663b\");\n",
       "          const url = new URL(\"/proxy/6006/\", window.location);\n",
       "          const port = 0;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir=$TB_LOGS_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8341545a-cadf-448a-b746-c5f74e33e58c",
   "metadata": {},
   "source": [
    "# Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e2fe35-7abe-4d76-9189-c58c4b5c1026",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mservice_account\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnetwork\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mrestart_job_on_worker_restart\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0menable_web_access\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtensorboard\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msync\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcreate_request_timeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Run this configured CustomJob.\n",
       "\n",
       "Args:\n",
       "    service_account (str):\n",
       "        Optional. Specifies the service account for workload run-as account.\n",
       "        Users submitting jobs must have act-as permission on this run-as account.\n",
       "    network (str):\n",
       "        Optional. The full name of the Compute Engine network to which the job\n",
       "        should be peered. For example, projects/12345/global/networks/myVPC.\n",
       "        Private services access must already be configured for the network.\n",
       "        If left unspecified, the job is not peered with any network.\n",
       "    timeout (int):\n",
       "        The maximum job running time in seconds. The default is 7 days.\n",
       "    restart_job_on_worker_restart (bool):\n",
       "        Restarts the entire CustomJob if a worker\n",
       "        gets restarted. This feature can be used by\n",
       "        distributed training jobs that are not resilient\n",
       "        to workers leaving and joining a job.\n",
       "    enable_web_access (bool):\n",
       "        Whether you want Vertex AI to enable interactive shell access\n",
       "        to training containers.\n",
       "        https://cloud.google.com/vertex-ai/docs/training/monitor-debug-interactive-shell\n",
       "    tensorboard (str):\n",
       "        Optional. The name of a Vertex AI\n",
       "        [Tensorboard][google.cloud.aiplatform.v1beta1.Tensorboard]\n",
       "        resource to which this CustomJob will upload Tensorboard\n",
       "        logs. Format:\n",
       "        ``projects/{project}/locations/{location}/tensorboards/{tensorboard}``\n",
       "\n",
       "        The training script should write Tensorboard to following Vertex AI environment\n",
       "        variable:\n",
       "\n",
       "        AIP_TENSORBOARD_LOG_DIR\n",
       "\n",
       "        `service_account` is required with provided `tensorboard`.\n",
       "        For more information on configuring your service account please visit:\n",
       "        https://cloud.google.com/vertex-ai/docs/experiments/tensorboard-training\n",
       "    sync (bool):\n",
       "        Whether to execute this method synchronously. If False, this method\n",
       "        will unblock and it will be executed in a concurrent Future.\n",
       "    create_request_timeout (float):\n",
       "        Optional. The timeout for the create request in seconds.\n",
       "\u001b[0;31mFile:\u001b[0m      /opt/conda/lib/python3.7/site-packages/google/cloud/aiplatform/jobs.py\n",
       "\u001b[0;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?job.run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e0ce91-a30a-474c-9f85-07a4ce6dde6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-10.m103",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-10:m103"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
