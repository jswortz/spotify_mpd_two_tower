{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ae44dcc-9f01-4d78-b5e6-4981f3a6558b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install apache-beam[gcp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6dcba672-f968-4bc4-bbdf-e9b6929b5169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF version: 2.9.0-rc2\n",
      "TFDV version: 1.9.0\n",
      "Beam version: 2.40.0\n",
      "BQ SDK version: 2.34.4\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf \n",
    "import apache_beam as beam \n",
    "import tensorflow_data_validation as tfdv\n",
    "from google.cloud import bigquery\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "# from tensorflow_transform.tf_metadata import dataset_metadata\n",
    "# from tensorflow_transform.tf_metadata import dataset_schema\n",
    "\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "from apache_beam.options.pipeline_options import SetupOptions\n",
    "\n",
    "print(\"TF version:\", tf.__version__)\n",
    "print(\"TFDV version:\", tfdv.__version__)\n",
    "print(\"Beam version:\", beam.__version__)\n",
    "print(\"BQ SDK version:\", bigquery.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92cb94ca-f4dc-43cd-85b9-ef24a0797cb8",
   "metadata": {},
   "source": [
    "## Helpful Guides\n",
    "* [tfdv-structured-data-flights](https://notebook.community/GoogleCloudPlatform/mlops-on-gcp/examples/tfdv-structured-data/tfdv-flights)\n",
    "* [sequential-data from BQ to TF-Records with apache beam](https://blog.ivanukhov.com/2019/11/08/sequential-data.html)\n",
    "* [forecast-pipeline - from BQ to TF-Records with apache beam](https://github.com/chain-rule/example-weather-forecast/blob/master/forecast/pipeline.py)\n",
    "* [Dataflow - Create TF-Records](https://quantiphi.com/scaling-etl-pipeline-for-creating-tensorflow-records-using-apache-beam-python-sdk-on-google-cloud-dataflow/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d9340c-3bfe-473f-aafc-bf592a1e0b82",
   "metadata": {},
   "source": [
    "# Apache Beam Pipeline -> write tf-records from BQ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977ab1aa-162c-4cc5-8696-10fccf088015",
   "metadata": {},
   "source": [
    "### Table rows\n",
    "* BigQueryIO read and write transforms produce and consume data as a `PCollection` of dictionaries, where each element in the `PCollection` represents a single row in the table\n",
    "\n",
    "### Schemas\n",
    "* When writing to BigQuery, you must supply a table schema for the destination table that you want to write to, unless you specify a create disposition of CREATE_NEVER. \n",
    "* Creating a table schema covers schemas in more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c87133f9-56a5-4db2-882e-c30a46a3abf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = 'hybrid-vertex'\n",
    "BUCKET_NAME = 'spotify-beam-v1' # 'spotify-tfrecords-blog' # Set your Bucket name\n",
    "REGION = 'us-central1' # Set the region for Dataflow jobs\n",
    "VERSION = 'v1'\n",
    "\n",
    "ROOT = f'gs://{BUCKET_NAME}/{VERSION}'\n",
    "\n",
    "CANDIDATE_DIR = ROOT + \"/candidates/\"\n",
    "DATA_DIR = ROOT + '/data/' # Location to store data\n",
    "STATS_DIR = ROOT +'/stats/' # Location to store stats \n",
    "STAGING_DIR = ROOT + '/job/staging/' # Dataflow staging directory on GCP\n",
    "TEMP_DIR =  ROOT + '/job/temp/' # Dataflow temporary directory on GCP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e828016-71f9-49d3-81f4-32979e27d7f8",
   "metadata": {},
   "source": [
    "### Cleanup working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "45e2526c-a27a-4692-af4e-c1f73ba0321b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_root ='gs://spotify-tfrecords-blog/candidate_tracks_v2'\n",
    "# files = np.array(tf.io.gfile.listdir(test_root))\n",
    "# for file in files:\n",
    "#     file_to_remove = f'{test_root}/{file}'\n",
    "#     print(file_to_remove)\n",
    "#     break\n",
    "#     # tf.io.gfile.remove(f'{test_root}/{file})\n",
    "# # files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dbb5c992-e209-4511-854b-3cf15c0a483a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating working directory: gs://spotify-beam-v1/v1\n"
     ]
    }
   ],
   "source": [
    "if tf.io.gfile.exists(ROOT):\n",
    "    print(\"Removing {} contents...\".format(ROOT))\n",
    "    files = np.array(tf.io.gfile.listdir(ROOT))\n",
    "    for file in files:\n",
    "        tf.io.gfile.remove(f'{ROOT}/{file}')\n",
    "\n",
    "print(\"Creating working directory: {}\".format(ROOT))\n",
    "tf.io.gfile.mkdir(ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d647f8-397b-4c16-9c00-cba56887cf31",
   "metadata": {},
   "source": [
    "## Extract data from BigQuery to GCS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d67b621-fe22-472a-854a-0dd7c5bd0d4d",
   "metadata": {},
   "source": [
    "* extract data from BigQuery, \n",
    "* convert data to TFRecord files, \n",
    "* store data files in Google Cloud Storage (GCS). \n",
    "> * This data file in GCS will then be used by TFDV for stats & vocabs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1277a70-fb1d-4964-8901-aac53a49d5de",
   "metadata": {},
   "source": [
    "### Define SQL Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fd1052d9-5f85-4cbd-8bb5-1c497a47859f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_query(project=None, bq_dataset=None, bq_table=None, limit=None):\n",
    "    query =f\"\"\"\n",
    "        SELECT *\n",
    "        FROM \n",
    "          `{project}.{bq_dataset}.{bq_table}`\n",
    "        \"\"\"\n",
    "    if limit:\n",
    "        query  += \"LIMIT {}\".format(limit)\n",
    "        \n",
    "    return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b88469e3-7ee5-4aeb-849e-17ec9cd923d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.train import BytesList, Feature, FeatureList, Int64List, FloatList\n",
    "from tensorflow.train import SequenceExample, FeatureLists\n",
    "\n",
    "\n",
    "def string_array(value):\n",
    "    \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[v.encode('utf-8') for v in value]))\n",
    "\n",
    "\n",
    "def float_feature(value):\n",
    "    \"\"\"Returns a float_list from a float / double.\"\"\"\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=[float(v) for v in value]))\n",
    "\n",
    "\n",
    "def int64_feature(value):\n",
    "    \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[int(v) for v in value]))\n",
    "\n",
    "\n",
    "def float_feature_list(value):\n",
    "    \"\"\"Returns a list of float_list from a float / double.\"\"\"\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=value))\n",
    "\n",
    "def get_type_map(query):\n",
    "    bq_client = bigquery.Client()\n",
    "    query_job = bq_client.query(f\"{query}\")\n",
    "    results = query_job.result()\n",
    "    \n",
    "    type_map = {}\n",
    "    \n",
    "    for field in results.schema:\n",
    "        type_map[field.name] = field.field_type\n",
    "    \n",
    "    return type_map\n",
    "\n",
    "# def get_mode_map(query):\n",
    "#     bq_client = bigquery.Client()\n",
    "#     query_job = bq_client.query(f\"{query}\")\n",
    "#     results = query_job.result()\n",
    "    \n",
    "#     mode_map = {}\n",
    "    \n",
    "#     for field in results.schema:\n",
    "#         mode_map[field.name] = field.mode\n",
    "    \n",
    "#     return mode_map\n",
    "\n",
    "\n",
    "def row_to_example(instance, type_map):\n",
    "    feature = {}\n",
    "    for key, value in instance.items():\n",
    "        data_type = type_map[key]\n",
    "        if value is None:\n",
    "            feature[key] = tf.train.Feature()\n",
    "        elif data_type == 'INTEGER':\n",
    "            feature[key] = tf.train.Feature(\n",
    "                int64_list=tf.train.Int64List(value=[value]))\n",
    "        elif data_type == 'FLOAT':\n",
    "            feature[key] = tf.train.Feature(\n",
    "                float_list=tf.train.FloatList(value=[value]))\n",
    "        else:\n",
    "            feature[key] = tf.train.Feature(\n",
    "                bytes_list=tf.train.BytesList(value=[tf.compat.as_bytes(value)]))\n",
    "            \n",
    "    return tf.train.Example(features=tf.train.Features(feature=feature))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d4ab57-209d-4e39-beaf-ebf3811179c4",
   "metadata": {},
   "source": [
    "## Submit Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "23376698-cf92-4529-be82-5023f30b5ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipeline(args):\n",
    "\n",
    "    source_query = args.pop('source_query')\n",
    "    sink_data_location = args.pop('sink_data_location')\n",
    "    runner = args['runner']\n",
    "    \n",
    "    pipeline_options = beam.options.pipeline_options.GoogleCloudOptions(**args)\n",
    "    print(pipeline_options)\n",
    "    \n",
    "    with beam.Pipeline(runner, options=pipeline_options) as pipeline:\n",
    "        (pipeline \n",
    "         | \"Read from BigQuery\">> beam.io.Read(beam.io.BigQuerySource(query = source_query, use_standard_sql = True))\n",
    "         | 'Convert to tf Example' >> beam.Map(lambda instance: row_to_example(instance, type_map))\n",
    "         | 'Serialize to String' >> beam.Map(lambda example: example.SerializeToString(deterministic=True))\n",
    "         | \"Write as TFRecords to GCS\" >> beam.io.WriteToTFRecord(\n",
    "                    file_path_prefix = sink_data_location+\"candidates\", \n",
    "                    file_name_suffix=\".tfrecord\")\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a107aa0e-3c42-4d3d-af6c-b1a003abf406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating source query...\n",
      "Retrieving data type...\n",
      "Pipeline args are set.\n"
     ]
    }
   ],
   "source": [
    "runner = 'DataflowRunner'\n",
    "job_name = 'spotify-tfrecord-beam-{}'.format(datetime.utcnow().strftime('%y%m%d-%H%M%S'))\n",
    "\n",
    "\n",
    "print(\"Generating source query...\")\n",
    "LIMIT = 10000\n",
    "PROJECT_ID='hybrid-vertex'\n",
    "BQ_DATASET='spotify_train_4'\n",
    "BQ_TABLE = 'candidate_features_v2'\n",
    "\n",
    "data_location = CANDIDATE_DIR\n",
    "\n",
    "source_query = generate_query(project=PROJECT_ID, bq_dataset=BQ_DATASET, bq_table=BQ_TABLE, limit=LIMIT)\n",
    "\n",
    "print(\"Retrieving data type...\")\n",
    "type_map = get_type_map(source_query)\n",
    "\n",
    "args = {\n",
    "    'job_name': job_name,\n",
    "    'runner': runner,\n",
    "    'source_query': source_query,\n",
    "    'type_map': type_map,\n",
    "    'sink_data_location': data_location,\n",
    "    'project': PROJECT_ID,\n",
    "    'region': REGION,\n",
    "    'staging_location': STAGING_DIR,\n",
    "    'temp_location': TEMP_DIR,\n",
    "    # 'save_main_session': True,\n",
    "    # 'setup_file': './setup.py'\n",
    "}\n",
    "print(\"Pipeline args are set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ba05b26b-542a-4a33-8460-0e371b2fe2a4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running data extraction pipeline...\n",
      "GoogleCloudOptions(create_from_snapshot=None, dataflow_endpoint=https://dataflow.googleapis.com, dataflow_kms_key=None, dataflow_service_options=None, enable_artifact_caching=False, enable_hot_key_logging=False, enable_streaming_engine=False, flexrs_goal=None, impersonate_service_account=None, job_name=spotify-tfrecord-beam-220711-142630, labels=None, no_auth=False, project=hybrid-vertex, region=us-central1, service_account_email=None, staging_location=gs://spotify-beam-v1/v1/job/staging/, temp_location=gs://spotify-beam-v1/v1/job/temp/, template_location=None, transform_name_mapping=None, update=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: BeamDeprecationWarning: BigQuerySource is deprecated since 2.25.0. Use ReadFromBigQuery instead.\n",
      "  from ipykernel import kernelapp as app\n",
      "WARNING:apache_beam.runners.interactive.interactive_environment:Dependencies required for Interactive Beam PCollection visualization are not available, please use: `pip install apache-beam[interactive]` to install necessary dependencies to enable all data visualization features.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (typeof window.interactive_beam_jquery == 'undefined') {\n",
       "          var jqueryScript = document.createElement('script');\n",
       "          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n",
       "          jqueryScript.type = 'text/javascript';\n",
       "          jqueryScript.onload = function() {\n",
       "            var datatableScript = document.createElement('script');\n",
       "            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n",
       "            datatableScript.type = 'text/javascript';\n",
       "            datatableScript.onload = function() {\n",
       "              window.interactive_beam_jquery = jQuery.noConflict(true);\n",
       "              window.interactive_beam_jquery(document).ready(function($){\n",
       "                \n",
       "              });\n",
       "            }\n",
       "            document.head.appendChild(datatableScript);\n",
       "          };\n",
       "          document.head.appendChild(jqueryScript);\n",
       "        } else {\n",
       "          window.interactive_beam_jquery(document).ready(function($){\n",
       "            \n",
       "          });\n",
       "        }"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery.py:2527: BeamDeprecationWarning: options is deprecated since First stable release. References to <pipeline>.options will not be supported\n",
      "  temp_location = pcoll.pipeline.options.view_as(\n",
      "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['-f', '/home/jupyter/.local/share/jupyter/runtime/kernel-ca31d5fd-50e5-40c5-8f41-000eeb90aec0.json']\n",
      "WARNING:apache_beam.options.pipeline_options:Discarding invalid overrides: {'type_map': {'track_uri': 'STRING', 'track_name': 'STRING', 'artist_name': 'STRING', 'artist_uri': 'STRING', 'album_name': 'STRING', 'album_uri': 'STRING', 'duration_ms': 'INTEGER', 'track_pop': 'INTEGER', 'artist_pop': 'FLOAT', 'artist_genres': 'STRING', 'artist_followers': 'FLOAT'}}\n",
      "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['-f', '/home/jupyter/.local/share/jupyter/runtime/kernel-ca31d5fd-50e5-40c5-8f41-000eeb90aec0.json']\n",
      "WARNING:apache_beam.options.pipeline_options:Discarding invalid overrides: {'type_map': {'track_uri': 'STRING', 'track_name': 'STRING', 'artist_name': 'STRING', 'artist_uri': 'STRING', 'album_name': 'STRING', 'album_uri': 'STRING', 'duration_ms': 'INTEGER', 'track_pop': 'INTEGER', 'artist_pop': 'FLOAT', 'artist_genres': 'STRING', 'artist_followers': 'FLOAT'}}\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Job did not reach to a terminal state after waiting indefinitely. Console URL: https://console.cloud.google.com/dataflow/jobs/<RegionId>/2022-07-11_07_27_47-12338818112686689332?project=<ProjectId>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4185/1368270305.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Running data extraction pipeline...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mrun_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Pipeline is done.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_4185/866826840.py\u001b[0m in \u001b[0;36mrun_pipeline\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     15\u001b[0m          | \"Write as TFRecords to GCS\" >> beam.io.WriteToTFRecord(\n\u001b[1;32m     16\u001b[0m                     \u001b[0mfile_path_prefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msink_data_location\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"candidates\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m                     file_name_suffix=\".tfrecord\")\n\u001b[0m\u001b[1;32m     18\u001b[0m         )\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/apache_beam/pipeline.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    596\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mexc_type\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_until_finish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    599\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extra_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_tb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/apache_beam/runners/dataflow/dataflow_runner.py\u001b[0m in \u001b[0;36mwait_until_finish\u001b[0;34m(self, duration)\u001b[0m\n\u001b[1;32m   1663\u001b[0m       assert duration or terminated, (\n\u001b[1;32m   1664\u001b[0m           \u001b[0;34m'Job did not reach to a terminal state after waiting indefinitely. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1665\u001b[0;31m           '{}'.format(consoleUrl))\n\u001b[0m\u001b[1;32m   1666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m       \u001b[0;31m# TODO(https://github.com/apache/beam/issues/21695): Also run this check\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Job did not reach to a terminal state after waiting indefinitely. Console URL: https://console.cloud.google.com/dataflow/jobs/<RegionId>/2022-07-11_07_27_47-12338818112686689332?project=<ProjectId>"
     ]
    }
   ],
   "source": [
    "# tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "print(\"Running data extraction pipeline...\")\n",
    "run_pipeline(args)\n",
    "print(\"Pipeline is done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30106e3-5496-4379-9249-60d688b4b1d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!gsutil ls {DATA_DIR}/*\n",
    "!ls {CANDIDATE_DIR}/*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0585195b-7f12-470c-94dd-87be46bfc339",
   "metadata": {},
   "source": [
    "# Version 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0feb88-2bf0-4af1-b18a-745621df5385",
   "metadata": {},
   "source": [
    "### Reading from BigQuery\n",
    "\n",
    "* To read an entire BigQuery table, use the table parameter with the BigQuery table name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c39b2c-5377-4030-8d10-13a0042d6109",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = 'hybrid-vertex'\n",
    "BUCKET_NAME = 'spotify-beam-v1' # 'spotify-tfrecords-blog' # Set your Bucket name\n",
    "REGION = 'us-central1' # Set the region for Dataflow jobs\n",
    "VERSION = 'v1'\n",
    "\n",
    "ROOT = f'gs://{BUCKET_NAME}/{VERSION}'\n",
    "\n",
    "CANDIDATE_DIR = ROOT + \"/candidates/\"\n",
    "DATA_DIR = ROOT + '/data/' # Location to store data\n",
    "STATS_DIR = ROOT +'/stats/' # Location to store stats \n",
    "STAGING_DIR = ROOT + '/job/staging/' # Dataflow staging directory on GCP\n",
    "TEMP_DIR =  ROOT + '/job/temp/' # Dataflow temporary directory on GCP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff715629-cc87-4053-8014-a1916ef80471",
   "metadata": {},
   "source": [
    "### Define Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "840f95e1-e863-4219-9d9e-f96c2891548c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Expected TFRecords: 5344\n"
     ]
    }
   ],
   "source": [
    "PROJECT_ID = 'hybrid-vertex'\n",
    "BQ_DATASET = 'spotify_train_3'\n",
    "BQ_TABLE = 'train_flatten'\n",
    "\n",
    "table_spec = f'{PROJECT_ID}:{BQ_DATASET}.{BQ_TABLE}'\n",
    "output_path = DATA_DIR\n",
    "\n",
    "total_samples = 65346428 \n",
    "samples_per_file = 12228 \n",
    "num_tfrecords = total_samples // samples_per_file\n",
    "\n",
    "if num_tfrecords % total_samples:\n",
    "    num_tfrecords += 1\n",
    "    \n",
    "print(\"Number of Expected TFRecords: {}\".format(num_tfrecords)) # 5343"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "cbe84fdf-bb44-402b-aaf3-f42fdeb3950d",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    \"data\": {\n",
    "        \"bq_source_table\": table_spec,\n",
    "        \"output_path\": output_path,\n",
    "        'samples_per_file': samples_per_file,\n",
    "        'num_tfrecords': num_tfrecords,\n",
    "        \"schema\": [\n",
    "            # playlist - context features\n",
    "            { \"name\": \"name\", \"kind\": \"tf.string\", \"mode\": \"NULLABLE\" },\n",
    "            { \"name\": \"collaborative\", \"kind\": \"tf.string\", \"mode\": \"NULLABLE\" },\n",
    "            { \"name\": \"n_songs_pl\", \"kind\": \"tf.float32\", \"mode\": \"NULLABLE\" },\n",
    "            { \"name\": \"num_artists_pl\", \"kind\": \"tf.float32\", \"mode\": \"NULLABLE\" },\n",
    "            { \"name\": \"num_albums_pl\", \"kind\": \"tf.float32\", \"mode\": \"NULLABLE\" },\n",
    "            { \"name\": \"description_pl\", \"kind\": \"tf.string\", \"mode\": \"NULLABLE\" },\n",
    "            # seed track - context features\n",
    "            { \"name\": \"track_name_seed_track\", \"kind\": \"tf.string\", \"mode\": \"NULLABLE\" },\n",
    "            { \"name\": \"artist_name_seed_track\", \"kind\": \"tf.string\", \"mode\": \"NULLABLE\" },\n",
    "            { \"name\": \"album_name_seed_track\", \"kind\": \"tf.string\", \"mode\": \"NULLABLE\" },\n",
    "            { \"name\": \"track_uri_seed_track\", \"kind\": \"tf.string\", \"mode\": \"NULLABLE\" },\n",
    "            { \"name\": \"artist_uri_seed_track\", \"kind\": \"tf.string\", \"mode\": \"NULLABLE\" },\n",
    "            { \"name\": \"album_uri_seed_track\", \"kind\": \"tf.string\", \"mode\": \"NULLABLE\" },\n",
    "            { \"name\": \"duration_seed_track\", \"kind\": \"tf.float32\", \"mode\": \"NULLABLE\" },\n",
    "            { \"name\": \"track_pop_seed_track\", \"kind\": \"tf.float32\", \"mode\": \"NULLABLE\" },\n",
    "            { \"name\": \"artist_pop_seed_track\", \"kind\": \"tf.float32\", \"mode\": \"NULLABLE\" },\n",
    "            { \"name\": \"artist_genres_seed_track\", \"kind\": \"tf.string\", \"mode\": \"NULLABLE\" },\n",
    "            { \"name\": \"artist_followers_seed_track\", \"kind\": \"tf.float32\", \"mode\": \"NULLABLE\" },\n",
    "            # candidate - context features\n",
    "            { \"name\": \"track_name_can\", \"kind\": \"tf.string\", \"mode\": \"NULLABLE\" },\n",
    "            { \"name\": \"artist_name_can\", \"kind\": \"tf.string\", \"mode\": \"NULLABLE\" },\n",
    "            { \"name\": \"album_name_can\", \"kind\": \"tf.string\", \"mode\": \"NULLABLE\" },\n",
    "            { \"name\": \"track_uri_can\", \"kind\": \"tf.string\", \"mode\": \"NULLABLE\" },\n",
    "            { \"name\": \"artist_uri_can\", \"kind\": \"tf.string\", \"mode\": \"NULLABLE\" },\n",
    "            { \"name\": \"album_uri_can\", \"kind\": \"tf.string\", \"mode\": \"NULLABLE\" },\n",
    "            { \"name\": \"duration_ms_can\", \"kind\": \"tf.float32\", \"mode\": \"NULLABLE\" },\n",
    "            { \"name\": \"track_pop_can\", \"kind\": \"tf.float32\", \"mode\": \"NULLABLE\" },\n",
    "            { \"name\": \"artist_pop_can\", \"kind\": \"tf.float32\", \"mode\": \"NULLABLE\" },\n",
    "            { \"name\": \"artist_genres_can\", \"kind\": \"tf.string\", \"mode\": \"NULLABLE\" },\n",
    "            { \"name\": \"artist_followers_can\", \"kind\": \"tf.float32\", \"mode\": \"NULLABLE\" },\n",
    "            # Ragged Features\n",
    "            { \"name\": \"track_name_pl\", \"kind\": \"tf.string\", \"mode\": \"REPEATED\" },\n",
    "            { \"name\": \"artist_name_pl\", \"kind\": \"tf.string\", \"mode\": \"REPEATED\" },\n",
    "            { \"name\": \"album_name_pl\", \"kind\": \"tf.string\", \"mode\": \"REPEATED\" },\n",
    "            { \"name\": \"track_uri_pl\", \"kind\": \"tf.string\", \"mode\": \"REPEATED\" },\n",
    "            { \"name\": \"duration_ms_songs_pl\", \"kind\": \"tf.float32\", \"mode\": \"REPEATED\" },\n",
    "            { \"name\": \"artist_pop_pl\", \"kind\": \"tf.float32\", \"mode\": \"REPEATED\" },\n",
    "            { \"name\": \"artists_followers_pl\", \"kind\": \"tf.float32\", \"mode\": \"REPEATED\" },\n",
    "            { \"name\": \"track_pop_pl\", \"kind\": \"tf.float32\", \"mode\": \"REPEATED\" },\n",
    "            { \"name\": \"artist_genres_pl\", \"kind\": \"tf.string\", \"mode\": \"REPEATED\" },\n",
    "        ]\n",
    "    },\n",
    "    \"tasks\": [\n",
    "        { \"name\": \"analysis\" },\n",
    "        { \"name\": \"training\", \"transform\": \"analysis\", \"shuffle\": \"True\" },\n",
    "        # { \"name\": \"validation\", \"transform\": \"analysis\" },\n",
    "        # { \"name\": \"testing\", \"transform\": \"identity\" }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d63cf414-4942-4dcb-a339-359e660deb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainTfSeqExampleDoFn(beam.DoFn):\n",
    "    \"\"\"\n",
    "    Convert training sample into TFExample\n",
    "    \"\"\"\n",
    "    def __init__(self, task):\n",
    "        \"\"\"\n",
    "        Initialization\n",
    "        \"\"\"\n",
    "        self.task = task\n",
    "\n",
    "    @staticmethod\n",
    "    def _bytes_feature(value):\n",
    "        \"\"\"\n",
    "        Get byte features\n",
    "        \"\"\"\n",
    "        return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "    @staticmethod\n",
    "    def _int64_feature(value):\n",
    "        \"\"\"\n",
    "        Get int64 feature\n",
    "        \"\"\"\n",
    "        return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "    \n",
    "    @staticmethod\n",
    "    def _string_array(value):\n",
    "        \"\"\"\n",
    "        Returns a bytes_list from a string / byte.\n",
    "        \"\"\"\n",
    "        return tf.train.Feature(bytes_list=tf.train.BytesList(value=[v.encode('utf-8') for v in value]))\n",
    "    \n",
    "    @staticmethod\n",
    "    def _float_feature(value):\n",
    "        \"\"\"Returns a float_list from a float / double.\"\"\"\n",
    "        return tf.train.Feature(float_list=tf.train.FloatList(value=[float(v) for v in value]))\n",
    "    \n",
    "    def process(self, data):\n",
    "        \"\"\"\n",
    "        Convert BQ row to tf-example\n",
    "        \"\"\"\n",
    "    \n",
    "        # ===============================\n",
    "        # Ragged Features - Query\n",
    "        # ===============================\n",
    "        ragged_key_list = [\n",
    "            'track_name_pl',\n",
    "            'artist_name_pl',\n",
    "            'album_name_pl',\n",
    "            # 'track_uri_pl',\n",
    "            'duration_ms_songs_pl',\n",
    "            'artist_pop_pl',\n",
    "            'artists_followers_pl',\n",
    "            'track_pop_pl',\n",
    "            'artist_genres_pl',\n",
    "        ]\n",
    "\n",
    "        ragged_dict = {}\n",
    "\n",
    "        for _ in ragged_key_list:\n",
    "            ragged_dict[_] = []\n",
    "\n",
    "        for x in data['track_name_pl']:\n",
    "            ragged_dict['track_name_pl'].append(x.encode('utf8'))\n",
    "\n",
    "        for x in data['artist_name_pl']:\n",
    "            ragged_dict['artist_name_pl'].append(x.encode('utf8'))\n",
    "\n",
    "        for x in data['album_name_pl']:\n",
    "            ragged_dict['album_name_pl'].append(x.encode('utf8'))\n",
    "\n",
    "        # for x in data['track_uri_pl']:\n",
    "        #     ragged_dict['track_uri_pl'].append(x.encode('utf8'))\n",
    "\n",
    "        for x in data['duration_ms_songs_pl']:\n",
    "            ragged_dict['duration_ms_songs_pl'].append(x)\n",
    "\n",
    "        for x in data['artist_pop_pl']:\n",
    "            ragged_dict['artist_pop_pl'].append(x)\n",
    "\n",
    "        for x in data['artists_followers_pl']:\n",
    "            ragged_dict['artists_followers_pl'].append(x)\n",
    "\n",
    "        for x in data['track_pop_pl']:\n",
    "            ragged_dict['track_pop_pl'].append(x)\n",
    "\n",
    "        for x in data['artist_genres_pl']:\n",
    "            ragged_dict['artist_genres_pl'].append(x.encode('utf8'))\n",
    "\n",
    "        # Set List Types\n",
    "        # Bytes\n",
    "        track_name_pl = BytesList(value=ragged_dict['track_name_pl'])\n",
    "        artist_name_pl = BytesList(value=ragged_dict['artist_name_pl'])\n",
    "        album_name_pl = BytesList(value=ragged_dict['album_name_pl'])\n",
    "        # track_uri_pl = BytesList(value=ragged_dict['track_uri_pl'])\n",
    "        artist_genres_pl = BytesList(value=ragged_dict['artist_genres_pl'])\n",
    "\n",
    "        # Float List\n",
    "        duration_ms_songs_pl = FloatList(value=ragged_dict['duration_ms_songs_pl'])\n",
    "        artist_pop_pl = FloatList(value=ragged_dict['artist_pop_pl'])\n",
    "        artists_followers_pl = FloatList(value=ragged_dict['artists_followers_pl'])\n",
    "        track_pop_pl = FloatList(value=ragged_dict['track_pop_pl'])\n",
    "\n",
    "        # Set FeatureLists\n",
    "        # Bytes\n",
    "        track_name_pl = FeatureList(feature=[Feature(bytes_list=track_name_pl)])\n",
    "        artist_name_pl = FeatureList(feature=[Feature(bytes_list=artist_name_pl)])\n",
    "        album_name_pl = FeatureList(feature=[Feature(bytes_list=album_name_pl)])\n",
    "        # track_uri_pl = FeatureList(feature=[Feature(bytes_list=track_uri_pl)])\n",
    "        artist_genres_pl = FeatureList(feature=[Feature(bytes_list=artist_genres_pl)])\n",
    "\n",
    "        # Float Lists\n",
    "        duration_ms_songs_pl = FeatureList(feature=[Feature(float_list=duration_ms_songs_pl)])\n",
    "        artist_pop_pl = FeatureList(feature=[Feature(float_list=artist_pop_pl)])\n",
    "        artists_followers_pl = FeatureList(feature=[Feature(float_list=artists_followers_pl)])\n",
    "        track_pop_pl = FeatureList(feature=[Feature(float_list=track_pop_pl)])\n",
    "        \n",
    "        # ===============================\n",
    "        # Create Context Features\n",
    "        # ===============================\n",
    "        context_features = {\n",
    "            # playlist - context features\n",
    "            \"name\": _string_array(data['name']),\n",
    "            'collaborative' : _string_array(data['collaborative']),\n",
    "            # 'duration_ms_seed_pl' : _float_feature(data['duration_ms_seed_pl']),\n",
    "            'n_songs_pl' : _float_feature(data['n_songs_pl']),\n",
    "            'num_artists_pl' : _float_feature(data['num_artists_pl']),\n",
    "            'num_albums_pl' : _float_feature(data['num_albums_pl']),\n",
    "            'description_pl' : _string_array(data['description_pl']),\n",
    "\n",
    "            # seed track - context features\n",
    "            'track_name_seed_track' : _string_array(data['track_name_seed_track']),\n",
    "            'artist_name_seed_track' : _string_array(data['artist_name_seed_track']),\n",
    "            'album_name_seed_track' : _string_array(data['album_name_seed_track']),\n",
    "            # 'track_uri_seed_track' : _string_array(data['track_uri_seed_track']),\n",
    "            # 'artist_uri_seed_track' : _string_array(data['artist_uri_seed_track']),\n",
    "            # 'album_uri_seed_track' : _string_array(data['album_uri_seed_track']),\n",
    "            'duration_seed_track' : _float_feature(data['duration_seed_track']),\n",
    "            'track_pop_seed_track' : _float_feature(data['track_pop_seed_track']),\n",
    "            'artist_pop_seed_track' : _float_feature(data['artist_pop_seed_track']),\n",
    "            'artist_genres_seed_track' : _string_array(data['artist_genres_seed_track']),\n",
    "            'artist_followers_seed_track' : _float_feature(data['artist_followers_seed_track']),\n",
    "\n",
    "            #candidate features\n",
    "            \"track_name_can\": _string_array(data['track_name_can']), \n",
    "            \"artist_name_can\": _string_array(data['artist_name_can']),\n",
    "            \"album_name_can\": _string_array(data['album_name_can']),\n",
    "            \"track_uri_can\": _string_array(data['track_uri_can']),\n",
    "            # \"artist_uri_can\": _string_array(data['artist_uri_can']),\n",
    "            # \"album_uri_can\": _string_array(data['album_uri_can']),\n",
    "            \"duration_ms_can\": _float_feature(data['duration_ms_can']),\n",
    "            \"track_pop_can\": _float_feature(data['track_pop_can']), \n",
    "            \"artist_pop_can\": _float_feature(data['artist_pop_can']),\n",
    "            \"artist_genres_can\": _string_array(data['artist_genres_can']),\n",
    "            \"artist_followers_can\": _float_feature(data['artist_followers_can']),\n",
    "        }\n",
    "        \n",
    "        # ===============================\n",
    "        # Create Sequence\n",
    "        # ===============================\n",
    "        seq = SequenceExample(\n",
    "            context=tf.train.Features(\n",
    "                feature=context_features\n",
    "            ),\n",
    "            feature_lists=FeatureLists(\n",
    "                feature_list={\n",
    "                    \"track_name_pl\": track_name_pl,\n",
    "                    \"artist_name_pl\": artist_name_pl,\n",
    "                    \"album_name_pl\": album_name_pl,\n",
    "                    # \"track_uri_pl\": track_uri_pl,\n",
    "                    \"duration_ms_songs_pl\": duration_ms_songs_pl,\n",
    "                    \"artist_pop_pl\": artist_pop_pl,\n",
    "                    \"artists_followers_pl\": artists_followers_pl,\n",
    "                    \"track_pop_pl\": track_pop_pl,\n",
    "                    \"artist_genres_pl\": artist_genres_pl\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "\n",
    "        yield seq\n",
    "        \n",
    "# def _filter(mode, example):\n",
    "#         return mode['name'] in example['mode']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963d423f-76d0-492c-a442-4c5690d0dcac",
   "metadata": {},
   "source": [
    "### Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "79dff549-3a94-44a9-a1fd-02726f5180ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = 'hybrid-vertex'\n",
    "BUCKET_NAME = 'spotify-beam-v1' # 'spotify-tfrecords-blog' # Set your Bucket name\n",
    "REGION = 'us-central1' # Set the region for Dataflow jobs\n",
    "VERSION = 'v1'\n",
    "\n",
    "ROOT = f'gs://{BUCKET_NAME}/{VERSION}'\n",
    "\n",
    "CANDIDATE_DIR = ROOT + \"/candidates/\"\n",
    "DATA_DIR = ROOT + '/data/' # Location to store data\n",
    "STATS_DIR = ROOT +'/stats/' # Location to store stats \n",
    "STAGING_DIR = ROOT + '/job/staging/' # Dataflow staging directory on GCP\n",
    "TEMP_DIR =  ROOT + '/job/temp/' # Dataflow temporary directory on GCP\n",
    "\n",
    "MAX_WORKERS = '20'\n",
    "\n",
    "RUNNER = 'DataflowRunner'\n",
    "STAGING_LOCATION = STAGING_DIR\n",
    "TEMP_LOCATION = TEMP_DIR\n",
    "\n",
    "TIMESTAMP = datetime.utcnow().strftime('%y%m%d-%H%M%S')\n",
    "JOB_NAME = f'spotify-tfrecords-{VERSION}-{TIMESTAMP}'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "10889bb5-268b-4ee6-bcb1-64b938b16c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "\n",
    "# sys.path\n",
    "\n",
    "# spotify_mpd_two_tower/jt-dev/pipeline/schema.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0811cdf5-4152-4155-8d0d-3904bb5f4054",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    div.alert {\n",
       "      white-space: pre-line;\n",
       "    }\n",
       "  </style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <link rel=\"stylesheet\" href=\"https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css\" integrity=\"sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh\" crossorigin=\"anonymous\">\n",
       "            <div class=\"alert alert-info\">No cache_root detected. Defaulting to staging_location gs://spotify-beam-v1/v1/job/staging/ for cache location.</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AssertionError",
     "evalue": "Job did not reach to a terminal state after waiting indefinitely. Console URL: https://console.cloud.google.com/dataflow/jobs/<RegionId>/2022-07-11_15_15_45-16356143904307732923?project=<ProjectId>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4185/3746660675.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     50\u001b[0m          \u001b[0;34m|\u001b[0m \u001b[0;34m'TrainToSequenceExample'\u001b[0m \u001b[0;34m>>\u001b[0m \u001b[0mbeam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParDo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcreate_sequence_examples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m          \u001b[0;34m|\u001b[0m \u001b[0;34m'SerializeProto'\u001b[0m \u001b[0;34m>>\u001b[0m \u001b[0mbeam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerializeToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m          | 'WriteTfrecord' >> write_to_tf_record)\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/apache_beam/pipeline.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    596\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mexc_type\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_until_finish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    599\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extra_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_tb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/apache_beam/runners/dataflow/dataflow_runner.py\u001b[0m in \u001b[0;36mwait_until_finish\u001b[0;34m(self, duration)\u001b[0m\n\u001b[1;32m   1663\u001b[0m       assert duration or terminated, (\n\u001b[1;32m   1664\u001b[0m           \u001b[0;34m'Job did not reach to a terminal state after waiting indefinitely. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1665\u001b[0;31m           '{}'.format(consoleUrl))\n\u001b[0m\u001b[1;32m   1666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m       \u001b[0;31m# TODO(https://github.com/apache/beam/issues/21695): Also run this check\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Job did not reach to a terminal state after waiting indefinitely. Console URL: https://console.cloud.google.com/dataflow/jobs/<RegionId>/2022-07-11_15_15_45-16356143904307732923?project=<ProjectId>"
     ]
    }
   ],
   "source": [
    "from schema import Schema\n",
    "\n",
    "# def pipeline():\n",
    "\n",
    "\"\"\"\n",
    "Main function to create tfrecords (ETL pipeline)\n",
    "\"\"\"\n",
    "RUNNER = 'DataflowRunner'\n",
    "\n",
    "# Read the SQL code\n",
    "table = CONFIG['data']['bq_source_table'] #.read()\n",
    "schema = CONFIG['data']['schema']\n",
    "output_path = CONFIG['data']['output_path']\n",
    "num_tfrecords = CONFIG['data']['num_tfrecords']\n",
    "\n",
    "# --> EXTRACTION <--\n",
    "\n",
    "# Create a BigQuery source\n",
    "source = beam.io.gcp.bigquery.ReadFromBigQuery(table=table, use_standard_sql=True, flatten_results=False)\n",
    "\n",
    "# Transform to Sequence Example\n",
    "create_sequence_examples = TrainTfSeqExampleDoFn(task = 'train')\n",
    "\n",
    "# --> LOADING <--\n",
    "# Write serialized example to tfrecords\n",
    "write_to_tf_record = beam.io.tfrecordio.WriteToTFRecord(\n",
    "    file_path_prefix=output_path,\n",
    "    file_name_suffix='.tfrecord',\n",
    "    num_shards=num_tfrecords)\n",
    "\n",
    "# TODO: Enter required pipeline arguments (Method 1)\n",
    "pipeline_args = [\n",
    "    '--runner', RUNNER,\n",
    "    '--project', PROJECT_ID,\n",
    "    '--region', REGION,\n",
    "    '--staging_location', STAGING_DIR,\n",
    "    '--temp_location', TEMP_DIR,\n",
    "    # '--template_location', TEMPLATE_LOCATION,\n",
    "    '--job_name', JOB_NAME,\n",
    "    '--num_workers', MAX_WORKERS,\n",
    "    # '--setup_file', './setup.py'\n",
    "]\n",
    "pipeline_options = PipelineOptions(pipeline_args)\n",
    "pipeline_options.view_as(SetupOptions).save_main_session = True\n",
    "\n",
    "# Defining Apache Beam Pipeline\n",
    "with beam.Pipeline(options=pipeline_options) as pipe:\n",
    "    _ = (pipe\n",
    "         | 'ReadBQTable' >> source\n",
    "         | 'TrainToSequenceExample' >> beam.ParDo(create_sequence_examples)\n",
    "         | 'SerializeProto' >> beam.Map(lambda x: x.SerializeToString())\n",
    "         | 'WriteTfrecord' >> write_to_tf_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc1c567-4b3c-455f-ab6a-e6d81f5978f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da6ee21-a4ea-4703-86fd-10b8a6d479ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50857dbb-42c6-461f-ac55-a4b4bd5abe85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5c3bca-1a92-45d5-b3e2-3d94f540d699",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9fc0cc53-3537-4ef8-bd98-bbd0ff39ce98",
   "metadata": {},
   "source": [
    "# Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cd1a9dce-5ea5-435e-95ab-b0cc1ff8fb05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ReadFromBigQuery(PTransform) label=[ReadFromBigQuery] at 0x7f2aecbdcf10>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spotify_mpd_two_tower import Schema\n",
    "\n",
    "# Read the SQL code\n",
    "table = CONFIG['data']['bq_source_table'] #.read()\n",
    "schema = CONFIG['data']['schema']\n",
    "output_path = CONFIG['data']['output_path']\n",
    "\n",
    "# Create a BigQuery source\n",
    "source = beam.io.gcp.bigquery.ReadFromBigQuery(table=table, use_standard_sql=True, flatten_results=False)\n",
    "\n",
    "# Create metadata needed later\n",
    "spec = schema.to_feature_spec()\n",
    "meta = dataset_metadata.DatasetMetadata(schema=dataset_schema.from_feature_spec(spec))\n",
    "\n",
    "# Read from BigQuery\n",
    "data = pipeline \\\n",
    "    | 'read' >> beam.io.Read(source)\n",
    "\n",
    "# Loop over tasks whose purpose is analysis\n",
    "transform_functions = {}\n",
    "for task in config['tasks']:\n",
    "    if 'transform' in task:\n",
    "        continue\n",
    "    name = task['name']\n",
    "    \n",
    "transform_functions[name] \\\n",
    "    | name + '-write-transform' >> transform_fn_io.WriteTransformFn(output_path)\n",
    "\n",
    "# Loop over tasks whose purpose is transformation\n",
    "for task in config['tasks']:\n",
    "    if not 'transform' in task:\n",
    "        continue\n",
    "    name = task['name']\n",
    "    # Select examples that belong to the current mode\n",
    "    data_ = data \\\n",
    "        | name + '-filter' >> beam.Filter(partial(_filter, task))\n",
    "    # Shuffle examples if needed\n",
    "    if task.get('shuffle', False):\n",
    "        data_ = data_ \\\n",
    "            | name + '-shuffle' >> beam.transforms.Reshuffle()\n",
    "        \n",
    "        # Transform the examples using an appropriate transform function\n",
    "        if task['transform'] == 'identity':\n",
    "            coder = tft.coders.ExampleProtoCoder(meta.schema)\n",
    "        else:\n",
    "            data_, meta_ = ((data_, meta), transform_functions[task['transform']]) \\\n",
    "                | name + '-transform' >> tt_beam.TransformDataset()\n",
    "            coder = tft.coders.ExampleProtoCoder(meta_.schema)\n",
    "            \n",
    "            \n",
    "        # Store the transformed examples as TFRecords\n",
    "        data_ \\\n",
    "            | name + '-encode' >> beam.Map(coder.encode) \\\n",
    "            | name + '-write-examples' >> beam.io.tfrecordio.WriteToTFRecord(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a86be58-dd03-4adc-aed4-ca7acc608b6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a6015e-d1b6-4c80-a21d-d40984f2da50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92b946d-e98a-4757-ae4e-d745bf62b5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the SQL code\n",
    "table = open(config['data']['bq_source_table']).read()\n",
    "\n",
    "# Create a BigQuery source\n",
    "source = beam.io.BigQuerySource(table=table, use_standard_sql=True, flatten_results=False)\n",
    "\n",
    "# Create metadata needed later\n",
    "spec = schema.to_feature_spec()\n",
    "meta = dataset_metadata.DatasetMetadata(\n",
    "    schema=dataset_schema.from_feature_spec(spec)\n",
    ")\n",
    "\n",
    "data = pipeline \\\n",
    "    | 'read' >> beam.io.Read(source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d5ba81-768f-419f-bc78-afb62a88d61a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a4d571-76d8-4ec0-8bf8-5ffc69f64cd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307291ac-e414-498f-9249-c12ed57aec13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abeea77-adf8-4f73-bee8-36dfb3bebec8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d3714e-97ff-4de9-908c-e2a86638a718",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2054824-c72f-41ff-9989-a48d7782b6d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-9.m94",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-9:m94"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
