{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ae44dcc-9f01-4d78-b5e6-4981f3a6558b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install apache-beam[gcp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6dcba672-f968-4bc4-bbdf-e9b6929b5169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF version: 2.9.0-rc2\n",
      "TFDV version: 1.9.0\n",
      "Beam version: 2.40.0\n",
      "BQ SDK version: 2.34.4\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf \n",
    "import apache_beam as beam \n",
    "import tensorflow_data_validation as tfdv\n",
    "from google.cloud import bigquery\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "# from tensorflow_transform.tf_metadata import dataset_metadata\n",
    "# from tensorflow_transform.tf_metadata import dataset_schema\n",
    "\n",
    "print(\"TF version:\", tf.__version__)\n",
    "print(\"TFDV version:\", tfdv.__version__)\n",
    "print(\"Beam version:\", beam.__version__)\n",
    "print(\"BQ SDK version:\", bigquery.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d9340c-3bfe-473f-aafc-bf592a1e0b82",
   "metadata": {},
   "source": [
    "# Apache Beam Pipeline -> write tf-records from BQ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977ab1aa-162c-4cc5-8696-10fccf088015",
   "metadata": {},
   "source": [
    "### Table rows\n",
    "* BigQueryIO read and write transforms produce and consume data as a `PCollection` of dictionaries, where each element in the `PCollection` represents a single row in the table\n",
    "\n",
    "### Schemas\n",
    "* When writing to BigQuery, you must supply a table schema for the destination table that you want to write to, unless you specify a create disposition of CREATE_NEVER. \n",
    "* Creating a table schema covers schemas in more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c87133f9-56a5-4db2-882e-c30a46a3abf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = 'hybrid-vertex'\n",
    "BUCKET_NAME = 'spotify-beam-v1' # 'spotify-tfrecords-blog' # Set your Bucket name\n",
    "REGION = 'us-central1' # Set the region for Dataflow jobs\n",
    "VERSION = 'v1'\n",
    "\n",
    "ROOT = f'gs://{BUCKET_NAME}/{VERSION}'\n",
    "\n",
    "CANDIDATE_DIR = ROOT + \"/candidates/\"\n",
    "DATA_DIR = ROOT + '/data/' # Location to store data\n",
    "STATS_DIR = ROOT +'/stats/' # Location to store stats \n",
    "STAGING_DIR = ROOT + '/job/staging/' # Dataflow staging directory on GCP\n",
    "TEMP_DIR =  ROOT + '/job/temp/' # Dataflow temporary directory on GCP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e828016-71f9-49d3-81f4-32979e27d7f8",
   "metadata": {},
   "source": [
    "### Cleanup working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "45e2526c-a27a-4692-af4e-c1f73ba0321b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_root ='gs://spotify-tfrecords-blog/candidate_tracks_v2'\n",
    "# files = np.array(tf.io.gfile.listdir(test_root))\n",
    "# for file in files:\n",
    "#     file_to_remove = f'{test_root}/{file}'\n",
    "#     print(file_to_remove)\n",
    "#     break\n",
    "#     # tf.io.gfile.remove(f'{test_root}/{file})\n",
    "# # files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dbb5c992-e209-4511-854b-3cf15c0a483a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating working directory: gs://spotify-beam-v1/v1\n"
     ]
    }
   ],
   "source": [
    "if tf.io.gfile.exists(ROOT):\n",
    "    print(\"Removing {} contents...\".format(ROOT))\n",
    "    files = np.array(tf.io.gfile.listdir(ROOT))\n",
    "    for file in files:\n",
    "        tf.io.gfile.remove(f'{ROOT}/{file}')\n",
    "\n",
    "print(\"Creating working directory: {}\".format(ROOT))\n",
    "tf.io.gfile.mkdir(ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d647f8-397b-4c16-9c00-cba56887cf31",
   "metadata": {},
   "source": [
    "## Extract data from BigQuery to GCS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d67b621-fe22-472a-854a-0dd7c5bd0d4d",
   "metadata": {},
   "source": [
    "* extract data from BigQuery, \n",
    "* convert data to TFRecord files, \n",
    "* store data files in Google Cloud Storage (GCS). \n",
    "> * This data file in GCS will then be used by TFDV for stats & vocabs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1277a70-fb1d-4964-8901-aac53a49d5de",
   "metadata": {},
   "source": [
    "### Define SQL Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fd1052d9-5f85-4cbd-8bb5-1c497a47859f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_query(project=None, bq_dataset=None, bq_table=None, limit=None):\n",
    "    query =f\"\"\"\n",
    "        SELECT *\n",
    "        FROM \n",
    "          `{project}.{bq_dataset}.{bq_table}`\n",
    "        \"\"\"\n",
    "    if limit:\n",
    "        query  += \"LIMIT {}\".format(limit)\n",
    "        \n",
    "    return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b88469e3-7ee5-4aeb-849e-17ec9cd923d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.train import BytesList, Feature, FeatureList, Int64List, FloatList\n",
    "from tensorflow.train import SequenceExample, FeatureLists\n",
    "\n",
    "\n",
    "def string_array(value):\n",
    "    \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[v.encode('utf-8') for v in value]))\n",
    "\n",
    "\n",
    "def float_feature(value):\n",
    "    \"\"\"Returns a float_list from a float / double.\"\"\"\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=[float(v) for v in value]))\n",
    "\n",
    "\n",
    "def int64_feature(value):\n",
    "    \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[int(v) for v in value]))\n",
    "\n",
    "\n",
    "def float_feature_list(value):\n",
    "    \"\"\"Returns a list of float_list from a float / double.\"\"\"\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=value))\n",
    "\n",
    "def get_type_map(query):\n",
    "    bq_client = bigquery.Client()\n",
    "    query_job = bq_client.query(f\"{query}\")\n",
    "    results = query_job.result()\n",
    "    \n",
    "    type_map = {}\n",
    "    \n",
    "    for field in results.schema:\n",
    "        type_map[field.name] = field.field_type\n",
    "    \n",
    "    return type_map\n",
    "\n",
    "# def get_mode_map(query):\n",
    "#     bq_client = bigquery.Client()\n",
    "#     query_job = bq_client.query(f\"{query}\")\n",
    "#     results = query_job.result()\n",
    "    \n",
    "#     mode_map = {}\n",
    "    \n",
    "#     for field in results.schema:\n",
    "#         mode_map[field.name] = field.mode\n",
    "    \n",
    "#     return mode_map\n",
    "\n",
    "\n",
    "def row_to_example(instance, type_map):\n",
    "    feature = {}\n",
    "    for key, value in instance.items():\n",
    "        data_type = type_map[key]\n",
    "        if value is None:\n",
    "            feature[key] = tf.train.Feature()\n",
    "        elif data_type == 'INTEGER':\n",
    "            feature[key] = tf.train.Feature(\n",
    "                int64_list=tf.train.Int64List(value=[value]))\n",
    "        elif data_type == 'FLOAT':\n",
    "            feature[key] = tf.train.Feature(\n",
    "                float_list=tf.train.FloatList(value=[value]))\n",
    "        else:\n",
    "            feature[key] = tf.train.Feature(\n",
    "                bytes_list=tf.train.BytesList(value=[tf.compat.as_bytes(value)]))\n",
    "            \n",
    "    return tf.train.Example(features=tf.train.Features(feature=feature))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d4ab57-209d-4e39-beaf-ebf3811179c4",
   "metadata": {},
   "source": [
    "## Submit Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "23376698-cf92-4529-be82-5023f30b5ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipeline(args):\n",
    "\n",
    "    source_query = args.pop('source_query')\n",
    "    sink_data_location = args.pop('sink_data_location')\n",
    "    runner = args['runner']\n",
    "    \n",
    "    pipeline_options = beam.options.pipeline_options.GoogleCloudOptions(**args)\n",
    "    print(pipeline_options)\n",
    "    \n",
    "    with beam.Pipeline(runner, options=pipeline_options) as pipeline:\n",
    "        (pipeline \n",
    "         | \"Read from BigQuery\">> beam.io.Read(beam.io.BigQuerySource(query = source_query, use_standard_sql = True))\n",
    "         | 'Convert to tf Example' >> beam.Map(lambda instance: row_to_example(instance, type_map))\n",
    "         | 'Serialize to String' >> beam.Map(lambda example: example.SerializeToString(deterministic=True))\n",
    "         | \"Write as TFRecords to GCS\" >> beam.io.WriteToTFRecord(\n",
    "                    file_path_prefix = sink_data_location+\"candidates\", \n",
    "                    file_name_suffix=\".tfrecord\")\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a107aa0e-3c42-4d3d-af6c-b1a003abf406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating source query...\n",
      "Retrieving data type...\n",
      "Pipeline args are set.\n"
     ]
    }
   ],
   "source": [
    "runner = 'DataflowRunner'\n",
    "job_name = 'spotify-tfrecord-beam-{}'.format(datetime.utcnow().strftime('%y%m%d-%H%M%S'))\n",
    "\n",
    "\n",
    "print(\"Generating source query...\")\n",
    "LIMIT = 10000\n",
    "PROJECT_ID='hybrid-vertex'\n",
    "BQ_DATASET='spotify_train_4'\n",
    "BQ_TABLE = 'candidate_features_v2'\n",
    "\n",
    "data_location = CANDIDATE_DIR\n",
    "\n",
    "source_query = generate_query(project=PROJECT_ID, bq_dataset=BQ_DATASET, bq_table=BQ_TABLE, limit=LIMIT)\n",
    "\n",
    "print(\"Retrieving data type...\")\n",
    "type_map = get_type_map(source_query)\n",
    "\n",
    "args = {\n",
    "    'job_name': job_name,\n",
    "    'runner': runner,\n",
    "    'source_query': source_query,\n",
    "    'type_map': type_map,\n",
    "    'sink_data_location': data_location,\n",
    "    'project': PROJECT_ID,\n",
    "    'region': REGION,\n",
    "    'staging_location': STAGING_DIR,\n",
    "    'temp_location': TEMP_DIR,\n",
    "    # 'save_main_session': True,\n",
    "    # 'setup_file': './setup.py'\n",
    "}\n",
    "print(\"Pipeline args are set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ba05b26b-542a-4a33-8460-0e371b2fe2a4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running data extraction pipeline...\n",
      "GoogleCloudOptions(create_from_snapshot=None, dataflow_endpoint=https://dataflow.googleapis.com, dataflow_kms_key=None, dataflow_service_options=None, enable_artifact_caching=False, enable_hot_key_logging=False, enable_streaming_engine=False, flexrs_goal=None, impersonate_service_account=None, job_name=spotify-tfrecord-beam-220711-142630, labels=None, no_auth=False, project=hybrid-vertex, region=us-central1, service_account_email=None, staging_location=gs://spotify-beam-v1/v1/job/staging/, temp_location=gs://spotify-beam-v1/v1/job/temp/, template_location=None, transform_name_mapping=None, update=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: BeamDeprecationWarning: BigQuerySource is deprecated since 2.25.0. Use ReadFromBigQuery instead.\n",
      "  from ipykernel import kernelapp as app\n",
      "WARNING:apache_beam.runners.interactive.interactive_environment:Dependencies required for Interactive Beam PCollection visualization are not available, please use: `pip install apache-beam[interactive]` to install necessary dependencies to enable all data visualization features.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (typeof window.interactive_beam_jquery == 'undefined') {\n",
       "          var jqueryScript = document.createElement('script');\n",
       "          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n",
       "          jqueryScript.type = 'text/javascript';\n",
       "          jqueryScript.onload = function() {\n",
       "            var datatableScript = document.createElement('script');\n",
       "            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n",
       "            datatableScript.type = 'text/javascript';\n",
       "            datatableScript.onload = function() {\n",
       "              window.interactive_beam_jquery = jQuery.noConflict(true);\n",
       "              window.interactive_beam_jquery(document).ready(function($){\n",
       "                \n",
       "              });\n",
       "            }\n",
       "            document.head.appendChild(datatableScript);\n",
       "          };\n",
       "          document.head.appendChild(jqueryScript);\n",
       "        } else {\n",
       "          window.interactive_beam_jquery(document).ready(function($){\n",
       "            \n",
       "          });\n",
       "        }"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery.py:2527: BeamDeprecationWarning: options is deprecated since First stable release. References to <pipeline>.options will not be supported\n",
      "  temp_location = pcoll.pipeline.options.view_as(\n",
      "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['-f', '/home/jupyter/.local/share/jupyter/runtime/kernel-ca31d5fd-50e5-40c5-8f41-000eeb90aec0.json']\n",
      "WARNING:apache_beam.options.pipeline_options:Discarding invalid overrides: {'type_map': {'track_uri': 'STRING', 'track_name': 'STRING', 'artist_name': 'STRING', 'artist_uri': 'STRING', 'album_name': 'STRING', 'album_uri': 'STRING', 'duration_ms': 'INTEGER', 'track_pop': 'INTEGER', 'artist_pop': 'FLOAT', 'artist_genres': 'STRING', 'artist_followers': 'FLOAT'}}\n",
      "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['-f', '/home/jupyter/.local/share/jupyter/runtime/kernel-ca31d5fd-50e5-40c5-8f41-000eeb90aec0.json']\n",
      "WARNING:apache_beam.options.pipeline_options:Discarding invalid overrides: {'type_map': {'track_uri': 'STRING', 'track_name': 'STRING', 'artist_name': 'STRING', 'artist_uri': 'STRING', 'album_name': 'STRING', 'album_uri': 'STRING', 'duration_ms': 'INTEGER', 'track_pop': 'INTEGER', 'artist_pop': 'FLOAT', 'artist_genres': 'STRING', 'artist_followers': 'FLOAT'}}\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Job did not reach to a terminal state after waiting indefinitely. Console URL: https://console.cloud.google.com/dataflow/jobs/<RegionId>/2022-07-11_07_27_47-12338818112686689332?project=<ProjectId>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4185/1368270305.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Running data extraction pipeline...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mrun_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Pipeline is done.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_4185/866826840.py\u001b[0m in \u001b[0;36mrun_pipeline\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     15\u001b[0m          | \"Write as TFRecords to GCS\" >> beam.io.WriteToTFRecord(\n\u001b[1;32m     16\u001b[0m                     \u001b[0mfile_path_prefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msink_data_location\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"candidates\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m                     file_name_suffix=\".tfrecord\")\n\u001b[0m\u001b[1;32m     18\u001b[0m         )\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/apache_beam/pipeline.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    596\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mexc_type\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_until_finish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    599\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extra_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_tb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/apache_beam/runners/dataflow/dataflow_runner.py\u001b[0m in \u001b[0;36mwait_until_finish\u001b[0;34m(self, duration)\u001b[0m\n\u001b[1;32m   1663\u001b[0m       assert duration or terminated, (\n\u001b[1;32m   1664\u001b[0m           \u001b[0;34m'Job did not reach to a terminal state after waiting indefinitely. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1665\u001b[0;31m           '{}'.format(consoleUrl))\n\u001b[0m\u001b[1;32m   1666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m       \u001b[0;31m# TODO(https://github.com/apache/beam/issues/21695): Also run this check\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Job did not reach to a terminal state after waiting indefinitely. Console URL: https://console.cloud.google.com/dataflow/jobs/<RegionId>/2022-07-11_07_27_47-12338818112686689332?project=<ProjectId>"
     ]
    }
   ],
   "source": [
    "# tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "print(\"Running data extraction pipeline...\")\n",
    "run_pipeline(args)\n",
    "print(\"Pipeline is done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30106e3-5496-4379-9249-60d688b4b1d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!gsutil ls {DATA_DIR}/*\n",
    "!ls {CANDIDATE_DIR}/*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0585195b-7f12-470c-94dd-87be46bfc339",
   "metadata": {},
   "source": [
    "# Version 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0feb88-2bf0-4af1-b18a-745621df5385",
   "metadata": {},
   "source": [
    "### Reading from BigQuery\n",
    "\n",
    "* To read an entire BigQuery table, use the table parameter with the BigQuery table name."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff715629-cc87-4053-8014-a1916ef80471",
   "metadata": {},
   "source": [
    "### Define Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "840f95e1-e863-4219-9d9e-f96c2891548c",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = 'hybrid-vertex'\n",
    "BQ_DATASET = 'spotify_train_3'\n",
    "BQ_TABLE = 'train_flatten'\n",
    "\n",
    "table_spec = f'{PROJECT_ID}.{BQ_DATASET}.{BQ_TABLE}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cbe84fdf-bb44-402b-aaf3-f42fdeb3950d",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    \"data\": {\n",
    "        \"bq_source_table\": table_spec,\n",
    "        \"schema\": [\n",
    "            # playlist - context features\n",
    "            { \"name\": \"name\", \"kind\": \"string\", \"mode\": \"NULLABLE\" },\n",
    "            { \"name\": \"collaborative\", \"kind\": \"string\", \"mode\": \"NULLABLE\" },\n",
    "            { \"name\": \"n_songs_pl\", \"kind\": \"float32\", \"mode\": \"NULLABLE\" },\n",
    "            { \"name\": \"num_artists_pl\", \"kind\": \"float32\", \"mode\": \"NULLABLE\" },\n",
    "            { \"name\": \"num_albums_pl\", \"kind\": \"float32\", \"mode\": \"NULLABLE\" },\n",
    "            { \"name\": \"description_pl\", \"kind\": \"string\", \"mode\": \"NULLABLE\" },\n",
    "            # seed track - context features\n",
    "            { \"name\": \"track_name_seed_track\", \"kind\": \"string\", \"mode\": \"NULLABLE\" },\n",
    "            { \"name\": \"artist_name_seed_track\", \"kind\": \"string\", \"mode\": \"NULLABLE\" },\n",
    "            { \"name\": \"album_name_seed_track\", \"kind\": \"string\", \"mode\": \"NULLABLE\" },\n",
    "            { \"name\": \"track_uri_seed_track\", \"kind\": \"string\", \"mode\": \"NULLABLE\" },\n",
    "            { \"name\": \"artist_uri_seed_track\", \"kind\": \"string\", \"mode\": \"NULLABLE\" },\n",
    "            { \"name\": \"album_uri_seed_track\", \"kind\": \"string\", \"mode\": \"NULLABLE\" },\n",
    "            { \"name\": \"duration_seed_track\", \"kind\": \"float32\", \"mode\": \"NULLABLE\" },\n",
    "            { \"name\": \"track_pop_seed_track\", \"kind\": \"float32\", \"mode\": \"NULLABLE\" },\n",
    "            { \"name\": \"artist_pop_seed_track\", \"kind\": \"float32\", \"mode\": \"NULLABLE\" },\n",
    "            { \"name\": \"artist_genres_seed_track\", \"kind\": \"string\", \"mode\": \"NULLABLE\" },\n",
    "            { \"name\": \"artist_followers_seed_track\", \"kind\": \"float32\", \"mode\": \"NULLABLE\" },\n",
    "            # candidate - context features\n",
    "            { \"name\": \"track_name_can\", \"kind\": \"string\", \"mode\": \"NULLABLE\" },\n",
    "            { \"name\": \"artist_name_can\", \"kind\": \"string\", \"mode\": \"NULLABLE\" },\n",
    "            { \"name\": \"album_name_can\", \"kind\": \"string\", \"mode\": \"NULLABLE\" },\n",
    "            { \"name\": \"track_uri_can\", \"kind\": \"string\", \"mode\": \"NULLABLE\" },\n",
    "            { \"name\": \"artist_uri_can\", \"kind\": \"string\", \"mode\": \"NULLABLE\" },\n",
    "            { \"name\": \"album_uri_can\", \"kind\": \"string\", \"mode\": \"NULLABLE\" },\n",
    "            { \"name\": \"duration_ms_can\", \"kind\": \"float32\", \"mode\": \"NULLABLE\" },\n",
    "            { \"name\": \"track_pop_can\", \"kind\": \"float32\", \"mode\": \"NULLABLE\" },\n",
    "            { \"name\": \"artist_pop_can\", \"kind\": \"float32\", \"mode\": \"NULLABLE\" },\n",
    "            { \"name\": \"artist_genres_can\", \"kind\": \"string\", \"mode\": \"NULLABLE\" },\n",
    "            { \"name\": \"artist_followers_can\", \"kind\": \"float32\", \"mode\": \"NULLABLE\" },\n",
    "            # Ragged Features\n",
    "            { \"name\": \"track_name_pl\", \"kind\": \"string\", \"mode\": \"REPEATED\" },\n",
    "            { \"name\": \"artist_name_pl\", \"kind\": \"string\", \"mode\": \"REPEATED\" },\n",
    "            { \"name\": \"album_name_pl\", \"kind\": \"string\", \"mode\": \"REPEATED\" },\n",
    "            { \"name\": \"track_uri_pl\", \"kind\": \"string\", \"mode\": \"REPEATED\" },\n",
    "            { \"name\": \"duration_ms_songs_pl\", \"kind\": \"float32\", \"mode\": \"REPEATED\" },\n",
    "            { \"name\": \"artist_pop_pl\", \"kind\": \"float32\", \"mode\": \"REPEATED\" },\n",
    "            { \"name\": \"artists_followers_pl\", \"kind\": \"float32\", \"mode\": \"REPEATED\" },\n",
    "            { \"name\": \"track_pop_pl\", \"kind\": \"float32\", \"mode\": \"REPEATED\" },\n",
    "            { \"name\": \"artist_genres_pl\", \"kind\": \"string\", \"mode\": \"REPEATED\" },\n",
    "        ]\n",
    "    },\n",
    "    \"modes\": [\n",
    "        { \"name\": \"analysis\" },\n",
    "        { \"name\": \"training\", \"transform\": \"analysis\", \"shuffle\": \"True\" },\n",
    "        { \"name\": \"validation\", \"transform\": \"analysis\" },\n",
    "        { \"name\": \"testing\", \"transform\": \"identity\" }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cd1a9dce-5ea5-435e-95ab-b0cc1ff8fb05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ReadFromBigQuery(PTransform) label=[ReadFromBigQuery] at 0x7f2aecbdcf10>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the SQL code\n",
    "table = CONFIG['data']['bq_source_table'] #.read()\n",
    "schema = CONFIG['data']['schema']\n",
    "\n",
    "# Create a BigQuery source\n",
    "source = beam.io.gcp.bigquery.ReadFromBigQuery(table=table, use_standard_sql=True, flatten_results=False)\n",
    "source\n",
    "# Create metadata needed later\n",
    "# spec = schema.to_feature_spec()\n",
    "# meta = dataset_metadata.DatasetMetadata(\n",
    "#     schema=dataset_schema.from_feature_spec(spec)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d63cf414-4942-4dcb-a339-359e660deb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_test = CONFIG['data']['schema']\n",
    "# schema_test.to_feature_spec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a86be58-dd03-4adc-aed4-ca7acc608b6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a6015e-d1b6-4c80-a21d-d40984f2da50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92b946d-e98a-4757-ae4e-d745bf62b5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the SQL code\n",
    "table = open(config['data']['bq_source_table']).read()\n",
    "\n",
    "# Create a BigQuery source\n",
    "source = beam.io.BigQuerySource(table=table, use_standard_sql=True, flatten_results=False)\n",
    "\n",
    "# Create metadata needed later\n",
    "spec = schema.to_feature_spec()\n",
    "meta = dataset_metadata.DatasetMetadata(\n",
    "    schema=dataset_schema.from_feature_spec(spec)\n",
    ")\n",
    "\n",
    "data = pipeline \\\n",
    "    | 'read' >> beam.io.Read(source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d5ba81-768f-419f-bc78-afb62a88d61a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a4d571-76d8-4ec0-8bf8-5ffc69f64cd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307291ac-e414-498f-9249-c12ed57aec13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abeea77-adf8-4f73-bee8-36dfb3bebec8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d3714e-97ff-4de9-908c-e2a86638a718",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2054824-c72f-41ff-9989-a48d7782b6d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-9.m94",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-9:m94"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
