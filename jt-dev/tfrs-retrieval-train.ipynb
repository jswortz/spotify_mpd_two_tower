{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b950b35-cba2-4469-ba8c-05557a73c6d3",
   "metadata": {},
   "source": [
    "# Multi-Tower Encoders with TFRS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2eb22515-5157-406b-a810-046ab13afaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set variables\n",
    "SEED = 41781897\n",
    "PROJECT_ID = 'hybrid-vertex'\n",
    "BQ_LOCATION='us-central1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0133923-e691-4897-8586-064b9e466d64",
   "metadata": {},
   "source": [
    "### Pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fa7f56cc-ec19-4a94-aec3-c403fb2cdda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow-recommenders -q --user\n",
    "# !pip install -U tensorflow-io==0.15.0 --user"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89fdc03-2386-47c6-92c0-a9400958b829",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2cff2c99-3c3c-42ea-8e63-65e987f3404c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") #do this b/c there's an info-level bug that can safely be ignored\n",
    "\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.framework import dtypes\n",
    "from tensorflow_io.bigquery import BigQueryClient\n",
    "from tensorflow_io.bigquery import BigQueryReadSession\n",
    "\n",
    "import json\n",
    "import tensorflow as tf\n",
    "# import tensorflow_recommenders as tfrs\n",
    "import datetime\n",
    "from tensorflow.python.lib.io import file_io\n",
    "from tensorflow.train import BytesList, Feature, FeatureList, Int64List, FloatList\n",
    "from tensorflow.train import SequenceExample, FeatureLists\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6353d90-bca2-46f3-9c83-53984611aed4",
   "metadata": {},
   "source": [
    "## Prep Train Data\n",
    "\n",
    "* Use tensorflow-io to import from BigQuery\n",
    "* Creates a [TF Dataset](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#methods_2) object \n",
    "* Vocab files for `TextVectorization` and `StringLookup` layers pre-computed and saved to GCS\n",
    "\n",
    "\n",
    "See [end-to-end guide BQ -> TF Dataset](https://www.tensorflow.org/io/tutorials/bigquery#load_census_data_in_tensorflow_dataset_using_bigquery_reader) for tips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "355b7091-facd-4d5e-adbb-66b39120b890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! rm -rf data_prep\n",
    "# ! mkdir data_prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90b91bbd-c765-4f96-b58a-daf944f1c992",
   "metadata": {},
   "outputs": [],
   "source": [
    "bq_2_tf_dict = {\n",
    "    'name': {'mode': BigQueryClient.FieldMode.NULLABLE, 'output_type': dtypes.string},\n",
    "    'collaborative': {'mode': BigQueryClient.FieldMode.NULLABLE, 'output_type': dtypes.string},\n",
    "    'pid': {'mode': BigQueryClient.FieldMode.NULLABLE, 'output_type': dtypes.int64},\n",
    "    # 'duration_ms_playlist': {'mode': BigQueryClient.FieldMode.NULLABLE, 'output_type': dtypes.int64},\n",
    "    # 'pid_pos_id': {'mode':BigQueryClient.FieldMode.NULLABLE, 'output_type': dtypes.string},\n",
    "    # candidate features\n",
    "    'pos_can': {'mode':BigQueryClient.FieldMode.NULLABLE, 'output_type': dtypes.int64},\n",
    "    'artist_name_can': {'mode':BigQueryClient.FieldMode.NULLABLE, 'output_type': dtypes.string},\n",
    "    'track_uri_can': {'mode':BigQueryClient.FieldMode.NULLABLE, 'output_type': dtypes.string},\n",
    "    'artist_uri_can': {'mode':BigQueryClient.FieldMode.NULLABLE, 'output_type': dtypes.string},\n",
    "    'track_name_can': {'mode':BigQueryClient.FieldMode.NULLABLE, 'output_type': dtypes.string},\n",
    "    'album_uri_can': {'mode':BigQueryClient.FieldMode.NULLABLE, 'output_type': dtypes.string},\n",
    "    'duration_ms_can': {'mode':BigQueryClient.FieldMode.NULLABLE, 'output_type': dtypes.float64},\n",
    "    'album_name_can': {'mode':BigQueryClient.FieldMode.NULLABLE, 'output_type': dtypes.string},\n",
    "    'track_pop_can': {'mode':BigQueryClient.FieldMode.NULLABLE, 'output_type': dtypes.float64},\n",
    "    'artist_pop_can': {'mode':BigQueryClient.FieldMode.NULLABLE, 'output_type': dtypes.float64},\n",
    "    'artist_genres_can': {'mode': BigQueryClient.FieldMode.NULLABLE, 'output_type': dtypes.string},\n",
    "    'artist_followers_can': {'mode':BigQueryClient.FieldMode.NULLABLE, 'output_type': dtypes.float64},\n",
    "    # seed track features\n",
    "    'pos_seed_track': {'mode':BigQueryClient.FieldMode.NULLABLE, 'output_type': dtypes.int64},\n",
    "    'artist_name_seed_track': {'mode':BigQueryClient.FieldMode.NULLABLE, 'output_type': dtypes.string},\n",
    "    'artist_uri_seed_track': {'mode':BigQueryClient.FieldMode.NULLABLE, 'output_type': dtypes.string},\n",
    "    'track_name_seed_track': {'mode': BigQueryClient.FieldMode.NULLABLE, 'output_type': dtypes.string},\n",
    "    'track_uri_seed_track': {'mode': BigQueryClient.FieldMode.NULLABLE, 'output_type': dtypes.string},\n",
    "    'album_name_seed_track': {'mode': BigQueryClient.FieldMode.NULLABLE, 'output_type': dtypes.string},\n",
    "    'album_uri_seed_track': {'mode': BigQueryClient.FieldMode.NULLABLE, 'output_type': dtypes.string},\n",
    "    'duration_seed_track': {'mode': BigQueryClient.FieldMode.NULLABLE, 'output_type': dtypes.float64},\n",
    "    'track_pop_seed_track': {'mode':BigQueryClient.FieldMode.NULLABLE, 'output_type': dtypes.float64},\n",
    "    'artist_pop_seed_track': {'mode':BigQueryClient.FieldMode.NULLABLE, 'output_type': dtypes.float64},\n",
    "    'artist_genres_seed_track': {'mode': BigQueryClient.FieldMode.NULLABLE, 'output_type': dtypes.string},\n",
    "    'artist_followers_seed_track': {'mode':BigQueryClient.FieldMode.NULLABLE, 'output_type': dtypes.float64},\n",
    "    ### playlist features\n",
    "    'duration_ms_seed_pl': {'mode': BigQueryClient.FieldMode.NULLABLE, 'output_type': dtypes.float64},\n",
    "    'n_songs_pl': {'mode': BigQueryClient.FieldMode.NULLABLE, 'output_type': dtypes.float64},\n",
    "    'num_artists_pl': {'mode': BigQueryClient.FieldMode.NULLABLE, 'output_type': dtypes.float64},\n",
    "    'num_albums_pl': {'mode': BigQueryClient.FieldMode.NULLABLE, 'output_type': dtypes.float64},\n",
    "    'description_pl': {'mode': BigQueryClient.FieldMode.NULLABLE, 'output_type': dtypes.string},\n",
    "    # 'pos_pl': {'mode': BigQueryClient.FieldMode.REPEATED, 'output_type': dtypes.int64},\n",
    "    'artist_name_pl': {'mode': BigQueryClient.FieldMode.REPEATED, 'output_type': dtypes.string},\n",
    "    'track_uri_pl': {'mode': BigQueryClient.FieldMode.REPEATED, 'output_type': dtypes.string},\n",
    "    'track_name_pl': {'mode': BigQueryClient.FieldMode.REPEATED, 'output_type': dtypes.string},\n",
    "    'duration_ms_songs_pl': {'mode': BigQueryClient.FieldMode.REPEATED, 'output_type': dtypes.float64},\n",
    "    'album_name_pl': {'mode': BigQueryClient.FieldMode.REPEATED, 'output_type': dtypes.string},\n",
    "    'artist_pop_pl': {'mode': BigQueryClient.FieldMode.REPEATED, 'output_type': dtypes.float64},\n",
    "    'artists_followers_pl': {'mode': BigQueryClient.FieldMode.REPEATED, 'output_type': dtypes.float64},              \n",
    "    'track_pop_pl': {'mode': BigQueryClient.FieldMode.REPEATED, 'output_type': dtypes.int64},\n",
    "    'artist_genres_pl': {'mode': BigQueryClient.FieldMode.REPEATED, 'output_type': dtypes.string},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "014e8c7c-8af0-40d1-ae94-460fe3a781df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-05 20:48:29.884955: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2022-07-05 20:48:29.884995: W tensorflow/stream_executor/cuda/cuda_driver.cc:312] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-07-05 20:48:29.885017: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (spotify-tf-2-9-jt-dev): /proc/driver/nvidia/version does not exist\n",
      "2022-07-05 20:48:29.885379: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-07-05 20:48:29.896237: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2299995000 Hz\n",
      "2022-07-05 20:48:29.900277: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x564afd6282a0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2022-07-05 20:48:29.900317: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n"
     ]
    }
   ],
   "source": [
    "BQ_TABLE_TRAIN = 'train_flatten'\n",
    "BQ_DATASET_TRAIN = 'spotify_train_3'\n",
    "io_batch_size = 1\n",
    "\n",
    "bq_client = BigQueryClient()\n",
    "\n",
    "bqsession = bq_client.read_session(\n",
    "    \"projects/\" + PROJECT_ID,\n",
    "    PROJECT_ID, \n",
    "    f'{BQ_TABLE_TRAIN}', \n",
    "    f'{BQ_DATASET_TRAIN}',\n",
    "    bq_2_tf_dict,\n",
    "    requested_streams=2,\n",
    ")\n",
    "\n",
    "dataset = bqsession.parallel_read_rows()\n",
    "dataset = dataset.prefetch(1).shuffle(io_batch_size*10).batch(io_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cdbd10-0975-4fd3-8fe1-5bf8ed01d9ee",
   "metadata": {},
   "source": [
    "# Multi-Tower Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab908ab-590e-4bc1-a011-df278b974f0f",
   "metadata": {},
   "source": [
    "Create a DNN nominator based on a 3-tower architecture, with `playlist`, `seed_track`, and `candidate_track` towers (TODO: insert diagram)\n",
    "\n",
    "* In this model, we have **two** kinds of query embeddings: `playlist` and `seed_track`\n",
    "* Within `playlist`, we include the sequence of tracks in each playlist\n",
    "* We propose an attention-based playlist model to summarize the track sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb63b5d1-79a8-4724-b643-ed33766fe55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "from google.cloud import storage\n",
    "\n",
    "BUCKET_NAME = 'spotify-v1'\n",
    "FILE_PATH = 'vocabs/v1_string_vocabs'\n",
    "FILE_NAME = 'string_vocabs_v1_20220705-202905.txt'\n",
    "DESTINATION_FILE = 'downloaded_vocabs.txt'\n",
    "\n",
    "client = storage.Client()\n",
    "\n",
    "with open(f'{DESTINATION_FILE}', 'wb') as file_obj:\n",
    "    client.download_blob_to_file(\n",
    "        f'gs://{BUCKET_NAME}/{FILE_PATH}/{FILE_NAME}', file_obj)\n",
    "\n",
    "    \n",
    "with open(f'{DESTINATION_FILE}', 'rb') as pickle_file:\n",
    "    vocab_dict_load = pkl.load(pickle_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93765ecd-93e3-4b4e-ade4-87632cd812a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab_dict_load"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0bccf90-2f88-4546-bffe-0e4bf8e8da50",
   "metadata": {},
   "source": [
    "## Playlist Tower\n",
    "* TODO: Add sequence model for playlist_seed_sequence features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f4698d-4595-48f8-b5ee-afc74d55d88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIM = 32\n",
    "PROJECTION_DIM = 5\n",
    "SEED = 1234\n",
    "\n",
    "class Playlist_Model(tf.keras.Model):\n",
    "    def __init__(self, layer_sizes, vocab_dict):\n",
    "        super().__init__()\n",
    "        \n",
    "        # ========================================\n",
    "        # non-sequence playlist features\n",
    "        # ========================================\n",
    "        \n",
    "        # Feature: playlist name\n",
    "        self.pl_name_text_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.TextVectorization(\n",
    "                    max_tokens=len(vocab_dict[\"name\"]),\n",
    "                    vocabulary=vocab_dict['name'], \n",
    "                    name=\"pl_name_txt_vectorizer\", \n",
    "                    ngrams=2\n",
    "                ),\n",
    "                tf.keras.layers.Embedding(\n",
    "                    input_dim=len(vocab_dict[\"name\"]),\n",
    "                    output_dim=OUTPUT_DIM,\n",
    "                    mask_zero=False,\n",
    "                    name=\"pl_name_emb_layer\",\n",
    "                ),\n",
    "                tf.keras.layers.GlobalAveragePooling1D(name=\"pl_name_pooling\"),\n",
    "            ], name=\"pl_name_emb_model\"\n",
    "        )\n",
    "        \n",
    "        # Feature: collaborative\n",
    "        collaborative_vocab = np.array([b'false', b'true'])\n",
    "        \n",
    "        self.pl_collaborative_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.StringLookup(\n",
    "                    vocabulary=collaborative_vocab, \n",
    "                    mask_token=None, \n",
    "                    name=\"pl_collaborative_lookup\", \n",
    "                    output_mode='int'\n",
    "                ),\n",
    "                tf.keras.layers.Embedding(\n",
    "                    input_dim=len(collaborative_vocab),\n",
    "                    output_dim=OUTPUT_DIM,\n",
    "                    mask_zero=False,\n",
    "                    name=\"pl_collaborative_emb_layer\",\n",
    "                ),\n",
    "            ], name=\"pl_collaborative_emb_model\"\n",
    "        )\n",
    "\n",
    "        # Feature: pid\n",
    "        self.pl_pid_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.StringLookup(\n",
    "                    vocabulary=vocab_dict['unique_pids'], \n",
    "                    mask_token=None, \n",
    "                    name=\"pl_pid_lookup\", \n",
    "                    output_mode='int'\n",
    "                ),\n",
    "                tf.keras.layers.Embedding(\n",
    "                    input_dim=len(vocab_dict['unique_pids']),\n",
    "                    output_dim=OUTPUT_DIM,\n",
    "                    mask_zero=False,\n",
    "                    name=\"pl_pid_emb_layer\",\n",
    "                ),\n",
    "            ], name=\"pl_pid_emb_model\"\n",
    "        )\n",
    "        \n",
    "        # Feature: description_pl\n",
    "        self.pl_description_text_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.TextVectorization(\n",
    "                    max_tokens=len(vocab_dict[\"description_pl\"]),\n",
    "                    vocabulary=vocab_dict['description_pl'], \n",
    "                    name=\"description_pl_vectorizer\", \n",
    "                    ngrams=2,\n",
    "                ),\n",
    "                tf.keras.layers.Embedding(\n",
    "                    input_dim=len(vocab_dict[\"description_pl\"]),\n",
    "                    output_dim=OUTPUT_DIM,\n",
    "                    mask_zero=False,\n",
    "                    name=\"description_pl_emb_layer\",\n",
    "                ),\n",
    "                tf.keras.layers.GlobalAveragePooling1D(name=\"description_pl_pooling\"),\n",
    "            ], name=\"pl_description_emb_model\"\n",
    "        )\n",
    "        \n",
    "        # Feature: duration_ms_seed_pl\n",
    "        duration_ms_seed_pl_buckets = np.linspace(\n",
    "            vocab_dict['min_duration_ms_seed_pl'], \n",
    "            vocab_dict['max_duration_ms_seed_pl'], \n",
    "            num=1000\n",
    "        )\n",
    "        self.duration_ms_seed_pl_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.Discretization(duration_ms_seed_pl_buckets.tolist()),\n",
    "                tf.keras.layers.Embedding(\n",
    "                    input_dim=len(duration_ms_seed_pl_buckets) + 1, \n",
    "                    output_dim=OUTPUT_DIM, \n",
    "                    name=\"duration_ms_seed_pl_emb_layer\",\n",
    "                )\n",
    "            ], name=\"duration_ms_seed_pl_emb_model\"\n",
    "        )\n",
    "        # self.duration_ms_seed_pl_normalization = tf.keras.layers.Normalization(\n",
    "        #     axis=None\n",
    "        # )\n",
    "        \n",
    "        # Feature: n_songs_pl\n",
    "        n_songs_pl_buckets = np.linspace(\n",
    "            vocab_dict['min_n_songs_pl'], \n",
    "            vocab_dict['max_n_songs_pl'], \n",
    "            num=100\n",
    "        )\n",
    "        self.n_songs_pl_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.Discretization(n_songs_pl_buckets.tolist()),\n",
    "                tf.keras.layers.Embedding(\n",
    "                    input_dim=len(n_songs_pl_buckets) + 1, \n",
    "                    output_dim=OUTPUT_DIM, \n",
    "                    name=\"n_songs_pl_emb_layer\",\n",
    "                )\n",
    "            ], name=\"n_songs_pl_emb_model\"\n",
    "        )\n",
    "        # self.n_songs_pl_normalization = tf.keras.layers.Normalization(\n",
    "        #     axis=None\n",
    "        # )\n",
    "        \n",
    "        # Feature: num_artists_pl\n",
    "        n_artists_pl_buckets = np.linspace(\n",
    "            vocab_dict['min_n_artists_pl'], \n",
    "            vocab_dict['max_n_artists_pl'], \n",
    "            num=100\n",
    "        )\n",
    "        self.n_artists_pl_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.Discretization(n_artists_pl_buckets.tolist()),\n",
    "                tf.keras.layers.Embedding(\n",
    "                    input_dim=len(n_artists_pl_buckets) + 1, \n",
    "                    output_dim=OUTPUT_DIM, \n",
    "                    name=\"n_artists_pl_emb_layer\",\n",
    "                )\n",
    "            ], name=\"n_artists_pl_emb_model\"\n",
    "        )\n",
    "        # self.n_artists_pl_normalization = tf.keras.layers.Normalization(\n",
    "        #     axis=None\n",
    "        # )\n",
    "        \n",
    "        # Feature: num_albums_pl\n",
    "        n_albums_pl_buckets = np.linspace(\n",
    "            vocab_dict['min_n_albums_pl'], \n",
    "            vocab_dict['max_n_albums_pl'],\n",
    "            num=100\n",
    "        )\n",
    "        self.n_albums_pl_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.Discretization(n_albums_pl_buckets.tolist()),\n",
    "                tf.keras.layers.Embedding(\n",
    "                    input_dim=len(n_albums_pl_buckets) + 1, \n",
    "                    output_dim=OUTPUT_DIM, \n",
    "                    name=\"n_albums_pl_emb_layer\",\n",
    "                )\n",
    "            ], name=\"n_albums_pl_emb_model\"\n",
    "        )\n",
    "        # self.n_albums_pl_normalization = tf.keras.layers.Normalization(\n",
    "        #     axis=None\n",
    "        # )\n",
    "\n",
    "        # ========================================\n",
    "        # sequence playlist features\n",
    "        # ========================================\n",
    "        \n",
    "        # Feature: pos_pl\n",
    "        # self.pos_pl_embedding = XXX\n",
    "        \n",
    "        # Feature: artist_name_pl\n",
    "        self.artist_name_pl_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.StringLookup(\n",
    "                    vocabulary=vocab_dict['artist_name_pl'], mask_token=None),\n",
    "                tf.keras.layers.Embedding(\n",
    "                    input_dim=len(vocab_dict['artist_name_pl']) + 1, \n",
    "                    OUTPUT_DIM,\n",
    "                    name=\"artist_name_pl_emb_layer\",\n",
    "                ),\n",
    "                tf.keras.layers.GRU(OUTPUT_DIM),\n",
    "            ], name=\"artist_name_pl_emb_model\"\n",
    "        )\n",
    "        \n",
    "        # Feature: track_uri_pl\n",
    "        self.track_uri_pl_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.Hashing(num_bins=200_000),\n",
    "                tf.keras.layers.Embedding(\n",
    "                    input_dim=len(vocab_dict['track_uri_pl']) + 1, \n",
    "                    OUTPUT_DIM,\n",
    "                    name=\"track_uri_pl_emb_layer\",\n",
    "                ),\n",
    "                tf.keras.layers.GRU(OUTPUT_DIM),\n",
    "            ], name=\"track_uri_pl_emb_model\"\n",
    "        )\n",
    "        \n",
    "        # Feature: track_name_pl\n",
    "        self.track_name_pl_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.StringLookup(\n",
    "                    vocabulary=vocab_dict['track_name_pl'], mask_token=None),\n",
    "                tf.keras.layers.Embedding(\n",
    "                    input_dim=len(vocab_dict['track_name_pl']) + 1, \n",
    "                    OUTPUT_DIM,\n",
    "                    name=\"track_name_pl_emb_layer\",\n",
    "                ),\n",
    "                tf.keras.layers.GRU(OUTPUT_DIM),\n",
    "            ], name=\"track_name_pl_emb_model\"\n",
    "        )\n",
    "        \n",
    "        # Feature: duration_ms_songs_pl\n",
    "        duration_ms_songs_pl_buckets = np.linspace(\n",
    "            vocab_dict['min_duration_ms_songs_pl'], \n",
    "            vocab_dict['max_duration_ms_songs_pl'], \n",
    "            num=1000\n",
    "        )\n",
    "        self.duration_ms_songs_pl_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.Discretization(duration_ms_songs_pl_buckets.tolist()),\n",
    "                tf.keras.layers.Embedding(\n",
    "                    input_dim=len(duration_ms_songs_pl_buckets) + 1, \n",
    "                    OUTPUT_DIM,\n",
    "                    name=\"duration_ms_songs_pl_emb_layer\",\n",
    "                ),\n",
    "                tf.keras.layers.GRU(OUTPUT_DIM),\n",
    "            ], name=\"duration_ms_songs_pl_emb_model\"\n",
    "        )\n",
    "        \n",
    "        # Feature: album_name_pl\n",
    "        self.album_name_pl_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.StringLookup(\n",
    "                    vocabulary=vocab_dict['album_name_pl'], mask_token=None),\n",
    "                tf.keras.layers.Embedding(\n",
    "                    input_dim=len(vocab_dict['album_name_pl']) + 1, \n",
    "                    OUTPUT_DIM,\n",
    "                    name=\"album_name_pl_emb_layer\",\n",
    "                ),\n",
    "                tf.keras.layers.GRU(OUTPUT_DIM),\n",
    "            ], name=\"album_name_pl_emb_model\"\n",
    "        )\n",
    "        \n",
    "        # Feature: artist_pop_pl\n",
    "        artist_pop_pl_buckets = np.linspace(\n",
    "            vocab_dict['min_artist_pop'], \n",
    "            vocab_dict['max_artist_pop'], \n",
    "            num=10\n",
    "        )\n",
    "        self.artist_pop_pl_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.Discretization(artist_pop_pl_buckets.tolist()),\n",
    "                tf.keras.layers.Embedding(\n",
    "                    input_dim=len(artist_pop_pl_buckets) + 1, \n",
    "                    OUTPUT_DIM,\n",
    "                    name=\"artist_pop_pl_emb_layer\",\n",
    "                ),\n",
    "                tf.keras.layers.GRU(OUTPUT_DIM),\n",
    "            ], name=\"artist_pop_pl_emb_model\"\n",
    "        )\n",
    "        \n",
    "        # Feature: artists_followers_pl\n",
    "        artists_followers_pl_buckets = np.linspace(\n",
    "            vocab_dict['min_artist_followers'], \n",
    "            vocab_dict['max_artist_followers'], \n",
    "            num=10\n",
    "        )\n",
    "        self.artists_followers_pl_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.Discretization(artists_followers_pl_buckets.tolist()),\n",
    "                tf.keras.layers.Embedding(\n",
    "                    input_dim=len(artists_followers_pl_buckets) + 1, \n",
    "                    OUTPUT_DIM,\n",
    "                    name=\"artists_followers_pl_emb_layer\",\n",
    "                ),\n",
    "                tf.keras.layers.GRU(OUTPUT_DIM),\n",
    "            ], name=\"artists_followers_pl_emb_model\"\n",
    "        )\n",
    "        \n",
    "        # Feature: track_pop_pl\n",
    "        track_pop_pl_buckets = np.linspace(\n",
    "            vocab_dict['min_track_pop'], \n",
    "            vocab_dict['max_track_pop'], \n",
    "            num=10\n",
    "        )\n",
    "        self.track_pop_pl_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.Discretization(track_pop_pl_buckets.tolist()),\n",
    "                tf.keras.layers.Embedding(\n",
    "                    input_dim=len(track_pop_pl_buckets) + 1, \n",
    "                    OUTPUT_DIM,\n",
    "                    name=\"track_pop_pl_emb_layer\",\n",
    "                ),\n",
    "                tf.keras.layers.GRU(OUTPUT_DIM),\n",
    "            ], name=\"track_pop_pl_emb_model\"\n",
    "        )\n",
    "        \n",
    "        # Feature: artist_genres_pl\n",
    "        self.artist_genres_pl_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.StringLookup(\n",
    "                    vocabulary=vocab_dict['artist_genres_pl'], mask_token=None),\n",
    "                tf.keras.layers.Embedding(\n",
    "                    input_dim=len(vocab_dict['artist_genres_pl']) + 1, \n",
    "                    OUTPUT_DIM,\n",
    "                    name=\"artist_genres_pl_emb_layer\",\n",
    "                ),\n",
    "                tf.keras.layers.GRU(OUTPUT_DIM),\n",
    "            ], name=\"artist_genres_pl_emb_model\"\n",
    "        )\n",
    "\n",
    "\n",
    "        # ========================================\n",
    "        # Cross Layers\n",
    "        # ========================================\n",
    "        # Cross Layers\n",
    "        if USE_CROSS_LAYER:\n",
    "            self._cross_layer = tfrs.layers.dcn.Cross(\n",
    "                projection_dim=PROJECTION_DIM,\n",
    "                kernel_initializer=\"glorot_uniform\", \n",
    "                name=\"pl_cross_layer\"\n",
    "            )\n",
    "        else:\n",
    "            self._cross_layer = None\n",
    "            \n",
    "        # Dense Layer\n",
    "        self.dense_layers = tf.keras.Sequential(name=\"pl_dense_layers\")\n",
    "        initializer = tf.keras.initializers.GlorotUniform(seed=SEED)\n",
    "        \n",
    "        # Use the ReLU activation for all but the last layer.\n",
    "        for layer_size in layer_sizes[:-1]:\n",
    "            self.dense_layers.add(\n",
    "                tf.keras.layers.Dense(\n",
    "                    layer_size, \n",
    "                    activation=\"relu\", \n",
    "                    kernel_initializer=initializer,\n",
    "                )\n",
    "            )\n",
    "            if DROPOUT:\n",
    "                self.dense_layers.add(\n",
    "                    tf.keras.layers.Dropout(\n",
    "                        DROPOUT_RATE\n",
    "                    )\n",
    "                )\n",
    "                \n",
    "        # No activation for the last layer\n",
    "        for layer_size in layer_sizes[-1:]:\n",
    "            self.dense_layers.add(\n",
    "                tf.keras.layers.Dense(\n",
    "                    layer_size, \n",
    "                    kernel_initializer=initializer\n",
    "                )\n",
    "            )\n",
    "        ### ADDING L2 NORM AT THE END\n",
    "        self.dense_layers.add(\n",
    "            tf.keras.layers.Lambda(\n",
    "                lambda x: tf.nn.l2_normalize(\n",
    "                    x, 1, epsilon=1e-12, name=\"normalize_dense\"\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    def call(self, data):\n",
    "        '''\n",
    "        The call method defines what happens when\n",
    "        the model is called\n",
    "        '''\n",
    "        \n",
    "        all_embs = tf.concat(\n",
    "            [\n",
    "                self.pl_name_text_embedding(data['name']),\n",
    "                self.pl_collaborative_embedding(data['collaborative']),\n",
    "                self.pl_pid_embedding(data[\"pid\"]),\n",
    "                # data[\"pid_pos_id\"],\n",
    "                self.pl_description_text_embedding(data['description_pl']),\n",
    "                self.duration_ms_seed_pl_embedding(data[\"duration_ms_seed_pl\"]),\n",
    "                self.n_songs_pl_embedding(data[\"n_songs_pl\"]),\n",
    "                self.n_artists_pl_embedding(data[\"num_artists_pl\"]),\n",
    "                self.n_albums_pl_embedding(data[\"num_albums_pl\"]),\n",
    "                # sequence features\n",
    "                # data[\"pos_pl\"],\n",
    "                self.artist_name_pl_embedding(data[\"artist_name_pl\"]),\n",
    "                self.track_uri_pl_embedding(data[\"track_uri_pl\"]),\n",
    "                self.track_name_pl_embedding(data[\"track_name_pl\"]),\n",
    "                self.duration_ms_songs_pl_embedding(data[\"duration_ms_songs_pl\"]),\n",
    "                self.album_name_pl_embedding(data[\"album_name_pl\"]),\n",
    "                self.artist_pop_pl_embedding(data[\"artist_pop_pl\"]),\n",
    "                self.artists_followers_pl_embedding(data[\"artists_followers_pl\"]),\n",
    "                self.track_pop_pl_embedding(data[\"track_pop_pl\"]),\n",
    "                self.artist_genres_pl_embedding(data[\"artist_genres_pl\"]),\n",
    "            ], axis=1)\n",
    "        # Build Cross Network\n",
    "        if self._cross_layer is not None:\n",
    "            cross_embs = self._cross_layer(all_embs)\n",
    "            return self.dense_layers(cross_embs)\n",
    "        else:\n",
    "            return self.dense_layers(all_embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbee0a35-e1a9-4afa-b9c6-90e4af3b3527",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bfab5a01-14cf-4bcc-874c-b8c965e69661",
   "metadata": {},
   "source": [
    "## Seed Track Tower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9697418d-10b0-4118-88a3-5296a735df5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seed_Track_Model(tf.keras.Model):\n",
    "    def __init__(self, layer_sizes, vocab_dict):\n",
    "        super().__init__()\n",
    "        \n",
    "        # ========================================\n",
    "        # seed track features\n",
    "        # ========================================\n",
    "        \n",
    "        # Feature: pos_seed_track\n",
    "        \n",
    "        # Feature: artist_name_seed_track\n",
    "        self.artist_name_seed_track_text_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.TextVectorization(\n",
    "                    max_tokens=len(vocab_dict[\"artist_name_seed_track\"]),\n",
    "                    vocabulary=vocab_dict[\"artist_name_seed_track\"],\n",
    "                    name=\"artist_name_seed_track_txt_vectorizer\",\n",
    "                    ngrams=2,\n",
    "                ),\n",
    "                tf.keras.layers.Embedding(\n",
    "                    input_dim=(vocab_dict[\"artist_name_seed_track\"]),\n",
    "                    output_dim=OUTPUT_DIM,\n",
    "                    mask_zero=False,\n",
    "                    name=\"artist_name_seed_track_emb_layer\",\n",
    "                ),\n",
    "                tf.keras.layers.GlobalAveragePooling1D(name=\"artist_name_seed_track_pooling\"),\n",
    "            ], name=\"artist_name_seed_track_emb_model\"\n",
    "        )\n",
    "        \n",
    "        # Feature: artist_uri_seed_track\n",
    "        # self.artist_uri_seed_track_hasher = tf.keras.layers.Hashing(num_bins=200_000)\n",
    "        # self.artist_uri_seed_track_embedding = tf.keras.layers.CategoryEncoding(num_tokens=64, output_mode=\"int\")\n",
    "        self.artist_uri_seed_track_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.StringLookup(\n",
    "                    vocabulary=vocab_dict['artist_uri_seed_track'], mask_token=None),\n",
    "                tf.keras.layers.Embedding(\n",
    "                    input_dim=len(vocab_dict['artist_uri_seed_track']) + 1, \n",
    "                    OUTPUT_DIM,\n",
    "                    name=\"artist_uri_seed_track_emb_layer\",\n",
    "                ),\n",
    "            ], name=\"artist_uri_seed_track_emb_model\"\n",
    "        )\n",
    "        \n",
    "        # Feature: track_name_seed_track\n",
    "        self.track_name_seed_track_text_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.TextVectorization(\n",
    "                    max_tokens=len(vocab_dict[\"track_name_seed_track\"]),\n",
    "                    vocabulary=vocab_dict[\"track_name_seed_track\"],\n",
    "                    name=\"track_name_seed_track_txt_vectorizer\",\n",
    "                    ngrams=2,\n",
    "                ),\n",
    "                tf.keras.layers.Embedding(\n",
    "                    input_dim=(vocab_dict[\"track_name_seed_track\"]),\n",
    "                    output_dim=OUTPUT_DIM,\n",
    "                    mask_zero=False,\n",
    "                    name=\"track_name_seed_track_emb_layer\",\n",
    "                ),\n",
    "                tf.keras.layers.GlobalAveragePooling1D(name=\"track_name_seed_track_pooling\"),\n",
    "            ], name=\"track_name_seed_track_emb_model\"\n",
    "        )\n",
    "        # Feature: track_uri_seed_track\n",
    "        self.track_uri_seed_track_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.StringLookup(\n",
    "                    vocabulary=vocab_dict['track_uri_seed_track'], mask_token=None),\n",
    "                tf.keras.layers.Embedding(\n",
    "                    input_dim=len(vocab_dict['track_uri_seed_track']) + 1, \n",
    "                    OUTPUT_DIM,\n",
    "                    name=\"track_uri_seed_track_emb_layer\",\n",
    "                ),\n",
    "            ], name=\"track_uri_seed_track_emb_model\"\n",
    "        )\n",
    "        \n",
    "        # Feature: album_name_seed_track\n",
    "        self.album_name_seed_track_text_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.TextVectorization(\n",
    "                    max_tokens=len(vocab_dict[\"album_name_seed_track\"]),\n",
    "                    vocabulary=vocab_dict[\"album_name_seed_track\"],\n",
    "                    name=\"album_name_seed_track_txt_vectorizer\",\n",
    "                    ngrams=2,\n",
    "                ),\n",
    "                tf.keras.layers.Embedding(\n",
    "                    input_dim=(vocab_dict[\"album_name_seed_track\"]),\n",
    "                    output_dim=OUTPUT_DIM,\n",
    "                    mask_zero=False,\n",
    "                    name=\"album_name_seed_track_emb_layer\",\n",
    "                ),\n",
    "                tf.keras.layers.GlobalAveragePooling1D(name=\"album_name_seed_track_pooling\"),\n",
    "            ], name=\"album_name_seed_track_emb_model\"\n",
    "        )\n",
    "        # Feature: album_uri_seed_track\n",
    "        self.album_uri_seed_track_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.StringLookup(\n",
    "                    vocabulary=vocab_dict['album_uri_seed_track'], mask_token=None),\n",
    "                tf.keras.layers.Embedding(\n",
    "                    input_dim=len(vocab_dict['album_uri_seed_track']) + 1, \n",
    "                    OUTPUT_DIM,\n",
    "                    name=\"album_uri_seed_track_emb_layer\",\n",
    "                ),\n",
    "            ], name=\"album_uri_seed_track_emb_model\"\n",
    "        )\n",
    "        # Feature: duration_seed_track\n",
    "        self.duration_seed_track_normalized = tf.keras.layers.Normalization(axis=None)\n",
    "        \n",
    "        # Feature: track_pop_seed_track\n",
    "        self.track_pop_seed_track_normalized = tf.keras.layers.Normalization(axis=None)\n",
    "        \n",
    "        # Feature: artist_pop_seed_track\n",
    "        self.artist_pop_seed_track_normalized = tf.keras.layers.Normalization(axis=None)\n",
    "        \n",
    "        # Feature: artist_genres_seed_track\n",
    "        self.artist_genres_seed_track_text_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.TextVectorization(\n",
    "                    max_tokens=len(vocab_dict[\"artist_genres_seed_track\"]),\n",
    "                    vocabulary=vocab_dict[\"artist_genres_seed_track\"],\n",
    "                    name=\"artist_genres_seed_track_txt_vectorizer\",\n",
    "                    ngrams=2,\n",
    "                ),\n",
    "                tf.keras.layers.Embedding(\n",
    "                    input_dim=(vocab_dict[\"artist_genres_seed_track\"]),\n",
    "                    output_dim=OUTPUT_DIM,\n",
    "                    mask_zero=False,\n",
    "                    name=\"artist_genres_seed_track_emb_layer\",\n",
    "                ),\n",
    "                tf.keras.layers.GlobalAveragePooling1D(name=\"artist_genres_seed_track_pooling\"),\n",
    "            ], name=\"artist_genres_seed_track_emb_model\"\n",
    "        )\n",
    "        \n",
    "        # Feature: artist_followers_seed_track\n",
    "        self.artist_followers_seed_track_normalized = tf.keras.layers.Normalization(axis=None)\n",
    "        \n",
    "        # ========================================\n",
    "        # seed track cross_layers\n",
    "        # ========================================\n",
    "        \n",
    "        # Cross Layers\n",
    "        if USE_CROSS_LAYER:\n",
    "            self._cross_layer = tfrs.layers.dcn.Cross(\n",
    "                projection_dim=PROJECTION_DIM,\n",
    "                kernel_initializer=\"glorot_uniform\", \n",
    "                name=\"seed_track_cross_layer\"\n",
    "            )\n",
    "        else:\n",
    "            self._cross_layer = None\n",
    "            \n",
    "        # Dense Layer\n",
    "        self.dense_layers = tf.keras.Sequential(name=\"seed_track_dense_layers\")\n",
    "        initializer = tf.keras.initializers.GlorotUniform(seed=SEED)\n",
    "        \n",
    "        # Use the ReLU activation for all but the last layer.\n",
    "        for layer_size in layer_sizes[:-1]:\n",
    "            self.dense_layers.add(\n",
    "                tf.keras.layers.Dense(\n",
    "                    layer_size, \n",
    "                    activation=\"relu\", \n",
    "                    kernel_initializer=initializer,\n",
    "                )\n",
    "            )\n",
    "            if DROPOUT:\n",
    "                self.dense_layers.add(\n",
    "                    tf.keras.layers.Dropout(\n",
    "                        DROPOUT_RATE\n",
    "                    )\n",
    "                )\n",
    "                \n",
    "        # No activation for the last layer\n",
    "        for layer_size in layer_sizes[-1:]:\n",
    "            self.dense_layers.add(\n",
    "                tf.keras.layers.Dense(\n",
    "                    layer_size, \n",
    "                    kernel_initializer=initializer\n",
    "                )\n",
    "            )\n",
    "        ### ADDING L2 NORM AT THE END\n",
    "        self.dense_layers.add(\n",
    "            tf.keras.layers.Lambda(\n",
    "                lambda x: tf.nn.l2_normalize(\n",
    "                    x, 1, epsilon=1e-12, name=\"normalize_dense\"\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    def call(self, data):\n",
    "        \n",
    "        all_embs = tf.concat(\n",
    "            [\n",
    "                data['pos_seed_track'],\n",
    "                self.artist_name_seed_track_text_embedding(data['artist_name_seed_track']),\n",
    "                self.artist_uri_seed_track_embedding(data['artist_uri_seed_track']),\n",
    "                self.track_name_seed_track_text_embedding(data['track_name_seed_track']),\n",
    "                self.track_uri_seed_track_embedding(data[\"track_uri_seed_track\"]),\n",
    "                self.album_name_seed_track_text_embedding(data[\"album_name_seed_track\"]),\n",
    "                self.album_uri_seed_track_embedding(data[\"album_uri_seed_track\"]),\n",
    "                tf.reshape(self.duration_seed_track_normalized(data[\"duration_seed_track\"]), (-1, 1))\n",
    "                tf.reshape(self.duration_seed_track_normalized(data[\"track_pop_seed_track\"]), (-1, 1))\n",
    "                tf.reshape(self.duration_seed_track_normalized(data[\"artist_pop_seed_track\"]), (-1, 1))\n",
    "                self.artist_genres_seed_track_text_embedding(data[\"artist_genres_seed_track\"]),\n",
    "                tf.reshape(self.artist_followers_seed_track_normalized(data[\"artist_followers_seed_track\"]), (-1, 1))\n",
    "            ], axis=1)\n",
    "        # Build Cross Network\n",
    "        if self._cross_layer is not None:\n",
    "            cross_embs = self._cross_layer(all_embs)\n",
    "            return self.dense_layers(cross_embs)\n",
    "        else:\n",
    "            return self.dense_layers(all_embs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14a5958-afc8-4671-af48-618eb2ed4374",
   "metadata": {},
   "source": [
    "## Candidate Track Tower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90ac61e-fa2b-4bea-b182-f3d8eb79d395",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Candidate_Track_Model(tf.keras.Model):\n",
    "    def __init__(self, layer_sizes, vocab_dict):\n",
    "        super().__init__()\n",
    "        \n",
    "        # ========================================\n",
    "        # seed track features\n",
    "        # ========================================\n",
    "        \n",
    "        # Feature: pos_can\n",
    "        \n",
    "        # Feature: artist_name_can\n",
    "        self.artist_name_can_text_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.TextVectorization(\n",
    "                    max_tokens=len(vocab_dict[\"artist_name_can\"]),\n",
    "                    vocabulary=vocab_dict[\"artist_name_can\"],\n",
    "                    name=\"artist_name_can_txt_vectorizer\",\n",
    "                    ngrams=2,\n",
    "                ),\n",
    "                tf.keras.layers.Embedding(\n",
    "                    input_dim=(vocab_dict[\"artist_name_can\"]),\n",
    "                    output_dim=OUTPUT_DIM,\n",
    "                    mask_zero=False,\n",
    "                    name=\"artist_name_can_emb_layer\",\n",
    "                ),\n",
    "                tf.keras.layers.GlobalAveragePooling1D(name=\"artist_name_can_pooling\"),\n",
    "            ], name=\"artist_name_can_emb_model\"\n",
    "        )\n",
    "        \n",
    "        # Feature: track_uri_can\n",
    "        self.track_uri_can_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.StringLookup(\n",
    "                    vocabulary=vocab_dict['track_uri_can'], mask_token=None),\n",
    "                tf.keras.layers.Embedding(\n",
    "                    input_dim=len(vocab_dict['track_uri_can']) + 1, \n",
    "                    OUTPUT_DIM,\n",
    "                    name=\"track_uri_can_emb_layer\",\n",
    "                ),\n",
    "            ], name=\"track_uri_can_emb_model\"\n",
    "        )\n",
    "        \n",
    "        # Feature: artist_uri_can\n",
    "        \n",
    "        # Feature: track_name_can\n",
    "        \n",
    "        # Feature: album_uri_can\n",
    "        \n",
    "        # Feature: duration_ms_can\n",
    "        \n",
    "        # Feature: album_name_can\n",
    "        \n",
    "        # Feature: track_pop_can\n",
    "        \n",
    "        # Feature: artist_pop_can\n",
    "        \n",
    "        # Feature: artist_genres_can\n",
    "        \n",
    "        # Feature: artist_followers_can"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d410a94-92cc-4097-94f3-ed24578ebae0",
   "metadata": {},
   "source": [
    "## Combined Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0add47-89c8-408c-bb47-bf76f47391c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63406535-cd99-4f17-81bb-d911c5c4474f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5f0b0b-6117-4df9-9a77-dc98a07e9c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# context_features_ = {\n",
    "#     # playlist features\n",
    "#     \"name\": tf.io.FixedLenFeature(dtype=tf.string, shape=(1)),\n",
    "#     \"collaborative\": tf.io.FixedLenFeature(dtype=tf.string, shape=(1)),\n",
    "#     \"pid\": tf.io.FixedLenFeature(dtype=tf.int64, shape=(1)),\n",
    "#     \"duration_ms_seed_pl\": tf.io.FixedLenFeature(dtype=tf.float32, shape=(1)),\n",
    "#     \"n_songs_pl\": tf.io.FixedLenFeature(dtype=tf.float32, shape=(1)),\n",
    "#     \"num_artists_pl\": tf.io.FixedLenFeature(dtype=tf.float32, shape=(1)),\n",
    "#     \"num_albums_pl\": tf.io.FixedLenFeature(dtype=tf.float32, shape=(1)),\n",
    "#     \"description_pl\": tf.io.FixedLenFeature(dtype=tf.string, shape=(1)),\n",
    "\n",
    "#     # candidate features\n",
    "#     \"pos_can\": tf.io.FixedLenFeature(dtype=tf.int64, shape=(1)),\n",
    "#     \"artist_name_can\": tf.io.FixedLenFeature(dtype=tf.string, shape=(1)),\n",
    "#     \"track_uri_can\": tf.io.FixedLenFeature(dtype=tf.string, shape=(1)),\n",
    "#     \"artist_uri_can\": tf.io.FixedLenFeature(dtype=tf.string, shape=(1)),\n",
    "#     \"track_name_can\": tf.io.FixedLenFeature(dtype=tf.string, shape=(1)),\n",
    "#     \"album_uri_can\": tf.io.FixedLenFeature(dtype=tf.string, shape=(1)),\n",
    "#     \"duration_ms_can\": tf.io.FixedLenFeature(dtype=tf.float32, shape=(1)),\n",
    "#     \"album_name_can\": tf.io.FixedLenFeature(dtype=tf.string, shape=(1)),\n",
    "#     \"track_pop_can\": tf.io.FixedLenFeature(dtype=tf.float32, shape=(1)),\n",
    "#     \"artist_pop_can\": tf.io.FixedLenFeature(dtype=tf.float32, shape=(1)),\n",
    "#     \"artist_genres_can\": tf.io.FixedLenFeature(dtype=tf.float32, shape=(1)),\n",
    "#     \"artist_followers_can\": tf.io.FixedLenFeature(dtype=tf.float32, shape=(1)),\n",
    "\n",
    "#     # seed track features\n",
    "#     \"pos_seed_track\": tf.io.FixedLenFeature(dtype=tf.int64, shape=(1)),\n",
    "#     \"artist_name_seed_track\": tf.io.FixedLenFeature(dtype=tf.string, shape=(1)),\n",
    "#     \"artist_uri_seed_track\": tf.io.FixedLenFeature(dtype=tf.string, shape=(1)),\n",
    "#     \"track_name_seed_track\": tf.io.FixedLenFeature(dtype=tf.string, shape=(1)),\n",
    "#     \"track_uri_seed_track\": tf.io.FixedLenFeature(dtype=tf.string, shape=(1)),\n",
    "#     \"album_name_seed_track\": tf.io.FixedLenFeature(dtype=tf.string, shape=(1)),\n",
    "#     \"album_uri_seed_track\": tf.io.FixedLenFeature(dtype=tf.string, shape=(1)),\n",
    "#     \"duration_seed_track\": tf.io.FixedLenFeature(dtype=tf.float32, shape=(1)),\n",
    "#     \"track_pop_seed_track\": tf.io.FixedLenFeature(dtype=tf.float32, shape=(1)),\n",
    "#     \"artist_pop_seed_track\": tf.io.FixedLenFeature(dtype=tf.float32, shape=(1)),\n",
    "#     \"artist_genres_seed_track\": tf.io.FixedLenFeature(dtype=tf.string, shape=(1)),\n",
    "#     \"artist_followers_seed_track\": tf.io.FixedLenFeature(dtype=tf.float32, shape=(1)),\n",
    "# }\n",
    "\n",
    "# sequence_features_ = {\n",
    "#     \"pos_pl\": tf.io.RaggedFeature(tf.int64),\n",
    "#     \"artist_name_pl\": tf.io.RaggedFeature(tf.string),\n",
    "#     \"track_uri_pl\": tf.io.RaggedFeature(tf.string),\n",
    "#     \"track_name_pl\": tf.io.RaggedFeature(tf.string),\n",
    "#     \"duration_ms_songs_pl\": tf.io.RaggedFeature(tf.float32),\n",
    "#     \"album_name_pl\": tf.io.RaggedFeature(tf.string),\n",
    "#     \"artist_pop_pl\": tf.io.RaggedFeature(tf.float32),\n",
    "#     \"artists_followers_pl\": tf.io.RaggedFeature(tf.float32),\n",
    "#     \"track_pop_pl\": tf.io.RaggedFeature(tf.float32),\n",
    "#     \"artist_genres_pl\": tf.io.RaggedFeature(tf.string),\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611ba282-3a5d-4b01-8e0e-c543f7f2982e",
   "metadata": {},
   "source": [
    "#### References\n",
    "* [multi-head_and torso: Dugtrio code](https://source.corp.google.com/piper///depot/google3/learning/brain/models/recommendations/dugtrio/torso/multi_head_torso.py)\n",
    "* [CPU-optimized sparse implementation of attention](https://source.corp.google.com/piper///depot/google3/ads/ubaq/brain_embeddings/tensorflow/layers.py;l=1234;rcl=202730546?q=layers.py%20f:brain_embeddings&dr=)\n",
    "\n",
    "#### Notes\n",
    "* `tf.feature_column.sequence_categorical_column_with_identity` - \"Pass this to embedding_column or indicator_column to convert sequence categorical data into dense representation for input to sequence NN, such as RN\" [docs](https://www.tensorflow.org/api_docs/python/tf/feature_column/sequence_categorical_column_with_identity)\n",
    "* Implementing a sequential model with Two Tower [docs](https://www.tensorflow.org/recommenders/examples/sequential_retrieval#implementing_a_sequential_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9bdfd1-ee59-474e-9f2b-1f24a0ca4594",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-9.m93",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-9:m93"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
