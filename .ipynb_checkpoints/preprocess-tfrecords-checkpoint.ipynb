{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5297b397-a3c1-407d-b314-654d9dad67f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = 'hybrid-vertex'  # <--- TODO: CHANGE THIS\n",
    "LOCATION = 'us-central1' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72d74d51-c921-4583-be0d-550390eb1197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow-recommenders==0.6.0 --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be5d86b6-0ed5-4b59-a588-de65ea85ba0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "TF_GPU_THREAD_MODE='gpu_private'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1ad020a-1e43-44ea-a68b-dd1e2066aebb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-28 00:13:29.477868: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-09-28 00:13:30.120914: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38238 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:00:04.0, compute capability: 8.0\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import tensorflow as tf\n",
    "import tensorflow_recommenders as tfrs\n",
    "\n",
    "from google.cloud import storage\n",
    "\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "from pprint import pprint\n",
    "\n",
    "from two_tower_src import two_tower as tt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d15cd75-4a58-44ee-b7ef-7a942355d742",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create Dataset for local training and testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219fd48f-0f57-42ca-ab15-beb744d13f27",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Playlist dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c846eda-201f-47e5-aa79-305c93fc48eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files = []\n",
    "for blob in client.list_blobs(f'{train_dir}', prefix=f'{train_dir_prefix}', delimiter=\"/\"):\n",
    "    train_files.append(blob.public_url.replace(\"https://storage.googleapis.com/\", \"gs://\"))\n",
    "\n",
    "# OPTIMIZE DATA INPUT PIPELINE\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_files)\n",
    "train_dataset = train_dataset.interleave(\n",
    "    lambda x: tf.data.TFRecordDataset(x),\n",
    "    cycle_length=tf.data.AUTOTUNE, \n",
    "    num_parallel_calls=tf.data.AUTOTUNE,\n",
    "    deterministic=False\n",
    ").map(\n",
    "    tt.parse_tfrecord,\n",
    "    num_parallel_calls=tf.data.AUTOTUNE,\n",
    ").map(\n",
    "    tt.return_padded_tensors,\n",
    "    num_parallel_calls=tf.data.AUTOTUNE,)\n",
    "# ).batch(\n",
    "#     batch_size \n",
    "# ).prefetch(\n",
    "#     tf.data.AUTOTUNE,\n",
    "# )\n",
    "\n",
    "\n",
    "valid_files = []\n",
    "for blob in client.list_blobs(f'{valid_dir}', prefix=f'{valid_dir_prefix}', delimiter=\"/\"):\n",
    "    valid_files.append(blob.public_url.replace(\"https://storage.googleapis.com/\", \"gs://\"))\n",
    "\n",
    "\n",
    "# OPTIMIZE DATA INPUT PIPELINE\n",
    "valid_dataset = tf.data.Dataset.from_tensor_slices(valid_files)\n",
    "valid_dataset = valid_dataset.interleave(\n",
    "    lambda x: tf.data.TFRecordDataset(x),\n",
    "    cycle_length=tf.data.AUTOTUNE, \n",
    "    num_parallel_calls=tf.data.AUTOTUNE,\n",
    "    deterministic=False\n",
    ").map(\n",
    "    tt.parse_tfrecord,\n",
    "    num_parallel_calls=tf.data.AUTOTUNE,\n",
    ").map(\n",
    "    tt.return_padded_tensors,\n",
    "    num_parallel_calls=tf.data.AUTOTUNE,)\n",
    "# ).batch(\n",
    "#     batch_size\n",
    "# ).prefetch(\n",
    "#     tf.data.AUTOTUNE,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73a785dd-84e6-4c07-8045-bc9de9f68568",
   "metadata": {},
   "outputs": [],
   "source": [
    "from two_tower_src import preprocess as hash_data\n",
    "\n",
    "valid_hashed = valid_dataset.map(hash_data.pre_hash_records)\n",
    "train_hashed = train_dataset.map(hash_data.pre_hash_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d431ab3b-71e0-469b-988e-19837af30098",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_hashed = tt.parsed_candidate_dataset.map(lambda x: \n",
    "                                            hash_data.pre_hash_records(x, candidate_mode=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de97d30f-8b8d-4d26-9805-718beb23ac5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir_hash = 'gs://spotify-beam-v3/v3/train-hashed/'\n",
    "\n",
    "valid_dir_hash = 'gs://spotify-beam-v3/v3/valid-hashed/'\n",
    "\n",
    "candidate_dir_hash = 'gs://spotify-beam-v3/v3/candidates-hashed/'\n",
    "\n",
    "def custom_shard_func_train(element, n_shards=5000):\n",
    "    x = element['track_uri_can'] % n_shards\n",
    "    return(x)\n",
    "\n",
    "def custom_shard_func_valid(element,  n_shards=510) -> tf.int64:\n",
    "    x = tf.dtypes.cast(element['track_name_can'], tf.int64) % n_shards\n",
    "    return(x)\n",
    "\n",
    "def custom_shard_func_candidate(element,  n_shards=8) -> tf.int64:\n",
    "    x = tf.dtypes.cast(element['track_name_can'], tf.int64) % n_shards\n",
    "    return(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b7555b-0784-4c0b-bb3a-01b9464ac000",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.data.experimental.save(valid_hashed, valid_dir_hash, shard_func=custom_shard_func_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf1f742-822f-4e8c-b3e0-2f4e6ecdab15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.data.experimental.save(train_hashed, train_dir_hash, shard_func=custom_shard_func_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282836c3-c409-4f34-91ba-cc6b68667f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.data.experimental.save(candidate_hashed, candidate_dir_hash, shard_func=custom_shard_func_candidate)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m96",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m96"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
