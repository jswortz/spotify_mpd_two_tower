{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8757f6-8f36-4b0d-8c37-ba31a1773f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install google-cloud-aiplatform[prediction]>=1.16.0 fastapi nvtabular git+https://github.com/NVIDIA-Merlin/models.git --user"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201e92f9-980b-43e9-8c76-b2aa02260e96",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Buidling a custom Vertex AI endpoint for Merlin Query Tower\n",
    "\n",
    "**IMPORTANT** Make sure you are running this notebook in a DLVM (e.g. tensorflow enterprise 2.8) to build the image\n",
    "\n",
    "________\n",
    "**This will not work in the training container**\n",
    "________\n",
    "\n",
    "Your output should look like this - you are going to use the query model endpoint to create a custom container\n",
    "\n",
    "![](img/merlin-bucket.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9cbe8a06-ed15-40ac-bc2f-d387bb406301",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "\n",
    "PROJECT = 'hybrid-vertex'  # <--- TODO: CHANGE THIS\n",
    "REGION = 'us-central1' \n",
    "BUCKET = 'gs://spotify-beam-v3'\n",
    "REPOSITORY = 'merlin-spotify-cpr'\n",
    "ARTIFACT_URI = f'{BUCKET}/merlin-processed'\n",
    "MODEL_DIR = f'{BUCKET}/merlin-processed/query_model_merlin'\n",
    "PREFIX = 'merlin-spotify'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b65e26b-31d7-459e-8f80-92e2c206fc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !gsutil mb -l us-central1 gs://wortz-project-bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5075f2f7-c075-4e56-a188-4adfee578a2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !echo y | docker container prune\n",
    "# !echo y | docker image prune"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d045aab2-d485-4bf1-9319-b7afc5cccb34",
   "metadata": {},
   "source": [
    "### Set up repo and configure Docker (one-time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e72acda9-ae76-4ce9-8600-71238d545f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mERROR:\u001b[0m (gcloud.beta.artifacts.repositories.create) ALREADY_EXISTS: the repository already exists\n"
     ]
    }
   ],
   "source": [
    "# Create the repo if needed for the artifacts\n",
    "\n",
    "! gcloud beta artifacts repositories create {REPOSITORY} \\\n",
    "    --repository-format=docker \\\n",
    "    --location=$REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1858a130-739b-4752-bf02-d0e8356a0829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;33mWARNING:\u001b[0m Your config file at [/home/jupyter/.docker/config.json] contains these credential helper entries:\n",
      "\n",
      "{\n",
      "  \"credHelpers\": {\n",
      "    \"gcr.io\": \"gcloud\",\n",
      "    \"us.gcr.io\": \"gcloud\",\n",
      "    \"eu.gcr.io\": \"gcloud\",\n",
      "    \"asia.gcr.io\": \"gcloud\",\n",
      "    \"staging-k8s.gcr.io\": \"gcloud\",\n",
      "    \"marketplace.gcr.io\": \"gcloud\",\n",
      "    \"us-central1-docker.pkg.dev\": \"gcloud\"\n",
      "  }\n",
      "}\n",
      "Adding credentials for: us-central1-docker.pkg.dev\n",
      "gcloud credential helpers already registered correctly.\n"
     ]
    }
   ],
   "source": [
    "! gcloud auth configure-docker {REGION}-docker.pkg.dev --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7bf429ed-e23c-47ac-93da-8a70c4e3e92a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! rm -rf app\n",
    "! mkdir app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4af962a7-3196-4b9a-9c36-3f35fd525b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing app/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile app/requirements.txt\n",
    "uvicorn[standard]==0.15.0\n",
    "gunicorn==20.1.0\n",
    "fastapi==0.68.1\n",
    "google-cloud-aiplatform\n",
    "git+https://github.com/NVIDIA-Merlin/models.git\n",
    "nvtabular==1.3.3\n",
    "gcsfs\n",
    "google-cloud-storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d80c477-d403-4d39-9220-98d90511a180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing app/predictor.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app/predictor.py\n",
    "\n",
    "import nvtabular as nvt\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import merlin.models.tf as mm\n",
    "from nvtabular.loader.tf_utils import configure_tensorflow\n",
    "configure_tensorflow()\n",
    "import tensorflow as tf\n",
    "\n",
    "class Predictor():\n",
    "    \"\"\"Interface of the Predictor class for Custom Prediction Routines.\n",
    "    The Predictor is responsible for the ML logic for processing a prediction request.\n",
    "    Specifically, the Predictor must define:\n",
    "    (1) How to load all model artifacts used during prediction into memory.\n",
    "    (2) The logic that should be executed at predict time.\n",
    "    When using the default PredictionHandler, the Predictor will be invoked as follows:\n",
    "      predictor.postprocess(predictor.predict(predictor.preprocess(prediction_input)))\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        return\n",
    "    \n",
    "    def load(self, artifacts_uri):\n",
    "        \"\"\"Loads the model artifact.\n",
    "        Args:\n",
    "            artifacts_uri (str):\n",
    "                Required. The value of the environment variable AIP_STORAGE_URI.\n",
    "        \"\"\"\n",
    "        self.model = tf.keras.models.load_model(os.path.join(artifacts_uri, \"query_model_merlin\" ))\n",
    "        self.workflow = nvt.Workflow.load(os.path.join(artifacts_uri, \"workflow/2t-spotify-workflow\"))\n",
    "        self.workflow = self.workflow.remove_inputs(['track_pop_can', 'track_uri_can', 'duration_ms_can', \n",
    "                                      'track_name_can', 'artist_name_can','album_name_can',\n",
    "                                      'album_uri_can','artist_followers_can', 'artist_genres_can',\n",
    "                                      'artist_name_can', 'artist_pop_can','artist_pop_pl','artist_uri_can', \n",
    "                                      'artists_followers_pl'])  \n",
    "        return self\n",
    "        \n",
    "    def preprocess(self, prediction_input):\n",
    "        \"\"\"Preprocesses the prediction input before doing the prediction.\n",
    "        Args:\n",
    "            prediction_input (Any):\n",
    "                Required. The prediction input that needs to be preprocessed.\n",
    "        Returns:\n",
    "            The preprocessed prediction input.\n",
    "        \"\"\"\n",
    "        #handle different input types, can take a dict or list of dicts\n",
    "        TEST_INSTANCE = prediction_input\n",
    "        if type(TEST_INSTANCE) == list:\n",
    "            pandas_instance = pd.DataFrame.from_dict(TEST_INSTANCE[0], orient='index').T\n",
    "            if len(TEST_INSTANCE) > 1:\n",
    "                for ti in TEST_INSTANCE[0:]:\n",
    "                    pandas_instance = pandas_instance.append(pd.DataFrame.from_dict(ti, orient='index').T)\n",
    "        else:\n",
    "            pandas_instance = pd.DataFrame.from_dict(TEST_INSTANCE, orient='index').T\n",
    "        transformed_inputs = nvt.Dataset(pandas_instance)\n",
    "        transformed_instance = self.workflow.transform(transformed_inputs)\n",
    "        return transformed_instance\n",
    "\n",
    "    def predict(self, instances):\n",
    "        \"\"\"Performs prediction.\n",
    "        Args:\n",
    "            instances (Any):\n",
    "                Required. The instance(s) used for performing prediction.\n",
    "        Returns:\n",
    "            Prediction results.\n",
    "        \"\"\"  \n",
    "        \n",
    "        loader = mm.Loader(instances, batch_size=instances.num_rows, shuffle=False)\n",
    "        batch = next(iter(loader))\n",
    "        return self.model(batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b9a8771-cae3-494d-a56b-060616889463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing app/main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app/main.py\n",
    "from fastapi import FastAPI, Request\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from google.cloud import storage\n",
    "from predictor import Predictor\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "predictor_instance = Predictor()\n",
    "loaded_predictor = predictor_instance.load(artifacts_uri = os.environ['AIP_STORAGE_URI'])\n",
    "\n",
    "@app.get(os.environ['AIP_HEALTH_ROUTE'], status_code=200)\n",
    "def health():\n",
    "    return {}\n",
    "\n",
    "\n",
    "@app.post(os.environ['AIP_PREDICT_ROUTE'])\n",
    "async def predict(request: Request):\n",
    "    body = await request.json()\n",
    "\n",
    "    instances = body[\"instances\"]\n",
    "    # inputs = np.asarray(instances)\n",
    "    preprocessed_inputs = loaded_predictor.preprocess(instances)\n",
    "    outputs = loaded_predictor.predict(preprocessed_inputs)\n",
    "\n",
    "    return {\"predictions\": outputs.numpy().tolist()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2dfde697-cec3-470c-9c6d-3be275ce6b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing app/prestart.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile app/prestart.sh\n",
    "#!/bin/bash\n",
    "export PORT=$AIP_HTTP_PORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "486a3c72-75d0-45be-98e3-b8451ea7f8cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing app/instances.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile app/instances.json\n",
    "{\"instances\": {\"collaborative\": \"false\", \"album_name_pl\": [\"There's Really A Wolf\", \"Late Nights: The Album\", \"American Teen\", \"Crazy In Love\", \"Pony\"], \"album_uri_can\": \"spotify:album:5l83t3mbVgCrIe1VU9uJZR\", \"artist_followers_can\": 4339757.0, \"artist_genres_can\": \"'hawaiian hip hop', 'rap'\", \"artist_genres_pl\": [\"'hawaiian hip hop', 'rap'\", \"'chicago rap', 'dance pop', 'pop', 'pop rap', 'r&b', 'southern hip hop', 'trap', 'urban contemporary'\", \"'pop', 'pop r&b'\", \"'dance pop', 'pop', 'r&b'\", \"'chill r&b', 'pop', 'pop r&b', 'r&b', 'urban contemporary'\"], \"artist_name_can\": \"Russ\", \"artist_name_pl\": [\"Russ\", \"Jeremih\", \"Khalid\", \"Beyonc\\u00c3\\u00a9\", \"William Singe\"], \"artist_pop_can\": 82.0, \"artist_pop_pl\": [82.0, 80.0, 90.0, 87.0, 65.0], \"artist_uri_can\": \"spotify:artist:1z7b1Pr1rSlvWRzsW3HOrS\", \"artists_followers_pl\": [4339757.0, 5611842.0, 15046756.0, 30713126.0, 603837.0], \"description_pl\": \"\", \"duration_ms_can\": 237322.0, \"duration_ms_songs_pl\": [237506.0, 217200.0, 219080.0, 226400.0, 121739.0], \"n_songs_pl\": 8.0, \"name\": \"Lit Tunes \", \"num_albums_pl\": 8.0, \"num_artists_pl\": 8.0, \"track_name_can\": \"We Just Havent Met Yet\", \"track_name_pl\": [\"Losin Control\", \"Paradise\", \"Location\", \"Crazy In Love - Remix\", \"Pony\"], \"track_pop_can\": 57.0, \"track_pop_pl\": [79.0, 58.0, 83.0, 71.0, 57.0], \"duration_ms_seed_pl\": 51023.1, \"pid\": 1, \"track_uri_can\": \"spotify:track:0VzDv4wiuZsLsNOmfaUy2W\", \"track_uri_pl\": [\"spotify:track:4cxMGhkinTocPSVVKWIw0d\", \"spotify:track:1wNEBPo3nsbGCZRryI832I\", \"spotify:track:152lZdxL1OR0ZMW6KquMif\", \"spotify:track:2f4IuijXLxYOeBncS60GUD\", \"spotify:track:4Lj8paMFwyKTGfILLELVxt\"]}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "970ed68e-37f6-406c-9615-3aab9570daeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make it a package\n",
    "!touch app/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0cb65bef-d269-4f80-a40b-e0b43f844c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile Dockerfile\n",
    "\n",
    "FROM nvcr.io/nvidia/merlin/merlin-tensorflow:22.09\n",
    "WORKDIR /app \n",
    "\n",
    "COPY ./app/requirements.txt /requirements.txt\n",
    "RUN pip install -r /requirements.txt\n",
    "\n",
    "COPY ./app /app\n",
    "EXPOSE 80\n",
    "    \n",
    "CMD [\"sh\", \"-c\", \"uvicorn main:app --host 0.0.0.0 --port $AIP_HTTP_PORT\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "22af2064-0460-4dd5-b352-180f33fbdd1c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  4.442GB\n",
      "Step 1/7 : FROM nvcr.io/nvidia/merlin/merlin-tensorflow:22.09\n",
      " ---> ec90adb8185e\n",
      "Step 2/7 : WORKDIR /app\n",
      " ---> Using cache\n",
      " ---> c54a8d620b68\n",
      "Step 3/7 : COPY ./app/requirements.txt /requirements.txt\n",
      " ---> Using cache\n",
      " ---> 89b93f7c10dd\n",
      "Step 4/7 : RUN pip install -r /requirements.txt\n",
      " ---> Using cache\n",
      " ---> 0d03b023e3a3\n",
      "Step 5/7 : COPY ./app /app\n",
      " ---> 267ef97b42f2\n",
      "Step 6/7 : EXPOSE 80\n",
      " ---> Running in 2f2dcd615a29\n",
      "Removing intermediate container 2f2dcd615a29\n",
      " ---> 195e9e1578e5\n",
      "Step 7/7 : CMD [\"sh\", \"-c\", \"uvicorn main:app --host 0.0.0.0 --port $AIP_HTTP_PORT\"]\n",
      " ---> Running in 0e29f8a9de1b\n",
      "Removing intermediate container 0e29f8a9de1b\n",
      " ---> 754e889372cb\n",
      "Successfully built 754e889372cb\n",
      "Successfully tagged us-central1-docker.pkg.dev/hybrid-vertex/merlin-spotify-cpr/merlin-prediction-cpr:latest\n"
     ]
    }
   ],
   "source": [
    "SERVER_IMAGE = \"merlin-prediction-cpr\"  # @param {type:\"string\"} \n",
    "REMOTE_IMAGE_NAME=f\"{REGION}-docker.pkg.dev/{PROJECT}/{REPOSITORY}/{SERVER_IMAGE}\"\n",
    "\n",
    "!docker build -t $REMOTE_IMAGE_NAME ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3db156d-4675-43cc-ac8f-305d5fc79230",
   "metadata": {},
   "source": [
    "#### If you are debugging, be sure to set `-d` detached flag off and run the commands in console"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4b2c6d-add5-4753-9069-8499b09ba5c6",
   "metadata": {},
   "source": [
    "### Copy/paste if you want to run from console for testing\n",
    "```python\n",
    "docker run --gpus all -p 80:8080 \\\n",
    "            --name=merlin-prediction-cpr \\\n",
    "            -e AIP_HTTP_PORT=8080 \\\n",
    "            -e AIP_HEALTH_ROUTE=/health \\\n",
    "            -e AIP_PREDICT_ROUTE=/predict \\\n",
    "            -e AIP_STORAGE_URI=gs://spotify-beam-v3/merlin-processed \\\n",
    "            us-central1-docker.pkg.dev/hybrid-vertex/merlin-spotify-cpr/merlin-prediction-cpr\n",
    "```\n",
    "\n",
    "##### No GPU:\n",
    "```python\n",
    "docker run -p 80:8080 \\\n",
    "            --name=merlin-prediction-cpr \\\n",
    "            -e AIP_HTTP_PORT=8080 \\\n",
    "            -e AIP_HEALTH_ROUTE=/health \\\n",
    "            -e AIP_PREDICT_ROUTE=/predict \\\n",
    "            -e AIP_STORAGE_URI=gs://spotify-beam-v3/merlin-processed \\\n",
    "            us-central1-docker.pkg.dev/hybrid-vertex/merlin-spotify-cpr/merlin-prediction-cpr\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b072387-50e8-4a24-b61b-7c375534fb4a",
   "metadata": {},
   "source": [
    "## Stop the images if they are running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e52cf577-dcb3-4534-87a1-5acb460d9302",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merlin-prediction-cpr\n",
      "merlin-prediction-cpr\n"
     ]
    }
   ],
   "source": [
    "! docker stop $SERVER_IMAGE\n",
    "! docker rm $SERVER_IMAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ae9f6a29-7e69-4bf6-bb22-926578ff1181",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}"
     ]
    }
   ],
   "source": [
    "! curl localhost/health"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fae134d5-92da-48b5-afdb-7a330c88ace0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ground truth candidate:\n",
    "    # 'album_uri_can': 'spotify:album:5l83t3mbVgCrIe1VU9uJZR', \n",
    "    # 'artist_name_can': 'Russ', \n",
    "    # 'track_name_can': 'We Just Havent Met Yet', \n",
    "## TODO - we have to overload with candidate data because of the workflow transform, add overloaded values in the predictor\n",
    "TEST_INSTANCE = {'collaborative': 'false',\n",
    "                 'album_name_pl': [\"There's Really A Wolf\", 'Late Nights: The Album',\n",
    "                       'American Teen', 'Crazy In Love', 'Pony'], \n",
    "                 'artist_genres_pl': [\"'hawaiian hip hop', 'rap'\",\n",
    "                       \"'chicago rap', 'dance pop', 'pop', 'pop rap', 'r&b', 'southern hip hop', 'trap', 'urban contemporary'\",\n",
    "                       \"'pop', 'pop r&b'\", \"'dance pop', 'pop', 'r&b'\",\n",
    "                       \"'chill r&b', 'pop', 'pop r&b', 'r&b', 'urban contemporary'\"], \n",
    "                 'artist_name_pl': ['Russ', 'Jeremih', 'Khalid', 'Beyonc\\xc3\\xa9',\n",
    "                       'William Singe'], \n",
    "                 'artist_pop_can': 82.0, \n",
    "                 'description_pl': '', \n",
    "                 'duration_ms_songs_pl': [237506., 217200., 219080., 226400., 121739.], \n",
    "                 'n_songs_pl': 8.0, \n",
    "                 'name': 'Lit Tunes ', \n",
    "                 'num_albums_pl': 8.0, \n",
    "                 'num_artists_pl': 8.0, \n",
    "                 'track_name_pl': ['Losin Control', 'Paradise', 'Location',\n",
    "                       'Crazy In Love - Remix', 'Pony'], \n",
    "                 'track_pop_pl': [79., 58., 83., 71., 57.],\n",
    "                 'duration_ms_seed_pl': 51023.1,\n",
    "                 'pid': 1,\n",
    "                 'track_uri_pl': ['spotify:track:4cxMGhkinTocPSVVKWIw0d',\n",
    "                       'spotify:track:1wNEBPo3nsbGCZRryI832I',\n",
    "                       'spotify:track:152lZdxL1OR0ZMW6KquMif',\n",
    "                       'spotify:track:2f4IuijXLxYOeBncS60GUD',\n",
    "                       'spotify:track:4Lj8paMFwyKTGfILLELVxt']\n",
    "                     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "acde41b9-5025-4d57-80a9-b9f0213c2e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"instances\": {\"collaborative\": \"false\", \"album_name_pl\": [\"There's Really A Wolf\", \"Late Nights: The Album\", \"American Teen\", \"Crazy In Love\", \"Pony\"], \"artist_genres_pl\": [\"'hawaiian hip hop', 'rap'\", \"'chicago rap', 'dance pop', 'pop', 'pop rap', 'r&b', 'southern hip hop', 'trap', 'urban contemporary'\", \"'pop', 'pop r&b'\", \"'dance pop', 'pop', 'r&b'\", \"'chill r&b', 'pop', 'pop r&b', 'r&b', 'urban contemporary'\"], \"artist_name_pl\": [\"Russ\", \"Jeremih\", \"Khalid\", \"Beyonc\\u00c3\\u00a9\", \"William Singe\"], \"artist_pop_can\": 82.0, \"description_pl\": \"\", \"duration_ms_songs_pl\": [237506.0, 217200.0, 219080.0, 226400.0, 121739.0], \"n_songs_pl\": 8.0, \"name\": \"Lit Tunes \", \"num_albums_pl\": 8.0, \"num_artists_pl\": 8.0, \"track_name_pl\": [\"Losin Control\", \"Paradise\", \"Location\", \"Crazy In Love - Remix\", \"Pony\"], \"track_pop_pl\": [79.0, 58.0, 83.0, 71.0, 57.0], \"duration_ms_seed_pl\": 51023.1, \"pid\": 1, \"track_uri_pl\": [\"spotify:track:4cxMGhkinTocPSVVKWIw0d\", \"spotify:track:1wNEBPo3nsbGCZRryI832I\", \"spotify:track:152lZdxL1OR0ZMW6KquMif\", \"spotify:track:2f4IuijXLxYOeBncS60GUD\", \"spotify:track:4Lj8paMFwyKTGfILLELVxt\"]}}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "json_instance = json.dumps({\"instances\": TEST_INSTANCE})\n",
    "print(json_instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5c9659cb-5e93-4ecf-8553-9058934e090e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting instances.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile instances.json\n",
    "{\"instances\": {\"collaborative\": \"false\", \"album_name_pl\": [\"There's Really A Wolf\", \"Late Nights: The Album\", \"American Teen\", \"Crazy In Love\", \"Pony\"], \"artist_genres_pl\": [\"'hawaiian hip hop', 'rap'\", \"'chicago rap', 'dance pop', 'pop', 'pop rap', 'r&b', 'southern hip hop', 'trap', 'urban contemporary'\", \"'pop', 'pop r&b'\", \"'dance pop', 'pop', 'r&b'\", \"'chill r&b', 'pop', 'pop r&b', 'r&b', 'urban contemporary'\"], \"artist_name_pl\": [\"Russ\", \"Jeremih\", \"Khalid\", \"Beyonc\\u00c3\\u00a9\", \"William Singe\"], \"artist_pop_can\": 82.0, \"description_pl\": \"\", \"duration_ms_songs_pl\": [237506.0, 217200.0, 219080.0, 226400.0, 121739.0], \"n_songs_pl\": 8.0, \"name\": \"Lit Tunes \", \"num_albums_pl\": 8.0, \"num_artists_pl\": 8.0, \"track_name_pl\": [\"Losin Control\", \"Paradise\", \"Location\", \"Crazy In Love - Remix\", \"Pony\"], \"track_pop_pl\": [79.0, 58.0, 83.0, 71.0, 57.0], \"duration_ms_seed_pl\": 51023.1, \"pid\": 1, \"track_uri_pl\": [\"spotify:track:4cxMGhkinTocPSVVKWIw0d\", \"spotify:track:1wNEBPo3nsbGCZRryI832I\", \"spotify:track:152lZdxL1OR0ZMW6KquMif\", \"spotify:track:2f4IuijXLxYOeBncS60GUD\", \"spotify:track:4Lj8paMFwyKTGfILLELVxt\"]}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "18ca1d02-548d-4d5d-a09e-9a5a6ddf7f45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"predictions\":[[0.5919173359870911,1.1092438697814941,0.0,0.5321710109710693,0.0,0.0,0.0,0.7351390719413757,0.0,0.4420093595981598,0.0,0.09786917269229889,0.11914101243019104,0.0,0.0,0.0,0.0,0.0,2.2194886207580566,0.0,0.3245813846588135,1.4900052547454834,0.0,1.1713331937789917,0.867906391620636,0.0,1.010358214378357,1.0915064811706543,0.6065486669540405,0.0,1.2675611972808838,0.0,0.0,0.0,0.0,3.3520569801330566,0.0,0.0,0.0,0.9523360133171082,0.7877380847930908,0.06262616068124771,0.08143562078475952,0.8745517134666443,2.8739030361175537,1.5691848993301392,0.6309142112731934,0.5168806910514832,0.0,0.7615355849266052,1.1100037097930908,0.20900781452655792,0.0,1.148346185684204,0.0,0.0,0.0,0.9303957223892212,0.0,0.0,0.2855423092842102,0.26085713505744934,0.0,0.45607706904411316,0.7987809181213379,1.007262110710144,0.0,1.4150218963623047,0.11576466262340546,0.0,3.5614173412323,0.0,0.0,1.4880270957946777,0.0,0.0,0.0,0.0,0.0,0.0,1.0274288654327393,0.0,1.6470527648925781,0.0,1.2647912502288818,0.45633432269096375,0.2748670279979706,0.0,0.0,1.1476340293884277,0.0,0.0,3.0344247817993164,0.9904715418815613,0.45293113589286804,3.8916711807250977,3.4132940769195557,0.9972118735313416,0.6069631576538086,0.8225669860839844,0.0,0.0,1.046805500984192,0.0,0.8828969597816467,0.0,0.0,1.6712911128997803,0.0,0.0,0.0,0.9217772483825684,0.0,0.6888653039932251,0.0,0.2225726991891861,1.146008014678955,0.5971589684486389,0.0,0.0,0.0964106097817421,1.1994199752807617,0.3218831419944763,0.0,1.552603840827942,0.0,1.127319097518921,0.0]]}"
     ]
    }
   ],
   "source": [
    "! curl -X POST \\\n",
    "      -d @instances.json \\\n",
    "      -H \"Content-Type: application/json; charset=utf-8\" \\\n",
    "      localhost/predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd45707-2c87-4148-bf69-25feb40a74a3",
   "metadata": {},
   "source": [
    "### Push the container once ready and testing is complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b9cf27ed-0c42-4cee-8116-44962bb57c13",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default tag: latest\n",
      "The push refers to repository [us-central1-docker.pkg.dev/hybrid-vertex/merlin-spotify-cpr/merlin-prediction-cpr]\n",
      "\n",
      "\u001b[1B82f5b7f4: Preparing \n",
      "\u001b[1Bc5dd757b: Preparing \n",
      "\u001b[1B98822505: Preparing \n",
      "\u001b[1Bd63dfea4: Preparing \n",
      "\u001b[1B82180673: Preparing \n",
      "\u001b[1B1d0a83a8: Preparing \n",
      "\u001b[1B70192997: Preparing \n",
      "\u001b[1Bc214ee0d: Preparing \n",
      "\u001b[1Becdb2469: Preparing \n",
      "\u001b[1B97d6ba5c: Preparing \n",
      "\u001b[1Bca2c28f6: Preparing \n",
      "\u001b[1B011a84bf: Preparing \n",
      "\u001b[1B76e335df: Preparing \n",
      "\u001b[1B6d0a11d6: Preparing \n",
      "\u001b[1B7d03c0bd: Preparing \n",
      "\u001b[1B2abf19ca: Preparing \n",
      "\u001b[1Bb185c3d4: Preparing \n",
      "\u001b[1Bdb8c2ade: Preparing \n",
      "\u001b[1Ba6f2f042: Preparing \n",
      "\u001b[1B95a49008: Preparing \n",
      "\u001b[1B2022891e: Preparing \n",
      "\u001b[1Beaa89a94: Preparing \n",
      "\u001b[1B1144ae49: Preparing \n",
      "\u001b[1Bafc73435: Preparing \n",
      "\u001b[1B3bad788b: Preparing \n",
      "\u001b[1Ba478afcb: Preparing \n",
      "\u001b[1B6441ec4f: Preparing \n",
      "\u001b[1B23294fa4: Preparing \n",
      "\u001b[1B1a1e4253: Preparing \n",
      "\u001b[1Bc346ccfc: Preparing \n",
      "\u001b[1Bfee3746e: Preparing \n",
      "\u001b[1B803f9277: Preparing \n",
      "\u001b[1Ba37e5c8e: Preparing \n",
      "\u001b[1B62840d41: Preparing \n",
      "\u001b[1B29e87f77: Preparing \n",
      "\u001b[1B1e712f2d: Preparing \n",
      "\u001b[1Bd99fe986: Preparing \n",
      "\u001b[1B53ee8e4e: Preparing \n",
      "\u001b[1Bca691741: Preparing \n",
      "\u001b[1B36bb7f17: Preparing \n",
      "\u001b[1B9045d0b0: Preparing \n",
      "\u001b[1B79425552: Preparing \n",
      "\u001b[1B8c7f7aca: Preparing \n",
      "\u001b[1Ba62a5147: Preparing \n",
      "\u001b[1Bed0a4582: Preparing \n",
      "\u001b[1Bda39f322: Preparing \n",
      "\u001b[1Be7e6e9c6: Preparing \n",
      "\u001b[1B42e6c79e: Preparing \n",
      "\u001b[1B61533862: Preparing \n",
      "\u001b[1B15257c6c: Preparing \n",
      "\u001b[46Bd0a83a8: Waiting g \n",
      "\u001b[36B185c3d4: Waiting g \n",
      "\u001b[47B0192997: Waiting g \n",
      "\u001b[47B214ee0d: Waiting g \n",
      "\u001b[47Bcdb2469: Waiting g \n",
      "\u001b[1Bed5b61b3: Preparing \n",
      "\u001b[48B7d6ba5c: Waiting g \n",
      "\u001b[38B022891e: Waiting g \n",
      "\u001b[49Ba2c28f6: Waiting g \n",
      "\u001b[39Baa89a94: Waiting g \n",
      "\u001b[50B11a84bf: Waiting g \n",
      "\u001b[40B144ae49: Waiting g \n",
      "\u001b[51B6e335df: Waiting g \n",
      "\u001b[39B478afcb: Waiting g \n",
      "\u001b[51Bd03c0bd: Waiting g \n",
      "\u001b[51Babf19ca: Waiting g \n",
      "\u001b[1Bbfae03e4: Preparing \n",
      "\u001b[1Bbf18a086: Preparing \n",
      "\u001b[42B3294fa4: Waiting g \n",
      "\u001b[1B2804b89d: Preparing \n",
      "\u001b[37B9e87f77: Waiting g \n",
      "\u001b[44Ba1e4253: Waiting g \n",
      "\u001b[38Be712f2d: Waiting g \n",
      "\u001b[45B346ccfc: Waiting g \n",
      "\u001b[39B99fe986: Waiting g \n",
      "\u001b[1Bf3b3096a: Preparing \n",
      "\u001b[47Bee3746e: Waiting g \n",
      "\u001b[41B3ee8e4e: Waiting g \n",
      "\u001b[1B74ab1503: Preparing \n",
      "\u001b[42Ba691741: Waiting g \n",
      "\u001b[80B5dd757b: Pushed   104.5MB/103MBkB8A\u001b[2K\u001b[75A\u001b[2K\u001b[81A\u001b[2K\u001b[72A\u001b[2K\u001b[69A\u001b[2K\u001b[68A\u001b[2K\u001b[65A\u001b[2K\u001b[63A\u001b[2K\u001b[58A\u001b[2K\u001b[54A\u001b[2K\u001b[50A\u001b[2K\u001b[46A\u001b[2K\u001b[40A\u001b[2K\u001b[36A\u001b[2K\u001b[30A\u001b[2K\u001b[26A\u001b[2K\u001b[22A\u001b[2K\u001b[16A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[80A\u001b[2K\u001b[80A\u001b[2K\u001b[80A\u001b[2K\u001b[80A\u001b[2K\u001b[80A\u001b[2K\u001b[80A\u001b[2K\u001b[80A\u001b[2K\u001b[80A\u001b[2K\u001b[80A\u001b[2K\u001b[80A\u001b[2K\u001b[80A\u001b[2K\u001b[80A\u001b[2K\u001b[80A\u001b[2K\u001b[80A\u001b[2K\u001b[80A\u001b[2K\u001b[80A\u001b[2K\u001b[80A\u001b[2K\u001b[80A\u001b[2K\u001b[80A\u001b[2K\u001b[80A\u001b[2K\u001b[80A\u001b[2K\u001b[80A\u001b[2K\u001b[80A\u001b[2K\u001b[80A\u001b[2K\u001b[80A\u001b[2K\u001b[80A\u001b[2K\u001b[80A\u001b[2K\u001b[80A\u001b[2K\u001b[80A\u001b[2K\u001b[80A\u001b[2K\u001b[80A\u001b[2K\u001b[80A\u001b[2K\u001b[80A\u001b[2K\u001b[80A\u001b[2K\u001b[80A\u001b[2Klatest: digest: sha256:dfd16e665ac477340c394fc313e76f397d157113e08ced85b237590ef5a10994 size: 17359\n"
     ]
    }
   ],
   "source": [
    "# ### push the container to registry\n",
    "!docker push $REMOTE_IMAGE_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a9a707-6965-4b6f-a3fe-7312dfddbf24",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Deploy to Vertex AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b166b499-b407-4e3e-99f3-da3f3031a147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Model\n",
      "Create Model backing LRO: projects/934903580331/locations/us-central1/models/269718998386475008/operations/7127246920585576448\n",
      "Model created. Resource name: projects/934903580331/locations/us-central1/models/269718998386475008@1\n",
      "To use this Model in another session:\n",
      "model = aiplatform.Model('projects/934903580331/locations/us-central1/models/269718998386475008@1')\n"
     ]
    }
   ],
   "source": [
    "MODEL_DISPLAY_NAME = \"Merlin Spotify Query Tower Model\"\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "model = aiplatform.Model.upload(\n",
    "        display_name=MODEL_DISPLAY_NAME,\n",
    "        artifact_uri=ARTIFACT_URI,\n",
    "        serving_container_image_uri=REMOTE_IMAGE_NAME,\n",
    "        serving_container_predict_route='/predict',\n",
    "        serving_container_health_route='/health',\n",
    "        # serving_container_command=[\"sh\", \"-c\", \"uvicorn main:app --host 0.0.0.0 --port $AIP_HTTP_PORT\"],\n",
    "        serving_container_args='--gpus all',\n",
    "        sync=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9e80231d-0430-478e-989a-4802adca17ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Endpoint\n",
      "Create Endpoint backing LRO: projects/934903580331/locations/us-central1/endpoints/1625663126038904832/operations/4492641138573836288\n",
      "Endpoint created. Resource name: projects/934903580331/locations/us-central1/endpoints/1625663126038904832\n",
      "To use this Endpoint in another session:\n",
      "endpoint = aiplatform.Endpoint('projects/934903580331/locations/us-central1/endpoints/1625663126038904832')\n",
      "Deploying model to Endpoint : projects/934903580331/locations/us-central1/endpoints/1625663126038904832\n",
      "Deploy Endpoint model backing LRO: projects/934903580331/locations/us-central1/endpoints/1625663126038904832/operations/8492963507585679360\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4736/2705951153.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mendpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeploy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmachine_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"n1-standard-4\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/google/cloud/aiplatform/models.py\u001b[0m in \u001b[0;36mdeploy\u001b[0;34m(self, endpoint, deployed_model_display_name, traffic_percentage, traffic_split, machine_type, min_replica_count, max_replica_count, accelerator_type, accelerator_count, service_account, explanation_metadata, explanation_parameters, metadata, encryption_spec_key_name, network, sync, deploy_request_timeout, autoscaling_target_cpu_utilization, autoscaling_target_accelerator_duty_cycle)\u001b[0m\n\u001b[1;32m   3313\u001b[0m             \u001b[0mdeploy_request_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdeploy_request_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3314\u001b[0m             \u001b[0mautoscaling_target_cpu_utilization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mautoscaling_target_cpu_utilization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3315\u001b[0;31m             \u001b[0mautoscaling_target_accelerator_duty_cycle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mautoscaling_target_accelerator_duty_cycle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3316\u001b[0m         )\n\u001b[1;32m   3317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/google/cloud/aiplatform/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    805\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m                     \u001b[0mVertexAiResourceNounWithFutureManager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0;31m# callbacks to call within the Future (in same Thread)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/google/cloud/aiplatform/models.py\u001b[0m in \u001b[0;36m_deploy\u001b[0;34m(self, endpoint, deployed_model_display_name, traffic_percentage, traffic_split, machine_type, min_replica_count, max_replica_count, accelerator_type, accelerator_count, service_account, explanation_metadata, explanation_parameters, metadata, encryption_spec_key_name, network, sync, deploy_request_timeout, autoscaling_target_cpu_utilization, autoscaling_target_accelerator_duty_cycle)\u001b[0m\n\u001b[1;32m   3488\u001b[0m             \u001b[0mdeploy_request_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdeploy_request_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3489\u001b[0m             \u001b[0mautoscaling_target_cpu_utilization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mautoscaling_target_cpu_utilization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3490\u001b[0;31m             \u001b[0mautoscaling_target_accelerator_duty_cycle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mautoscaling_target_accelerator_duty_cycle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3491\u001b[0m         )\n\u001b[1;32m   3492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/google/cloud/aiplatform/models.py\u001b[0m in \u001b[0;36m_deploy_call\u001b[0;34m(cls, api_client, endpoint_resource_name, model, endpoint_resource_traffic_split, network, deployed_model_display_name, traffic_percentage, traffic_split, machine_type, min_replica_count, max_replica_count, accelerator_type, accelerator_count, service_account, explanation_metadata, explanation_parameters, metadata, deploy_request_timeout, autoscaling_target_cpu_utilization, autoscaling_target_accelerator_duty_cycle)\u001b[0m\n\u001b[1;32m   1239\u001b[0m         )\n\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1241\u001b[0;31m         \u001b[0moperation_future\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1243\u001b[0m     def undeploy(\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/google/api_core/future/polling.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout, retry)\u001b[0m\n\u001b[1;32m    130\u001b[0m         \"\"\"\n\u001b[1;32m    131\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mretry\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mDEFAULT_RETRY\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"retry\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mretry\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_blocking_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/google/api_core/future/polling.py\u001b[0m in \u001b[0;36m_blocking_poll\u001b[0;34m(self, timeout, retry)\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mretry\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mDEFAULT_RETRY\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"retry\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mretry\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m             \u001b[0mretry_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_done_or_raise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRetryError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             raise concurrent.futures.TimeoutError(\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/google/api_core/retry.py\u001b[0m in \u001b[0;36mretry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    286\u001b[0m                 \u001b[0msleep_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_deadline\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m                 \u001b[0mon_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mon_error\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m             )\n\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/google/api_core/retry.py\u001b[0m in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, deadline, on_error)\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0;34m\"Retrying due to {}, sleeping {:.1f}s ...\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_exc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msleep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         )\n\u001b[0;32m--> 218\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Sleep generator stopped yielding sleep values.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "endpoint = model.deploy(machine_type=\"n1-standard-4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b8fa58-c587-4b5d-9958-8ed904e0c162",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint.predict(instances=[TEST_INSTANCE])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d761b027-df5e-4f18-95f7-6d6714882b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498737fe-97f9-417d-92f0-e7cef34331e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m98",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m98"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
