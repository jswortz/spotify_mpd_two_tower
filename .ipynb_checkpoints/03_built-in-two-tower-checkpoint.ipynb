{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": [
    "# Copyright 2022 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPoU8Sm5E6e"
   },
   "source": [
    "<table align=\"left\">\n",
    "\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/master/notebooks/official/matching_engine/two-tower-model-introduction.ipynb\"\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/tree/master/notebooks/official/matching_engine/two-tower-model-introduction.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "## Overview\n",
    "\n",
    "This tutorial demonstrates how to use the Two-Tower built-in algorithm on the Vertex AI platform.\n",
    "\n",
    "Two-tower models learn to represent two items of various types (such as user profiles, search queries, web documents, answer passages, or images) in the same vector space, so that similar or related items are close to each other. These two items are referred to as the query and candidate object, since when paired with a nearest neighbor search service such as Vertex Matching Engine, the two-tower model can retrieve candidate objects related to an input query object. These objects are encoded by a query and candidate encoder (the two \"towers\") respectively, which are trained on pairs of relevant items. This built-in algorithm exports trained query and candidate encoders as model artifacts, which can be deployed in Vertex Prediction for usage in a recommendation system.\n",
    "\n",
    "### Dataset\n",
    "\n",
    "This tutorial uses the `movielens_100k sample dataset` in the public bucket `gs://cloud-samples-data/vertex-ai/matching-engine/two-tower`, which was generated from the [MovieLens movie rating dataset](https://grouplens.org/datasets/movielens/100k/). For simplicity, the data for this tutorial only includes the user id feature for users, and the movie id and movie title features for movies. In this example, the user is the query object and the movie is the candidate object, and each training example in the dataset contains a user and a movie they rated (we only include positive ratings in the dataset). The two-tower model will embed the user and the movie in the same embedding space, so that given a user, the model will recommend movies it thinks the user will like.\n",
    "\n",
    "### Objective\n",
    "\n",
    "In this notebook, you will learn how to run the two-tower model.\n",
    "The tutorial covers the following steps:\n",
    "1. **Setup**: Importing the required libraries and setting your global variables.\n",
    "2. **Configure parameters**: Setting the appropriate parameter values for the training job.\n",
    "3. **Train on Vertex Training**: Submitting a training job.\n",
    "4. **Deploy on Vertex Prediction**: Importing and deploying the trained model to a callable endpoint.\n",
    "5. **Predict**: Calling the deployed endpoint using online or batch prediction.\n",
    "6. **Hyperparameter tuning**: Running a hyperparameter tuning job.\n",
    "7. **Cleaning up**: Deleting resources created by this tutorial.\n",
    "\n",
    "\n",
    "### Costs \n",
    "\n",
    "\n",
    "This tutorial uses billable components of Google Cloud:\n",
    "\n",
    "* Vertex AI\n",
    "* Cloud Storage\n",
    "\n",
    "\n",
    "Learn about [Vertex AI\n",
    "pricing](https://cloud.google.com/vertex-ai/pricing) and [Cloud Storage\n",
    "pricing](https://cloud.google.com/storage/pricing), and use the [Pricing\n",
    "Calculator](https://cloud.google.com/products/calculator/)\n",
    "to generate a cost estimate based on your projected usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ze4-nDLfK4pw"
   },
   "source": [
    "### Set up your local development environment\n",
    "\n",
    "**If you are using Colab or Google Cloud Notebooks**, your environment already meets\n",
    "all the requirements to run this notebook. You can skip this step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gCuSR8GkAgzl"
   },
   "source": [
    "**Otherwise**, make sure your environment meets this notebook's requirements.\n",
    "You need the following:\n",
    "\n",
    "* The Google Cloud SDK\n",
    "* Git\n",
    "* Python 3\n",
    "* virtualenv\n",
    "* Jupyter notebook running in a virtual environment with Python 3\n",
    "\n",
    "The Google Cloud guide to [Setting up a Python development\n",
    "environment](https://cloud.google.com/python/setup) and the [Jupyter\n",
    "installation guide](https://jupyter.org/install) provide detailed instructions\n",
    "for meeting these requirements. The following steps provide a condensed set of\n",
    "instructions:\n",
    "\n",
    "1. [Install and initialize the Cloud SDK.](https://cloud.google.com/sdk/docs/)\n",
    "\n",
    "1. [Install Python 3.](https://cloud.google.com/python/setup#installing_python)\n",
    "\n",
    "1. [Install\n",
    "   virtualenv](https://cloud.google.com/python/setup#installing_and_using_virtualenv)\n",
    "   and create a virtual environment that uses Python 3. Activate the virtual environment.\n",
    "\n",
    "1. To install Jupyter, run `pip3 install jupyter` on the\n",
    "command-line in a terminal shell.\n",
    "\n",
    "1. To launch Jupyter, run `jupyter notebook` on the command-line in a terminal shell.\n",
    "\n",
    "1. Open this notebook in the Jupyter Notebook Dashboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7EUnXsZhAGF"
   },
   "source": [
    "### Install additional packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2b4ef9b72d43"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# The Google Cloud Notebook product has specific requirements\n",
    "IS_GOOGLE_CLOUD_NOTEBOOK = os.path.exists(\"/opt/deeplearning/metadata/env_version\")\n",
    "\n",
    "# Google Cloud Notebook requires dependencies to be installed with '--user'\n",
    "USER_FLAG = \"\"\n",
    "if IS_GOOGLE_CLOUD_NOTEBOOK:\n",
    "    USER_FLAG = \"--user\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wyy5Lbnzg5fi"
   },
   "outputs": [],
   "source": [
    "! pip3 install {USER_FLAG} --upgrade tensorflow\n",
    "! pip3 install {USER_FLAG} --upgrade google-cloud-aiplatform tensorboard-plugin-profile\n",
    "! gcloud components update --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hhq5zEbGg0XX"
   },
   "source": [
    "### Restart the kernel\n",
    "\n",
    "After you install the additional packages, you need to restart the notebook kernel so it can find the packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EzrelQZ22IZj"
   },
   "outputs": [],
   "source": [
    "# Automatically restart kernel after installs\n",
    "import os\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lWEdiXsJg0XY"
   },
   "source": [
    "## Before you begin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BF1j6f9HApxa"
   },
   "source": [
    "### Set up your Google Cloud project\n",
    "\n",
    "**The following steps are required, regardless of your notebook environment.**\n",
    "\n",
    "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
    "\n",
    "1. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
    "\n",
    "1. [Enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com).\n",
    "\n",
    "1. If you are running this notebook locally, you will need to install the [Cloud SDK](https://cloud.google.com/sdk).\n",
    "\n",
    "1. Enter your project ID in the cell below. Then run the cell to make sure the\n",
    "Cloud SDK uses the right project for all the commands in this notebook.\n",
    "\n",
    "**Note**: Jupyter runs lines prefixed with `!` as shell commands, and it interpolates Python variables prefixed with `$` into these commands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WReHDGG5g0XY"
   },
   "source": [
    "#### Set your project ID\n",
    "\n",
    "**If you do not know your project ID**, you may be able to get your project ID using `gcloud`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "oM1iC_MfAts1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project ID:  hybrid-vertex\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "PROJECT_ID = 'hybrid-vertex'\n",
    "\n",
    "# Get your Google Cloud project ID from gcloud\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    shell_output = ! gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = shell_output[0]\n",
    "    print(\"Project ID: \", PROJECT_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qJYoRfYng0XZ"
   },
   "source": [
    "Otherwise, set your project ID here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "riG_qUokg0XZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n"
     ]
    }
   ],
   "source": [
    "if PROJECT_ID == \"\" or PROJECT_ID is None:\n",
    "    PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}\n",
    "\n",
    "! gcloud config set project {PROJECT_ID}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "06571eb4063b"
   },
   "source": [
    "#### Timestamp\n",
    "\n",
    "If you are in a live tutorial session, you might be using a shared test account or project. To avoid name collisions between users on resources created, you create a timestamp for each instance session, and append it onto the name of resources you create in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "697568e92bd6"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dr--iN2kAylZ"
   },
   "source": [
    "### Authenticate your Google Cloud account\n",
    "\n",
    "**If you are using Google Cloud Notebooks**, your environment is already\n",
    "authenticated. Skip this step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sBCra4QMA2wR"
   },
   "source": [
    "**If you are using Colab**, run the cell below and follow the instructions\n",
    "when prompted to authenticate your account via oAuth.\n",
    "\n",
    "**Otherwise**, follow these steps:\n",
    "\n",
    "1. In the Cloud Console, go to the [**Create service account key**\n",
    "   page](https://console.cloud.google.com/apis/credentials/serviceaccountkey).\n",
    "\n",
    "2. Click **Create service account**.\n",
    "\n",
    "3. In the **Service account name** field, enter a name, and\n",
    "   click **Create**.\n",
    "\n",
    "4. In the **Grant this service account access to project** section, click the **Role** drop-down list. Type \"Vertex AI\"\n",
    "into the filter box, and select\n",
    "   **Vertex AI Administrator**. Type \"Storage Object Admin\" into the filter box, and select **Storage Object Admin**.\n",
    "\n",
    "5. Click *Create*. A JSON file that contains your key downloads to your\n",
    "local environment.\n",
    "\n",
    "6. Enter the path to your service account key as the\n",
    "`GOOGLE_APPLICATION_CREDENTIALS` variable in the cell below and run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PyQmSRbKA8r-"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# If you are running this notebook in Colab, run this cell and follow the\n",
    "# instructions to authenticate your GCP account. This provides access to your\n",
    "# Cloud Storage bucket and lets you submit training jobs and prediction\n",
    "# requests.\n",
    "\n",
    "# The Google Cloud Notebook product has specific requirements\n",
    "IS_GOOGLE_CLOUD_NOTEBOOK = os.path.exists(\"/opt/deeplearning/metadata/env_version\")\n",
    "\n",
    "# If on Google Cloud Notebooks, then don't execute this code\n",
    "if not IS_GOOGLE_CLOUD_NOTEBOOK:\n",
    "    if \"google.colab\" in sys.modules:\n",
    "        from google.colab import auth as google_auth\n",
    "\n",
    "        google_auth.authenticate_user()\n",
    "\n",
    "    # If you are running this notebook locally, replace the string below with the\n",
    "    # path to your service account key and run this cell to authenticate your GCP\n",
    "    # account.\n",
    "    elif not os.getenv(\"IS_TESTING\"):\n",
    "        %env GOOGLE_APPLICATION_CREDENTIALS ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zgPO1eR3CYjk"
   },
   "source": [
    "### Create a Cloud Storage bucket\n",
    "\n",
    "**The following steps are required, regardless of your notebook environment.**\n",
    "\n",
    "Before you submit a training job for the two-tower model, you need to upload your training data and schema to Cloud Storage. Vertex AI trains the model using this input data. In this tutorial, the Two-Tower built-in algorithm also saves the trained model that results from your job in the same bucket. Using this model artifact, you can then create Vertex AI model and endpoint resources in order to serve online predictions.\n",
    "\n",
    "Set the name of your Cloud Storage bucket below. It must be unique across all\n",
    "Cloud Storage buckets.\n",
    "\n",
    "You may also change the `REGION` variable, which is used for operations\n",
    "throughout the rest of this notebook. Make sure to [choose a region where Vertex AI services are\n",
    "available](https://cloud.google.com/vertex-ai/docs/general/locations#available_regions). You may\n",
    "not use a Multi-Regional Storage bucket for training with Vertex AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "MzGDU7TWdts_"
   },
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"gs://spotify-builtin-2t\"  # @param {type:\"string\"}\n",
    "REGION = \"us-central1\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cf221059d072"
   },
   "outputs": [],
   "source": [
    "if BUCKET_NAME == \"\" or BUCKET_NAME is None or BUCKET_NAME == \"gs://[your-bucket-name]\":\n",
    "    BUCKET_NAME = \"gs://\" + PROJECT_ID + \"aip-\" + TIMESTAMP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-EcIXiGsCePi"
   },
   "source": [
    "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NIq7R4HZCfIc"
   },
   "outputs": [],
   "source": [
    "! gsutil mb -l $REGION $BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ucvCsknMCims"
   },
   "source": [
    "Finally, validate access to your Cloud Storage bucket by examining its contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vhOb7YnwClBb"
   },
   "outputs": [],
   "source": [
    "! gsutil ls -al $BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "### Import libraries and define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "pRUOFELefqf1"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ixC92jeHQMxk"
   },
   "source": [
    "## Configure parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GgsNm8aim0Ym"
   },
   "source": [
    "The following table shows parameters that are common to all Vertex Training jobs created using the `gcloud ai custom-jobs create` command. See the [official documentation](https://cloud.google.com/sdk/gcloud/reference/ai/custom-jobs/create) for all the possible arguments.\n",
    "\n",
    "| Parameter | Data type | Description | Required |\n",
    "|--|--|--|--|\n",
    "| `display-name` | string | Name of the job. | Yes |\n",
    "| `worker-pool-spec` | string | Comma-separated list of arguments specifying a worker pool configuration (see below). | Yes |\n",
    "| `region` | string | Region to submit the job to. | No |\n",
    "\n",
    "The `worker-pool-spec` flag can be specified multiple times, one for each worker pool. The following table shows the arguments used to specify a worker pool.\n",
    "\n",
    "| Parameter | Data type | Description | Required |\n",
    "|--|--|--|--|\n",
    "| `machine-type` | string | Machine type for the pool. See the [official documentation](https://cloud.google.com/vertex-ai/docs/training/configure-compute) for supported machines. | Yes |\n",
    "| `replica-count` | int | The number of replicas of the machine in the pool. | No |\n",
    "| `container-image-uri` | string | Docker image to run on each worker. | No |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0MvQ22Sbm8lh"
   },
   "source": [
    "The following table shows the parameters for the two-tower model training job:\n",
    "\n",
    "| Parameter | Data type | Description | Required |\n",
    "|--|--|--|--|\n",
    "| `training_data_path` | string | Cloud Storage pattern where training data is stored. | Yes |\n",
    "| `input_schema_path` | string | Cloud Storage path where the JSON input schema is stored. | Yes |\n",
    "| `input_file_format` | string | The file format of input. Currently supports `jsonl` and `tfrecord`. | No - default is `jsonl`. |\n",
    "| `job_dir` | string | Cloud Storage directory where the model output files will be stored. | Yes |\n",
    "| `eval_data_path` | string | Cloud Storage pattern where eval data is stored. | No |\n",
    "| `candidate_data_path` | string | Cloud Storage pattern where candidate data is stored. Only used for top_k_categorical_accuracy metrics. If not set, it's generated from training/eval data. | No |\n",
    "| `train_batch_size` | int | Batch size for training. | No - Default is 100. |\n",
    "| `eval_batch_size` | int | Batch size for evaluation. | No - Default is 100. |\n",
    "| `eval_split` | float | Split fraction to use for the evaluation dataset, if `eval_data_path` is not provided. | No - Default is 0.2 |\n",
    "| `optimizer` | string | Training optimizer. Lowercase string name of any TF2.3 Keras optimizer is supported ('sgd', 'nadam', 'ftrl', etc.). See [TensorFlow documentation](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers). | No - Default is 'adagrad'. |\n",
    "| `learning_rate` | float | Learning rate for training. | No - Default is the default learning rate of the specified optimizer. |\n",
    "| `momentum` | float | Momentum for optimizer, if specified. | No - Default is the default momentum value for the specified optimizer. |\n",
    "| `metrics` | string | Metrics used to evaluate the model. Can be either `auc`, `top_k_categorical_accuracy` or `precision_at_1`. | No - Default is `auc`. |\n",
    "| `num_epochs` | int | Number of epochs for training. | No - Default is 10. |\n",
    "| `num_hidden_layers` | int | Number of hidden layers. | No |\n",
    "| `num_nodes_hidden_layer{index}` | int | Num of nodes in hidden layer {index}. The range of index is 1 to 20. | No |\n",
    "| `output_dim` | int | The output embedding dimension for each encoder tower of the two-tower model. | No - Default is 64. |\n",
    "| `training_steps_per_epoch` | int | Number of steps per epoch to run the training for.  Only needed if you are using more than 1 machine or using a master machine with more than 1 gpu. | No - Default is None. |\n",
    "| `eval_steps_per_epoch` | int | Number of steps per epoch to run the evaluation for.  Only needed if you are using more than 1 machine or using a master machine with more than 1 gpu. | No - Default is None. |\n",
    "| `gpu_memory_alloc` | int | Amount of memory allocated per GPU (in MB). | No - Default is no limit. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "2sEfn2ZVnI_s"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data path: gs://spotify-builtin-2t/train_data/*\n",
      "Input schema path: gs://spotify-builtin-2t/schema.json\n",
      "Output directory: gs://spotify-builtin-2t/experiment-20220629210427/output\n",
      "Train batch size: 100\n",
      "Number of epochs: 3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Change to your data and schema paths. These are paths to the movielens_100k\n",
    "# sample data.\n",
    "TRAINING_DATA_PATH = f\"{BUCKET_NAME}/train_data/*\"\n",
    "INPUT_SCHEMA_PATH = f\"{BUCKET_NAME}/schema.json\"\n",
    "\n",
    "# URI of the two-tower training Docker image.\n",
    "LEARNER_IMAGE_URI = \"us-docker.pkg.dev/vertex-ai-restricted/builtin-algorithm/two-tower\"\n",
    "\n",
    "# Change to your output location.\n",
    "OUTPUT_DIR = f\"{BUCKET_NAME}/experiment-{TIMESTAMP}/output\"\n",
    "\n",
    "TRAIN_BATCH_SIZE = 100  # Batch size for training.\n",
    "NUM_EPOCHS = 3  # Number of epochs for training.\n",
    "\n",
    "print(f\"Training data path: {TRAINING_DATA_PATH}\")\n",
    "print(f\"Input schema path: {INPUT_SCHEMA_PATH}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"Train batch size: {TRAIN_BATCH_SIZE}\")\n",
    "print(f\"Number of epochs: {NUM_EPOCHS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"query\": {\"name\": {\"feature_type\": \"Text\"}, \"collaborative\": {\"feature_type\": \"Text\"}, \"duration_ms_playlist\": {\"feature_type\": \"Numeric\"}, \"artist_name_seed_track\": {\"feature_type\": \"Text\"}, \"artist_uri_seed_track\": {\"feature_type\": \"Id\", \"config\": {\"num_buckets\": 1000}}, \"track_name_seed_track\": {\"feature_type\": \"Text\"}, \"track_uri_seed_track\": {\"feature_type\": \"Id\", \"config\": {\"num_buckets\": 10000}}, \"album_name_seed_track\": {\"feature_type\": \"Text\"}, \"album_uri_seed_track\": {\"feature_type\": \"Id\", \"config\": {\"num_buckets\": 1000}}, \"duration_seed_track\": {\"feature_type\": \"Numeric\"}, \"track_pop_seed_track\": {\"feature_type\": \"Numeric\"}, \"artist_pop_seed_track\": {\"feature_type\": \"Numeric\"}, \"artist_followers_seed_track\": {\"feature_type\": \"Numeric\"}, \"duration_ms_seed_pl\": {\"feature_type\": \"Numeric\"}, \"n_songs_pl\": {\"feature_type\": \"Numeric\"}, \"num_artists_pl\": {\"feature_type\": \"Numeric\"}, \"num_albums_pl\": {\"feature_type\": \"Numeric\"}, \"artist_genres_seed_track\": {\"feature_type\": \"Text\"}, \"description_pl\": {\"feature_type\": \"Text\"}, \"artist_name_pl\": {\"feature_type\": \"Text\"}, \"track_uri_pl\": {\"feature_type\": \"Id\", \"config\": {\"num_buckets\": 10000}}, \"track_name_pl\": {\"feature_type\": \"Text\"}, \"duration_ms_songs_pl\": {\"feature_type\": \"Numeric\"}, \"album_name_pl\": {\"feature_type\": \"Text\"}, \"artist_pop_pl\": {\"feature_type\": \"Numeric\"}, \"artists_followers_pl\": {\"feature_type\": \"Numeric\"}, \"track_pop_pl\": {\"feature_type\": \"Numeric\"}, \"artist_genres_pl\": {\"feature_type\": \"Text\"}}, \"candidate\": {\"artist_name_can\": {\"feature_type\": \"Text\"}, \"track_uri_can\": {\"feature_type\": \"Id\", \"config\": {\"num_buckets\": 10000}}, \"album_uri_can\": {\"feature_type\": \"Id\", \"config\": {\"num_buckets\": 1000}}, \"track_name_can\": {\"feature_type\": \"Text\"}, \"artist_uri_can\": {\"feature_type\": \"Id\", \"config\": {\"num_buckets\": 1000}}, \"duration_ms_can\": {\"feature_type\": \"Numeric\"}, \"album_name_can\": {\"feature_type\": \"Text\"}, \"track_pop_can\": {\"feature_type\": \"Numeric\"}, \"artist_pop_can\": {\"feature_type\": \"Numeric\"}, \"artist_followers_can\": {\"feature_type\": \"Numeric\"}, \"artist_genres_can\": {\"feature_type\": \"Text\"}}}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "output = ! gsutil cat $BUCKET_NAME/schema.json\n",
    "print(output[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "upLZ8kcankwj"
   },
   "source": [
    "## Train on Vertex Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6M_O_L55nwQ0"
   },
   "source": [
    "Submit the two-tower training job to Vertex Training. The following command uses a single CPU machine for training. When using single node training, `training_steps_per_epoch` and `eval_steps_per_epoch` do not need to be set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "1gXZRq80nl2S"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Using endpoint [https://us-central1-aiplatform.googleapis.com/]', 'CustomJob [projects/934903580331/locations/us-central1/customJobs/4890078239411666944] is submitted successfully.', '', 'Your job is still active. You may view the status of your job with the command', '', '  $ gcloud ai custom-jobs describe projects/934903580331/locations/us-central1/customJobs/4890078239411666944', '', 'or continue streaming the logs with the command', '', '  $ gcloud ai custom-jobs stream-logs projects/934903580331/locations/us-central1/customJobs/4890078239411666944']\n"
     ]
    }
   ],
   "source": [
    "DATASET_NAME = \"spotify\"\n",
    "learning_job_name = f\"two_tower_cpu_{DATASET_NAME}_{TIMESTAMP}\"\n",
    "\n",
    "CREATION_LOG = ! gcloud ai custom-jobs create \\\n",
    "  --display-name={learning_job_name} \\\n",
    "  --worker-pool-spec=machine-type=n1-standard-8,replica-count=1,container-image-uri={LEARNER_IMAGE_URI} \\\n",
    "  --region={REGION} \\\n",
    "  --args=--training_data_path={TRAINING_DATA_PATH} \\\n",
    "  --args=--input_schema_path={INPUT_SCHEMA_PATH} \\\n",
    "  --args=--job-dir={OUTPUT_DIR} \\\n",
    "  --args=--train_batch_size={TRAIN_BATCH_SIZE} \\\n",
    "  --args=--num_epochs={NUM_EPOCHS}\n",
    "\n",
    "print(CREATION_LOG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RaIkcFT2n4_U"
   },
   "source": [
    "If you want to train using GPUs, you need to write configuration to a YAML file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "      # - --training_steps_per_epoch=1500\n",
    "      # - --eval_steps_per_epoch=1500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "### Example Record\n",
    "```\n",
    "{\n",
    "   \"candidate\":{\n",
    "      \"artist_name_can\":[\n",
    "         \"The Head Assembly\"\n",
    "      ],\n",
    "      \"track_uri_can\":[\n",
    "         \"spotify:track:5Q2LDZsjtscyce03gyG08Q\"\n",
    "      ],\n",
    "      \"track_name_can\":[\n",
    "         \"Tickle My... - Julissa Veloz Club Mix\"\n",
    "      ],\n",
    "      \"duration_ms_can\":[\n",
    "         354573\n",
    "      ],\n",
    "      \"album_name_can\":[\n",
    "         \"Tickle My...\"\n",
    "      ],\n",
    "      \"track_pop_can\":[\n",
    "         \"0\"\n",
    "      ],\n",
    "      \"artist_pop_can\":[\n",
    "         0\n",
    "      ],\n",
    "      \"artist_followers_can\":[\n",
    "         0\n",
    "      ],\n",
    "      \"artist_genres_can\":[\n",
    "         \"\"\n",
    "      ]\n",
    "   },\n",
    "   \"query\":{\n",
    "      \"id_pl\":[\n",
    "         \"986484-124\"\n",
    "      ],\n",
    "      \"name\":[\n",
    "         \"New Music\"\n",
    "      ],\n",
    "      \"collaborative\":[\n",
    "         \"false\"\n",
    "      ],\n",
    "      \"artist_name_seed_track\":[\n",
    "         \"Jason Aldean\"\n",
    "      ],\n",
    "      \"track_name_seed_track\":[\n",
    "         \"Burnin' It Down\"\n",
    "      ],\n",
    "      \"album_name_seed_track\":[\n",
    "         \"Old Boots, New Dirt\"\n",
    "      ],\n",
    "      \"duration_seed_track\":[\n",
    "         219146\n",
    "      ],\n",
    "      \"track_pop_seed_track\":[\n",
    "         \"50\"\n",
    "      ],\n",
    "      \"artist_pop_seed_track\":[\n",
    "         78\n",
    "      ],\n",
    "      \"artist_followers_seed_track\":[\n",
    "         5139127\n",
    "      ],\n",
    "      \"duration_ms_seed_pl\":[\n",
    "         29578370\n",
    "      ],\n",
    "      \"n_songs_pl\":[\n",
    "         \"124\"\n",
    "      ],\n",
    "      \"num_artists_pl\":[\n",
    "         \"85\"\n",
    "      ],\n",
    "      \"num_albums_pl\":[\n",
    "         \"99\"\n",
    "      ],\n",
    "      \"artist_genres_seed_track\":[\n",
    "         \"'contemporary country', 'country', 'country road'\"\n",
    "      ],\n",
    "      \"description_pl\":[\n",
    "         \"\"\n",
    "      ],\n",
    "      \"artist_name_pl\":[\n",
    "         \"Tove Lo\",\n",
    "         \"Ellie Goulding\",\n",
    "         \"Kanye West\",\n",
    "         \"Becky G\",\n",
    "         \"Coldplay\",\n",
    "         \"MAGIC!\",\n",
    "         \"OneRepublic\",\n",
    "         \"Alex Clare\",\n",
    "         \"Sampha\",\n",
    "         \"Hackman\",\n",
    "         \"Ellie Goulding\",\n",
    "         \"OneRepublic\",\n",
    "         \"Jennifer Lopez\",\n",
    "         \"Jason Derulo\",\n",
    "         \"Sam Smith\",\n",
    "         \"Sia\",\n",
    "         \"Sia\",\n",
    "         \"Christina Perri\",\n",
    "         \"Jhene Aiko\",\n",
    "         \"Jhene Aiko\",\n",
    "         \"Jason Derulo\",\n",
    "         \"Frank Ocean\",\n",
    "         \"Miley Cyrus\",\n",
    "         \"Miley Cyrus\",\n",
    "         \"Tito \\\"El Bambino\\\"\",\n",
    "         \"Demi Lovato\",\n",
    "         \"Selena Gomez \\u0026 The Scene\",\n",
    "         \"Usher\",\n",
    "         \"Usher\",\n",
    "         \"Maroon 5\",\n",
    "         \"DEV\",\n",
    "         \"Manufactured Superstars\",\n",
    "         \"Tiësto\",\n",
    "         \"Diplo\",\n",
    "         \"Steve Aoki\",\n",
    "         \"Steve Aoki\",\n",
    "         \"Issues\",\n",
    "         \"Chiodos\",\n",
    "         \"Clean Bandit\",\n",
    "         \"You Me At Six\",\n",
    "         \"NERO\",\n",
    "         \"NERO\",\n",
    "         \"NERO\",\n",
    "         \"NERO\",\n",
    "         \"Farruko\",\n",
    "         \"Juanes\",\n",
    "         \"Prince Royce\",\n",
    "         \"Paramore\",\n",
    "         \"Kiesza\",\n",
    "         \"Snootie Wild\",\n",
    "         \"Hoobastank\",\n",
    "         \"Zedd\",\n",
    "         \"Future\",\n",
    "         \"Kat Dahlia\",\n",
    "         \"Naughty Boy\",\n",
    "         \"Nico \\u0026 Vinz\",\n",
    "         \"Disclosure\",\n",
    "         \"Ariana Grande\",\n",
    "         \"Jessie Ware\",\n",
    "         \"Miley Cyrus\",\n",
    "         \"Amy Winehouse\",\n",
    "         \"Gemini\",\n",
    "         \"Vance Joy\",\n",
    "         \"Ed Sheeran\",\n",
    "         \"Frank Ocean\",\n",
    "         \"Tonight Alive\",\n",
    "         \"Ariana Grande\",\n",
    "         \"Ty Dolla $ign\",\n",
    "         \"The Weeknd\",\n",
    "         \"The Weeknd\",\n",
    "         \"The Weeknd\",\n",
    "         \"The Weeknd\",\n",
    "         \"Jeremih\",\n",
    "         \"Breathe Carolina\",\n",
    "         \"Sam Smith\",\n",
    "         \"Rita Ora\",\n",
    "         \"Muse\",\n",
    "         \"The Used\",\n",
    "         \"Austin Mahone\",\n",
    "         \"Seven Lions\",\n",
    "         \"Seven Lions\",\n",
    "         \"Seven Lions\",\n",
    "         \"Seven Lions\",\n",
    "         \"Skrillex\",\n",
    "         \"AJR\",\n",
    "         \"Niykee Heaton\",\n",
    "         \"Chris Brown\",\n",
    "         \"Skrillex\",\n",
    "         \"Tonight Alive\",\n",
    "         \"Shelco Garcia \\u0026 Teenwolf\",\n",
    "         \"Gareth Emery\",\n",
    "         \"Lights\",\n",
    "         \"Charli XCX\",\n",
    "         \"Pharrell Williams\",\n",
    "         \"Trinidad James\",\n",
    "         \"Tonight Alive\",\n",
    "         \"Tonight Alive\",\n",
    "         \"Milkman\",\n",
    "         \"Maroon 5\",\n",
    "         \"Iggy Azalea\",\n",
    "         \"The Chainsmokers\",\n",
    "         \"Diplo\",\n",
    "         \"Diplo\",\n",
    "         \"Yellow Claw\",\n",
    "         \"Steve Aoki\",\n",
    "         \"Diplo\",\n",
    "         \"Diplo\",\n",
    "         \"Diplo\",\n",
    "         \"Diplo\",\n",
    "         \"Diplo\",\n",
    "         \"Steve Aoki\",\n",
    "         \"Diplo\",\n",
    "         \"Diplo\",\n",
    "         \"K CAMP\",\n",
    "         \"K CAMP\",\n",
    "         \"Jessie J\",\n",
    "         \"Drake\",\n",
    "         \"PARTYNEXTDOOR\",\n",
    "         \"Tiësto\",\n",
    "         \"Beyoncé\",\n",
    "         \"John Legend\",\n",
    "         \"Nicki Minaj\",\n",
    "         \"Echosmith\",\n",
    "         \"Jason Aldean\"\n",
    "      ],\n",
    "      \"track_name_pl\":[\n",
    "         \"Habits (Stay High) - Hippie Sabotage Remix\",\n",
    "         \"Beating Heart\",\n",
    "         \"Lost In The World\",\n",
    "         \"Shower\",\n",
    "         \"Magic\",\n",
    "         \"Don't Kill the Magic\",\n",
    "         \"Something I Need\",\n",
    "         \"Relax My Beloved\",\n",
    "         \"Too Much\",\n",
    "         \"Forgotten Notes\",\n",
    "         \"Tessellate - Bonus Track\",\n",
    "         \"Love Runs Out\",\n",
    "         \"First Love\",\n",
    "         \"Stupid Love\",\n",
    "         \"Money On My Mind\",\n",
    "         \"Chandelier\",\n",
    "         \"Elastic Heart - From \\\"The Hunger Games: Catching Fire\\\"/Soundtrack\",\n",
    "         \"human\",\n",
    "         \"To Love \\u0026 Die\",\n",
    "         \"The Worst\",\n",
    "         \"Trumpets\",\n",
    "         \"Novacane\",\n",
    "         \"Someone Else\",\n",
    "         \"Drive\",\n",
    "         \"El Gran Perdedor\",\n",
    "         \"Really Don't Care\",\n",
    "         \"I Won't Apologize\",\n",
    "         \"Trading Places\",\n",
    "         \"Good Kisser\",\n",
    "         \"Maps\",\n",
    "         \"Naked\",\n",
    "         \"Like Satellites - Extended Mix\",\n",
    "         \"Wasted\",\n",
    "         \"Express Yourself - feat. Nicky Da B\",\n",
    "         \"Rage the Night Away\",\n",
    "         \"Freak - feat. Steve Bays\",\n",
    "         \"Never Lose Your Flames\",\n",
    "         \"Hey Zeus! the Dungeon\",\n",
    "         \"Rather Be (feat. Jess Glynne)\",\n",
    "         \"Stay With Me\",\n",
    "         \"Me And You\",\n",
    "         \"Guilt\",\n",
    "         \"My Eyes\",\n",
    "         \"Satisfy\",\n",
    "         \"Passion Whine - Remastered Version\",\n",
    "         \"La Luz\",\n",
    "         \"Soy el Mismo\",\n",
    "         \"Ain't It Fun\",\n",
    "         \"Hideaway\",\n",
    "         \"Yayo\",\n",
    "         \"The Letter\",\n",
    "         \"Find You\",\n",
    "         \"I Won\",\n",
    "         \"Crazy\",\n",
    "         \"La La La\",\n",
    "         \"Am I Wrong\",\n",
    "         \"Latch\",\n",
    "         \"Problem\",\n",
    "         \"Running\",\n",
    "         \"Maybe You're Right\",\n",
    "         \"Back To Black\",\n",
    "         \"Blue\",\n",
    "         \"Riptide\",\n",
    "         \"Don't\",\n",
    "         \"Lost\",\n",
    "         \"Little Lion Man\",\n",
    "         \"Break Free\",\n",
    "         \"Or Nah (feat. The Weeknd, Wiz Khalifa and DJ Mustard) - Remix\",\n",
    "         \"Adaptation\",\n",
    "         \"Pretty\",\n",
    "         \"Belong To The World\",\n",
    "         \"Wicked Games\",\n",
    "         \"Don't Tell 'Em\",\n",
    "         \"Blackout\",\n",
    "         \"Stay With Me\",\n",
    "         \"I Will Never Let You Down\",\n",
    "         \"Madness\",\n",
    "         \"The Bird And The Worm\",\n",
    "         \"Mmm Yeah (feat. Pitbull)\",\n",
    "         \"Don’t Leave\",\n",
    "         \"Worlds Apart\",\n",
    "         \"Strangers\",\n",
    "         \"Keep It Close\",\n",
    "         \"Recess\",\n",
    "         \"I'm Ready\",\n",
    "         \"Bad Intentions\",\n",
    "         \"New Flame\",\n",
    "         \"Dirty Vibe\",\n",
    "         \"The Edge (From the Motion Picture \\\"The Amazing Spider-Man 2\\\")\",\n",
    "         \"That's My Jam - Original Mix\",\n",
    "         \"Concrete Angel\",\n",
    "         \"Up We Go\",\n",
    "         \"Boom Clap\",\n",
    "         \"Come Get It Bae\",\n",
    "         \"Female$ Welcomed\",\n",
    "         \"Lonely Girl\",\n",
    "         \"Say Please\",\n",
    "         \"Somebody Find Me (feat. Kait Weston)\",\n",
    "         \"It Was Always You\",\n",
    "         \"Black Widow\",\n",
    "         \"Kanye\",\n",
    "         \"Revolution - feat. Faustix \\u0026 Imanos and Kai\",\n",
    "         \"6th Gear - feat. Kstylis\",\n",
    "         \"Techno - feat. Wacka Flocka Flame\",\n",
    "         \"Freak - feat. Steve Bays\",\n",
    "         \"Boy Oh Boy\",\n",
    "         \"Biggie Bounce - feat. Angger Dimas \\u0026 Travis Porte\",\n",
    "         \"Express Yourself - feat. Nicky Da B\",\n",
    "         \"Revolution (Danny Diggz Remix) - feat. Faustix \\u0026 Imanos and Kai\",\n",
    "         \"Boy Oh Boy - Thugli Remix\",\n",
    "         \"Freak (Rickyxsan Remix) - feat. Steve Bays\",\n",
    "         \"Biggie Bounce (Tony Romera Remix) - feat. Angger Dimas \\u0026 Travis Porter\",\n",
    "         \"Express Yourself (Party Favor Extended Remix) - feat. Nicky Da B\",\n",
    "         \"Money Baby\",\n",
    "         \"Cut Her Off\",\n",
    "         \"Bang Bang\",\n",
    "         \"From Time\",\n",
    "         \"Wus Good / Curious\",\n",
    "         \"Who Wants To Be Alone\",\n",
    "         \"XO\",\n",
    "         \"You \\u0026 I (Nobody in the World)\",\n",
    "         \"Anaconda\",\n",
    "         \"Cool Kids\",\n",
    "         \"Burnin' It Down\"\n",
    "      ],\n",
    "      \"duration_ms_songs_pl\":[\n",
    "         258933,\n",
    "         212125,\n",
    "         256586,\n",
    "         206166,\n",
    "         285014,\n",
    "         217226,\n",
    "         240080,\n",
    "         211986,\n",
    "         177795,\n",
    "         268511,\n",
    "         236960,\n",
    "         224853,\n",
    "         215786,\n",
    "         214493,\n",
    "         192670,\n",
    "         216120,\n",
    "         257986,\n",
    "         250706,\n",
    "         203453,\n",
    "         254493,\n",
    "         217306,\n",
    "         302346,\n",
    "         288333,\n",
    "         255213,\n",
    "         182896,\n",
    "         201600,\n",
    "         186546,\n",
    "         268240,\n",
    "         249626,\n",
    "         189959,\n",
    "         236933,\n",
    "         340157,\n",
    "         190013,\n",
    "         277566,\n",
    "         285186,\n",
    "         281250,\n",
    "         221279,\n",
    "         264160,\n",
    "         227833,\n",
    "         194973,\n",
    "         247520,\n",
    "         284120,\n",
    "         289666,\n",
    "         284516,\n",
    "         213200,\n",
    "         176986,\n",
    "         223053,\n",
    "         296520,\n",
    "         251986,\n",
    "         220240,\n",
    "         234853,\n",
    "         204346,\n",
    "         239733,\n",
    "         211320,\n",
    "         222200,\n",
    "         247520,\n",
    "         255631,\n",
    "         193893,\n",
    "         268746,\n",
    "         213386,\n",
    "         240440,\n",
    "         316266,\n",
    "         204280,\n",
    "         219840,\n",
    "         234093,\n",
    "         233146,\n",
    "         214840,\n",
    "         242983,\n",
    "         283933,\n",
    "         375400,\n",
    "         307173,\n",
    "         323746,\n",
    "         266840,\n",
    "         210200,\n",
    "         172723,\n",
    "         203466,\n",
    "         281040,\n",
    "         225040,\n",
    "         231624,\n",
    "         362040,\n",
    "         376242,\n",
    "         204240,\n",
    "         312506,\n",
    "         237680,\n",
    "         227311,\n",
    "         198167,\n",
    "         244133,\n",
    "         206680,\n",
    "         180600,\n",
    "         270000,\n",
    "         236733,\n",
    "         171293,\n",
    "         169866,\n",
    "         201933,\n",
    "         190080,\n",
    "         191133,\n",
    "         194013,\n",
    "         225584,\n",
    "         239919,\n",
    "         209423,\n",
    "         229946,\n",
    "         263716,\n",
    "         211900,\n",
    "         208012,\n",
    "         281250,\n",
    "         174398,\n",
    "         224769,\n",
    "         277566,\n",
    "         193000,\n",
    "         197635,\n",
    "         311999,\n",
    "         270000,\n",
    "         195728,\n",
    "         219120,\n",
    "         243693,\n",
    "         199320,\n",
    "         322160,\n",
    "         212880,\n",
    "         276226,\n",
    "         215946,\n",
    "         252653,\n",
    "         260240,\n",
    "         237626,\n",
    "         219146\n",
    "      ],\n",
    "      \"album_name_pl\":[\n",
    "         \"Queen Of The Clouds\",\n",
    "         \"Halcyon Days\",\n",
    "         \"My Beautiful Dark Twisted Fantasy\",\n",
    "         \"Shower\",\n",
    "         \"Ghost Stories\",\n",
    "         \"Don't Kill the Magic\",\n",
    "         \"Native\",\n",
    "         \"The Lateness Of The Hour\",\n",
    "         \"Too Much / Happens\",\n",
    "         \"Forgotten Notes\",\n",
    "         \"Halcyon Days\",\n",
    "         \"Native\",\n",
    "         \"A.K.A.\",\n",
    "         \"Tattoos\",\n",
    "         \"In The Lonely Hour\",\n",
    "         \"1000 Forms Of Fear\",\n",
    "         \"Elastic Heart\",\n",
    "         \"head or heart\",\n",
    "         \"Souled Out\",\n",
    "         \"Sail Out\",\n",
    "         \"Tattoos\",\n",
    "         \"Novacane\",\n",
    "         \"Bangerz (Deluxe Version)\",\n",
    "         \"Bangerz (Deluxe Version)\",\n",
    "         \"El Gran Perdedor\",\n",
    "         \"Demi\",\n",
    "         \"Kiss \\u0026 Tell\",\n",
    "         \"Here I Stand (Deluxe Version)\",\n",
    "         \"Hard II Love\",\n",
    "         \"V\",\n",
    "         \"The Night The Sun Came Up\",\n",
    "         \"Like Satellites [Remixes]\",\n",
    "         \"A Town Called Paradise\",\n",
    "         \"Random White Dude Be Everywhere\",\n",
    "         \"Neon Future I\",\n",
    "         \"Random White Dude Be Everywhere\",\n",
    "         \"Issues\",\n",
    "         \"Illuminaudio\",\n",
    "         \"I Cry When I Laugh\",\n",
    "         \"Hold Me Down\",\n",
    "         \"Welcome Reality\",\n",
    "         \"Welcome Reality\",\n",
    "         \"Welcome Reality\",\n",
    "         \"Satisfy\",\n",
    "         \"Farruko Presents Los Menores\",\n",
    "         \"Loco De Amor\",\n",
    "         \"Soy el Mismo\",\n",
    "         \"Paramore\",\n",
    "         \"Sound Of A Woman\",\n",
    "         \"Yayo\",\n",
    "         \"FOR(N)EVER\",\n",
    "         \"Clarity\",\n",
    "         \"Honest\",\n",
    "         \"My Garden\",\n",
    "         \"Hotel Cabana\",\n",
    "         \"Black Star Elephant\",\n",
    "         \"Settle\",\n",
    "         \"My Everything\",\n",
    "         \"Devotion\",\n",
    "         \"Bangerz (Deluxe Version)\",\n",
    "         \"Back To Black\",\n",
    "         \"Blue\",\n",
    "         \"Dream Your Life Away\",\n",
    "         \"x\",\n",
    "         \"channel ORANGE\",\n",
    "         \"Punk Goes Pop, Vol. 4\",\n",
    "         \"My Everything\",\n",
    "         \"Or Nah (feat. The Weeknd, Wiz Khalifa and DJ Mustard)\",\n",
    "         \"Kiss Land\",\n",
    "         \"Kiss Land\",\n",
    "         \"Kiss Land\",\n",
    "         \"Trilogy\",\n",
    "         \"Late Nights: The Album\",\n",
    "         \"Hell Is What You Make It: Reloaded\",\n",
    "         \"In The Lonely Hour\",\n",
    "         \"I Will Never Let You Down\",\n",
    "         \"The 2nd Law\",\n",
    "         \"Lies For The Liars\",\n",
    "         \"The Secret\",\n",
    "         \"Worlds Apart\",\n",
    "         \"Worlds Apart\",\n",
    "         \"Strangers\",\n",
    "         \"Worlds Apart\",\n",
    "         \"Recess\",\n",
    "         \"Living Room\",\n",
    "         \"Bad Intentions\",\n",
    "         \"X (Deluxe Version)\",\n",
    "         \"Recess\",\n",
    "         \"The Edge (From the Motion Picture \\\"The Amazing Spider-Man 2\\\")\",\n",
    "         \"That's My Jam\",\n",
    "         \"Concrete Angel\",\n",
    "         \"Little Machines\",\n",
    "         \"SUCKER\",\n",
    "         \"G I R L\",\n",
    "         \"Don't Be S.A.F.E.\",\n",
    "         \"The Other Side\",\n",
    "         \"The Other Side\",\n",
    "         \"Reboot\",\n",
    "         \"V\",\n",
    "         \"The New Classic\",\n",
    "         \"Kanye\",\n",
    "         \"Random White Dude Be Everywhere\",\n",
    "         \"Random White Dude Be Everywhere\",\n",
    "         \"Random White Dude Be Everywhere\",\n",
    "         \"Random White Dude Be Everywhere\",\n",
    "         \"Random White Dude Be Everywhere\",\n",
    "         \"Random White Dude Be Everywhere\",\n",
    "         \"Random White Dude Be Everywhere\",\n",
    "         \"Random White Dude Be Everywhere\",\n",
    "         \"Random White Dude Be Everywhere\",\n",
    "         \"Random White Dude Be Everywhere\",\n",
    "         \"Random White Dude Be Everywhere\",\n",
    "         \"Random White Dude Be Everywhere\",\n",
    "         \"Money Baby\",\n",
    "         \"Cut Her Off\",\n",
    "         \"My Everything\",\n",
    "         \"Nothing Was The Same\",\n",
    "         \"PARTYNEXTDOOR\",\n",
    "         \"The Best of Nelly Furtado\",\n",
    "         \"BEYONCÉ [Platinum Edition]\",\n",
    "         \"Love In The Future\",\n",
    "         \"The Pinkprint\",\n",
    "         \"Talking Dreams\",\n",
    "         \"Old Boots, New Dirt\"\n",
    "      ],\n",
    "      \"artist_pop_pl\":[\n",
    "         79,\n",
    "         81,\n",
    "         96,\n",
    "         86,\n",
    "         92,\n",
    "         70,\n",
    "         86,\n",
    "         59,\n",
    "         64,\n",
    "         32,\n",
    "         81,\n",
    "         86,\n",
    "         81,\n",
    "         85,\n",
    "         86,\n",
    "         88,\n",
    "         88,\n",
    "         75,\n",
    "         82,\n",
    "         82,\n",
    "         85,\n",
    "         87,\n",
    "         86,\n",
    "         86,\n",
    "         71,\n",
    "         84,\n",
    "         72,\n",
    "         83,\n",
    "         83,\n",
    "         89,\n",
    "         66,\n",
    "         25,\n",
    "         87,\n",
    "         84,\n",
    "         78,\n",
    "         78,\n",
    "         55,\n",
    "         52,\n",
    "         80,\n",
    "         65,\n",
    "         60,\n",
    "         60,\n",
    "         60,\n",
    "         60,\n",
    "         90,\n",
    "         78,\n",
    "         80,\n",
    "         80,\n",
    "         60,\n",
    "         43,\n",
    "         69,\n",
    "         80,\n",
    "         92,\n",
    "         59,\n",
    "         65,\n",
    "         67,\n",
    "         77,\n",
    "         93,\n",
    "         66,\n",
    "         86,\n",
    "         78,\n",
    "         39,\n",
    "         79,\n",
    "         95,\n",
    "         87,\n",
    "         55,\n",
    "         93,\n",
    "         87,\n",
    "         97,\n",
    "         97,\n",
    "         97,\n",
    "         97,\n",
    "         80,\n",
    "         61,\n",
    "         86,\n",
    "         79,\n",
    "         79,\n",
    "         66,\n",
    "         65,\n",
    "         65,\n",
    "         65,\n",
    "         65,\n",
    "         65,\n",
    "         81,\n",
    "         78,\n",
    "         60,\n",
    "         90,\n",
    "         81,\n",
    "         55,\n",
    "         35,\n",
    "         63,\n",
    "         63,\n",
    "         82,\n",
    "         83,\n",
    "         62,\n",
    "         55,\n",
    "         55,\n",
    "         40,\n",
    "         89,\n",
    "         75,\n",
    "         85,\n",
    "         84,\n",
    "         84,\n",
    "         69,\n",
    "         78,\n",
    "         84,\n",
    "         84,\n",
    "         84,\n",
    "         84,\n",
    "         84,\n",
    "         78,\n",
    "         84,\n",
    "         84,\n",
    "         68,\n",
    "         68,\n",
    "         76,\n",
    "         98,\n",
    "         80,\n",
    "         87,\n",
    "         87,\n",
    "         81,\n",
    "         90,\n",
    "         63,\n",
    "         78\n",
    "      ],\n",
    "      \"artists_followers_pl\":[\n",
    "         2979355,\n",
    "         10616880,\n",
    "         16780345,\n",
    "         11124521,\n",
    "         34753939,\n",
    "         1252851,\n",
    "         13351301,\n",
    "         642317,\n",
    "         545398,\n",
    "         9565,\n",
    "         10616880,\n",
    "         13351301,\n",
    "         11559186,\n",
    "         10917261,\n",
    "         19080875,\n",
    "         22715182,\n",
    "         22715182,\n",
    "         3243327,\n",
    "         5334113,\n",
    "         5334113,\n",
    "         10917261,\n",
    "         9142911,\n",
    "         17680286,\n",
    "         17680286,\n",
    "         2987540,\n",
    "         21918370,\n",
    "         7426513,\n",
    "         9537420,\n",
    "         9537420,\n",
    "         35660772,\n",
    "         255226,\n",
    "         4430,\n",
    "         6259946,\n",
    "         2457277,\n",
    "         3638728,\n",
    "         3638728,\n",
    "         320870,\n",
    "         263185,\n",
    "         4717503,\n",
    "         687147,\n",
    "         814804,\n",
    "         814804,\n",
    "         814804,\n",
    "         814804,\n",
    "         12543291,\n",
    "         3704387,\n",
    "         6825828,\n",
    "         6781282,\n",
    "         283160,\n",
    "         186274,\n",
    "         2040593,\n",
    "         5710363,\n",
    "         11270410,\n",
    "         284539,\n",
    "         317269,\n",
    "         484909,\n",
    "         2006723,\n",
    "         77395215,\n",
    "         650384,\n",
    "         17680286,\n",
    "         7770093,\n",
    "         48393,\n",
    "         2571148,\n",
    "         94437255,\n",
    "         9142911,\n",
    "         316592,\n",
    "         77395215,\n",
    "         4168033,\n",
    "         42984888,\n",
    "         42984888,\n",
    "         42984888,\n",
    "         42984888,\n",
    "         5611842,\n",
    "         339954,\n",
    "         19080875,\n",
    "         7312799,\n",
    "         6919697,\n",
    "         985769,\n",
    "         1943504,\n",
    "         419066,\n",
    "         419066,\n",
    "         419066,\n",
    "         419066,\n",
    "         7984163,\n",
    "         2151505,\n",
    "         455793,\n",
    "         15618321,\n",
    "         7984163,\n",
    "         316592,\n",
    "         3706,\n",
    "         281099,\n",
    "         278431,\n",
    "         2589435,\n",
    "         3864604,\n",
    "         280347,\n",
    "         316592,\n",
    "         316592,\n",
    "         16966,\n",
    "         35660772,\n",
    "         5928632,\n",
    "         18906144,\n",
    "         2457277,\n",
    "         2457277,\n",
    "         994597,\n",
    "         3638728,\n",
    "         2457277,\n",
    "         2457277,\n",
    "         2457277,\n",
    "         2457277,\n",
    "         2457277,\n",
    "         3638728,\n",
    "         2457277,\n",
    "         2457277,\n",
    "         1434353,\n",
    "         1434353,\n",
    "         9737642,\n",
    "         62021773,\n",
    "         3688081,\n",
    "         6259946,\n",
    "         30713126,\n",
    "         6131889,\n",
    "         24503334,\n",
    "         576379,\n",
    "         5139127\n",
    "      ],\n",
    "      \"track_pop_pl\":[\n",
    "         \"69\",\n",
    "         \"55\",\n",
    "         \"69\",\n",
    "         \"82\",\n",
    "         \"76\",\n",
    "         \"46\",\n",
    "         \"0\",\n",
    "         \"35\",\n",
    "         \"0\",\n",
    "         \"0\",\n",
    "         \"44\",\n",
    "         \"68\",\n",
    "         \"0\",\n",
    "         \"51\",\n",
    "         \"0\",\n",
    "         \"82\",\n",
    "         \"0\",\n",
    "         \"67\",\n",
    "         \"0\",\n",
    "         \"75\",\n",
    "         \"71\",\n",
    "         \"79\",\n",
    "         \"58\",\n",
    "         \"55\",\n",
    "         \"0\",\n",
    "         \"74\",\n",
    "         \"44\",\n",
    "         \"0\",\n",
    "         \"0\",\n",
    "         \"86\",\n",
    "         \"46\",\n",
    "         \"0\",\n",
    "         \"0\",\n",
    "         \"19\",\n",
    "         \"49\",\n",
    "         \"28\",\n",
    "         \"47\",\n",
    "         \"31\",\n",
    "         \"54\",\n",
    "         \"51\",\n",
    "         \"0\",\n",
    "         \"0\",\n",
    "         \"0\",\n",
    "         \"0\",\n",
    "         \"0\",\n",
    "         \"56\",\n",
    "         \"60\",\n",
    "         \"77\",\n",
    "         \"69\",\n",
    "         \"28\",\n",
    "         \"0\",\n",
    "         \"58\",\n",
    "         \"64\",\n",
    "         \"49\",\n",
    "         \"75\",\n",
    "         \"81\",\n",
    "         \"1\",\n",
    "         \"0\",\n",
    "         \"32\",\n",
    "         \"51\",\n",
    "         \"0\",\n",
    "         \"47\",\n",
    "         \"85\",\n",
    "         \"54\",\n",
    "         \"82\",\n",
    "         \"0\",\n",
    "         \"0\",\n",
    "         \"82\",\n",
    "         \"57\",\n",
    "         \"59\",\n",
    "         \"55\",\n",
    "         \"76\",\n",
    "         \"79\",\n",
    "         \"60\",\n",
    "         \"0\",\n",
    "         \"70\",\n",
    "         \"72\",\n",
    "         \"64\",\n",
    "         \"71\",\n",
    "         \"44\",\n",
    "         \"25\",\n",
    "         \"0\",\n",
    "         \"34\",\n",
    "         \"60\",\n",
    "         \"66\",\n",
    "         \"55\",\n",
    "         \"0\",\n",
    "         \"55\",\n",
    "         \"48\",\n",
    "         \"26\",\n",
    "         \"50\",\n",
    "         \"45\",\n",
    "         \"54\",\n",
    "         \"0\",\n",
    "         \"45\",\n",
    "         \"52\",\n",
    "         \"31\",\n",
    "         \"27\",\n",
    "         \"61\",\n",
    "         \"70\",\n",
    "         \"62\",\n",
    "         \"20\",\n",
    "         \"2\",\n",
    "         \"4\",\n",
    "         \"28\",\n",
    "         \"41\",\n",
    "         \"1\",\n",
    "         \"19\",\n",
    "         \"1\",\n",
    "         \"1\",\n",
    "         \"0\",\n",
    "         \"0\",\n",
    "         \"0\",\n",
    "         \"0\",\n",
    "         \"0\",\n",
    "         \"0\",\n",
    "         \"0\",\n",
    "         \"70\",\n",
    "         \"0\",\n",
    "         \"67\",\n",
    "         \"0\",\n",
    "         \"71\",\n",
    "         \"75\",\n",
    "         \"50\"\n",
    "      ],\n",
    "      \"artist_genres_pl\":[\n",
    "         \"'art pop', 'dance pop', 'electropop', 'metropopolis', 'pop', 'swedish electropop', 'swedish pop', 'swedish synthpop'\",\n",
    "         \"'dance pop', 'edm', 'electropop', 'indietronica', 'metropopolis', 'pop', 'uk pop'\",\n",
    "         \"'chicago rap', 'rap'\",\n",
    "         \"'dance pop', 'latin', 'latin pop', 'latin viral pop', 'pop', 'rap latina', 'reggaeton', 'trap latino'\",\n",
    "         \"'permanent wave', 'pop'\",\n",
    "         \"'reggae fusion'\",\n",
    "         \"'dance pop', 'piano rock', 'pop', 'pop rock'\",\n",
    "         \"'modern alternative rock'\",\n",
    "         \"'alternative r\\u0026b', 'indie soul'\",\n",
    "         \"'future garage', 'uk bass'\",\n",
    "         \"'dance pop', 'edm', 'electropop', 'indietronica', 'metropopolis', 'pop', 'uk pop'\",\n",
    "         \"'dance pop', 'piano rock', 'pop', 'pop rock'\",\n",
    "         \"'dance pop', 'pop', 'pop rap', 'urban contemporary'\",\n",
    "         \"'dance pop', 'pop', 'pop rap', 'post-teen pop'\",\n",
    "         \"'dance pop', 'pop', 'uk pop'\",\n",
    "         \"'australian dance', 'australian pop', 'pop'\",\n",
    "         \"'australian dance', 'australian pop', 'pop'\",\n",
    "         \"'dance pop', 'neo mellow', 'pop', 'pop rock', 'post-teen pop', 'viral pop'\",\n",
    "         \"'alternative r\\u0026b', 'dance pop', 'pop', 'r\\u0026b', 'urban contemporary'\",\n",
    "         \"'alternative r\\u0026b', 'dance pop', 'pop', 'r\\u0026b', 'urban contemporary'\",\n",
    "         \"'dance pop', 'pop', 'pop rap', 'post-teen pop'\",\n",
    "         \"'alternative r\\u0026b', 'hip hop', 'lgbtq+ hip hop', 'neo soul', 'pop'\",\n",
    "         \"'dance pop', 'pop', 'post-teen pop'\",\n",
    "         \"'dance pop', 'pop', 'post-teen pop'\",\n",
    "         \"'latin', 'latin hip hop', 'reggaeton', 'trap latino'\",\n",
    "         \"'dance pop', 'pop', 'post-teen pop'\",\n",
    "         \"'dance pop', 'electropop', 'pop', 'post-teen pop', 'viral pop'\",\n",
    "         \"'atl hip hop', 'contemporary r\\u0026b', 'dance pop', 'pop', 'r\\u0026b', 'south carolina hip hop', 'urban contemporary'\",\n",
    "         \"'atl hip hop', 'contemporary r\\u0026b', 'dance pop', 'pop', 'r\\u0026b', 'south carolina hip hop', 'urban contemporary'\",\n",
    "         \"'pop', 'pop rock'\",\n",
    "         \"'dance pop', 'electropop'\",\n",
    "         \"\",\n",
    "         \"'big room', 'brostep', 'dance pop', 'dutch edm', 'edm', 'house', 'pop', 'pop dance', 'slap house', 'trance', 'tropical house'\",\n",
    "         \"'dance pop', 'edm', 'electro house', 'house', 'moombahton', 'ninja', 'pop', 'pop dance', 'tropical house'\",\n",
    "         \"'dance pop', 'edm', 'electro house', 'pop dance', 'pop rap', 'tropical house'\",\n",
    "         \"'dance pop', 'edm', 'electro house', 'pop dance', 'pop rap', 'tropical house'\",\n",
    "         \"'metalcore', 'nu-metalcore', 'trancecore'\",\n",
    "         \"'metalcore', 'pop punk', 'post-hardcore', 'progressive post-hardcore', 'screamo'\",\n",
    "         \"'dance pop', 'edm', 'pop', 'pop dance', 'tropical house', 'uk dance', 'uk funky'\",\n",
    "         \"'modern alternative rock', 'modern rock', 'neon pop punk', 'pop emo', 'pop punk', 'rock'\",\n",
    "         \"'drum and bass', 'edm', 'electro house', 'melodic dubstep'\",\n",
    "         \"'drum and bass', 'edm', 'electro house', 'melodic dubstep'\",\n",
    "         \"'drum and bass', 'edm', 'electro house', 'melodic dubstep'\",\n",
    "         \"'drum and bass', 'edm', 'electro house', 'melodic dubstep'\",\n",
    "         \"'latin', 'latin hip hop', 'reggaeton', 'trap latino'\",\n",
    "         \"'colombian pop', 'latin', 'latin pop', 'mexican pop', 'rock en espanol', 'tropical'\",\n",
    "         \"'bachata', 'latin', 'latin hip hop', 'latin pop'\",\n",
    "         \"'candy pop', 'pixie', 'pop emo', 'pop punk'\",\n",
    "         \"'electropop', 'pop edm'\",\n",
    "         \"'memphis hip hop', 'trap'\",\n",
    "         \"'alternative metal', 'funk metal', 'nu metal', 'pop rock', 'post-grunge', 'rap rock', 'rock'\",\n",
    "         \"'complextro', 'dance pop', 'edm', 'electro house', 'electropop', 'german techno', 'pop', 'pop dance', 'pop rap', 'tropical house'\",\n",
    "         \"'atl hip hop', 'hip hop', 'pop rap', 'rap', 'southern hip hop', 'trap'\",\n",
    "         \"'hip pop', 'miami hip hop'\",\n",
    "         \"'tropical house', 'uk contemporary r\\u0026b'\",\n",
    "         \"'pop rap'\",\n",
    "         \"'edm', 'house', 'pop', 'uk dance'\",\n",
    "         \"'dance pop', 'pop'\",\n",
    "         \"'art pop', 'british soul', 'dance pop', 'electropop', 'neo soul', 'pop', 'pop soul', 'tropical house'\",\n",
    "         \"'dance pop', 'pop', 'post-teen pop'\",\n",
    "         \"'british soul', 'indie r\\u0026b', 'neo soul'\",\n",
    "         \"'uk dance'\",\n",
    "         \"'folk-pop', 'modern rock', 'pop', 'pop rock'\",\n",
    "         \"'pop', 'uk pop'\",\n",
    "         \"'alternative r\\u0026b', 'hip hop', 'lgbtq+ hip hop', 'neo soul', 'pop'\",\n",
    "         \"'candy pop', 'neon pop punk', 'pixie', 'pop emo', 'pop punk'\",\n",
    "         \"'dance pop', 'pop'\",\n",
    "         \"'hip hop', 'pop', 'pop rap', 'r\\u0026b', 'southern hip hop', 'trap', 'trap soul'\",\n",
    "         \"'canadian contemporary r\\u0026b', 'canadian pop', 'pop'\",\n",
    "         \"'canadian contemporary r\\u0026b', 'canadian pop', 'pop'\",\n",
    "         \"'canadian contemporary r\\u0026b', 'canadian pop', 'pop'\",\n",
    "         \"'canadian contemporary r\\u0026b', 'canadian pop', 'pop'\",\n",
    "         \"'chicago rap', 'dance pop', 'pop', 'pop rap', 'r\\u0026b', 'southern hip hop', 'trap', 'urban contemporary'\",\n",
    "         \"'electropowerpop', 'neon pop punk', 'pop punk', 'screamo', 'trancecore'\",\n",
    "         \"'dance pop', 'pop', 'uk pop'\",\n",
    "         \"'dance pop', 'edm', 'electropop', 'pop', 'pop rap', 'post-teen pop', 'tropical house', 'uk pop'\",\n",
    "         \"'modern rock', 'permanent wave', 'rock'\",\n",
    "         \"'post-hardcore', 'rock', 'screamo'\",\n",
    "         \"'dance pop', 'pop', 'post-teen pop', 'viral pop'\",\n",
    "         \"'dubstep', 'edm', 'electro house', 'future bass', 'melodic dubstep', 'pop dance', 'pop edm', 'progressive trance'\",\n",
    "         \"'dubstep', 'edm', 'electro house', 'future bass', 'melodic dubstep', 'pop dance', 'pop edm', 'progressive trance'\",\n",
    "         \"'dubstep', 'edm', 'electro house', 'future bass', 'melodic dubstep', 'pop dance', 'pop edm', 'progressive trance'\",\n",
    "         \"'dubstep', 'edm', 'electro house', 'future bass', 'melodic dubstep', 'pop dance', 'pop edm', 'progressive trance'\",\n",
    "         \"'brostep', 'complextro', 'edm', 'electro', 'pop rap'\",\n",
    "         \"'modern rock'\",\n",
    "         \"'alt z', 'pop'\",\n",
    "         \"'dance pop', 'pop', 'pop rap', 'r\\u0026b', 'rap'\",\n",
    "         \"'brostep', 'complextro', 'edm', 'electro', 'pop rap'\",\n",
    "         \"'candy pop', 'neon pop punk', 'pixie', 'pop emo', 'pop punk'\",\n",
    "         \"\",\n",
    "         \"'edm', 'pop dance', 'progressive electro house', 'progressive house', 'progressive trance', 'trance', 'uplifting trance'\",\n",
    "         \"'canadian folk', 'canadian pop', 'electropop', 'indie poptimism'\",\n",
    "         \"'art pop', 'candy pop', 'dance pop', 'electropop', 'metropopolis', 'pop', 'uk pop'\",\n",
    "         \"'pop rap'\",\n",
    "         \"'atl hip hop', 'rap', 'southern hip hop', 'trap'\",\n",
    "         \"'candy pop', 'neon pop punk', 'pixie', 'pop emo', 'pop punk'\",\n",
    "         \"'candy pop', 'neon pop punk', 'pixie', 'pop emo', 'pop punk'\",\n",
    "         \"\",\n",
    "         \"'pop', 'pop rock'\",\n",
    "         \"'australian hip hop', 'dance pop', 'hip pop', 'pop', 'pop rap', 'post-teen pop'\",\n",
    "         \"'dance pop', 'edm', 'electropop', 'pop', 'pop dance', 'tropical house'\",\n",
    "         \"'dance pop', 'edm', 'electro house', 'house', 'moombahton', 'ninja', 'pop', 'pop dance', 'tropical house'\",\n",
    "         \"'dance pop', 'edm', 'electro house', 'house', 'moombahton', 'ninja', 'pop', 'pop dance', 'tropical house'\",\n",
    "         \"'bass trap', 'edm', 'electro house', 'electronic trap', 'pop dance'\",\n",
    "         \"'dance pop', 'edm', 'electro house', 'pop dance', 'pop rap', 'tropical house'\",\n",
    "         \"'dance pop', 'edm', 'electro house', 'house', 'moombahton', 'ninja', 'pop', 'pop dance', 'tropical house'\",\n",
    "         \"'dance pop', 'edm', 'electro house', 'house', 'moombahton', 'ninja', 'pop', 'pop dance', 'tropical house'\",\n",
    "         \"'dance pop', 'edm', 'electro house', 'house', 'moombahton', 'ninja', 'pop', 'pop dance', 'tropical house'\",\n",
    "         \"'dance pop', 'edm', 'electro house', 'house', 'moombahton', 'ninja', 'pop', 'pop dance', 'tropical house'\",\n",
    "         \"'dance pop', 'edm', 'electro house', 'house', 'moombahton', 'ninja', 'pop', 'pop dance', 'tropical house'\",\n",
    "         \"'dance pop', 'edm', 'electro house', 'pop dance', 'pop rap', 'tropical house'\",\n",
    "         \"'dance pop', 'edm', 'electro house', 'house', 'moombahton', 'ninja', 'pop', 'pop dance', 'tropical house'\",\n",
    "         \"'dance pop', 'edm', 'electro house', 'house', 'moombahton', 'ninja', 'pop', 'pop dance', 'tropical house'\",\n",
    "         \"'atl hip hop', 'hip hop', 'melodic rap', 'pop r\\u0026b', 'pop rap', 'r\\u0026b', 'rap', 'southern hip hop', 'trap'\",\n",
    "         \"'atl hip hop', 'hip hop', 'melodic rap', 'pop r\\u0026b', 'pop rap', 'r\\u0026b', 'rap', 'southern hip hop', 'trap'\",\n",
    "         \"'dance pop', 'pop', 'pop rap', 'post-teen pop'\",\n",
    "         \"'canadian hip hop', 'canadian pop', 'hip hop', 'rap', 'toronto rap'\",\n",
    "         \"'pop', 'pop rap', 'r\\u0026b', 'rap', 'toronto rap', 'trap', 'urban contemporary'\",\n",
    "         \"'big room', 'brostep', 'dance pop', 'dutch edm', 'edm', 'house', 'pop', 'pop dance', 'slap house', 'trance', 'tropical house'\",\n",
    "         \"'dance pop', 'pop', 'r\\u0026b'\",\n",
    "         \"'neo soul', 'pop', 'pop soul', 'r\\u0026b', 'urban contemporary'\",\n",
    "         \"'dance pop', 'hip pop', 'pop', 'pop rap', 'queens hip hop'\",\n",
    "         \"'dance pop', 'indie poptimism', 'pop rock'\",\n",
    "         \"'contemporary country', 'country', 'country road'\"\n",
    "      ]\n",
    "   }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "nAod1hbSn5yw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Using endpoint [https://us-central1-aiplatform.googleapis.com/]', 'CustomJob [projects/934903580331/locations/us-central1/customJobs/624465704850030592] is submitted successfully.', '', 'Your job is still active. You may view the status of your job with the command', '', '  $ gcloud ai custom-jobs describe projects/934903580331/locations/us-central1/customJobs/624465704850030592', '', 'or continue streaming the logs with the command', '', '  $ gcloud ai custom-jobs stream-logs projects/934903580331/locations/us-central1/customJobs/624465704850030592']\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "learning_job_name = f\"two_tower_gpu_{DATASET_NAME}_{TIMESTAMP}\"\n",
    "\n",
    "config = f\"\"\"workerPoolSpecs:\n",
    "  -\n",
    "    machineSpec:\n",
    "      machineType: n1-highmem-8\n",
    "      acceleratorType: NVIDIA_TESLA_K80\n",
    "      acceleratorCount: 1\n",
    "    replicaCount: 1\n",
    "    containerSpec:\n",
    "      imageUri: {LEARNER_IMAGE_URI}\n",
    "      args:\n",
    "      - --training_data_path={TRAINING_DATA_PATH}\n",
    "      - --input_schema_path={INPUT_SCHEMA_PATH}\n",
    "      - --job-dir={OUTPUT_DIR}\n",
    "\"\"\"\n",
    "\n",
    "!echo $'{config}' > ./config.yaml\n",
    "\n",
    "CREATION_LOG = ! gcloud ai custom-jobs create \\\n",
    "  --display-name={learning_job_name} \\\n",
    "  --region={REGION} \\\n",
    "  --config=config.yaml\n",
    "\n",
    "print(CREATION_LOG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "94tmU59YrKfe"
   },
   "source": [
    "If you want to use TFRecord input file format, you can try the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8wZbRgUhrLD0"
   },
   "outputs": [],
   "source": [
    "TRAINING_DATA_PATH = f\"gs://cloud-samples-data/vertex-ai/matching-engine/two-tower/{DATASET_NAME}/tfrecord/*\"\n",
    "\n",
    "learning_job_name = f\"two_tower_cpu_tfrecord_{DATASET_NAME}_{TIMESTAMP}\"\n",
    "\n",
    "CREATION_LOG = ! gcloud ai custom-jobs create \\\n",
    "  --display-name={learning_job_name} \\\n",
    "  --worker-pool-spec=machine-type=n1-standard-8,replica-count=1,container-image-uri={LEARNER_IMAGE_URI} \\\n",
    "  --region={REGION} \\\n",
    "  --args=--training_data_path={TRAINING_DATA_PATH} \\\n",
    "  --args=--input_schema_path={INPUT_SCHEMA_PATH} \\\n",
    "  --args=--job-dir={OUTPUT_DIR} \\\n",
    "  --args=--train_batch_size={TRAIN_BATCH_SIZE} \\\n",
    "  --args=--num_epochs={NUM_EPOCHS} \\\n",
    "  --args=--input_file_format=tfrecord\n",
    "\n",
    "print(CREATION_LOG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yceUSlyWrWes"
   },
   "source": [
    "After the job is submitted successfully, you can view its details and logs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "XXDC7F_8rXWM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8188402016507133952\n"
     ]
    }
   ],
   "source": [
    "JOB_ID = re.search(r\"(?<=/customJobs/)\\d+\", CREATION_LOG[1]).group(0)\n",
    "print(JOB_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "NkbwWCEMoQcy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state: JOB_STATE_FAILED\n"
     ]
    }
   ],
   "source": [
    "# View the job's configuration and state.\n",
    "STATE = \"state: JOB_STATE_PENDING\"\n",
    "\n",
    "while STATE not in [\"state: JOB_STATE_SUCCEEDED\", \"state: JOB_STATE_FAILED\"]:\n",
    "    DESCRIPTION = ! gcloud ai custom-jobs describe {JOB_ID} --region={REGION}\n",
    "    STATE = DESCRIPTION[-2]\n",
    "    print(STATE)\n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wgs0qV_Nr-RN"
   },
   "source": [
    "When the training starts, you can view the logs in TensorBoard. Colab users can use the TensorBoard widget below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8SweSrkhr_DP"
   },
   "outputs": [],
   "source": [
    "TENSORBOARD_DIR = os.path.join(OUTPUT_DIR, \"tensorboard\")\n",
    "%tensorboard --logdir {TENSORBOARD_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RzCrdxgAsGll"
   },
   "source": [
    "For Google CLoud Notebooks users, the TensorBoard widget above won't work. We recommend you to launch TensorBoard through the Cloud Shell.\n",
    "\n",
    "1. In your Cloud Shell, launch Tensorboard on port 8080:\n",
    "\n",
    "    ```\n",
    "    export TENSORBOARD_DIR=gs://xxxxx/tensorboard\n",
    "    tensorboard --logdir=${TENSORBOARD_DIR} --port=8080 --load_fast=false\n",
    "    ```\n",
    "\n",
    "2. Click the \"Web Preview\" button at the top-right of the Cloud Shell window (looks like an eye in a rectangle). \n",
    "\n",
    "3. Select \"Preview on port 8080\". This should launch the TensorBoard webpage in a new tab in your browser.\n",
    "\n",
    "After the job finishes successfully, you can view the output directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mFPLfY4gsK1V"
   },
   "outputs": [],
   "source": [
    "! gsutil ls {OUTPUT_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZhY0h8ijsPlP"
   },
   "source": [
    "## Deploy on Vertex Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6oquDjRgsS2V"
   },
   "source": [
    "### Import the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gidmXBWysaeP"
   },
   "source": [
    "Our training job will export two TF SavedModels under `gs://<job_dir>/query_model` and `gs://<job_dir>/candidate_model`. These exported models can be used for online or batch prediction in Vertex Prediction. First, import the query (or candidate) model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yEFd3Og_sbdm"
   },
   "outputs": [],
   "source": [
    "# The following imports the query (user) encoder model.\n",
    "MODEL_TYPE = \"query\"\n",
    "# Use the following instead to import the candidate (movie) encoder model.\n",
    "# MODEL_TYPE = 'candidate'\n",
    "\n",
    "DISPLAY_NAME = f\"{DATASET_NAME}_{MODEL_TYPE}\"  # The display name of the model.\n",
    "MODEL_NAME = f\"{MODEL_TYPE}_model\"  # Used by the deployment container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GCrhxf7GsdZS"
   },
   "outputs": [],
   "source": [
    "aiplatform.init(\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    "    staging_bucket=BUCKET_NAME,\n",
    ")\n",
    "\n",
    "model = aiplatform.Model.upload(\n",
    "    display_name=DISPLAY_NAME,\n",
    "    artifact_uri=OUTPUT_DIR,\n",
    "    serving_container_image_uri=\"us-central1-docker.pkg.dev/cloud-ml-algos/two-tower/deploy\",\n",
    "    serving_container_health_route=f\"/v1/models/{MODEL_NAME}\",\n",
    "    serving_container_predict_route=f\"/v1/models/{MODEL_NAME}:predict\",\n",
    "    serving_container_environment_variables={\n",
    "        \"MODEL_BASE_PATH\": \"$(AIP_STORAGE_URI)\",\n",
    "        \"MODEL_NAME\": MODEL_NAME,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0x-pZJzUsh22"
   },
   "source": [
    "### Deploy the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QJECrcTdskix"
   },
   "source": [
    "After importing the model, you must deploy it to an endpoint so that you can get online predictions. More information about this process can be found in the [official documentation](https://cloud.google.com/vertex-ai/docs/predictions/deploy-model-api)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jB3yT5xassCt"
   },
   "outputs": [],
   "source": [
    "! gcloud ai models list --region={REGION} --filter={DISPLAY_NAME}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zkJh2rmysu2M"
   },
   "source": [
    "Create a model endpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "er94Wp82sxYW"
   },
   "outputs": [],
   "source": [
    "endpoint = aiplatform.Endpoint.create(display_name=DATASET_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PICvm8PhqtMw"
   },
   "source": [
    "Deploy model to the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OUEF7Yces4uD"
   },
   "outputs": [],
   "source": [
    "model.deploy(\n",
    "    endpoint=endpoint,\n",
    "    machine_type=\"n1-standard-4\",\n",
    "    traffic_split={\"0\": 100},\n",
    "    deployed_model_display_name=DISPLAY_NAME,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y_LMW1rjtMM6"
   },
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QAkHJlY7tOmu"
   },
   "source": [
    "Now that you have deployed the query/candidate encoder model on Vertex Prediction, you can call the model to calculate embeddings for live data. There are two methods of getting predictions, online and batch, which are shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rwiJRfJRtQ1V"
   },
   "source": [
    "### Online prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "THz5Gn5ftTsm"
   },
   "source": [
    "[Online prediction](https://cloud.google.com/vertex-ai/docs/predictions/online-predictions-custom-models) is used to synchronously query a model on a small batch of instances with minimal latency. The following function calls the deployed Vertex Prediction model endpoint using Vertex SDK for Python:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bFVnzRzltfDa"
   },
   "source": [
    "The input data you want predictions on should be provided as a stringified JSON in the `data` field. Note that you should also provide a unique `key` field (of type str) for each input instance so that you can associate each output embedding with its corresponding input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E8Wt7wYgtg_f"
   },
   "outputs": [],
   "source": [
    "# Input items for the query model:\n",
    "input_items = [\n",
    "    {\"data\": '{\"user_id\": [\"1\"]}', \"key\": \"key1\"},\n",
    "    {\"data\": '{\"user_id\": [\"2\"]}', \"key\": \"key2\"},\n",
    "]\n",
    "\n",
    "# Input items for the candidate model:\n",
    "# input_items = [{\n",
    "#     'data' : '{\"movie_id\": [\"1\"], \"movie_title\": [\"fake title\"]}',\n",
    "#     'key': 'key1'\n",
    "# }]\n",
    "\n",
    "encodings = endpoint.predict(input_items)\n",
    "print(f\"Number of encodings: {len(encodings.predictions)}\")\n",
    "print(encodings.predictions[0][\"encoding\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1k-_XJzlthfP"
   },
   "source": [
    "You can also do online prediction using the gcloud CLI, as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tn5L9V0utkpA"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "request = json.dumps({\"instances\": input_items})\n",
    "with open(\"request.json\", \"w\") as writer:\n",
    "    writer.write(f\"{request}\\n\")\n",
    "\n",
    "ENDPOINT_ID = endpoint.resource_name\n",
    "\n",
    "! gcloud ai endpoints predict {ENDPOINT_ID} \\\n",
    "  --region={REGION} \\\n",
    "  --json-request=request.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ocLE_U6ftnA3"
   },
   "source": [
    "### Batch prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U__JdxfPto-_"
   },
   "source": [
    "[Batch prediction](https://cloud.google.com/vertex-ai/docs/predictions/batch-predictions) is used to asynchronously make predictions on a batch of input data.  This is recommended if you have a large input size and do not need an immediate response, such as getting embeddings for candidate objects in order to create an index for a nearest neighbor search service such as [Vertex Matching Engine](https://cloud.google.com/vertex-ai/docs/matching-engine/overview).\n",
    "\n",
    "The input data needs to be on Cloud Storage and in JSONL format. You can use the sample query object file provided below. Like with online prediction, it's recommended to have the `key` field so that you can associate each output embedding with its corresponding input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ar13RZ4VtquX"
   },
   "outputs": [],
   "source": [
    "QUERY_SAMPLE_PATH = f\"gs://cloud-samples-data/vertex-ai/matching-engine/two-tower/{DATASET_NAME}/query_sample.jsonl\"\n",
    "\n",
    "! gsutil cat {QUERY_SAMPLE_PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z4f5yEmatu8P"
   },
   "source": [
    "The following function calls the deployed Vertex Prediction model using the sample query object input file. Note that it uses the model resource directly and doesn't require a deployed endpoint. Once you start the job, you can track its status on the [Cloud Console](https://console.cloud.google.com/vertex-ai/batch-predictions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EYEOOPS8txYY"
   },
   "outputs": [],
   "source": [
    "model.batch_predict(\n",
    "    job_display_name=f\"batch_predict_{DISPLAY_NAME}\",\n",
    "    gcs_source=[QUERY_SAMPLE_PATH],\n",
    "    gcs_destination_prefix=OUTPUT_DIR,\n",
    "    machine_type=\"n1-standard-4\",\n",
    "    starting_replica_count=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zImnlP2Yt6Sv"
   },
   "source": [
    "## Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I2nRxtLTt8xn"
   },
   "source": [
    "After successfully training your model, deploying it, and calling it to make predictions, you may want to optimize the hyperparameters used during training to improve your model's accuracy and performance. See the Vertex AI documentation for an [overview of hyperparameter tuning](https://cloud.google.com/vertex-ai/docs/training/hyperparameter-tuning-overview) and [how to use it in your Vertex Training jobs](https://cloud.google.com/vertex-ai/docs/training/using-hyperparameter-tuning).\n",
    "\n",
    "For this example, the following command runs a Vertex AI hyperparameter tuning job with 8 trials that attempts to maximize the validation AUC metric. The hyperparameters it optimizes are the number of hidden layers, the size of the hidden layers, and the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z_ea5wfjt_XD"
   },
   "outputs": [],
   "source": [
    "PARALLEL_TRIAL_COUNT = 4\n",
    "MAX_TRIAL_COUNT = 8\n",
    "METRIC = \"val_auc\"\n",
    "hyper_tune_job_name = f\"hyper_tune_{DATASET_NAME}_{TIMESTAMP}\"\n",
    "\n",
    "config = json.dumps(\n",
    "    {\n",
    "        \"displayName\": hyper_tune_job_name,\n",
    "        \"studySpec\": {\n",
    "            \"metrics\": [{\"metricId\": METRIC, \"goal\": \"MAXIMIZE\"}],\n",
    "            \"parameters\": [\n",
    "                {\n",
    "                    \"parameterId\": \"num_hidden_layers\",\n",
    "                    \"scaleType\": \"UNIT_LINEAR_SCALE\",\n",
    "                    \"integerValueSpec\": {\"minValue\": 0, \"maxValue\": 2},\n",
    "                    \"conditionalParameterSpecs\": [\n",
    "                        {\n",
    "                            \"parameterSpec\": {\n",
    "                                \"parameterId\": \"num_nodes_hidden_layer1\",\n",
    "                                \"scaleType\": \"UNIT_LOG_SCALE\",\n",
    "                                \"integerValueSpec\": {\"minValue\": 1, \"maxValue\": 128},\n",
    "                            },\n",
    "                            \"parentIntValues\": {\"values\": [1, 2]},\n",
    "                        },\n",
    "                        {\n",
    "                            \"parameterSpec\": {\n",
    "                                \"parameterId\": \"num_nodes_hidden_layer2\",\n",
    "                                \"scaleType\": \"UNIT_LOG_SCALE\",\n",
    "                                \"integerValueSpec\": {\"minValue\": 1, \"maxValue\": 128},\n",
    "                            },\n",
    "                            \"parentIntValues\": {\"values\": [2]},\n",
    "                        },\n",
    "                    ],\n",
    "                },\n",
    "                {\n",
    "                    \"parameterId\": \"learning_rate\",\n",
    "                    \"scaleType\": \"UNIT_LOG_SCALE\",\n",
    "                    \"doubleValueSpec\": {\"minValue\": 0.0001, \"maxValue\": 1.0},\n",
    "                },\n",
    "            ],\n",
    "            \"algorithm\": \"ALGORITHM_UNSPECIFIED\",\n",
    "        },\n",
    "        \"maxTrialCount\": MAX_TRIAL_COUNT,\n",
    "        \"parallelTrialCount\": PARALLEL_TRIAL_COUNT,\n",
    "        \"maxFailedTrialCount\": 3,\n",
    "        \"trialJobSpec\": {\n",
    "            \"workerPoolSpecs\": [\n",
    "                {\n",
    "                    \"machineSpec\": {\n",
    "                        \"machineType\": \"n1-standard-4\",\n",
    "                    },\n",
    "                    \"replicaCount\": 1,\n",
    "                    \"containerSpec\": {\n",
    "                        \"imageUri\": LEARNER_IMAGE_URI,\n",
    "                        \"args\": [\n",
    "                            f\"--training_data_path={TRAINING_DATA_PATH}\",\n",
    "                            f\"--input_schema_path={INPUT_SCHEMA_PATH}\",\n",
    "                            f\"--job-dir={OUTPUT_DIR}\",\n",
    "                        ],\n",
    "                    },\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "! curl -X POST -H \"Authorization: Bearer \"$(gcloud auth print-access-token) \\\n",
    " -H \"Content-Type: application/json; charset=utf-8\"  \\\n",
    " -d '{config}' https://us-central1-aiplatform.googleapis.com/v1/projects/{PROJECT_ID}/locations/{REGION}/hyperparameterTuningJobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TpV-iwP9qw9c"
   },
   "source": [
    "## Cleaning up\n",
    "\n",
    "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
    "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
    "\n",
    "Otherwise, you can delete the individual resources you created in this tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sx_vKniMq9ZX"
   },
   "outputs": [],
   "source": [
    "# Delete endpoint resource\n",
    "endpoint.delete(force=True)\n",
    "\n",
    "# Delete model resource\n",
    "model.delete()\n",
    "\n",
    "# Delete Cloud Storage objects that were created\n",
    "! gsutil -m rm -r $OUTPUT_DIR"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "two-tower-model-introduction.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-9.m93",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-9:m93"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
