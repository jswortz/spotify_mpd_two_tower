{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2804db33-2adb-4845-a6bb-087f168eb6a5",
   "metadata": {},
   "source": [
    "# Build baseline tfrs model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9289ca69-cd66-4161-88b2-db58e9218b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = 'hybrid-vertex'  # <--- TODO: CHANGE THIS\n",
    "LOCATION = 'us-central1' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "059c0d02-8de1-446e-9b2c-7366dfff042d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow-recommenders==0.6.0 --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef3417e6-72a8-48e2-ba9d-8fd207ab6229",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import tensorflow as tf\n",
    "import tensorflow_recommenders as tfrs\n",
    "\n",
    "from google.cloud import storage\n",
    "\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5015b9c5-f6f8-46b3-998c-9ceb0ca65962",
   "metadata": {},
   "source": [
    "## Create Small Dataset for testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93457c26-5e92-4ddd-81db-824922476d73",
   "metadata": {},
   "source": [
    "### features and helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a94ba1a-db9e-4416-95f8-2e1f34575d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_features = {\n",
    "        'track_name_can': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    'artist_name_can': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    'album_name_can': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    'track_uri_can': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    'artist_uri_can': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    'album_uri_can': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    'duration_ms_can': tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "    'track_pop_can': tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "    'artist_pop_can': tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "    'artist_genres_can': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    'artist_followers_can': tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "471a3a84-c965-407e-abbe-471fb846bc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_feats = {\n",
    "    'track_name_can': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    'artist_name_can': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    'album_name_can': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    'track_uri_can': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    'artist_uri_can': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    'album_uri_can': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    'duration_ms_can': tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "    'track_pop_can': tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "    'artist_pop_can': tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "    'artist_genres_can': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    'artist_followers_can': tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "    # 'pos_seed_track': tf.io.FixedLenFeature(dtype=tf.int64, shape=()),\n",
    "    'track_name_seed_track': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    'artist_name_seed_track': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    'album_name_seed_track': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    'track_uri_seed_track': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    'artist_uri_seed_track': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    'album_uri_seed_track': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    'duration_seed_track': tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "    'track_pop_seed_track': tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "    'artist_pop_seed_track': tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "    'artist_genres_seed_track': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    'artist_followers_seed_track': tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "    # 'pid': tf.io.FixedLenFeature(dtype=tf.int64, shape=()),\n",
    "    'name': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    'collaborative': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "    # 'duration_ms_seed_pl': tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "    'n_songs_pl': tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "    'num_artists_pl': tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "    'num_albums_pl': tf.io.FixedLenFeature(dtype=tf.float32, shape=()),\n",
    "    'description_pl': tf.io.FixedLenFeature(dtype=tf.string, shape=()),\n",
    "}\n",
    "    ###ragged\n",
    "\n",
    "seq_feats = {\n",
    "    'track_name_pl': tf.io.RaggedFeature(tf.string),\n",
    "    'artist_name_pl': tf.io.RaggedFeature(tf.string),\n",
    "    'album_name_pl': tf.io.RaggedFeature(tf.string),\n",
    "    'track_uri_pl': tf.io.RaggedFeature(tf.string),\n",
    "    'duration_ms_songs_pl': tf.io.RaggedFeature(tf.float32),\n",
    "    'artist_pop_pl': tf.io.RaggedFeature(tf.float32),\n",
    "    'artists_followers_pl': tf.io.RaggedFeature(tf.float32),\n",
    "    'track_pop_pl': tf.io.RaggedFeature(tf.float32),\n",
    "    'artist_genres_pl': tf.io.RaggedFeature(tf.string),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984676ac-e3bc-4c34-9875-3113130e418b",
   "metadata": {},
   "source": [
    "### Playlist dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "10c78641-4f19-4033-af80-cd841b4f009b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## testing output\n",
    "from google.cloud import storage\n",
    "\n",
    "client = storage.Client()\n",
    "# # gs://spotify-beam-v3/v3/candidates/*.tfrecords\n",
    "\n",
    "BUCKET = 'spotify-beam-v3'\n",
    "CANDIDATE_PREFIX = 'v3/train/'\n",
    "\n",
    "valid_files = []\n",
    "for blob in client.list_blobs(f\"{BUCKET}\", prefix=f'{CANDIDATE_PREFIX}', delimiter=\"/\"):\n",
    "    valid_files.append(blob.public_url.replace(\"https://storage.googleapis.com/\", \"gs://\"))\n",
    "    \n",
    "raw_dataset = tf.data.TFRecordDataset(valid_files[:3])\n",
    "\n",
    "BUCKET = 'spotify-beam-v3'\n",
    "CANDIDATE_PREFIX = 'v3/valid/'\n",
    "\n",
    "valid_files = []\n",
    "for blob in client.list_blobs(f\"{BUCKET}\", prefix=f'{CANDIDATE_PREFIX}', delimiter=\"/\"):\n",
    "    valid_files.append(blob.public_url.replace(\"https://storage.googleapis.com/\", \"gs://\"))\n",
    "    \n",
    "raw_dataset_valid = tf.data.TFRecordDataset(valid_files[:3])\n",
    "\n",
    "def parse_tfrecord(example):\n",
    "    example = tf.io.parse_single_sequence_example(\n",
    "        example, \n",
    "        context_features=cont_feats,\n",
    "        sequence_features=seq_feats\n",
    "    )\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5023e7e2-b25f-4fa0-94db-af4901925d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "parsed_dataset = raw_dataset.map(parse_tfrecord)\n",
    "parsed_dataset_valid = raw_dataset_valid.map(parse_tfrecord)\n",
    "\n",
    "\n",
    "MAX_PLAYLIST_LENGTH = 375\n",
    "\n",
    "\n",
    "# gives: \n",
    "# array([[ 1,  2, -1, -1],\n",
    "#       [ 3,  4, -1, -1]], dtype=int32)\n",
    "\n",
    "def pad_up_to(t, max_in_dims=[1 ,MAX_PLAYLIST_LENGTH], constant_value=''):\n",
    "    s = tf.shape(t)\n",
    "    paddings = [[0, m-s[i]] for (i,m) in enumerate(max_in_dims)]\n",
    "    return tf.pad(t, paddings, 'CONSTANT', constant_values=constant_value)\n",
    "\n",
    "def return_padded_tensors(context, data):\n",
    "    \n",
    "        a = data['track_name_pl'].to_tensor(default_value='', shape=[None, MAX_PLAYLIST_LENGTH]), \n",
    "        b = data['artist_name_pl'].to_tensor(default_value='', shape=[None, MAX_PLAYLIST_LENGTH]), \n",
    "        c = data['album_name_pl'].to_tensor(default_value='', shape=[None, MAX_PLAYLIST_LENGTH]), \n",
    "        d = data['track_uri_pl'].to_tensor(default_value='', shape=[None, MAX_PLAYLIST_LENGTH]), \n",
    "        e = data['duration_ms_songs_pl'].to_tensor(default_value=-1., shape=[None, MAX_PLAYLIST_LENGTH]), \n",
    "        f = data['artist_pop_pl'].to_tensor(default_value=-1., shape=[None, MAX_PLAYLIST_LENGTH]), \n",
    "        g = data['artists_followers_pl'].to_tensor(default_value=-1., shape=[None, MAX_PLAYLIST_LENGTH]), \n",
    "        h = data['track_pop_pl'].to_tensor(default_value=-1., shape=[None, MAX_PLAYLIST_LENGTH]), \n",
    "        i = data['artist_genres_pl'].to_tensor(default_value='', shape=[None, MAX_PLAYLIST_LENGTH]), \n",
    "        \n",
    "        padded_data = context.copy()\n",
    "        padded_data['track_name_pl'] = a\n",
    "        padded_data['artist_name_pl'] = b\n",
    "        padded_data['album_name_pl'] = c\n",
    "        padded_data['track_uri_pl'] = d\n",
    "        padded_data['duration_ms_songs_pl'] = e\n",
    "        padded_data['artist_pop_pl'] = f\n",
    "        padded_data['artists_followers_pl'] = g\n",
    "        padded_data['track_pop_pl'] = h\n",
    "        padded_data['artist_genres_pl'] = i\n",
    "        \n",
    "        return padded_data\n",
    "parsed_dataset_padded = parsed_dataset.map(return_padded_tensors)   \n",
    "parsed_dataset_padded_valid = parsed_dataset_valid.map(return_padded_tensors)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2b420022-53c8-464e-8306-53b0faccbb20",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'album_name_can': <tf.Tensor: shape=(), dtype=string, numpy=b'Word Of Mouth'>,\n",
      " 'album_name_pl': (<tf.Tensor: shape=(1, 375), dtype=string, numpy=\n",
      "array([[b'Stranglehold', b'Roadhouse 01', b'A Song For Every Moon',\n",
      "        b'Uncomfortable - Single', b'A Song For Every Moon', b'Freudian',\n",
      "        b'Michl', b'Salt', b'blkswn', b'blkswn', b'blkswn', b'blkswn',\n",
      "        b'Blonde', b'Chanel', b'Kiddo', b'I Learnt Some Jazz Today',\n",
      "        b\"Say It Here, While It's Safe\",\n",
      "        b'Piece of Cake (feat. Harrison Sands and Shota)',\n",
      "        b'Land of Lights', b'Blonde', b'Nat Love', b'Pine & Ginger',\n",
      "        b'She Say', b'S!ck S!ck S!ck', b'Telefone', b'Nocturnal',\n",
      "        b'Hotel Allan', b'alarm (prod. no sentences)',\n",
      "        b'Buttermilk - EP', b'Restoration of an American Idol',\n",
      "        b'Into the Flame', b'The Thirst (feat. Shiz)', b'Urban Flora',\n",
      "        b'A Song For Every Moon', b\"Sex n' Drugs\", b'Thoughts of You',\n",
      "        b\"Searchin'\", b'Tell Me Why', b'Kiddo', b'Por Vida', b'grey',\n",
      "        b'Lotto (feat. Cousin Neighbor)', b'Fading', b'Lemons',\n",
      "        b'Midnight Moonlight EP', b'Waste?', b'Afterglow', b'Afterglow',\n",
      "        b'Black Girl Magic', b'digital druglord', b'Changed',\n",
      "        b'Open Arms', b'This Is What It\\xe2\\x80\\x99s Like',\n",
      "        b'Something To Believe In', b'DAMN.', b'Coloring Book', b'Riot',\n",
      "        b'Dead', b'Uzutrap', b'LANY', b'Under the Influence',\n",
      "        b'About Me', b'Good For You', b'By Myself', b'The Afterglow',\n",
      "        b'In The Lonely Hour', b\"When It's Dark Out\", b'130 Mood : TRBL',\n",
      "        b'Been Calling', b'Whose Mans Is This_', b'Sound of Sinning',\n",
      "        b'Menu (SINGLE)', b'Mornings (See You Again)', b'Real Love',\n",
      "        b'Provider', b'Summer Daze - EP', b'Found', b'H.E.R.', b'92',\n",
      "        b'North', b'Belong To U', b'Indigo', b'SEPT 5TH', b'', b'', b'',\n",
      "        b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'',\n",
      "        b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'',\n",
      "        b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'',\n",
      "        b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'',\n",
      "        b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'',\n",
      "        b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'',\n",
      "        b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'',\n",
      "        b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'',\n",
      "        b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'',\n",
      "        b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'',\n",
      "        b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'',\n",
      "        b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'',\n",
      "        b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'',\n",
      "        b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'',\n",
      "        b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'',\n",
      "        b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'',\n",
      "        b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'',\n",
      "        b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'',\n",
      "        b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'',\n",
      "        b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'',\n",
      "        b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'',\n",
      "        b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'',\n",
      "        b'', b'', b'']], dtype=object)>,),\n",
      " 'album_name_seed_track': <tf.Tensor: shape=(), dtype=string, numpy=b'SEPT 5TH'>,\n",
      " 'album_uri_can': <tf.Tensor: shape=(), dtype=string, numpy=b'spotify:album:1urOknNCkViW2SDrfWOXo5'>,\n",
      " 'album_uri_seed_track': <tf.Tensor: shape=(), dtype=string, numpy=b'spotify:album:0jLynoED1FbV2Ky7vU6Pjc'>,\n",
      " 'artist_followers_can': <tf.Tensor: shape=(), dtype=float32, numpy=144490.0>,\n",
      " 'artist_followers_seed_track': <tf.Tensor: shape=(), dtype=float32, numpy=1051451.0>,\n",
      " 'artist_genres_can': <tf.Tensor: shape=(), dtype=string, numpy=b\"'canadian contemporary r&b', 'modern alternative rock'\">,\n",
      " 'artist_genres_pl': (<tf.Tensor: shape=(1, 375), dtype=string, numpy=\n",
      "array([[b\"'alternative r&b', 'electropop', 'nyc pop', 'pittsburgh indie'\",\n",
      "        b\"'canadian contemporary r&b', 'modern alternative rock'\",\n",
      "        b\"'pop'\", b'NONE', b\"'pop'\",\n",
      "        b\"'canadian contemporary r&b', 'pop', 'r&b'\", b'NONE',\n",
      "        b\"'alternative r&b', 'portland hip hop'\",\n",
      "        b\"'alternative r&b', 'hip hop', 'rap', 'underground hip hop'\",\n",
      "        b\"'alternative r&b', 'hip hop', 'rap', 'underground hip hop'\",\n",
      "        b\"'alternative r&b', 'hip hop', 'rap', 'underground hip hop'\",\n",
      "        b\"'alternative r&b', 'hip hop', 'rap', 'underground hip hop'\",\n",
      "        b\"'alternative r&b', 'hip hop', 'lgbtq+ hip hop', 'neo soul', 'pop'\",\n",
      "        b\"'alternative r&b', 'hip hop', 'lgbtq+ hip hop', 'neo soul', 'pop'\",\n",
      "        b\"'canadian contemporary r&b', 'canadian pop', 'dance pop', 'pop', 'r&b'\",\n",
      "        b\"'modern reggae'\", b'NONE',\n",
      "        b\"'desi hip hop', 'lo-fi rap', 'pop rap'\", b'NONE',\n",
      "        b\"'alternative r&b', 'hip hop', 'lgbtq+ hip hop', 'neo soul', 'pop'\",\n",
      "        b'NONE', b'NONE', b\"'modern reggae'\",\n",
      "        b\"'alternative r&b', 'hip hop', 'rap', 'underground hip hop'\",\n",
      "        b\"'alternative r&b', 'chicago rap', 'escape room', 'hip hop', 'indie soul', 'underground hip hop'\",\n",
      "        b\"'canadian contemporary r&b', 'canadian hip hop', 'pop rap', 'r&b', 'trap', 'trap soul'\",\n",
      "        b\"'canadian contemporary r&b', 'modern alternative rock'\",\n",
      "        b\"'lo-fi rap'\", b\"'lo-fi rap'\",\n",
      "        b\"'atl hip hop', 'pop rap', 'rap', 'southern hip hop', 'trap', 'underground hip hop'\",\n",
      "        b'NONE', b'NONE',\n",
      "        b\"'alternative r&b', 'electropop', 'etherpop', 'pop', 'r&b'\",\n",
      "        b\"'pop'\", b\"'desi hip hop', 'lo-fi rap', 'pop rap'\", b'NONE',\n",
      "        b\"'modern reggae'\", b\"'vapor twitch'\",\n",
      "        b\"'canadian contemporary r&b', 'canadian pop', 'dance pop', 'pop', 'r&b'\",\n",
      "        b\"'colombian pop', 'dance pop', 'pop', 'r&b'\", b'NONE',\n",
      "        b\"'christian hip hop', 'christian trap'\",\n",
      "        b\"'christian hip hop', 'christian trap'\", b'NONE',\n",
      "        b\"'alternative r&b', 'chill r&b', 'escape room', 'indie r&b', 'indie soul', 'neo soul', 'r&b'\",\n",
      "        b'NONE',\n",
      "        b\"'icelandic folk', 'icelandic indie', 'icelandic pop', 'indie folk'\",\n",
      "        b\"'icelandic folk', 'icelandic indie', 'icelandic pop', 'indie folk'\",\n",
      "        b\"'uk alternative hip hop'\", b\"'electropop', 'pop', 'pop rap'\",\n",
      "        b\"'alt z', 'canadian contemporary r&b', 'pop'\",\n",
      "        b\"'alternative r&b', 'electropop', 'indie electropop', 'indie poptimism', 'indie soul', 'vapor soul', 'vapor twitch'\",\n",
      "        b\"'alt z', 'electropop', 'indie electropop', 'indie poptimism', 'metropopolis'\",\n",
      "        b\"'modern alternative rock', 'modern rock', 'pop rock', 'rock', 'stomp and holler'\",\n",
      "        b\"'conscious hip hop', 'hip hop', 'rap', 'west coast rap'\",\n",
      "        b\"'chicago rap', 'conscious hip hop', 'hip hop', 'pop rap', 'rap'\",\n",
      "        b\"'memphis hip hop'\",\n",
      "        b\"'alt z', 'dance pop', 'electropop', 'pop', 'post-teen pop'\",\n",
      "        b'NONE', b\"'la pop', 'pop'\", b'NONE', b\"'indie electropop'\",\n",
      "        b\"'hip hop', 'pop', 'portland hip hop', 'rap', 'underground hip hop'\",\n",
      "        b\"'alt z', 'electropop', 'pop'\",\n",
      "        b\"'electropop', 'pop', 'pop rap'\",\n",
      "        b\"'dance pop', 'pop', 'uk pop'\",\n",
      "        b\"'indie pop rap', 'oakland hip hop', 'pop', 'pop rap', 'rap'\",\n",
      "        b\"'k-pop', 'korean r&b'\",\n",
      "        b\"'afro dancehall', 'afropop', 'azontobeats', 'nigerian pop', 'swedish dancehall', 'uk dancehall'\",\n",
      "        b'NONE',\n",
      "        b\"'afrobeat', 'bay area indie', 'funk', 'instrumental funk', 'psychedelic soul', 'soul'\",\n",
      "        b\"'alternative r&b', 'hip hop', 'rap', 'underground hip hop'\",\n",
      "        b\"'chill r&b'\", b'NONE',\n",
      "        b\"'alternative r&b', 'hip hop', 'lgbtq+ hip hop', 'neo soul', 'pop'\",\n",
      "        b\"'vapor soul'\", b\"'uk contemporary r&b'\",\n",
      "        b\"'alternative r&b', 'dance pop', 'pop', 'r&b'\",\n",
      "        b\"'dutch hip hop', 'dutch rap pop'\",\n",
      "        b\"'abstract beats', 'alternative r&b', 'indie r&b', 'indie soul', 'vapor twitch'\",\n",
      "        b\"'pop edm', 'progressive electro house'\",\n",
      "        b\"'alternative r&b', 'indie r&b', 'indie soul'\",\n",
      "        b\"'alternative r&b', 'canadian contemporary r&b', 'indie r&b', 'pop', 'pop r&b', 'pop rap', 'r&b', 'trap soul'\",\n",
      "        b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'',\n",
      "        b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'',\n",
      "        b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'',\n",
      "        b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'',\n",
      "        b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'',\n",
      "        b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'',\n",
      "        b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'',\n",
      "        b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'',\n",
      "        b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'',\n",
      "        b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'',\n",
      "        b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'',\n",
      "        b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'',\n",
      "        b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'',\n",
      "        b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'',\n",
      "        b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'',\n",
      "        b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'',\n",
      "        b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'',\n",
      "        b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'',\n",
      "        b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'',\n",
      "        b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'',\n",
      "        b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'',\n",
      "        b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'',\n",
      "        b'', b'', b'', b'', b'', b'']], dtype=object)>,),\n",
      " 'artist_genres_seed_track': <tf.Tensor: shape=(), dtype=string, numpy=b\"'alternative r&b', 'canadian contemporary r&b', 'indie r&b', 'pop', 'pop r&b', 'pop rap', 'r&b', 'trap soul'\">,\n",
      " 'artist_name_can': <tf.Tensor: shape=(), dtype=string, numpy=b'Allan Rayman'>,\n",
      " 'artist_name_pl': (<tf.Tensor: shape=(1, 375), dtype=string, numpy=\n",
      "array([[b'Kevin Garrett', b'Allan Rayman', b'Bruno Major', b'Mansa',\n",
      "        b'Bruno Major', b'Daniel Caesar', b'Michl', b'Shy Girls',\n",
      "        b'Smino', b'Smino', b'Smino', b'Smino', b'Frank Ocean',\n",
      "        b'Frank Ocean', b'Jessie Reyez', b'Tessellated',\n",
      "        b'Kweku Collins', b'Abhi The Nomad', b'Eventide', b'Frank Ocean',\n",
      "        b'Kweku Collins', b'Amindi K. Fro$t', b'Tessellated', b'Smino',\n",
      "        b'Noname', b'Roy Woods', b'Allan Rayman', b'atlas', b'Biskwiq',\n",
      "        b'Mike WiLL Made-It', b'Tay', b'Tay', b'Alina Baraz',\n",
      "        b'Bruno Major', b'Abhi The Nomad', b'William Bolton',\n",
      "        b'Tessellated', b'vbnd', b'Jessie Reyez', b'Kali Uchis',\n",
      "        b'Kweku Collins', b'JGivens', b'Tragic Hero', b'dylAn',\n",
      "        b'Ravyn Lenae', b'Michl', b'\\xc3\\x81sgeir', b'\\xc3\\x81sgeir',\n",
      "        b'Che Lingo', b'blackbear', b'JP Saxe', b'RKCB', b'Glades',\n",
      "        b'Young the Giant', b'Kendrick Lamar', b'Chance The Rapper',\n",
      "        b'Jon Waltz', b'Madison Beer', b'J. Han', b'LANY', b'Majik',\n",
      "        b'Elias Abid', b'Amin\\xc3\\xa9', b'Christian French',\n",
      "        b'blackbear', b'Sam Smith', b'G-Eazy', b'DEAN', b'Maleek Berry',\n",
      "        b'Charlie Powers', b'Monophonics', b'Smino', b'Alextbh',\n",
      "        b'Cortes', b'Frank Ocean', b'Cool Company', b'Seramic',\n",
      "        b'H.E.R.', b'SRNO', b'Sango', b'Fancy Cars', b'River Tiber',\n",
      "        b'dvsn', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'',\n",
      "        b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'',\n",
      "        b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'',\n",
      "        b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'',\n",
      "        b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'',\n",
      "        b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'',\n",
      "        b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'',\n",
      "        b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'',\n",
      "        b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'',\n",
      "        b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'',\n",
      "        b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'',\n",
      "        b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'',\n",
      "        b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'',\n",
      "        b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'',\n",
      "        b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'',\n",
      "        b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'',\n",
      "        b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'',\n",
      "        b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'',\n",
      "        b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'',\n",
      "        b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'',\n",
      "        b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'',\n",
      "        b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'',\n",
      "        b'', b'', b'', b'', b'', b'', b'', b'']], dtype=object)>,),\n",
      " 'artist_name_seed_track': <tf.Tensor: shape=(), dtype=string, numpy=b'dvsn'>,\n",
      " 'artist_pop_can': <tf.Tensor: shape=(), dtype=float32, numpy=57.0>,\n",
      " 'artist_pop_pl': (<tf.Tensor: shape=(1, 375), dtype=float32, numpy=\n",
      "array([[56., 57., 76., 49., 76., 82., 53., 46., 73., 73., 73., 73., 87.,\n",
      "        87., 72., 42., 42., 61., 32., 87., 42.,  0., 42., 73., 61., 67.,\n",
      "        57., 55., 59., 71.,  4.,  4., 70., 76., 61., 47., 42., 46., 72.,\n",
      "        85., 42., 31., 26., 30., 59., 53., 58., 58., 50., 85., 73., 58.,\n",
      "        51., 70., 91., 80., 41., 78., 20., 79., 34., 25., 76., 65., 85.,\n",
      "        86., 81., 67., 60., 31., 51., 73., 47.,  0., 87., 51., 26., 81.,\n",
      "        69., 55., 43., 51., 65., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.]],\n",
      "      dtype=float32)>,),\n",
      " 'artist_pop_seed_track': <tf.Tensor: shape=(), dtype=float32, numpy=65.0>,\n",
      " 'artist_uri_can': <tf.Tensor: shape=(), dtype=string, numpy=b'spotify:artist:6Yv6OBXD6ZQakEljaGaDAk'>,\n",
      " 'artist_uri_seed_track': <tf.Tensor: shape=(), dtype=string, numpy=b'spotify:artist:7e1ICztHM2Sc4JNLxeMXYl'>,\n",
      " 'artists_followers_pl': (<tf.Tensor: shape=(1, 375), dtype=float32, numpy=\n",
      "array([[ 1.2562500e+05,  1.4449000e+05,  8.2950100e+05,  8.6620000e+03,\n",
      "         8.2950100e+05,  3.1571950e+06,  1.5782400e+05,  5.8059000e+04,\n",
      "         6.2357700e+05,  6.2357700e+05,  6.2357700e+05,  6.2357700e+05,\n",
      "         9.1429110e+06,  9.1429110e+06,  1.0747770e+06,  1.7105000e+04,\n",
      "         4.9978000e+04,  1.0601400e+05,  7.9240000e+03,  9.1429110e+06,\n",
      "         4.9978000e+04,  2.8700000e+02,  1.7105000e+04,  6.2357700e+05,\n",
      "         6.0877000e+05,  8.4721000e+05,  1.4449000e+05,  1.6107500e+05,\n",
      "         5.6810000e+04,  1.3711760e+06,  5.2000000e+02,  5.2000000e+02,\n",
      "         1.0617970e+06,  8.2950100e+05,  1.0601400e+05,  1.9342000e+04,\n",
      "         1.7105000e+04,  3.2877000e+04,  1.0747770e+06,  2.7112680e+06,\n",
      "         4.9978000e+04,  2.2799000e+04,  7.1650000e+03,  4.6300000e+03,\n",
      "         2.8826700e+05,  1.5782400e+05,  2.7211500e+05,  2.7211500e+05,\n",
      "         1.8223000e+04,  4.6268480e+06,  3.3516400e+05,  1.0685900e+05,\n",
      "         8.1725000e+04,  1.3825310e+06,  1.9595648e+07,  5.5914660e+06,\n",
      "         9.4470000e+03,  4.6326380e+06,  3.1460000e+03,  4.4885300e+06,\n",
      "         1.2757000e+04,  8.9200000e+02,  1.7566430e+06,  2.4156600e+05,\n",
      "         4.6268480e+06,  1.9080876e+07,  5.0360250e+06,  1.2394940e+06,\n",
      "         3.3200000e+05,  3.2060000e+03,  7.8842000e+04,  6.2357700e+05,\n",
      "         7.8490000e+04,  1.2800000e+02,  9.1429110e+06,  3.1248000e+04,\n",
      "         9.3470000e+03,  4.9476360e+06,  1.6605000e+04,  1.7391700e+05,\n",
      "         9.3180000e+03,  4.5205000e+04,  1.0514510e+06, -1.0000000e+00,\n",
      "        -1.0000000e+00, -1.0000000e+00, -1.0000000e+00, -1.0000000e+00,\n",
      "        -1.0000000e+00, -1.0000000e+00, -1.0000000e+00, -1.0000000e+00,\n",
      "        -1.0000000e+00, -1.0000000e+00, -1.0000000e+00, -1.0000000e+00,\n",
      "        -1.0000000e+00, -1.0000000e+00, -1.0000000e+00, -1.0000000e+00,\n",
      "        -1.0000000e+00, -1.0000000e+00, -1.0000000e+00, -1.0000000e+00,\n",
      "        -1.0000000e+00, -1.0000000e+00, -1.0000000e+00, -1.0000000e+00,\n",
      "        -1.0000000e+00, -1.0000000e+00, -1.0000000e+00, -1.0000000e+00,\n",
      "        -1.0000000e+00, -1.0000000e+00, -1.0000000e+00, -1.0000000e+00,\n",
      "        -1.0000000e+00, -1.0000000e+00, -1.0000000e+00, -1.0000000e+00,\n",
      "        -1.0000000e+00, -1.0000000e+00, -1.0000000e+00, -1.0000000e+00,\n",
      "        -1.0000000e+00, -1.0000000e+00, -1.0000000e+00, -1.0000000e+00,\n",
      "        -1.0000000e+00, -1.0000000e+00, -1.0000000e+00, -1.0000000e+00,\n",
      "        -1.0000000e+00, -1.0000000e+00, -1.0000000e+00, -1.0000000e+00,\n",
      "        -1.0000000e+00, -1.0000000e+00, -1.0000000e+00, -1.0000000e+00,\n",
      "        -1.0000000e+00, -1.0000000e+00, -1.0000000e+00, -1.0000000e+00,\n",
      "        -1.0000000e+00, -1.0000000e+00, -1.0000000e+00, -1.0000000e+00,\n",
      "        -1.0000000e+00, -1.0000000e+00, -1.0000000e+00, -1.0000000e+00,\n",
      "        -1.0000000e+00, -1.0000000e+00, -1.0000000e+00, -1.0000000e+00,\n",
      "        -1.0000000e+00, -1.0000000e+00, -1.0000000e+00, -1.0000000e+00,\n",
      "        -1.0000000e+00, -1.0000000e+00, -1.0000000e+00, -1.0000000e+00,\n",
      "        -1.0000000e+00, -1.0000000e+00, -1.0000000e+00, -1.0000000e+00,\n",
      "        -1.0000000e+00, -1.0000000e+00, -1.0000000e+00, -1.0000000e+00,\n",
      "        -1.0000000e+00, -1.0000000e+00, -1.0000000e+00, -1.0000000e+00,\n",
      "        -1.0000000e+00, -1.0000000e+00, -1.0000000e+00, -1.0000000e+00,\n",
      "        -1.0000000e+00, -1.0000000e+00, -1.0000000e+00, -1.0000000e+00,\n",
      "        -1.0000000e+00, -1.0000000e+00, -1.0000000e+00, -1.0000000e+00,\n",
      "        -1.0000000e+00, -1.0000000e+00, -1.0000000e+00, -1.0000000e+00,\n",
      "        -1.0000000e+00, -1.0000000e+00, -1.0000000e+00, -1.0000000e+00,\n",
      "        -1.0000000e+00, -1.0000000e+00, -1.0000000e+00, -1.0000000e+00,\n",
      "        -1.0000000e+00, -1.0000000e+00, -1.0000000e+00, -1.0000000e+00,\n",
      "        -1.0000000e+00, -1.0000000e+00, -1.0000000e+00, -1.0000000e+00,\n",
      "        -1.0000000e+00, -1.0000000e+00, -1.0000000e+00, -1.0000000e+00,\n",
      "        -1.0000000e+00, -1.0000000e+00, -1.0000000e+00, -1.0000000e+00,\n",
      "        -1.0000000e+00, -1.0000000e+00, -1.0000000e+00, -1.0000000e+00,\n",
      "        -1.0000000e+00, -1.0000000e+00, -1.0000000e+00, -1.0000000e+00,\n",
      "        -1.0000000e+00, -1.0000000e+00, -1.0000000e+00, -1.0000000e+00,\n",
      "        -1.0000000e+00, -1.0000000e+00, -1.0000000e+00, -1.0000000e+00,\n",
      "        -1.0000000e+00, -1.0000000e+00, -1.0000000e+00, -1.0000000e+00,\n",
      "        -1.0000000e+00, -1.0000000e+00, -1.0000000e+00, -1.0000000e+00,\n",
      "        -1.0000000e+00, -1.0000000e+00, -1.0000000e+00, -1.0000000e+00,\n",
      "        -1.0000000e+00, -1.0000000e+00, -1.0000000e+00, -1.0000000e+00,\n",
      "        -1.0000000e+00, -1.0000000e+00, -1.0000000e+00, -1.0000000e+00,\n",
      "        -1.0000000e+00, -1.0000000e+00, -1.0000000e+00, -1.0000000e+00,\n",
      "        -1.0000000e+00, -1.0000000e+00, -1.0000000e+00, -1.0000000e+00,\n",
      "        -1.0000000e+00, -1.0000000e+00, -1.0000000e+00, -1.0000000e+00,\n",
      "        -1.0000000e+00, -1.0000000e+00, -1.0000000e+00, -1.0000000e+00,\n",
      "        -1.0000000e+00, -1.0000000e+00, -1.0000000e+00, -1.0000000e+00,\n",
      "        -1.0000000e+00, -1.0000000e+00, -1.0000000e+00, -1.0000000e+00,\n",
      "        -1.0000000e+00, -1.0000000e+00, -1.0000000e+00, -1.0000000e+00,\n",
      "        -1.0000000e+00, -1.0000000e+00, -1.0000000e+00, -1.0000000e+00,\n",
      "        -1.0000000e+00, -1.0000000e+00, -1.0000000e+00, -1.0000000e+00,\n",
      "        -1.0000000e+00, -1.0000000e+00, -1.0000000e+00, -1.0000000e+00,\n",
      "        -1.0000000e+00, -1.0000000e+00, -1.0000000e+00, -1.0000000e+00,\n",
      "        -1.0000000e+00, -1.0000000e+00, -1.0000000e+00, -1.0000000e+00,\n",
      "        -1.0000000e+00, -1.0000000e+00, -1.0000000e+00, -1.0000000e+00,\n",
      "        -1.0000000e+00, -1.0000000e+00, -1.0000000e+00, -1.0000000e+00,\n",
      "        -1.0000000e+00, -1.0000000e+00, -1.0000000e+00, -1.0000000e+00,\n",
      "        -1.0000000e+00, -1.0000000e+00, -1.0000000e+00, -1.0000000e+00,\n",
      "        -1.0000000e+00, -1.0000000e+00, -1.0000000e+00, -1.0000000e+00,\n",
      "        -1.0000000e+00, -1.0000000e+00, -1.0000000e+00, -1.0000000e+00,\n",
      "        -1.0000000e+00, -1.0000000e+00, -1.0000000e+00, -1.0000000e+00,\n",
      "        -1.0000000e+00, -1.0000000e+00, -1.0000000e+00, -1.0000000e+00,\n",
      "        -1.0000000e+00, -1.0000000e+00, -1.0000000e+00, -1.0000000e+00,\n",
      "        -1.0000000e+00, -1.0000000e+00, -1.0000000e+00, -1.0000000e+00,\n",
      "        -1.0000000e+00, -1.0000000e+00, -1.0000000e+00, -1.0000000e+00,\n",
      "        -1.0000000e+00, -1.0000000e+00, -1.0000000e+00, -1.0000000e+00,\n",
      "        -1.0000000e+00, -1.0000000e+00, -1.0000000e+00, -1.0000000e+00,\n",
      "        -1.0000000e+00, -1.0000000e+00, -1.0000000e+00, -1.0000000e+00,\n",
      "        -1.0000000e+00, -1.0000000e+00, -1.0000000e+00, -1.0000000e+00,\n",
      "        -1.0000000e+00, -1.0000000e+00, -1.0000000e+00, -1.0000000e+00,\n",
      "        -1.0000000e+00, -1.0000000e+00, -1.0000000e+00, -1.0000000e+00,\n",
      "        -1.0000000e+00, -1.0000000e+00, -1.0000000e+00, -1.0000000e+00,\n",
      "        -1.0000000e+00, -1.0000000e+00, -1.0000000e+00]], dtype=float32)>,),\n",
      " 'collaborative': <tf.Tensor: shape=(), dtype=string, numpy=b'false'>,\n",
      " 'description_pl': <tf.Tensor: shape=(), dtype=string, numpy=b''>,\n",
      " 'duration_ms_can': <tf.Tensor: shape=(), dtype=float32, numpy=179533.0>,\n",
      " 'duration_ms_songs_pl': (<tf.Tensor: shape=(1, 375), dtype=float32, numpy=\n",
      "array([[ 2.01360e+05,  2.25880e+05,  2.10240e+05,  4.00615e+05,\n",
      "         1.74167e+05,  2.78179e+05,  2.08214e+05,  2.30720e+05,\n",
      "         1.88209e+05,  1.69319e+05,  2.05199e+05,  1.74805e+05,\n",
      "         3.07151e+05,  2.10285e+05,  2.40906e+05,  1.06909e+05,\n",
      "         2.05984e+05,  2.16923e+05,  2.77590e+05,  1.84516e+05,\n",
      "         2.38546e+05,  1.89230e+05,  2.16000e+05,  2.64125e+05,\n",
      "         2.18627e+05,  2.01290e+05,  1.58186e+05,  1.38239e+05,\n",
      "         1.05600e+05,  2.34022e+05,  1.71000e+05,  1.93142e+05,\n",
      "         2.76436e+05,  1.55616e+05,  2.17777e+05,  2.09970e+05,\n",
      "         1.50000e+05,  1.31677e+05,  1.85240e+05,  2.12000e+05,\n",
      "         2.61955e+05,  2.12371e+05,  2.44500e+05,  1.05128e+05,\n",
      "         2.19648e+05,  1.97323e+05,  2.47120e+05,  3.07826e+05,\n",
      "         2.10000e+05,  1.68428e+05,  2.79269e+05,  2.28965e+05,\n",
      "         1.84425e+05,  2.24400e+05,  2.75253e+05,  2.90316e+05,\n",
      "         2.14153e+05,  1.94826e+05,  2.33532e+05,  2.34818e+05,\n",
      "         2.14727e+05,  2.25207e+05,  2.20160e+05,  1.84624e+05,\n",
      "         2.17026e+05,  1.84748e+05,  2.22466e+05,  2.29359e+05,\n",
      "         1.96800e+05,  1.96946e+05,  2.00600e+05,  1.99040e+05,\n",
      "         2.13097e+05,  1.89048e+05,  2.43238e+05,  1.52500e+05,\n",
      "         1.86195e+05,  2.09400e+05,  1.72042e+05,  2.22000e+05,\n",
      "         2.02574e+05,  1.52250e+05,  2.46293e+05, -1.00000e+00,\n",
      "        -1.00000e+00, -1.00000e+00, -1.00000e+00, -1.00000e+00,\n",
      "        -1.00000e+00, -1.00000e+00, -1.00000e+00, -1.00000e+00,\n",
      "        -1.00000e+00, -1.00000e+00, -1.00000e+00, -1.00000e+00,\n",
      "        -1.00000e+00, -1.00000e+00, -1.00000e+00, -1.00000e+00,\n",
      "        -1.00000e+00, -1.00000e+00, -1.00000e+00, -1.00000e+00,\n",
      "        -1.00000e+00, -1.00000e+00, -1.00000e+00, -1.00000e+00,\n",
      "        -1.00000e+00, -1.00000e+00, -1.00000e+00, -1.00000e+00,\n",
      "        -1.00000e+00, -1.00000e+00, -1.00000e+00, -1.00000e+00,\n",
      "        -1.00000e+00, -1.00000e+00, -1.00000e+00, -1.00000e+00,\n",
      "        -1.00000e+00, -1.00000e+00, -1.00000e+00, -1.00000e+00,\n",
      "        -1.00000e+00, -1.00000e+00, -1.00000e+00, -1.00000e+00,\n",
      "        -1.00000e+00, -1.00000e+00, -1.00000e+00, -1.00000e+00,\n",
      "        -1.00000e+00, -1.00000e+00, -1.00000e+00, -1.00000e+00,\n",
      "        -1.00000e+00, -1.00000e+00, -1.00000e+00, -1.00000e+00,\n",
      "        -1.00000e+00, -1.00000e+00, -1.00000e+00, -1.00000e+00,\n",
      "        -1.00000e+00, -1.00000e+00, -1.00000e+00, -1.00000e+00,\n",
      "        -1.00000e+00, -1.00000e+00, -1.00000e+00, -1.00000e+00,\n",
      "        -1.00000e+00, -1.00000e+00, -1.00000e+00, -1.00000e+00,\n",
      "        -1.00000e+00, -1.00000e+00, -1.00000e+00, -1.00000e+00,\n",
      "        -1.00000e+00, -1.00000e+00, -1.00000e+00, -1.00000e+00,\n",
      "        -1.00000e+00, -1.00000e+00, -1.00000e+00, -1.00000e+00,\n",
      "        -1.00000e+00, -1.00000e+00, -1.00000e+00, -1.00000e+00,\n",
      "        -1.00000e+00, -1.00000e+00, -1.00000e+00, -1.00000e+00,\n",
      "        -1.00000e+00, -1.00000e+00, -1.00000e+00, -1.00000e+00,\n",
      "        -1.00000e+00, -1.00000e+00, -1.00000e+00, -1.00000e+00,\n",
      "        -1.00000e+00, -1.00000e+00, -1.00000e+00, -1.00000e+00,\n",
      "        -1.00000e+00, -1.00000e+00, -1.00000e+00, -1.00000e+00,\n",
      "        -1.00000e+00, -1.00000e+00, -1.00000e+00, -1.00000e+00,\n",
      "        -1.00000e+00, -1.00000e+00, -1.00000e+00, -1.00000e+00,\n",
      "        -1.00000e+00, -1.00000e+00, -1.00000e+00, -1.00000e+00,\n",
      "        -1.00000e+00, -1.00000e+00, -1.00000e+00, -1.00000e+00,\n",
      "        -1.00000e+00, -1.00000e+00, -1.00000e+00, -1.00000e+00,\n",
      "        -1.00000e+00, -1.00000e+00, -1.00000e+00, -1.00000e+00,\n",
      "        -1.00000e+00, -1.00000e+00, -1.00000e+00, -1.00000e+00,\n",
      "        -1.00000e+00, -1.00000e+00, -1.00000e+00, -1.00000e+00,\n",
      "        -1.00000e+00, -1.00000e+00, -1.00000e+00, -1.00000e+00,\n",
      "        -1.00000e+00, -1.00000e+00, -1.00000e+00, -1.00000e+00,\n",
      "        -1.00000e+00, -1.00000e+00, -1.00000e+00, -1.00000e+00,\n",
      "        -1.00000e+00, -1.00000e+00, -1.00000e+00, -1.00000e+00,\n",
      "        -1.00000e+00, -1.00000e+00, -1.00000e+00, -1.00000e+00,\n",
      "        -1.00000e+00, -1.00000e+00, -1.00000e+00, -1.00000e+00,\n",
      "        -1.00000e+00, -1.00000e+00, -1.00000e+00, -1.00000e+00,\n",
      "        -1.00000e+00, -1.00000e+00, -1.00000e+00, -1.00000e+00,\n",
      "        -1.00000e+00, -1.00000e+00, -1.00000e+00, -1.00000e+00,\n",
      "        -1.00000e+00, -1.00000e+00, -1.00000e+00, -1.00000e+00,\n",
      "        -1.00000e+00, -1.00000e+00, -1.00000e+00, -1.00000e+00,\n",
      "        -1.00000e+00, -1.00000e+00, -1.00000e+00, -1.00000e+00,\n",
      "        -1.00000e+00, -1.00000e+00, -1.00000e+00, -1.00000e+00,\n",
      "        -1.00000e+00, -1.00000e+00, -1.00000e+00, -1.00000e+00,\n",
      "        -1.00000e+00, -1.00000e+00, -1.00000e+00, -1.00000e+00,\n",
      "        -1.00000e+00, -1.00000e+00, -1.00000e+00, -1.00000e+00,\n",
      "        -1.00000e+00, -1.00000e+00, -1.00000e+00, -1.00000e+00,\n",
      "        -1.00000e+00, -1.00000e+00, -1.00000e+00, -1.00000e+00,\n",
      "        -1.00000e+00, -1.00000e+00, -1.00000e+00, -1.00000e+00,\n",
      "        -1.00000e+00, -1.00000e+00, -1.00000e+00, -1.00000e+00,\n",
      "        -1.00000e+00, -1.00000e+00, -1.00000e+00, -1.00000e+00,\n",
      "        -1.00000e+00, -1.00000e+00, -1.00000e+00, -1.00000e+00,\n",
      "        -1.00000e+00, -1.00000e+00, -1.00000e+00, -1.00000e+00,\n",
      "        -1.00000e+00, -1.00000e+00, -1.00000e+00, -1.00000e+00,\n",
      "        -1.00000e+00, -1.00000e+00, -1.00000e+00, -1.00000e+00,\n",
      "        -1.00000e+00, -1.00000e+00, -1.00000e+00, -1.00000e+00,\n",
      "        -1.00000e+00, -1.00000e+00, -1.00000e+00, -1.00000e+00,\n",
      "        -1.00000e+00, -1.00000e+00, -1.00000e+00, -1.00000e+00,\n",
      "        -1.00000e+00, -1.00000e+00, -1.00000e+00, -1.00000e+00,\n",
      "        -1.00000e+00, -1.00000e+00, -1.00000e+00, -1.00000e+00,\n",
      "        -1.00000e+00, -1.00000e+00, -1.00000e+00, -1.00000e+00,\n",
      "        -1.00000e+00, -1.00000e+00, -1.00000e+00, -1.00000e+00,\n",
      "        -1.00000e+00, -1.00000e+00, -1.00000e+00, -1.00000e+00,\n",
      "        -1.00000e+00, -1.00000e+00, -1.00000e+00, -1.00000e+00,\n",
      "        -1.00000e+00, -1.00000e+00, -1.00000e+00, -1.00000e+00,\n",
      "        -1.00000e+00, -1.00000e+00, -1.00000e+00, -1.00000e+00,\n",
      "        -1.00000e+00, -1.00000e+00, -1.00000e+00, -1.00000e+00,\n",
      "        -1.00000e+00, -1.00000e+00, -1.00000e+00]], dtype=float32)>,),\n",
      " 'duration_seed_track': <tf.Tensor: shape=(), dtype=float32, numpy=246293.0>,\n",
      " 'n_songs_pl': <tf.Tensor: shape=(), dtype=float32, numpy=83.0>,\n",
      " 'name': <tf.Tensor: shape=(), dtype=string, numpy=b'Vibezz'>,\n",
      " 'num_albums_pl': <tf.Tensor: shape=(), dtype=float32, numpy=75.0>,\n",
      " 'num_artists_pl': <tf.Tensor: shape=(), dtype=float32, numpy=62.0>,\n",
      " 'track_name_can': <tf.Tensor: shape=(), dtype=string, numpy=b'Word Of Mouth'>,\n",
      " 'track_name_pl': (<tf.Tensor: shape=(1, 375), dtype=string, numpy=\n",
      "array([[b'Stranglehold', b'Repeat', b'Easily', b'Uncomfortable',\n",
      "        b\"Wouldn't Mean A Thing\", b'Get You (feat. Kali Uchis)',\n",
      "        b'Die Trying', b'What If I Can', b'Glass Flows',\n",
      "        b'Wild Irish Roses', b'Maraca', b'Ricky Millions', b'Nights',\n",
      "        b'Chanel', b'Figures', b'I Learnt Some Jazz Today',\n",
      "        b'Lonely Lullabies',\n",
      "        b'Piece of Cake (feat. Harrison Sands and Shota)',\n",
      "        b'Land of Lights', b'Pink + White', b'Stupid Rose',\n",
      "        b'Pine & Ginger', b'She Say', b'Ruby Red',\n",
      "        b'Forever (feat. Joseph Chilliams & Ravyn Lenae)', b'Dangerous',\n",
      "        b'Song 512', b'alarm (prod. no sentences)',\n",
      "        b'Faucets (feat. Healy)',\n",
      "        b'Grown up Fairy Tales (feat. Chance the Rapper & Jeremih)',\n",
      "        b'Into the Flame', b'The Thirst (feat. Shiz)',\n",
      "        b'Pretty Thoughts - FKJ Remix', b'Second Time', b\"Sex n' Drugs\",\n",
      "        b'Thoughts of You', b\"Searchin'\", b'Tell Me Why', b'Fuck It',\n",
      "        b'Loner', b'Jump.i', b'Lotto (feat. Cousin Neighbor)', b'Fading',\n",
      "        b'Lemons', b'Spice', b'Waste?', b'Unbound', b'Fennir yfir',\n",
      "        b'Black Girl Magic',\n",
      "        b'hell is where i dreamt of u and woke up alone', b'Changed',\n",
      "        b'Open Arms', b'Drive // Stripped',\n",
      "        b'Something To Believe In - Live Acoustic', b'PRIDE.',\n",
      "        b'Summer Friends (feat. Jeremih & Francis & The Lights)',\n",
      "        b'Riot', b'Dead', b'Uzutrap', b'13', b'27', b'About Me',\n",
      "        b'Heebiejeebies - Bonus', b'By Myself', b'Califormula',\n",
      "        b'Safe With Me', b'Some Kind Of Drug', b'D (Half Moon)',\n",
      "        b'Been Calling', b'Whose Mans Is This_', b'La La La Love Me',\n",
      "        b'Menu', b'Lead The Way', b'Real Love', b'Provider',\n",
      "        b'Beneath the Lights', b'Found', b'Best Part', b'One Dance',\n",
      "        b'Middle of Things, Beautiful Wife (feat. Xavier Om\\xc3\\xa4r))',\n",
      "        b'Belong To U', b'West', b'Hallucinations', b'', b'', b'', b'',\n",
      "        b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'',\n",
      "        b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'',\n",
      "        b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'',\n",
      "        b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'',\n",
      "        b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'',\n",
      "        b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'',\n",
      "        b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'',\n",
      "        b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'',\n",
      "        b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'',\n",
      "        b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'',\n",
      "        b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'',\n",
      "        b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'',\n",
      "        b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'',\n",
      "        b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'',\n",
      "        b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'',\n",
      "        b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'',\n",
      "        b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'',\n",
      "        b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'',\n",
      "        b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'',\n",
      "        b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'',\n",
      "        b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'',\n",
      "        b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'',\n",
      "        b'', b'']], dtype=object)>,),\n",
      " 'track_name_seed_track': <tf.Tensor: shape=(), dtype=string, numpy=b'Hallucinations'>,\n",
      " 'track_pop_can': <tf.Tensor: shape=(), dtype=float32, numpy=0.0>,\n",
      " 'track_pop_pl': (<tf.Tensor: shape=(1, 375), dtype=float32, numpy=\n",
      "array([[ 0., 48., 81., 32., 57., 84., 41.,  0., 54., 71., 47., 45., 83.,\n",
      "        84., 69.,  0.,  0.,  0., 16., 84.,  0., 50.,  0., 47., 48., 47.,\n",
      "        35.,  0., 41., 49.,  0.,  0., 29., 63., 44.,  0.,  0.,  0., 50.,\n",
      "        68.,  0., 11.,  0.,  0., 52., 34., 28., 13., 22.,  1.,  0.,  0.,\n",
      "         0., 38., 81., 61.,  0., 55.,  0.,  0., 31., 13., 69., 64., 47.,\n",
      "        22., 68., 68.,  0.,  0., 49., 36.,  0.,  0., 68.,  0.,  0., 79.,\n",
      "         0., 50.,  0.,  0., 63., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.]],\n",
      "      dtype=float32)>,),\n",
      " 'track_pop_seed_track': <tf.Tensor: shape=(), dtype=float32, numpy=63.0>,\n",
      " 'track_uri_can': <tf.Tensor: shape=(), dtype=string, numpy=b'spotify:track:0aVQHaVi4hnNwS7ka194QT'>,\n",
      " 'track_uri_pl': (<tf.Tensor: shape=(1, 375), dtype=string, numpy=\n",
      "array([[b'spotify:track:5wYbTyBlqYiCN5vfp0JACL',\n",
      "        b'spotify:track:7tLWymW3ieb60ZI0aFa4lv',\n",
      "        b'spotify:track:2k9N4caeCIJLOWwWwssrEM',\n",
      "        b'spotify:track:7qdfQMpIUOwsIKTM69XVyW',\n",
      "        b'spotify:track:2My7DsxhZodhsguIHCqAGm',\n",
      "        b'spotify:track:7zFXmv6vqI4qOt4yGf3jYZ',\n",
      "        b'spotify:track:5O06nbk5wDRr1WR3Tyo0Af',\n",
      "        b'spotify:track:6x59xGENjPKBfBbFaJrx6w',\n",
      "        b'spotify:track:06BLANboqUEnvBHJH0aViQ',\n",
      "        b'spotify:track:6efkcs2aUBMFKxl0cl2JWQ',\n",
      "        b'spotify:track:4TtKJMwpf4Df6vacawP1k3',\n",
      "        b'spotify:track:4RcIM9FCaFFlAfC6pMMb59',\n",
      "        b'spotify:track:7eqoqGkKwgOaWNNHx90uEZ',\n",
      "        b'spotify:track:6Nle9hKrkL1wQpwNfEkxjh',\n",
      "        b'spotify:track:3U3J5v3rkx89WnFEQvAJD5',\n",
      "        b'spotify:track:5OsIEsKYHmARpNvVFkWjKB',\n",
      "        b'spotify:track:7BLBIXwIOXPSpTOh1YnKSA',\n",
      "        b'spotify:track:3aLKqv7FPLxC1CXWLKx8e5',\n",
      "        b'spotify:track:0JmzkMiu7ARjNLetSVICMY',\n",
      "        b'spotify:track:3xKsf9qdS1CyvXSMEid6g8',\n",
      "        b'spotify:track:5UO7PbqKIVIijPgEBNPEd1',\n",
      "        b'spotify:track:7iHsUPZjmTqzog933uh5lU',\n",
      "        b'spotify:track:32sxMJ44j8DDD5KiDXVn0J',\n",
      "        b'spotify:track:0E6GxVs1NLV9FxK9rK4K1G',\n",
      "        b'spotify:track:3cFqaY74NyS6rcitujSx2g',\n",
      "        b'spotify:track:313cdthw8naSCqGDNON7XB',\n",
      "        b'spotify:track:6AaBcAy4Bq9o9OdQ3WOWgG',\n",
      "        b'spotify:track:1UeaZu60NUVnJVK6ONpyPe',\n",
      "        b'spotify:track:5WJ3Py8BuLkt0RaDrN8U2X',\n",
      "        b'spotify:track:2w7CXqA8SonrkEC74zuBVM',\n",
      "        b'spotify:track:6EwkuhiNG7AdBP8afmFtO1',\n",
      "        b'spotify:track:0hYr8J9hFQmt7SFFUUT8c8',\n",
      "        b'spotify:track:3WrIsXM196pQz95nZyxSDD',\n",
      "        b'spotify:track:6Ub9ro6DKs0u0J0zIBw5Of',\n",
      "        b'spotify:track:6cvRSOoJKvQwS0oDt0TePP',\n",
      "        b'spotify:track:7lIIdjDhVx5l8kDbTqmWdk',\n",
      "        b'spotify:track:5u1jV3NN4lR1O0YyCqCphr',\n",
      "        b'spotify:track:65cwAfq5FM5UBEyD1w5oWY',\n",
      "        b'spotify:track:2iPo6qr1P0xQDWgbLCAJ7t',\n",
      "        b'spotify:track:6m6R6O2BOZDCNymhJ45spI',\n",
      "        b'spotify:track:171awMjY0IamtiXWfNnIus',\n",
      "        b'spotify:track:74tSdQK9QvmBlXlxAIxxrA',\n",
      "        b'spotify:track:55qULu9m5js4KPcshHAdyH',\n",
      "        b'spotify:track:68YAZg1HzCaDjTeEXlqkP7',\n",
      "        b'spotify:track:2QmU4B8fsyWcTQLVDl06Pk',\n",
      "        b'spotify:track:2NHBZioRzyMG6VTsyqqrLB',\n",
      "        b'spotify:track:2R3cKxBdjqpmeINbNpELOs',\n",
      "        b'spotify:track:1fsPtW8ff8iuK3vF5rR88R',\n",
      "        b'spotify:track:3v1ERynrK48XDoeTtav83V',\n",
      "        b'spotify:track:7mmf56XtAneYnuomOuvySJ',\n",
      "        b'spotify:track:0d9PbHaIlNHYqnizRF1odR',\n",
      "        b'spotify:track:5jQQDwJosT89Wy5vOLCafm',\n",
      "        b'spotify:track:7EpsAOwJU61tXSaT2xCk7z',\n",
      "        b'spotify:track:6j0OqIhUxvOgGPTXxNj124',\n",
      "        b'spotify:track:6IZvVAP7VPPnsGX6bvgkqg',\n",
      "        b'spotify:track:2fl0B0OaXjWbjHCQFx2O8W',\n",
      "        b'spotify:track:3WpA2qsihqFtTcuNzBlEYI',\n",
      "        b'spotify:track:5kApwSRDqF5CKclVLw1FBM',\n",
      "        b'spotify:track:7tZsT7e40yJ5rv729j2NrU',\n",
      "        b'spotify:track:3zgs1f6Tj0mn29qTXJuhSY',\n",
      "        b'spotify:track:3Gl6bd4gJog9CQmqS1H7ke',\n",
      "        b'spotify:track:4lvs8f2BwirfIe5hB04Rwi',\n",
      "        b'spotify:track:32xx0fAv3CIeGmNaWTHvEF',\n",
      "        b'spotify:track:6iRTYLWbxXtUz00KQIWiTN',\n",
      "        b'spotify:track:6eH0vTMo40p9Jbp67ZmUmC',\n",
      "        b'spotify:track:4Xy46vKRLab0hfzFlcI5fb',\n",
      "        b'spotify:track:1Bqxj0aH5KewYHKUg1IdrF',\n",
      "        b'spotify:track:3uA8SjMyDtwtt0jLPMQbVD',\n",
      "        b'spotify:track:6AtJkbqi8D9V7em7swWLR0',\n",
      "        b'spotify:track:0XUOcGbyiR2iiN8DsuJW5l',\n",
      "        b'spotify:track:1wlPuhYpOJRPHVUeizRdx2',\n",
      "        b'spotify:track:4y9uFK5dLF9aX423q7hHp0',\n",
      "        b'spotify:track:6900N5IPVHzAaYJDA99Ac6',\n",
      "        b'spotify:track:1LeotL0lJvp7DFQd5wj5DJ',\n",
      "        b'spotify:track:6R6ihJhRbgu7JxJKIbW57w',\n",
      "        b'spotify:track:668wD7a3RNNX4BlwOTHQCX',\n",
      "        b'spotify:track:3InoZulaW9jwR9992k8dnf',\n",
      "        b'spotify:track:4OBZT9EnhYIV17t4pGw7ig',\n",
      "        b'spotify:track:5bkIN5gDuV9Y9P3JjbTfI8',\n",
      "        b'spotify:track:1OjmLuc3Kf34WcEAasCjsO',\n",
      "        b'spotify:track:711J1I0VfLoAqKX3d2b7PE',\n",
      "        b'spotify:track:26oTYFTOdLZxDwfoHTGvmj',\n",
      "        b'spotify:track:0UE0RhnRaEYsiYgXpyLoZc', b'', b'', b'', b'', b'',\n",
      "        b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'',\n",
      "        b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'',\n",
      "        b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'',\n",
      "        b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'',\n",
      "        b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'',\n",
      "        b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'',\n",
      "        b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'',\n",
      "        b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'',\n",
      "        b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'',\n",
      "        b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'',\n",
      "        b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'',\n",
      "        b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'',\n",
      "        b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'',\n",
      "        b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'',\n",
      "        b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'',\n",
      "        b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'',\n",
      "        b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'',\n",
      "        b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'',\n",
      "        b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'',\n",
      "        b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'',\n",
      "        b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'',\n",
      "        b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'', b'',\n",
      "        b'']], dtype=object)>,),\n",
      " 'track_uri_seed_track': <tf.Tensor: shape=(), dtype=string, numpy=b'spotify:track:0UE0RhnRaEYsiYgXpyLoZc'>}\n",
      "_______________\n"
     ]
    }
   ],
   "source": [
    "for features in parsed_dataset_padded_valid.skip(3).take(1):\n",
    "    pprint(features)\n",
    "    print(\"_______________\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea17604-e4e9-45fe-9a6d-804b24c72acd",
   "metadata": {},
   "source": [
    "### Candidate Track dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53680f4f-9fa4-473b-91e6-b959fa5bfa65",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET = 'spotify-beam-v3'\n",
    "CANDIDATE_PREFIX = 'v3/candidates/'\n",
    "\n",
    "candidate_files = []\n",
    "for blob in client.list_blobs(f\"{BUCKET}\", prefix=f'{CANDIDATE_PREFIX}', delimiter=\"/\"):\n",
    "    candidate_files.append(blob.public_url.replace(\"https://storage.googleapis.com/\", \"gs://\"))\n",
    "    \n",
    "candidate_dataset = tf.data.TFRecordDataset(candidate_files)\n",
    "\n",
    "def parse_candidate_tfrecord_fn(example):\n",
    "    example = tf.io.parse_single_example(\n",
    "        example, \n",
    "        features=candidate_features\n",
    "    )\n",
    "    return example\n",
    "\n",
    "# parsed_candidate_dataset = candidate_dataset.map(parse_candidate_tfrecord_fn, num_parallel_calls=-1)\n",
    "\n",
    "parsed_candidate_dataset = candidate_dataset.map(parse_candidate_tfrecord_fn) ### THIS NEEDS TO BE FIXED SO THE UNIQUE PRODUCT DATASET HAS THE SAME FIELD NAMES (goes thru the same model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f78a3917-06e5-4b9b-b6e8-62ce10ea2826",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'album_name_can': <tf.Tensor: shape=(), dtype=string, numpy=b'The Sound of Everything Rmx'>,\n",
      " 'album_uri_can': <tf.Tensor: shape=(), dtype=string, numpy=b'spotify:album:4a8tMD6qq6GUuUwNae38VI'>,\n",
      " 'artist_followers_can': <tf.Tensor: shape=(), dtype=float32, numpy=277649.0>,\n",
      " 'artist_genres_can': <tf.Tensor: shape=(), dtype=string, numpy=b\"'downtempo', 'electronica', 'funk', 'latin alternative', 'nu jazz', 'nu-cumbia', 'trip hop', 'world'\">,\n",
      " 'artist_name_can': <tf.Tensor: shape=(), dtype=string, numpy=b'Quantic'>,\n",
      " 'artist_pop_can': <tf.Tensor: shape=(), dtype=float32, numpy=64.0>,\n",
      " 'artist_uri_can': <tf.Tensor: shape=(), dtype=string, numpy=b'spotify:artist:5ZMwoAjeDtLJ0XRwRTgaK8'>,\n",
      " 'duration_ms_can': <tf.Tensor: shape=(), dtype=float32, numpy=267130.0>,\n",
      " 'track_name_can': <tf.Tensor: shape=(), dtype=string, numpy=b'The Sound of Everything - Watch TV & Se\\xc3\\xb1orlobo Remix'>,\n",
      " 'track_pop_can': <tf.Tensor: shape=(), dtype=float32, numpy=53.0>,\n",
      " 'track_uri_can': <tf.Tensor: shape=(), dtype=string, numpy=b'spotify:track:27CDzo2P7Mf3dKoa76tNxb'>}\n",
      "_______________\n"
     ]
    }
   ],
   "source": [
    "for features in parsed_candidate_dataset.take(1):\n",
    "    pprint(features)\n",
    "    print(\"_______________\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f134d2e-aab7-45dd-a034-778acdc13c41",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Vocab files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca9208b3-af2a-439b-bca8-f05f1bb6f0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME = 'spotify-v1'\n",
    "FILE_PATH = 'vocabs/v1_string_vocabs'\n",
    "FILE_NAME = 'string_vocabs_v1_20220705-202905.txt'\n",
    "DESTINATION_FILE = 'downloaded_vocabs.txt'\n",
    "\n",
    "client = storage.Client()\n",
    "\n",
    "with open(f'{DESTINATION_FILE}', 'wb') as file_obj:\n",
    "    client.download_blob_to_file(\n",
    "        f'gs://{BUCKET_NAME}/{FILE_PATH}/{FILE_NAME}', file_obj)\n",
    "\n",
    "    \n",
    "with open(f'{DESTINATION_FILE}', 'rb') as pickle_file:\n",
    "    vocab_dict_load = pkl.load(pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab1cbcc2-f54e-44a6-a52f-7d9273bf8a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(vocab_dict_load[\"unique_pids\"])\n",
    "\n",
    "avg_duration_ms_seed_pl = 13000151.68\n",
    "var_duration_ms_seed_pl = 133092900971233.58\n",
    "vocab_dict_load['avg_duration_ms_seed_pl']=avg_duration_ms_seed_pl\n",
    "vocab_dict_load['var_duration_ms_seed_pl']=var_duration_ms_seed_pl\n",
    "\n",
    "avg_n_songs_pl = 55.21\n",
    "var_n_songs_pl = 2317.54\n",
    "vocab_dict_load['avg_n_songs_pl']=avg_n_songs_pl\n",
    "vocab_dict_load['var_n_songs_pl']=var_n_songs_pl\n",
    "\n",
    "avg_n_artists_pl = 30.56\n",
    "var_n_artists_pl = 769.26\n",
    "vocab_dict_load['avg_n_artists_pl']=avg_n_artists_pl\n",
    "vocab_dict_load['var_n_artists_pl']=var_n_artists_pl\n",
    "\n",
    "avg_n_albums_pl = 40.25\n",
    "var_n_albums_pl = 1305.54\n",
    "vocab_dict_load['avg_n_albums_pl']=avg_n_albums_pl\n",
    "vocab_dict_load['var_n_albums_pl']=var_n_albums_pl\n",
    "\n",
    "avg_artist_pop = 16.08\n",
    "var_artist_pop = 300.64\n",
    "vocab_dict_load['avg_artist_pop']=avg_artist_pop\n",
    "vocab_dict_load['var_artist_pop']=var_artist_pop\n",
    "\n",
    "avg_duration_ms_songs_pl = 234823.14\n",
    "var_duration_ms_songs_pl = 5558806228.41\n",
    "vocab_dict_load['avg_duration_ms_songs_pl']=avg_duration_ms_songs_pl\n",
    "vocab_dict_load['var_duration_ms_songs_pl']=var_duration_ms_songs_pl\n",
    "\n",
    "avg_artist_followers = 43337.77\n",
    "var_artist_followers = 377777790193.57\n",
    "vocab_dict_load['avg_artist_followers']=avg_artist_followers\n",
    "vocab_dict_load['var_artist_followers']=var_artist_followers\n",
    "\n",
    "avg_track_pop = 10.85\n",
    "var_track_pop = 202.18\n",
    "vocab_dict_load['avg_track_pop']=avg_track_pop\n",
    "vocab_dict_load['var_track_pop']=var_track_pop\n",
    "# vocab_dict_load['unique_pids_string']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4ae5c9f6-0328-4faa-9817-b90992bfde3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([     0,      1,      2, ..., 999997, 999998, 999999])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_dict_load['unique_pids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b10d7b1c-1e5c-4fb4-a078-ba01b03c9416",
   "metadata": {},
   "outputs": [],
   "source": [
    "VERSION='test-v1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "186a4826-a4b1-4cc3-976e-8685cc4dbace",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET='spotify-tfrecords-blog'\n",
    "bucket=client.bucket(BUCKET)\n",
    "blob = bucket.blob(f'vocabs_stats/vocab_dict_{VERSION}.txt')\n",
    "pickle_out = pkl.dumps(vocab_dict_load)\n",
    "blob.upload_from_string(pickle_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8ef10ce3-1e76-41a3-a36e-16731139a1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bucket=client.bucket(BUCKET)  \n",
    "# blob = bucket.blob('vocabs/string_vocabs')\n",
    "# blob.upload_from_filename('string_vocabs')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de4de4f-7b50-4ec5-bc1a-6eee6bde3e59",
   "metadata": {},
   "source": [
    "### Test instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ee785672-705b-4d7e-a2b3-1f3a6a529538",
   "metadata": {},
   "outputs": [],
   "source": [
    "playlist_test_instancet = {\n",
    "    'name': np.asarray([b'Best Christmas']),\n",
    "    'collaborative': np.asarray([b'false']),\n",
    "    'pid': np.asarray([173671]),\n",
    "    'description_pl': np.asarray([b'test description']),\n",
    "    'duration_ms_seed_pl': np.asarray([5458995.]),\n",
    "    'n_songs_pl': np.asarray([58.]),\n",
    "    'num_artists_pl': np.asarray([19.]),\n",
    "    'num_albums_pl': np.asarray([27.]),\n",
    "    'artist_name_pl': np.asarray([[b'Juan Luis Guerra 4.40', b'Prince Royce', b'Luis Vargas']]),\n",
    "    'track_uri_pl': np.asarray([[b'spotify:track:1g0IBPZTRP7VYkctJ4Qafg',b'spotify:track:43wUzbYxEFoXugYkgTzMWp']]),\n",
    "    'track_name_pl': np.asarray([[b'Lover Come Back', b'White Lightning', b'Shake Me Down']]),\n",
    "    'duration_ms_songs_pl': np.asarray([[245888., 195709., 283906., 271475., 300373., 275173., 236145.,]]),\n",
    "    'album_name_pl': np.asarray([[b'Silsulim', b'Sara Shara', b'Muzika Vesheket', b'Ba La Lirkod']]),\n",
    "    'artist_pop_pl': np.asarray([[81., 81., 70., 66., 66., 66., 46., 87.]]),\n",
    "    'artists_followers_pl': np.asarray([[3.556710e+05, 8.200000e+02, 1.510000e+02, 1.098080e+05,]]),\n",
    "    'artist_genres_pl': np.asarray([[b\"'israeli pop', 'jewish pop'\", b\"'israeli pop', 'jewish pop'\",]]),\n",
    "    'track_pop_pl': np.asarray([[70, 77, 50, 44, 30, 28, 15, 26, 15, 18, 46, 38,]])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6c7be542-9aa4-4ddf-aa7f-5abd4cde18e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_test_instance = {\n",
    "    'artist_name': np.asarray([b'Hellogoodbye']),\n",
    "    'track_name': np.asarray([b'When We First Met']),\n",
    "    'album_name': np.asarray([b'Would It Kill You?']),\n",
    "    'track_uri': np.asarray([b'Ba La Lirkod']),\n",
    "    'artist_uri': np.asarray([b'spotify:artist:6GH0NzpthMGxu1mcfAkOde']),\n",
    "    'album_uri': np.asarray([b'spotify:album:4dHXV7pJs6d8N9ACAMzhIw']),\n",
    "    'duration_ms': np.asarray([154813.0]),\n",
    "    'track_pop': np.asarray([45.0]),\n",
    "    'artist_pop': np.asarray([51.0]),\n",
    "    'artist_followers':np.asarray([205331.0]),\n",
    "    'artist_genres': np.asarray([b\"'neon pop punk', 'pop punk'\"]),\n",
    "    # 'test': np.asarray([b'test'])\n",
    "}\n",
    "\n",
    "# candidate_test_instance = {\n",
    "#     'artist_name': np.asarray([b'Hellogoodbye']),\n",
    "#     'track_name': np.asarray([b'When We First Met']),\n",
    "#     'album_name': np.asarray([b'Would It Kill You?']),\n",
    "#     'track_uri': np.asarray([b'Ba La Lirkod']),\n",
    "#     'artist_uri': np.asarray([b'spotify:artist:6GH0NzpthMGxu1mcfAkOde']),\n",
    "#     'album_uri': np.asarray([b'spotify:album:4dHXV7pJs6d8N9ACAMzhIw']),\n",
    "#     'duration_ms': np.asarray([154813.0]),\n",
    "#     'track_pop': np.asarray([45.0]),\n",
    "#     'artist_pop': np.asarray([51.0]),\n",
    "#     'artist_followers':np.asarray([205331.0]),\n",
    "#     'artist_genres': np.asarray([b\"'neon pop punk', 'pop punk'\"]),\n",
    "#     # 'test': np.asarray([b'test'])\n",
    "# }\n",
    "# pprint(can_test_instance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a87c3d-e54f-4e2e-93aa-b2d84c4d619f",
   "metadata": {},
   "source": [
    "# Two-Tower Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0387f3f-98be-464c-b080-ff08a8b6430e",
   "metadata": {},
   "source": [
    "## Playlist (query) Tower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "586e5b73-71d2-455a-9b5d-99175e7463f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 32\n",
    "PROJECTION_DIM = 5\n",
    "SEED = 1234\n",
    "USE_CROSS_LAYER=True\n",
    "DROPOUT='False'\n",
    "DROPOUT_RATE='0.33'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "74c47c08-b6f6-4afb-a213-c6234ff1a2ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74028"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vocab_dict_load[\"name\"]\n",
    "len(vocab_dict_load[\"name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1122ba98-978b-4bfe-8d82-835e9c2f2689",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': array([b'! 2017 Songs', b'! <3', b'! DJ', ...,\n",
       "        b'\\xf0\\x9f\\xa6\\x84\\xf0\\x9f\\xa6\\x84',\n",
       "        b'\\xf0\\x9f\\xa6\\x84\\xf0\\x9f\\xa6\\x84\\xf0\\x9f\\xa6\\x84',\n",
       "        b'\\xf0\\x9f\\xa6\\x8b\\xf0\\x9f\\xa6\\x8b\\xf0\\x9f\\xa6\\x8b'], dtype=object),\n",
       " 'artist_name_can': array([b'!!!', b'!Dela Dap', b'!Distain', ..., b'\\xed\\x9b\\x88 Hun',\n",
       "        b'\\xef\\xbc\\x92\\xef\\xbc\\x98\\xef\\xbc\\x91\\xef\\xbc\\x94',\n",
       "        b'\\xef\\xbc\\xad\\xef\\xbc\\xa9\\xef\\xbc\\xb9\\xef\\xbc\\xa1\\xef\\xbc\\xb6\\xef\\xbc\\xa9'],\n",
       "       dtype=object),\n",
       " 'track_uri_can': array([b'spotify:track:0000uJA4xCdxThagdLkkLR',\n",
       "        b'spotify:track:0002yNGLtYSYtc0X6ZnFvp',\n",
       "        b'spotify:track:00039MgrmLoIzSpuYKurn9', ...,\n",
       "        b'spotify:track:7zzwsf6tmZETUV26kR7D5z',\n",
       "        b'spotify:track:7zzxEH0xUl5k3p6IxUfgAO',\n",
       "        b'spotify:track:7zzyrYnZIfvYAGwl7lRb7X'], dtype=object),\n",
       " 'artist_uri_can': array([b'spotify:artist:0001ZVMPt41Vwzt1zsmuzp',\n",
       "        b'spotify:artist:0001cekkfdEBoMlwVQvpLg',\n",
       "        b'spotify:artist:0001wHqxbF2YYRQxGdbyER', ...,\n",
       "        b'spotify:artist:7zzVVpR5daxXjgmXNj5nfI',\n",
       "        b'spotify:artist:7zzZtvkqJ7GnXfCte5z3gm',\n",
       "        b'spotify:artist:7zzsdcNemyhcNk2wpNsXZt'], dtype=object),\n",
       " 'track_name_can': array([b'!', b'! (Foreword)', b'! (The Song Formerly Known As)', ...,\n",
       "        b'\\xef\\xbb\\xbfThe Right Move',\n",
       "        b'\\xef\\xbf\\xbc9 Fruitless Years of Total Fucking Agony',\n",
       "        b'\\xef\\xbf\\xbd\\xc3\\xa4k\\xc3\\xa4skero'], dtype=object),\n",
       " 'album_uri_can': array([b'spotify:album:00010fh2pSk7f1mGIhgorB',\n",
       "        b'spotify:album:00045VFusrXwCSietfmspc',\n",
       "        b'spotify:album:0005lpYtyKk9B3e0mWjdem', ...,\n",
       "        b'spotify:album:7zzZUxOJ7fnNz2McXzH8wz',\n",
       "        b'spotify:album:7zzu2Xa7v5lAlzSjGzjrBa',\n",
       "        b'spotify:album:7zzuO4Kbud2YPyGsacVA3h'], dtype=object),\n",
       " 'album_name_can': array([b'!', b'!! Going Places !!', b'!!! Chk Chik Chick', ...,\n",
       "        b'\\xed\\x9b\\x84\\xec\\x95\\x84\\xec\\x9c\\xa0-\\xed\\x95\\x99\\xea\\xb5\\x90 2015 (Original Television Soundtrack), Pt.3',\n",
       "        b'\\xed\\x9e\\x88\\xec\\x96\\xb4\\xeb\\xa1\\x9c - Single',\n",
       "        b'\\xef\\xbb\\xbfBPitch Control - Best of 2014'], dtype=object),\n",
       " 'artist_genres_can': array([b'', b'\"australian children\\'s music\"',\n",
       "        b'\"australian children\\'s music\", \"children\\'s folk\", \"children\\'s music\"',\n",
       "        ..., b\"'zouk', 'zouk riddim'\", b\"'zouk', 'zouk riddim' 'zouk'\",\n",
       "        b\"'zydeco'\"], dtype=object),\n",
       " 'unique_pids': array([     0,      1,      2, ..., 999997, 999998, 999999]),\n",
       " 'artist_name_seed_track': array([b'!!!', b'!Dela Dap', b'!Distain', ..., b'\\xed\\x9b\\x88 Hun',\n",
       "        b'\\xef\\xbc\\x92\\xef\\xbc\\x98\\xef\\xbc\\x91\\xef\\xbc\\x94',\n",
       "        b'\\xef\\xbc\\xad\\xef\\xbc\\xa9\\xef\\xbc\\xb9\\xef\\xbc\\xa1\\xef\\xbc\\xb6\\xef\\xbc\\xa9'],\n",
       "       dtype=object),\n",
       " 'artist_uri_seed_track': array([b'spotify:artist:0001ZVMPt41Vwzt1zsmuzp',\n",
       "        b'spotify:artist:0001cekkfdEBoMlwVQvpLg',\n",
       "        b'spotify:artist:0001wHqxbF2YYRQxGdbyER', ...,\n",
       "        b'spotify:artist:7zzVVpR5daxXjgmXNj5nfI',\n",
       "        b'spotify:artist:7zzZtvkqJ7GnXfCte5z3gm',\n",
       "        b'spotify:artist:7zzsdcNemyhcNk2wpNsXZt'], dtype=object),\n",
       " 'track_name_seed_track': array([b'!', b'! (Foreword)', b'! (The Song Formerly Known As)', ...,\n",
       "        b'\\xef\\xbb\\xbfThe Right Move',\n",
       "        b'\\xef\\xbf\\xbc9 Fruitless Years of Total Fucking Agony',\n",
       "        b'\\xef\\xbf\\xbd\\xc3\\xa4k\\xc3\\xa4skero'], dtype=object),\n",
       " 'track_uri_seed_track': array([b'spotify:track:0000uJA4xCdxThagdLkkLR',\n",
       "        b'spotify:track:0002yNGLtYSYtc0X6ZnFvp',\n",
       "        b'spotify:track:00039MgrmLoIzSpuYKurn9', ...,\n",
       "        b'spotify:track:7zzwsf6tmZETUV26kR7D5z',\n",
       "        b'spotify:track:7zzxEH0xUl5k3p6IxUfgAO',\n",
       "        b'spotify:track:7zzyrYnZIfvYAGwl7lRb7X'], dtype=object),\n",
       " 'album_name_seed_track': array([b'!', b'!! Going Places !!', b'!!! Chk Chik Chick', ...,\n",
       "        b'\\xed\\x9b\\x84\\xec\\x95\\x84\\xec\\x9c\\xa0-\\xed\\x95\\x99\\xea\\xb5\\x90 2015 (Original Television Soundtrack), Pt.3',\n",
       "        b'\\xed\\x9e\\x88\\xec\\x96\\xb4\\xeb\\xa1\\x9c - Single',\n",
       "        b'\\xef\\xbb\\xbfBPitch Control - Best of 2014'], dtype=object),\n",
       " 'album_uri_seed_track': array([b'spotify:album:00010fh2pSk7f1mGIhgorB',\n",
       "        b'spotify:album:00045VFusrXwCSietfmspc',\n",
       "        b'spotify:album:0005lpYtyKk9B3e0mWjdem', ...,\n",
       "        b'spotify:album:7zzZUxOJ7fnNz2McXzH8wz',\n",
       "        b'spotify:album:7zzu2Xa7v5lAlzSjGzjrBa',\n",
       "        b'spotify:album:7zzuO4Kbud2YPyGsacVA3h'], dtype=object),\n",
       " 'artist_genres_seed_track': array([b'', b'\"australian children\\'s music\"',\n",
       "        b'\"australian children\\'s music\", \"children\\'s folk\", \"children\\'s music\"',\n",
       "        ..., b\"'zouk', 'zouk riddim'\", b\"'zouk', 'zouk riddim' 'zouk'\",\n",
       "        b\"'zydeco'\"], dtype=object),\n",
       " 'description_pl': array([b'', b'!', b'! english&#x2F;spanish !', ..., b'\\xf0\\x9f\\x9a\\xa8',\n",
       "        b'\\xf0\\x9f\\xa4\\x91',\n",
       "        b'\\xf0\\x9f\\xa4\\x98\\xf0\\x9f\\xa4\\x98\\xf0\\x9f\\xa4\\x98\\xf0\\x9f\\xa4\\x98\\xf0\\x9f\\xa4\\x98\\xf0\\x9f\\xa4\\x98\\xf0\\x9f\\xa4\\x98\\xf0\\x9f\\xa4\\x98\\xf0\\x9f\\xa4\\x98\\xf0\\x9f\\xa4\\x98\\xf0\\x9f\\xa4\\x98\\xf0\\x9f\\xa4\\x98\\xf0\\x9f\\xa4\\x98\\xf0\\x9f\\xa4\\x98\\xf0\\x9f\\xa4\\x98\\xf0\\x9f\\xa4\\x98'],\n",
       "       dtype=object),\n",
       " 'artist_name_pl': array([b'!!!', b'!Dela Dap', b'!Distain', ..., b'\\xed\\x9b\\x88 Hun',\n",
       "        b'\\xef\\xbc\\x92\\xef\\xbc\\x98\\xef\\xbc\\x91\\xef\\xbc\\x94',\n",
       "        b'\\xef\\xbc\\xad\\xef\\xbc\\xa9\\xef\\xbc\\xb9\\xef\\xbc\\xa1\\xef\\xbc\\xb6\\xef\\xbc\\xa9'],\n",
       "       dtype=object),\n",
       " 'track_uri_pl': array([b'spotify:track:0000uJA4xCdxThagdLkkLR',\n",
       "        b'spotify:track:0002yNGLtYSYtc0X6ZnFvp',\n",
       "        b'spotify:track:00039MgrmLoIzSpuYKurn9', ...,\n",
       "        b'spotify:track:7zzwsf6tmZETUV26kR7D5z',\n",
       "        b'spotify:track:7zzxEH0xUl5k3p6IxUfgAO',\n",
       "        b'spotify:track:7zzyrYnZIfvYAGwl7lRb7X'], dtype=object),\n",
       " 'track_name_pl': array([b'!', b'! (Foreword)', b'! (The Song Formerly Known As)', ...,\n",
       "        b'\\xef\\xbb\\xbfThe Right Move',\n",
       "        b'\\xef\\xbf\\xbc9 Fruitless Years of Total Fucking Agony',\n",
       "        b'\\xef\\xbf\\xbd\\xc3\\xa4k\\xc3\\xa4skero'], dtype=object),\n",
       " 'album_name_pl': array([b'!', b'!! Going Places !!', b'!!! Chk Chik Chick', ...,\n",
       "        b'\\xed\\x9b\\x84\\xec\\x95\\x84\\xec\\x9c\\xa0-\\xed\\x95\\x99\\xea\\xb5\\x90 2015 (Original Television Soundtrack), Pt.3',\n",
       "        b'\\xed\\x9e\\x88\\xec\\x96\\xb4\\xeb\\xa1\\x9c - Single',\n",
       "        b'\\xef\\xbb\\xbfBPitch Control - Best of 2014'], dtype=object),\n",
       " 'artist_genres_pl': array([b'', b'\"australian children\\'s music\"',\n",
       "        b'\"australian children\\'s music\", \"children\\'s folk\", \"children\\'s music\"',\n",
       "        ..., b\"'zouk', 'zouk riddim'\", b\"'zouk', 'zouk riddim' 'zouk'\",\n",
       "        b\"'zydeco'\"], dtype=object),\n",
       " 'min_duration_ms_seed_pl': 0,\n",
       " 'max_duration_ms_seed_pl': 629322792,\n",
       " 'min_n_songs_pl': 1,\n",
       " 'max_n_songs_pl': 375,\n",
       " 'min_n_artists_pl': 1,\n",
       " 'max_n_artists_pl': 237,\n",
       " 'min_n_albums_pl': 1,\n",
       " 'max_n_albums_pl': 244,\n",
       " 'min_artist_pop': 0,\n",
       " 'max_artist_pop': 100,\n",
       " 'min_duration_ms_songs_pl': -1,\n",
       " 'max_duration_ms_songs_pl': 20744575,\n",
       " 'min_artist_followers': 0,\n",
       " 'max_artist_followers': 94437255,\n",
       " 'min_track_pop': 0,\n",
       " 'max_track_pop': 96,\n",
       " 'avg_duration_ms_seed_pl': 13000151.68,\n",
       " 'var_duration_ms_seed_pl': 133092900971233.58,\n",
       " 'avg_n_songs_pl': 55.21,\n",
       " 'var_n_songs_pl': 2317.54,\n",
       " 'avg_n_artists_pl': 30.56,\n",
       " 'var_n_artists_pl': 769.26,\n",
       " 'avg_n_albums_pl': 40.25,\n",
       " 'var_n_albums_pl': 1305.54,\n",
       " 'avg_artist_pop': 16.08,\n",
       " 'var_artist_pop': 300.64,\n",
       " 'avg_duration_ms_songs_pl': 234823.14,\n",
       " 'var_duration_ms_songs_pl': 5558806228.41,\n",
       " 'avg_artist_followers': 43337.77,\n",
       " 'var_artist_followers': 377777790193.57,\n",
       " 'avg_track_pop': 10.85,\n",
       " 'var_track_pop': 202.18}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_dict_load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6553e994-0c3e-4a3c-be3b-852ff3d0a586",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Playlist_Model(tf.keras.Model):\n",
    "    def __init__(self, layer_sizes, vocab_dict):\n",
    "        super().__init__()\n",
    "\n",
    "        # ========================================\n",
    "        # non-sequence playlist features\n",
    "        # ========================================\n",
    "        \n",
    "        # Feature: playlist name\n",
    "        self.pl_name_text_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.TextVectorization(\n",
    "                    max_tokens=len(vocab_dict[\"name\"]), # not needed if passing vocab\n",
    "                    # vocabulary=vocab_dict['name'], \n",
    "                    name=\"pl_name_txt_vectorizer\", \n",
    "                    ngrams=2\n",
    "                ),\n",
    "                tf.keras.layers.Embedding(\n",
    "                    input_dim=len(vocab_dict[\"name\"]) + 1,\n",
    "                    output_dim=EMBEDDING_DIM,\n",
    "                    mask_zero=False,\n",
    "                    name=\"pl_name_emb_layer\",\n",
    "                ),\n",
    "                tf.keras.layers.GlobalAveragePooling1D(name=\"pl_name_pooling\"),\n",
    "            ], name=\"pl_name_emb_model\"\n",
    "        )\n",
    "        \n",
    "        # Feature: collaborative\n",
    "        collaborative_vocab = np.array([b'false', b'true'])\n",
    "        \n",
    "        self.pl_collaborative_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.StringLookup(\n",
    "                    vocabulary=collaborative_vocab, \n",
    "                    mask_token=None, \n",
    "                    name=\"pl_collaborative_lookup\", \n",
    "                    output_mode='int'\n",
    "                ),\n",
    "                tf.keras.layers.Embedding(\n",
    "                    input_dim=len(collaborative_vocab) + 1,\n",
    "                    output_dim=EMBEDDING_DIM,\n",
    "                    mask_zero=False,\n",
    "                    name=\"pl_collaborative_emb_layer\",\n",
    "                ),\n",
    "            ], name=\"pl_collaborative_emb_model\"\n",
    "        )\n",
    "        \n",
    "        # Feature: pid\n",
    "        self.pl_track_uri_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.StringLookup(\n",
    "                    vocabulary=tf.constant(vocab_dict['track_uri_can']), \n",
    "                    mask_token=None, \n",
    "                    name=\"pl_track_uri_lookup\", \n",
    "                ),\n",
    "                tf.keras.layers.Embedding(\n",
    "                    input_dim=len(vocab_dict['track_uri_can'])+1,\n",
    "                    output_dim=EMBEDDING_DIM,\n",
    "                    mask_zero=False,\n",
    "                    name=\"pl_track_uri_layer\",\n",
    "                ),\n",
    "            ], name=\"pl_track_uri_emb_model\"\n",
    "        )\n",
    "        \n",
    "        # Feature: description_pl\n",
    "#         self.pl_description_text_embedding = tf.keras.Sequential(\n",
    "#             [\n",
    "#                 tf.keras.layers.TextVectorization(\n",
    "#                     max_tokens=len(vocab_dict[\"description_pl\"]), # not needed if passing vocab\n",
    "#                     # vocabulary=tf.constant(vocab_dict['description_pl']), \n",
    "#                     name=\"description_pl_vectorizer\", \n",
    "#                 ),\n",
    "#                 tf.keras.layers.Embedding(\n",
    "#                     input_dim=len(vocab_dict[\"description_pl\"]) + 1,\n",
    "#                     output_dim=EMBEDDING_DIM,\n",
    "#                     mask_zero=False,\n",
    "#                     name=\"description_pl_emb_layer\",\n",
    "#                 ),\n",
    "#                 tf.keras.layers.GlobalAveragePooling1D(name=\"description_pl_pooling\"),\n",
    "#             ], name=\"pl_description_emb_model\"\n",
    "#         )\n",
    "        \n",
    "        # Feature: duration_ms_seed_pl                      \n",
    "        # TODO: Noramlize or Descritize?\n",
    "        # duration_ms_seed_pl_buckets = np.linspace(\n",
    "        #     vocab_dict['min_duration_ms_seed_pl'], \n",
    "        #     vocab_dict['max_duration_ms_seed_pl'], \n",
    "        #     num=1000\n",
    "        # )\n",
    "        # self.duration_ms_seed_pl_embedding = tf.keras.Sequential(\n",
    "        #     [\n",
    "        #         tf.keras.layers.Discretization(duration_ms_seed_pl_buckets.tolist()),\n",
    "        #         tf.keras.layers.Embedding(\n",
    "        #             input_dim=len(duration_ms_seed_pl_buckets) + 1, \n",
    "        #             output_dim=EMBEDDING_DIM, \n",
    "        #             name=\"duration_ms_seed_pl_emb_layer\",\n",
    "        #         )\n",
    "        #     ], name=\"duration_ms_seed_pl_emb_model\"\n",
    "        # )\n",
    "        # self.duration_ms_seed_pl_normalization = tf.keras.layers.Normalization(\n",
    "        #     mean=vocab_dict['avg_duration_ms_seed_pl'],\n",
    "        #     variance=vocab_dict['var_duration_ms_seed_pl'],\n",
    "        #     axis=None\n",
    "        # )\n",
    "        \n",
    "        # Feature: n_songs_pl\n",
    "        # TODO: Noramlize or Descritize?\n",
    "        n_songs_pl_buckets = np.linspace(\n",
    "            vocab_dict['min_n_songs_pl'], \n",
    "            vocab_dict['max_n_songs_pl'], \n",
    "            num=100\n",
    "        )\n",
    "        self.n_songs_pl_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.Discretization(n_songs_pl_buckets.tolist()),\n",
    "                tf.keras.layers.Embedding(\n",
    "                    input_dim=len(n_songs_pl_buckets) + 1, \n",
    "                    output_dim=EMBEDDING_DIM, \n",
    "                    name=\"n_songs_pl_emb_layer\",\n",
    "                )\n",
    "            ], name=\"n_songs_pl_emb_model\"\n",
    "        )\n",
    "        # self.n_songs_pl_normalization = tf.keras.layers.Normalization(\n",
    "        #     mean=vocab_dict['avg_n_songs_pl'],\n",
    "        #     variance=vocab_dict['var_n_songs_pl'],\n",
    "        #     axis=None\n",
    "        # )\n",
    "        \n",
    "        # Feature: num_artists_pl\n",
    "        # TODO: Noramlize or Descritize?\n",
    "        n_artists_pl_buckets = np.linspace(\n",
    "            vocab_dict['min_n_artists_pl'], \n",
    "            vocab_dict['max_n_artists_pl'], \n",
    "            num=100\n",
    "        )\n",
    "        self.n_artists_pl_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.Discretization(n_artists_pl_buckets.tolist()),\n",
    "                tf.keras.layers.Embedding(\n",
    "                    input_dim=len(n_artists_pl_buckets) + 1, \n",
    "                    output_dim=EMBEDDING_DIM, \n",
    "                    name=\"n_artists_pl_emb_layer\",\n",
    "                    mask_zero=False\n",
    "                )\n",
    "            ], name=\"n_artists_pl_emb_model\"\n",
    "        )\n",
    "        # self.n_artists_pl_normalization = tf.keras.layers.Normalization(\n",
    "        #     mean=vocab_dict['avg_n_artists_pl'],\n",
    "        #     variance=vocab_dict['var_n_artists_pl'],\n",
    "        #     axis=None\n",
    "        # )\n",
    "        \n",
    "        # Feature: num_albums_pl\n",
    "        n_albums_pl_buckets = np.linspace(\n",
    "            vocab_dict['min_n_albums_pl'], \n",
    "            vocab_dict['max_n_albums_pl'],\n",
    "            num=100\n",
    "        )\n",
    "        self.n_albums_pl_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.Discretization(n_albums_pl_buckets.tolist()),\n",
    "                tf.keras.layers.Embedding(\n",
    "                    input_dim=len(n_albums_pl_buckets) + 1, \n",
    "                    output_dim=EMBEDDING_DIM, \n",
    "                    name=\"n_albums_pl_emb_layer\",\n",
    "                )\n",
    "            ], name=\"n_albums_pl_emb_model\"\n",
    "        )\n",
    "        # self.n_albums_pl_normalization = tf.keras.layers.Normalization(\n",
    "        #     mean=vocab_dict['avg_n_albums_pl'],\n",
    "        #     variance=vocab_dict['var_n_albums_pl'],\n",
    "        #     axis=None\n",
    "        # )\n",
    "        \n",
    "        # ========================================\n",
    "        # sequence playlist features\n",
    "        # ========================================\n",
    "        \n",
    "        # Feature: artist_name_pl\n",
    "        self.artist_name_pl_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                # tf.keras.layers.Flatten(),\n",
    "                tf.keras.layers.StringLookup(\n",
    "                    vocabulary=vocab_dict['artist_name_pl'], mask_token=''),\n",
    "                tf.keras.layers.Embedding(\n",
    "                    input_dim=len(vocab_dict['artist_name_pl']) + 1, \n",
    "                    output_dim=EMBEDDING_DIM,\n",
    "                    name=\"artist_name_pl_emb_layer\",\n",
    "                    mask_zero=False\n",
    "                ),\n",
    "                # tf.keras.layers.GlobalAveragePooling1D(EMBEDDING_DIM, name=\"artist_name_conv1d\"), # GlobalAveragePooling1D\n",
    "                tf.keras.layers.GlobalAveragePooling1D(name=\"artist_name_pl_1d\"),\n",
    "            ], name=\"artist_name_pl_emb_model\"\n",
    "        )\n",
    "        \n",
    "        # Feature: track_uri_pl\n",
    "        # 2.2M unique\n",
    "        self.track_uri_pl_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                # tf.keras.layers.Flatten(),\n",
    "                tf.keras.layers.StringLookup(\n",
    "                    vocabulary=vocab_dict['track_uri_can'], mask_token=''),\n",
    "                tf.keras.layers.Embedding(\n",
    "                    input_dim=len(vocab_dict['track_uri_can']) + 1, \n",
    "                    output_dim=EMBEDDING_DIM,\n",
    "                    name=\"track_uri_pl_emb_layer\",\n",
    "                    mask_zero=False\n",
    "                ),\n",
    "                tf.keras.layers.GlobalAveragePooling1D(name=\"track_uri_1d\"),\n",
    "            ], name=\"track_uri_pl_emb_model\"\n",
    "        )\n",
    "        \n",
    "        # Feature: track_name_pl\n",
    "        self.track_name_pl_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                # tf.keras.layers.Flatten(),\n",
    "                tf.keras.layers.StringLookup(\n",
    "                    vocabulary=vocab_dict['track_name_pl'], \n",
    "                    name=\"track_name_pl_lookup\",\n",
    "                    output_mode='int',\n",
    "                    mask_token=''\n",
    "                ),\n",
    "                tf.keras.layers.Embedding(\n",
    "                    input_dim=len(vocab_dict['track_name_pl']) + 2, \n",
    "                    output_dim=EMBEDDING_DIM,\n",
    "                    name=\"track_name_pl_emb_layer\",\n",
    "                    mask_zero=False\n",
    "                ),\n",
    "                tf.keras.layers.GlobalAveragePooling1D(name=\"track_name_pl_1d\"),\n",
    "            ], name=\"track_name_pl_emb_model\"\n",
    "        )\n",
    "        \n",
    "        Feature: duration_ms_songs_pl\n",
    "        duration_ms_songs_pl_buckets = np.linspace(\n",
    "            vocab_dict['min_duration_ms_songs_pl'], \n",
    "            vocab_dict['max_duration_ms_songs_pl'], \n",
    "            num=100\n",
    "        )\n",
    "        self.duration_ms_songs_pl_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                # tf.keras.layers.Flatten(),\n",
    "                tf.keras.layers.Discretization(duration_ms_songs_pl_buckets.tolist()),\n",
    "                tf.keras.layers.Embedding(\n",
    "                    input_dim=len(duration_ms_songs_pl_buckets) + 1, \n",
    "                    output_dim=EMBEDDING_DIM,\n",
    "                    name=\"duration_ms_songs_pl_emb_layer\",\n",
    "                    mask_zero=False\n",
    "                ),\n",
    "                tf.keras.layers.GlobalAveragePooling1D(name=\"duration_ms_songs_pl_emb_layer_pl_1d\"),\n",
    "            ], name=\"duration_ms_songs_pl_emb_model\"\n",
    "        )\n",
    "        \n",
    "        # Feature: album_name_pl\n",
    "        self.album_name_pl_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                # tf.keras.layers.Flatten(),\n",
    "                tf.keras.layers.StringLookup(\n",
    "                    vocabulary=tf.constant(vocab_dict['album_name_pl']), \n",
    "                    mask_token=None, \n",
    "                    name=\"album_name_pl_lookup\"\n",
    "                ),\n",
    "                tf.keras.layers.Embedding(\n",
    "                    input_dim=len(vocab_dict['album_name_pl']) + 1, \n",
    "                    output_dim=EMBEDDING_DIM,\n",
    "                    name=\"album_name_pl_emb_layer\",\n",
    "                    mask_zero=False\n",
    "                ),\n",
    "                tf.keras.layers.GlobalAveragePooling1D(name=\"album_name_pl_emb_layer_1d\"),\n",
    "            ], name=\"album_name_pl_emb_model\"\n",
    "        )\n",
    "        \n",
    "        # Feature: artist_pop_pl\n",
    "        artist_pop_pl_buckets = np.linspace(\n",
    "            vocab_dict['min_artist_pop'], \n",
    "            vocab_dict['max_artist_pop'], \n",
    "            num=10\n",
    "        )\n",
    "        self.artist_pop_pl_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                # tf.keras.layers.Flatten(),\n",
    "                tf.keras.layers.Discretization(artist_pop_pl_buckets.tolist()),\n",
    "                tf.keras.layers.Embedding(\n",
    "                    input_dim=len(artist_pop_pl_buckets) + 1, \n",
    "                    output_dim=EMBEDDING_DIM,\n",
    "                    name=\"artist_pop_pl_emb_layer\",\n",
    "                    mask_zero=False\n",
    "                ),\n",
    "                tf.keras.layers.GlobalAveragePooling1D(name=\"artist_pop_1d\"),\n",
    "            ], name=\"artist_pop_pl_emb_model\"\n",
    "        )\n",
    "        \n",
    "        # Feature: artists_followers_pl\n",
    "        artists_followers_pl_buckets = np.linspace(\n",
    "            vocab_dict['min_artist_followers'], \n",
    "            vocab_dict['max_artist_followers'], \n",
    "            num=10\n",
    "        )\n",
    "        self.artists_followers_pl_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                # tf.keras.layers.Flatten(),\n",
    "                tf.keras.layers.Discretization(artists_followers_pl_buckets.tolist()),\n",
    "                tf.keras.layers.Embedding(\n",
    "                    input_dim=len(artists_followers_pl_buckets) + 1, \n",
    "                    output_dim=EMBEDDING_DIM,\n",
    "                    name=\"artists_followers_pl_emb_layer\",\n",
    "                    mask_zero=False\n",
    "                ),\n",
    "                tf.keras.layers.GlobalAveragePooling1D(name=\"artists_followers_pl_1d\"),\n",
    "            ], name=\"artists_followers_pl_emb_model\"\n",
    "        )\n",
    "        \n",
    "        # Feature: track_pop_pl\n",
    "        track_pop_pl_buckets = np.linspace(\n",
    "            vocab_dict['min_track_pop'], \n",
    "            vocab_dict['max_track_pop'], \n",
    "            num=10\n",
    "        )\n",
    "        self.track_pop_pl_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                # tf.keras.layers.Flatten(dtype=tf.float32),\n",
    "                tf.keras.layers.Discretization(track_pop_pl_buckets.tolist()),\n",
    "                tf.keras.layers.Embedding(\n",
    "                    input_dim=len(track_pop_pl_buckets) + 1, \n",
    "                    output_dim=EMBEDDING_DIM,\n",
    "                    name=\"track_pop_pl_emb_layer\",\n",
    "                    mask_zero=False\n",
    "                ),\n",
    "                tf.keras.layers.GlobalAveragePooling1D(name=\"track_pop_pl_1d\"),\n",
    "            ], name=\"track_pop_pl_emb_model\"\n",
    "        )\n",
    "        \n",
    "        # Feature: artist_genres_pl\n",
    "        self.artist_genres_pl_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.Flatten(),\n",
    "                tf.keras.layers.StringLookup(\n",
    "                    vocabulary=vocab_dict['artist_genres_pl'], mask_token=''),\n",
    "                tf.keras.layers.Embedding(\n",
    "                    input_dim=len(vocab_dict['artist_genres_pl']) + 2, \n",
    "                    output_dim=EMBEDDING_DIM,\n",
    "                    name=\"artist_genres_pl_emb_layer\",\n",
    "                    mask_zero=False\n",
    "                ),\n",
    "                tf.keras.layers.GlobalAveragePooling1D(name=\"artist_genres_pl_1d\"),\n",
    "            ], name=\"artist_genres_pl_emb_model\"\n",
    "        )\n",
    "        \n",
    "        # ========================================\n",
    "        # dense and cross layers\n",
    "        # ========================================\n",
    "\n",
    "        # Cross Layers\n",
    "        if USE_CROSS_LAYER:\n",
    "            self._cross_layer = tfrs.layers.dcn.Cross(\n",
    "                projection_dim=PROJECTION_DIM,\n",
    "                kernel_initializer=\"glorot_uniform\", \n",
    "                name=\"pl_cross_layer\"\n",
    "            )\n",
    "        else:\n",
    "            self._cross_layer = None\n",
    "            \n",
    "        # Dense Layers\n",
    "        self.dense_layers = tf.keras.Sequential(name=\"pl_dense_layers\")\n",
    "        initializer = tf.keras.initializers.GlorotUniform(seed=SEED)\n",
    "        \n",
    "        # Use the ReLU activation for all but the last layer.\n",
    "        for layer_size in layer_sizes[:-1]:\n",
    "            self.dense_layers.add(\n",
    "                tf.keras.layers.Dense(\n",
    "                    layer_size, \n",
    "                    activation=\"relu\", \n",
    "                    kernel_initializer=initializer,\n",
    "                )\n",
    "            )\n",
    "            if DROPOUT:\n",
    "                self.dense_layers.add(tf.keras.layers.Dropout(DROPOUT_RATE))\n",
    "                \n",
    "        # No activation for the last layer\n",
    "        for layer_size in layer_sizes[-1:]:\n",
    "            self.dense_layers.add(\n",
    "                tf.keras.layers.Dense(\n",
    "                    layer_size, \n",
    "                    kernel_initializer=initializer\n",
    "                )\n",
    "            )\n",
    "        ### ADDING L2 NORM AT THE END\n",
    "        self.dense_layers.add(\n",
    "            tf.keras.layers.Lambda(\n",
    "                lambda x: tf.nn.l2_normalize(\n",
    "                    x, 1, epsilon=1e-12, name=\"normalize_dense\"\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        \n",
    "    # ========================================\n",
    "    # call\n",
    "    # ========================================\n",
    "    def call(self, data):\n",
    "        '''\n",
    "        The call method defines what happens when\n",
    "        the model is called\n",
    "        '''\n",
    "        \n",
    "        all_embs = tf.concat(\n",
    "            [\n",
    "                self.pl_name_text_embedding(data['name']),\n",
    "                self.pl_collaborative_embedding(data['collaborative']),\n",
    "                self.pl_track_uri_embedding(data[\"track_uri_can\"]),\n",
    "                # self.pl_description_text_embedding(data['description_pl']),\n",
    "                # self.duration_ms_seed_pl_embedding(data[\"duration_ms_seed_pl\"]),\n",
    "                # tf.reshape(self.duration_ms_seed_pl_normalization(data[\"duration_ms_seed_pl\"]), (-1, 1))      # Normalize or Discretize?\n",
    "                self.n_songs_pl_embedding(data[\"n_songs_pl\"]),\n",
    "                # tf.reshape(self.n_songs_pl_normalization(data[\"n_songs_pl\"]), (-1, 1))                        # Normalize or Discretize?\n",
    "                self.n_artists_pl_embedding(data['num_artists_pl']),\n",
    "                # tf.reshape(self.n_artists_pl_normalization(data[\"num_artists_pl\"]), (-1, 1))                  # Normalize or Discretize?\n",
    "                self.n_albums_pl_embedding(data[\"num_albums_pl\"]),\n",
    "                # tf.reshape(self.n_albums_pl_normalization(data[\"num_albums_pl\"]), (-1, 1))                    # Normalize or Discretize?\n",
    "                \n",
    "                # sequence features\n",
    "                # data[\"pos_pl\"],\n",
    "                self.artist_name_pl_embedding(tf.reshape(data[\"artist_name_pl\"], (-1, MAX_PLAYLIST_LENGTH))),\n",
    "                self.track_uri_pl_embedding(tf.reshape(data[\"track_uri_pl\"], (-1, MAX_PLAYLIST_LENGTH))),\n",
    "                self.track_name_pl_embedding(tf.reshape(data[\"track_name_pl\"], (-1, MAX_PLAYLIST_LENGTH))),\n",
    "                self.duration_ms_songs_pl_embedding(tf.reshape(data[\"duration_ms_songs_pl\"], (-1, MAX_PLAYLIST_LENGTH))),\n",
    "                self.album_name_pl_embedding(tf.reshape(data[\"album_name_pl\"], (-1, MAX_PLAYLIST_LENGTH))),\n",
    "                self.artist_pop_pl_embedding(tf.reshape(data[\"artist_pop_pl\"], (-1, MAX_PLAYLIST_LENGTH))),\n",
    "                self.artists_followers_pl_embedding(tf.reshape(data[\"artists_followers_pl\"], (-1, MAX_PLAYLIST_LENGTH))),\n",
    "                self.track_pop_pl_embedding(tf.reshape(data[\"track_pop_pl\"], (-1, MAX_PLAYLIST_LENGTH))),\n",
    "                self.artist_genres_pl_embedding(tf.reshape(data[\"artist_genres_pl\"], (-1, MAX_PLAYLIST_LENGTH))),\n",
    "            ], axis=1)\n",
    "        \n",
    "        # Build Cross Network\n",
    "        if self._cross_layer is not None:\n",
    "            cross_embs = self._cross_layer(all_embs)\n",
    "            return self.dense_layers(cross_embs)\n",
    "        else:\n",
    "            return self.dense_layers(all_embs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35bdd6e7-9b24-4093-86b0-a98f83b58eca",
   "metadata": {},
   "source": [
    "### test playlist tower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1893e877-873c-4c2b-813e-ea924b096b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adapts complete for name\n"
     ]
    }
   ],
   "source": [
    "layer_sizes=[64,32]\n",
    "\n",
    "test_playlist_model = Playlist_Model(layer_sizes,vocab_dict_load)\n",
    "\n",
    "test_playlist_model.pl_name_text_embedding.layers[0].adapt(parsed_dataset_padded.map(lambda x: x['name']).batch(1000))\n",
    "\n",
    "print(\"Adapts complete for name\")\n",
    "\n",
    "# test_playlist_model.pl_description_text_embedding.layers[0].adapt(parsed_dataset_padded.map(lambda x: x['description_pl']).batch(1000))\n",
    "\n",
    "# print(\"Adapts complete for description\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "38ae8e37-e44f-4352-88cc-bad1c8c21971",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '[UNK]',\n",
       " 'music',\n",
       " 'country',\n",
       " 'chill',\n",
       " 'summer',\n",
       " 'party',\n",
       " 'good',\n",
       " 'songs',\n",
       " 'rap',\n",
       " 'new',\n",
       " 'jams',\n",
       " 'rock',\n",
       " 'my',\n",
       " 'the',\n",
       " 'playlist',\n",
       " '2017',\n",
       " 'throwback',\n",
       " 'workout',\n",
       " '2016',\n",
       " 'work',\n",
       " 'oldies',\n",
       " '80s',\n",
       " 'wedding',\n",
       " 'feels',\n",
       " 'car',\n",
       " 'road',\n",
       " 'old',\n",
       " 'mix',\n",
       " 'it',\n",
       " 'christmas',\n",
       " 'pop',\n",
       " 'out',\n",
       " 'up',\n",
       " 'dance',\n",
       " 'vibes',\n",
       " 'gym',\n",
       " 'best',\n",
       " 'trip',\n",
       " 'time',\n",
       " 'stuff',\n",
       " '2015',\n",
       " 'school',\n",
       " 'road trip',\n",
       " 'love',\n",
       " 'hip',\n",
       " 'slow',\n",
       " 'hop',\n",
       " 'of',\n",
       " 'feel',\n",
       " 'classics',\n",
       " 'classic',\n",
       " '2014',\n",
       " 'spring',\n",
       " 'running',\n",
       " 'lit',\n",
       " 'jamz',\n",
       " 'hip hop',\n",
       " 'for',\n",
       " 'throwbacks',\n",
       " 'edm',\n",
       " 'disney',\n",
       " '90s',\n",
       " 'run',\n",
       " 'me',\n",
       " 'feel good',\n",
       " 'favorites',\n",
       " '3',\n",
       " '17',\n",
       " 'worship',\n",
       " 'down',\n",
       " 'all',\n",
       " 'summer 2017',\n",
       " 'spanish',\n",
       " 'sleep',\n",
       " 'old school',\n",
       " 'indie',\n",
       " 'hype',\n",
       " 'good stuff',\n",
       " 'fall',\n",
       " '16',\n",
       " 'you',\n",
       " 'study',\n",
       " 'morning',\n",
       " 'jesus',\n",
       " 'favs',\n",
       " 'favorite',\n",
       " 'baby',\n",
       " '2',\n",
       " 'work out',\n",
       " 'tunes',\n",
       " 'to',\n",
       " 'tbt',\n",
       " 'sad',\n",
       " 'random',\n",
       " 'play',\n",
       " 'mellow',\n",
       " 'march',\n",
       " 'just',\n",
       " 'driving',\n",
       " 'days',\n",
       " 'day',\n",
       " 'christian',\n",
       " 'but',\n",
       " 'best of',\n",
       " 'beach',\n",
       " 'and',\n",
       " 'alternative',\n",
       " 'the feels',\n",
       " 'summer 17',\n",
       " 'reggae',\n",
       " 'on',\n",
       " 'mood',\n",
       " 'jam',\n",
       " 'in',\n",
       " 'hits',\n",
       " 'hiphop',\n",
       " 'halloween',\n",
       " 'good vibes',\n",
       " 'game',\n",
       " 'fun',\n",
       " 'current',\n",
       " 'calm',\n",
       " 'acoustic',\n",
       " 'a',\n",
       " 'turn up',\n",
       " 'turn',\n",
       " 'trap',\n",
       " 'summer 2016',\n",
       " 'soft',\n",
       " 'shower',\n",
       " 'roadtrip',\n",
       " 'nostalgia',\n",
       " 'my songs',\n",
       " 'my music',\n",
       " 'mine',\n",
       " 'litty',\n",
       " 'life',\n",
       " 'house',\n",
       " 'happy',\n",
       " 'good songs',\n",
       " 'girl',\n",
       " 'favorite songs',\n",
       " 'everything',\n",
       " 'classic rock',\n",
       " 'back',\n",
       " '2000s',\n",
       " 'yes',\n",
       " 'wedding dance',\n",
       " 'top',\n",
       " 'sing',\n",
       " 'salsa',\n",
       " 'roll',\n",
       " 'rnb',\n",
       " 'rides',\n",
       " 'rb',\n",
       " 'rainy',\n",
       " 'pump',\n",
       " 'pregame',\n",
       " 'one',\n",
       " 'on the',\n",
       " 'now',\n",
       " 'musica',\n",
       " 'metal',\n",
       " 'list',\n",
       " 'latin',\n",
       " 'june',\n",
       " 'july',\n",
       " 'jazz',\n",
       " 'in the',\n",
       " 'homework',\n",
       " 'home',\n",
       " 'good music',\n",
       " 'go',\n",
       " 'girls',\n",
       " 'get',\n",
       " 'drive',\n",
       " 'dad',\n",
       " 'car rides',\n",
       " 'beats',\n",
       " 'all time',\n",
       " '',\n",
       " 'yeah',\n",
       " 'winter',\n",
       " 'wedding playlist',\n",
       " 'vibe',\n",
       " 'travel',\n",
       " 'the road',\n",
       " 'the good',\n",
       " 'the best',\n",
       " 'rainy days',\n",
       " 'punk',\n",
       " 'pump up',\n",
       " 'prom',\n",
       " 'party mix',\n",
       " 'oldies but',\n",
       " 'nye',\n",
       " 'not',\n",
       " 'no',\n",
       " 'n',\n",
       " 'my favorites',\n",
       " 'musicals',\n",
       " 'mixed',\n",
       " 'love songs',\n",
       " 'let',\n",
       " 'jesus jams',\n",
       " 'jammin',\n",
       " 'idk',\n",
       " 'i',\n",
       " 'heavy',\n",
       " 'heart',\n",
       " 'hard',\n",
       " 'gaming',\n",
       " 'fire',\n",
       " 'feelin',\n",
       " 'espaol',\n",
       " 'easy',\n",
       " 'cool',\n",
       " 'classical',\n",
       " 'chillin',\n",
       " 'chill vibes',\n",
       " 'break',\n",
       " 'banda',\n",
       " 'aux',\n",
       " '70s',\n",
       " '2013',\n",
       " '',\n",
       " 'young',\n",
       " 'yo',\n",
       " 'work it',\n",
       " 'white',\n",
       " 'wave',\n",
       " 'ultimate',\n",
       " 'turnt',\n",
       " 'tracks',\n",
       " 'things',\n",
       " 'techno',\n",
       " 'tb',\n",
       " 'tailgate',\n",
       " 'swing',\n",
       " 'sunday',\n",
       " 'summer 16',\n",
       " 'summa',\n",
       " 'studying',\n",
       " 'songs to',\n",
       " 'song',\n",
       " 'smooth',\n",
       " 'slow songs',\n",
       " 'rock n',\n",
       " 'relax',\n",
       " 'ready',\n",
       " 'rave',\n",
       " 'rage',\n",
       " 'quiet',\n",
       " 'plane',\n",
       " 'people',\n",
       " 'party time',\n",
       " 'party playlist',\n",
       " 'party bus',\n",
       " 'other',\n",
       " 'oldschool',\n",
       " 'new wave',\n",
       " 'new music',\n",
       " 'n roll',\n",
       " 'my favs',\n",
       " 'my country',\n",
       " 'musical',\n",
       " 'me to',\n",
       " 'lovely',\n",
       " 'like',\n",
       " 'let it',\n",
       " 'kpop',\n",
       " 'it out',\n",
       " 'instrumental',\n",
       " 'heat',\n",
       " 'groovy',\n",
       " 'groove',\n",
       " 'gospel',\n",
       " 'goodies',\n",
       " 'gold',\n",
       " 'games',\n",
       " 'fresh',\n",
       " 'for you',\n",
       " 'folk',\n",
       " 'flow',\n",
       " 'fall 17',\n",
       " 'english',\n",
       " 'emo',\n",
       " 'electro',\n",
       " 'drunk',\n",
       " 'december',\n",
       " 'dance party',\n",
       " 'dance music',\n",
       " 'country jams',\n",
       " 'country favorites',\n",
       " 'college',\n",
       " 'coffee',\n",
       " 'church',\n",
       " 'chilling',\n",
       " 'chill out',\n",
       " 'chill music',\n",
       " 'car jams',\n",
       " 'california',\n",
       " 'bus',\n",
       " 'bumpin',\n",
       " 'bro',\n",
       " 'breathe',\n",
       " 'boat',\n",
       " 'birthday',\n",
       " 'beat',\n",
       " 'be',\n",
       " 'bangerz',\n",
       " 'april',\n",
       " 'alt',\n",
       " 'all the',\n",
       " '80s hits',\n",
       " '60s',\n",
       " '2k17',\n",
       " '15',\n",
       " '1',\n",
       " '',\n",
       " 'zone',\n",
       " 'your',\n",
       " 'yeet',\n",
       " 'year',\n",
       " 'yas',\n",
       " 'yacht',\n",
       " 'xxx',\n",
       " 'wub',\n",
       " 'working out',\n",
       " 'working',\n",
       " 'workday',\n",
       " 'woop',\n",
       " 'winter 2016',\n",
       " 'windows down',\n",
       " 'windows',\n",
       " 'white people',\n",
       " 'wedding dancing',\n",
       " 'vroom',\n",
       " 'vibin',\n",
       " 'vegas',\n",
       " 'trouble',\n",
       " 'trippin',\n",
       " 'trance',\n",
       " 'top tracks',\n",
       " 'to the',\n",
       " 'to sing',\n",
       " 'titty',\n",
       " 'times',\n",
       " 'time favorites',\n",
       " 'throwbackz',\n",
       " 'throwback songs',\n",
       " 'those',\n",
       " 'this',\n",
       " 'thinking',\n",
       " 'the shower',\n",
       " 'that',\n",
       " 'teenage angst',\n",
       " 'teenage',\n",
       " 'tape',\n",
       " 'sweat',\n",
       " 'super chill',\n",
       " 'super',\n",
       " 'summertime',\n",
       " 'summer17',\n",
       " 'summer feels',\n",
       " 'summer country',\n",
       " 'summer 2014',\n",
       " 'summa 16',\n",
       " 'starred 2',\n",
       " 'starred',\n",
       " 'spring break',\n",
       " 'spring 17',\n",
       " 'spring 16',\n",
       " 'spin',\n",
       " 'spanish music',\n",
       " 'south',\n",
       " 'soul',\n",
       " 'snowboarding',\n",
       " 'slow jamz',\n",
       " 'slow it',\n",
       " 'sleepy',\n",
       " 'slaps',\n",
       " 'sing along',\n",
       " 'shop',\n",
       " 'sex',\n",
       " 'september',\n",
       " 'senior',\n",
       " 'sara',\n",
       " 'running mix',\n",
       " 'run run',\n",
       " 'routine',\n",
       " 'room',\n",
       " 'rocks',\n",
       " 'roadtrippin',\n",
       " 'road trippin',\n",
       " 'rise',\n",
       " 'right now',\n",
       " 'right',\n",
       " 'riding',\n",
       " 'ride',\n",
       " 'reggaeton',\n",
       " 'reception',\n",
       " 'real',\n",
       " 'rap music',\n",
       " 'randoms',\n",
       " 'rachel',\n",
       " 'pre game',\n",
       " 'pre',\n",
       " 'power',\n",
       " 'pool party',\n",
       " 'pool',\n",
       " 'pipe it',\n",
       " 'pipe',\n",
       " 'phora',\n",
       " 'partyparty',\n",
       " 'pancho',\n",
       " 'out music',\n",
       " 'ooo',\n",
       " 'oldie but',\n",
       " 'oldie',\n",
       " 'old stuff',\n",
       " 'old country',\n",
       " 'old but',\n",
       " 'oh',\n",
       " 'of the',\n",
       " 'of discover',\n",
       " 'of 2014',\n",
       " 'october',\n",
       " 'nye 2016',\n",
       " 'november',\n",
       " 'new worship',\n",
       " 'new stuff',\n",
       " 'never',\n",
       " 'myself',\n",
       " 'my vibe',\n",
       " 'my time',\n",
       " 'my heart',\n",
       " 'my girl',\n",
       " 'my feels',\n",
       " 'musiccc',\n",
       " 'morning routine',\n",
       " 'morning coffee',\n",
       " 'moody',\n",
       " 'montage',\n",
       " 'moms',\n",
       " 'modern',\n",
       " 'mode',\n",
       " 'mhmm',\n",
       " 'meh',\n",
       " 'meditation',\n",
       " 'maybe',\n",
       " 'matt',\n",
       " 'marathon',\n",
       " 'main',\n",
       " 'maddie',\n",
       " 'lsd',\n",
       " 'loud',\n",
       " 'long car',\n",
       " 'long',\n",
       " 'litty titty',\n",
       " 'lit list',\n",
       " 'listen',\n",
       " 'lifting',\n",
       " 'lets',\n",
       " 'lalala',\n",
       " 'kuntry',\n",
       " 'know',\n",
       " 'kick',\n",
       " 'just rock',\n",
       " 'just good',\n",
       " 'july 2017',\n",
       " 'joe',\n",
       " 'jesus time',\n",
       " 'jesus music',\n",
       " 'january',\n",
       " 'james',\n",
       " 'jake',\n",
       " 'its',\n",
       " 'it up',\n",
       " 'it down',\n",
       " 'it back',\n",
       " 'island',\n",
       " 'ignant',\n",
       " 'iconic',\n",
       " 'huh',\n",
       " 'hop rap',\n",
       " 'holy',\n",
       " 'hoco',\n",
       " 'high',\n",
       " 'hi',\n",
       " 'hey',\n",
       " 'hell',\n",
       " 'hanging',\n",
       " 'half marathon',\n",
       " 'half',\n",
       " 'gucci',\n",
       " 'grooves',\n",
       " 'grind',\n",
       " 'greys anatomy',\n",
       " 'greys',\n",
       " 'grad party',\n",
       " 'grad',\n",
       " 'gospel rap',\n",
       " 'goodie',\n",
       " 'good jams',\n",
       " 'goldies',\n",
       " 'going out',\n",
       " 'going',\n",
       " 'girls girls',\n",
       " 'girl power',\n",
       " 'getting ready',\n",
       " 'getting',\n",
       " 'get up',\n",
       " 'friday',\n",
       " 'for my',\n",
       " 'focus',\n",
       " 'feeling good',\n",
       " 'feeling',\n",
       " 'feelin good',\n",
       " 'favorite country',\n",
       " 'faves',\n",
       " 'fav',\n",
       " 'family party',\n",
       " 'family',\n",
       " 'fall17',\n",
       " 'fall 2016',\n",
       " 'exercise',\n",
       " 'everyday music',\n",
       " 'everyday',\n",
       " 'ever',\n",
       " 'ethereal',\n",
       " 'energy',\n",
       " 'en',\n",
       " 'emotional',\n",
       " 'elevator music',\n",
       " 'elevator',\n",
       " 'electro swing',\n",
       " 'dubstep',\n",
       " 'drank',\n",
       " 'download',\n",
       " 'dope',\n",
       " 'dont',\n",
       " 'discoveries',\n",
       " 'discover',\n",
       " 'dinner',\n",
       " 'dark',\n",
       " 'dancing',\n",
       " 'daily',\n",
       " 'cute',\n",
       " 'current jams',\n",
       " 'cruisin',\n",
       " 'cruise',\n",
       " 'crap',\n",
       " 'covers',\n",
       " 'country love',\n",
       " 'country girl',\n",
       " 'clean',\n",
       " 'classic country',\n",
       " 'christian music',\n",
       " 'chillax',\n",
       " 'chill mix',\n",
       " 'chill jams',\n",
       " 'chili',\n",
       " 'ch',\n",
       " 'cardio',\n",
       " 'car ride',\n",
       " 'car music',\n",
       " 'car jamz',\n",
       " 'candy',\n",
       " 'campfire',\n",
       " 'but goodies',\n",
       " 'but goldies',\n",
       " 'but gold',\n",
       " 'but a',\n",
       " 'bruh',\n",
       " 'boom',\n",
       " 'boat tunes',\n",
       " 'blues',\n",
       " 'better',\n",
       " 'best songs',\n",
       " 'bday party',\n",
       " 'bday',\n",
       " 'bass',\n",
       " 'bang',\n",
       " 'bailable',\n",
       " 'bahamas',\n",
       " 'bad',\n",
       " 'background',\n",
       " 'bachata',\n",
       " 'august 2015',\n",
       " 'august',\n",
       " 'art',\n",
       " 'april 2017',\n",
       " 'anime',\n",
       " 'angst',\n",
       " 'anatomy',\n",
       " 'america',\n",
       " 'along',\n",
       " 'alo',\n",
       " 'alex',\n",
       " 'age',\n",
       " 'a goodie',\n",
       " '90s rnb',\n",
       " '70s80s',\n",
       " '4',\n",
       " '2k16',\n",
       " '2015 top',\n",
       " '2012',\n",
       " '20',\n",
       " '14',\n",
       " '12',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'fire',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ' views',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'zzz',\n",
       " 'zouk',\n",
       " 'zone out',\n",
       " 'zombies',\n",
       " 'youth',\n",
       " 'yourself',\n",
       " 'your perfect',\n",
       " 'your body',\n",
       " 'young and',\n",
       " 'you like',\n",
       " 'you know',\n",
       " 'yin',\n",
       " 'yes please',\n",
       " 'yep',\n",
       " 'yellow',\n",
       " 'yeee',\n",
       " 'yee',\n",
       " 'years eve',\n",
       " 'years',\n",
       " 'yeah baby',\n",
       " 'yasssss',\n",
       " 'yassss',\n",
       " 'yah',\n",
       " 'yacht rock',\n",
       " 'ya know',\n",
       " 'ya',\n",
       " 'y salsa',\n",
       " 'y',\n",
       " 'xy',\n",
       " 'xxxx',\n",
       " 'xoxo',\n",
       " 'xmas',\n",
       " 'wut',\n",
       " 'wub wub',\n",
       " 'wtf',\n",
       " 'writing',\n",
       " 'wrecking ball',\n",
       " 'wrecking',\n",
       " 'wrap',\n",
       " 'wowza',\n",
       " 'wow',\n",
       " 'world',\n",
       " 'workout twerkout',\n",
       " 'workout music',\n",
       " 'workout jamz',\n",
       " 'workout 2016',\n",
       " 'workin',\n",
       " 'work playlist',\n",
       " 'work music',\n",
       " 'work mix',\n",
       " 'work hard',\n",
       " 'work 3',\n",
       " 'woop woop',\n",
       " 'woo',\n",
       " 'womp',\n",
       " 'wk',\n",
       " 'with me',\n",
       " 'with',\n",
       " 'witchy',\n",
       " 'wip',\n",
       " 'winter 2015',\n",
       " 'winter 15',\n",
       " 'whip',\n",
       " 'whatevs',\n",
       " 'what the',\n",
       " 'what',\n",
       " 'wet',\n",
       " 'westcoast',\n",
       " 'wesley',\n",
       " 'werkout',\n",
       " 'werk',\n",
       " 'weeknd',\n",
       " 'weekly',\n",
       " 'weekends',\n",
       " 'weekend',\n",
       " 'wednesday',\n",
       " 'wedding reception',\n",
       " 'wedding party',\n",
       " 'wedding music',\n",
       " 'wedding ideas',\n",
       " 'wedding dinner',\n",
       " 'wedding ceremony',\n",
       " 'wedding background',\n",
       " 'wavy',\n",
       " 'was a',\n",
       " 'was',\n",
       " 'warmups',\n",
       " 'warmth',\n",
       " 'wap',\n",
       " 'wanderlust',\n",
       " 'wake up',\n",
       " 'wake',\n",
       " 'vybz',\n",
       " 'vroom vroom',\n",
       " 'vol 1',\n",
       " 'vol',\n",
       " 'vocaloid',\n",
       " 'vm',\n",
       " 'vintage',\n",
       " 'vii',\n",
       " 'views',\n",
       " 'viejitas',\n",
       " 'vida',\n",
       " 'vibing',\n",
       " 'vibezz',\n",
       " 'vibez',\n",
       " 'vibey',\n",
       " 'variety',\n",
       " 'valentines day',\n",
       " 'valentines',\n",
       " 'us',\n",
       " 'upside down',\n",
       " 'upside',\n",
       " 'update',\n",
       " 'upbeat songs',\n",
       " 'upbeat',\n",
       " 'up the',\n",
       " 'up and',\n",
       " 'unwind',\n",
       " 'untitled',\n",
       " 'under the',\n",
       " 'under',\n",
       " 'ultimate playlist',\n",
       " 'ultimate party',\n",
       " 'uff',\n",
       " 'tyler',\n",
       " 'twerkout',\n",
       " 'turnt up',\n",
       " 'tunez',\n",
       " 'tune',\n",
       " 'truck',\n",
       " 'trips',\n",
       " 'trippy',\n",
       " 'trip music',\n",
       " 'trip 2',\n",
       " 'triggered',\n",
       " 'trey',\n",
       " 'trees',\n",
       " 'travel playlist',\n",
       " 'trappin',\n",
       " 'trap house',\n",
       " 'tranquility',\n",
       " 'top country',\n",
       " 'top 40',\n",
       " 'too much',\n",
       " 'too',\n",
       " 'todo',\n",
       " 'today',\n",
       " 'to this',\n",
       " 'to sleep',\n",
       " 'to learn',\n",
       " 'to church',\n",
       " 'tis the',\n",
       " 'tis',\n",
       " 'times roll',\n",
       " 'time low',\n",
       " 'time favs',\n",
       " 'tight',\n",
       " 'tiger',\n",
       " 'thug life',\n",
       " 'thug',\n",
       " 'throw',\n",
       " 'throwing it',\n",
       " 'throwing',\n",
       " 'throwin it',\n",
       " 'throwin',\n",
       " 'throwbacksss',\n",
       " 'throwbackkkkk',\n",
       " 'throwbackkkk',\n",
       " 'throwback jamz',\n",
       " 'throwback jams',\n",
       " 'throwback country',\n",
       " 'throw back',\n",
       " 'throw',\n",
       " 'three days',\n",
       " 'three',\n",
       " 'thoughts',\n",
       " 'those songs',\n",
       " 'those days',\n",
       " 'this one',\n",
       " 'thinking out',\n",
       " 'think',\n",
       " 'theatre',\n",
       " 'the weeknd',\n",
       " 'the vibes',\n",
       " 'the tiger',\n",
       " 'the season',\n",
       " 'the queen',\n",
       " 'the playlist',\n",
       " 'the party',\n",
       " 'the moon',\n",
       " 'the moment',\n",
       " 'the jams',\n",
       " 'the jam',\n",
       " 'the influence',\n",
       " 'the hunger',\n",
       " 'the hop',\n",
       " 'the grind',\n",
       " 'the go',\n",
       " 'the galaxy',\n",
       " 'the folk',\n",
       " 'the dumps',\n",
       " 'the day',\n",
       " 'the dark',\n",
       " 'the 80s',\n",
       " 'thats my',\n",
       " 'thats',\n",
       " 'that playlist',\n",
       " 'that i',\n",
       " 'thailand',\n",
       " 'texas',\n",
       " 'tejano',\n",
       " 'teen',\n",
       " 'tbs',\n",
       " 'talk is',\n",
       " 'talk',\n",
       " 'take me',\n",
       " 'take',\n",
       " 'tailgating',\n",
       " 'tahoe',\n",
       " 'sweg',\n",
       " 'sweet 16',\n",
       " 'sweet',\n",
       " 'sway',\n",
       " 'swaggy',\n",
       " 'sure',\n",
       " 'supernatural',\n",
       " 'sundaze',\n",
       " 'sunday morning',\n",
       " 'sunday afternoon',\n",
       " 'summit',\n",
       " 'summer ',\n",
       " 'summer songs',\n",
       " 'summer seventeen',\n",
       " 'summer playlist',\n",
       " 'summer jams',\n",
       " 'summer drive',\n",
       " 'summer daze',\n",
       " 'summer chill',\n",
       " 'summer 2k17',\n",
       " 'summer 2k16',\n",
       " 'summer 2k15',\n",
       " 'summer 2015',\n",
       " 'summer 2013',\n",
       " 'summer 2012',\n",
       " 'summer 15',\n",
       " 'summer 14',\n",
       " 'sum',\n",
       " 'suits',\n",
       " 'suh',\n",
       " 'sucks',\n",
       " 'study time',\n",
       " 'study sesh',\n",
       " 'study grind',\n",
       " 'stressed',\n",
       " 'stress relief',\n",
       " 'stress',\n",
       " 'stranger things',\n",
       " 'stranger',\n",
       " 'stop believin',\n",
       " 'stop',\n",
       " 'steven',\n",
       " 'step up',\n",
       " 'step',\n",
       " 'stay classy',\n",
       " 'stay',\n",
       " 'starter',\n",
       " 'stars',\n",
       " 'stargazing',\n",
       " 'stardust',\n",
       " 'springtime',\n",
       " 'spring17',\n",
       " 'spring summer',\n",
       " 'spring 2016',\n",
       " 'spring 2015',\n",
       " 'spring 14',\n",
       " 'sprang break',\n",
       " 'sprang',\n",
       " 'spotify',\n",
       " 'spoopy',\n",
       " 'splash',\n",
       " 'special',\n",
       " 'sparks',\n",
       " 'spanish songs',\n",
       " 'spacey',\n",
       " 'soundtracks',\n",
       " 'sounds good',\n",
       " 'sounds',\n",
       " 'soundhound',\n",
       " 'soulfunk',\n",
       " 'songz',\n",
       " 'songwriters',\n",
       " 'songs that',\n",
       " 'songs of',\n",
       " 'songs for',\n",
       " 'somewhere',\n",
       " 'sola',\n",
       " 'soft rock',\n",
       " 'soft jams',\n",
       " 'social',\n",
       " 'soca',\n",
       " 'so chill',\n",
       " 'so',\n",
       " 'snow',\n",
       " 'snoop',\n",
       " 'snap',\n",
       " 'smoove',\n",
       " 'smoke break',\n",
       " 'smoke',\n",
       " 'smile',\n",
       " 'slumber',\n",
       " 'slow jams',\n",
       " 'slow down',\n",
       " 'slow country',\n",
       " 'sleeping playlist',\n",
       " 'sleeping',\n",
       " 'sleep tight',\n",
       " 'sleep music',\n",
       " 'sleep 2',\n",
       " 'slapz',\n",
       " 'sky',\n",
       " 'ski',\n",
       " 'sit back',\n",
       " 'sit',\n",
       " 'singing in',\n",
       " 'singing',\n",
       " 'singer songwriters',\n",
       " 'singer',\n",
       " 'sing me',\n",
       " 'sing in',\n",
       " 'simpin',\n",
       " 'sigh',\n",
       " 'sierra',\n",
       " 'side',\n",
       " 'showtunes',\n",
       " 'shower time',\n",
       " 'shower mix',\n",
       " 'shotgun',\n",
       " 'shoegaze',\n",
       " 'shleep',\n",
       " 'shame',\n",
       " 'shadowhunters',\n",
       " 'sg',\n",
       " 'sex music',\n",
       " 'seventeen',\n",
       " ...]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_playlist_model.pl_name_text_embedding.layers[0].get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d020d381-738b-4d7e-80ff-3d56c5bb701b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dict_load['name_voacb'] = test_playlist_model.pl_name_text_embedding.layers[0].get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b87363d4-64d9-4cca-9290-b26efdfe9090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 1.75068378e-01  1.06176727e-01  1.19251013e-01 -3.13839406e-01\n",
      "  -1.65929541e-01  2.61445850e-01 -1.06377922e-01 -9.99871548e-03\n",
      "  -2.08328366e-01 -4.21047993e-02 -6.54220507e-02  3.51857007e-01\n",
      "  -1.27639668e-02 -2.96327975e-02  2.63485998e-01 -1.98547721e-01\n",
      "   2.12687925e-01 -2.71099448e-01 -2.73166075e-02  1.51845098e-01\n",
      "   3.00694518e-02  3.72495353e-01  1.68648049e-01 -1.08165391e-01\n",
      "  -1.02048248e-01  2.44667800e-03 -5.03552286e-03 -5.04281670e-02\n",
      "  -2.72486120e-01  8.97828415e-02 -6.43469244e-02  1.82053357e-01]\n",
      " [ 1.37110069e-01  1.75218642e-01  1.95286106e-02 -3.17005992e-01\n",
      "  -4.14202772e-02  4.48241055e-01  1.63688123e-01  2.20893938e-02\n",
      "  -1.22042693e-01  5.38237393e-03  8.96497890e-02  1.55782774e-01\n",
      "   1.17517099e-01 -1.45114735e-01  4.74771000e-02 -1.49342343e-01\n",
      "   1.03034601e-01 -2.80737877e-01  2.18786061e-01  6.89666718e-02\n",
      "  -9.43843573e-02  4.40004051e-01  8.42144564e-02 -8.30152035e-02\n",
      "  -1.29518285e-01 -2.68459227e-02 -1.60139322e-01 -3.16282809e-02\n",
      "  -3.16738784e-01 -3.48240137e-02 -4.31761406e-02  8.34440365e-02]\n",
      " [ 1.20945759e-01  2.28182063e-01  1.49382025e-01 -1.56113103e-01\n",
      "  -2.57717639e-01  4.62515771e-01 -1.59310281e-01 -3.54506560e-02\n",
      "  -5.25986999e-02 -4.32135090e-02  3.69317643e-02  3.72003257e-01\n",
      "  -3.87368398e-03  1.46957606e-01  2.25037545e-01 -1.62378728e-01\n",
      "   1.61985070e-01 -3.12295169e-01  4.68432717e-02  6.96289539e-02\n",
      "   1.15804270e-01  2.86466241e-01  1.99399740e-01 -8.13087076e-02\n",
      "   5.20683266e-02  8.65144730e-02  5.93840778e-02  4.32298519e-02\n",
      "  -6.44731894e-02  1.36654064e-01 -7.03335851e-02  1.43577084e-01]\n",
      " [ 2.13088378e-01  1.04634173e-01  1.04712814e-01 -3.83318901e-01\n",
      "  -2.42498562e-01  3.96441132e-01 -1.19868450e-01  1.01658344e-01\n",
      "  -3.18488143e-02  1.14089604e-02 -4.52347174e-02  1.66908264e-01\n",
      "   1.11118667e-01 -1.71210722e-03  1.45025298e-01 -1.69721961e-01\n",
      "   2.26841480e-01 -3.38090777e-01  1.97932884e-01  1.15538470e-01\n",
      "  -1.72001217e-03  1.20676294e-01  9.80259180e-02 -1.81937233e-01\n",
      "   4.52909879e-02 -2.12126017e-01  1.42620653e-01 -1.39561281e-01\n",
      "  -2.54913718e-01  6.84158430e-02 -1.29743144e-01  5.39324842e-02]\n",
      " [ 2.94113934e-01  1.89718679e-02 -2.99375169e-02 -3.46084267e-01\n",
      "  -6.43294081e-02  3.41194600e-01  9.09997746e-02  1.40580341e-01\n",
      "  -1.01915501e-01  6.46083057e-02  2.16691312e-03  4.53594550e-02\n",
      "   5.26671484e-02 -1.04468703e-01 -8.74165853e-04  4.47702743e-02\n",
      "   1.74097568e-01 -4.86410499e-01  3.43406290e-01 -1.84424762e-02\n",
      "  -1.15720436e-01  2.67657459e-01  6.75009273e-04 -2.17140839e-02\n",
      "  -2.53734994e-03 -1.34855554e-01 -4.78000566e-02 -9.21032652e-02\n",
      "  -2.33993992e-01  1.51780397e-01 -6.09772429e-02 -1.74381033e-01]\n",
      " [ 1.27446473e-01  2.00113252e-01  1.23842165e-01 -2.84822792e-01\n",
      "  -3.03180993e-01  1.82652861e-01  2.72089958e-01  1.14208259e-01\n",
      "  -1.20513409e-01  1.18723288e-01 -1.27919316e-02  1.05179965e-01\n",
      "   1.44096285e-01  4.86926585e-02  1.96398750e-01 -1.40643910e-01\n",
      "   2.64494359e-01 -2.95322269e-01  7.29844943e-02  2.18021289e-01\n",
      "  -1.03197023e-01  2.58413851e-01 -1.61360204e-02 -1.78272396e-01\n",
      "   9.02470201e-02  1.13583198e-02  1.69242378e-02 -2.03983769e-01\n",
      "  -3.76625985e-01 -1.31061841e-02  2.74458118e-02 -3.34382467e-02]\n",
      " [ 5.48221730e-02 -3.85404564e-02  7.11091757e-02 -4.03041750e-01\n",
      "  -2.79339373e-01  2.22101629e-01 -6.85212910e-02  6.45079985e-02\n",
      "  -1.11640595e-01 -6.59402981e-02 -1.08736210e-01  8.54321122e-02\n",
      "   1.63099542e-01  5.25952457e-03  1.68213591e-01 -2.18161672e-01\n",
      "   2.94623226e-01 -3.83243620e-01  1.97835743e-01  2.11246207e-01\n",
      "  -7.64177889e-02  2.10054398e-01 -4.79485802e-02 -1.06264412e-01\n",
      "  -1.21107027e-01 -6.48147166e-02 -7.90649373e-03 -3.19418728e-01\n",
      "  -9.77801010e-02  1.77966848e-01  1.00378320e-01 -8.75859708e-03]\n",
      " [ 1.37779906e-01  1.06987599e-02 -6.95117190e-02 -4.80190247e-01\n",
      "  -1.24722585e-01  3.94267768e-01  4.29471815e-03  1.20127089e-01\n",
      "  -1.57167971e-01 -4.47423458e-02  8.86860564e-02  3.57310325e-02\n",
      "   7.16923848e-02 -1.08145066e-01  2.64692008e-01 -2.33470947e-01\n",
      "   1.89625546e-01 -2.42971614e-01  2.63674974e-01 -6.36847690e-02\n",
      "   3.12410425e-02  1.83282763e-01 -1.38045812e-04 -1.52236462e-01\n",
      "  -4.63412050e-03 -5.82872741e-02  7.29914233e-02 -1.53929472e-01\n",
      "  -3.37815553e-01  6.54505119e-02  7.37752467e-02  1.34214647e-02]\n",
      " [ 1.36451438e-01 -5.57074323e-02  1.25262126e-01 -1.78378940e-01\n",
      "  -2.13184491e-01  2.15943292e-01 -1.34008795e-01  8.22942331e-02\n",
      "  -1.47665024e-01  1.71492651e-01 -3.88417160e-03  1.82940736e-01\n",
      "   1.27770916e-01 -1.74327299e-01  1.78485289e-01 -9.11461413e-02\n",
      "   2.40974098e-01 -5.00844717e-01  1.33041173e-01 -7.36448094e-02\n",
      "   5.42064980e-02  2.99897403e-01  6.25264570e-02 -9.56835877e-03\n",
      "  -4.83855680e-02  1.13550723e-01 -6.79036230e-02 -4.24152389e-02\n",
      "  -3.32145154e-01  2.60835797e-01  1.13713436e-01 -2.31136791e-02]\n",
      " [ 2.75978833e-01  2.43094489e-01  3.75220627e-02 -3.24567795e-01\n",
      "  -1.97435960e-01  2.81791955e-01 -1.36260420e-01 -4.34968099e-02\n",
      "  -5.82031086e-02 -1.88131128e-02  3.72906849e-02  3.00598145e-01\n",
      "   9.43270177e-02 -4.78123464e-02  1.67657081e-02 -3.82442363e-02\n",
      "   2.56490946e-01 -3.76155198e-01  6.15635701e-02 -1.68632325e-02\n",
      "  -6.28580004e-02  4.14187700e-01  2.57059336e-01 -6.39377087e-02\n",
      "  -5.39046824e-02 -4.44667268e-04 -4.55047078e-02  4.48307730e-02\n",
      "  -1.54183641e-01  5.88380285e-02  1.64396055e-02  1.38590232e-01]], shape=(10, 32), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#test with the source batched datset\n",
    "batched_dataset = parsed_dataset_padded.batch(10)\n",
    "for x in batched_dataset.take(1):\n",
    "    print(test_playlist_model(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "af26dfe7-85fa-4168-9cdd-c138587c037f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([b''], shape=(1,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "#diagnosing NANs\n",
    "batched_dataset = parsed_dataset_padded.skip(8).batch(1) # @JT this seems to matter on the data - I think there are bad lookups maybe\n",
    "for x in batched_dataset.take(1):\n",
    "    print(x['description_pl'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9f0be2a2-af20-47e0-8759-94a48fa6d996",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"playlist__model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " pl_name_emb_model (Sequenti  (None, 32)               2368928   \n",
      " al)                                                             \n",
      "||\n",
      "| pl_name_txt_vectorizer (Tex  (None, None)           0         |\n",
      "| tVectorization)                                               |\n",
      "|                                                               |\n",
      "| pl_name_emb_layer (Embeddin  (None, None, 32)       2368928   |\n",
      "| g)                                                            |\n",
      "|                                                               |\n",
      "| pl_name_pooling (GlobalAver  (None, 32)             0         |\n",
      "| agePooling1D)                                                 |\n",
      "\n",
      " pl_collaborative_emb_model   (10, 32)                 96        \n",
      " (Sequential)                                                    \n",
      "||\n",
      "| pl_collaborative_lookup (St  (10,)                  0         |\n",
      "| ringLookup)                                                   |\n",
      "|                                                               |\n",
      "| pl_collaborative_emb_layer   (10, 32)               96        |\n",
      "| (Embedding)                                                   |\n",
      "\n",
      " pl_track_uri_emb_model (Seq  (10, 32)                 72393376  \n",
      " uential)                                                        \n",
      "||\n",
      "| pl_track_uri_lookup (String  (10,)                  0         |\n",
      "| Lookup)                                                       |\n",
      "|                                                               |\n",
      "| pl_track_uri_layer (Embeddi  (10, 32)               72393376  |\n",
      "| ng)                                                           |\n",
      "\n",
      " n_songs_pl_emb_model (Seque  (10, 32)                 3232      \n",
      " ntial)                                                          \n",
      "||\n",
      "| discretization_7 (Discretiz  (10,)                  0         |\n",
      "| ation)                                                        |\n",
      "|                                                               |\n",
      "| n_songs_pl_emb_layer (Embed  (10, 32)               3232      |\n",
      "| ding)                                                         |\n",
      "\n",
      " n_artists_pl_emb_model (Seq  (10, 32)                 3232      \n",
      " uential)                                                        \n",
      "||\n",
      "| discretization_8 (Discretiz  (10,)                  0         |\n",
      "| ation)                                                        |\n",
      "|                                                               |\n",
      "| n_artists_pl_emb_layer (Emb  (10, 32)               3232      |\n",
      "| edding)                                                       |\n",
      "\n",
      " n_albums_pl_emb_model (Sequ  (10, 32)                 3232      \n",
      " ential)                                                         \n",
      "||\n",
      "| discretization_9 (Discretiz  (10,)                  0         |\n",
      "| ation)                                                        |\n",
      "|                                                               |\n",
      "| n_albums_pl_emb_layer (Embe  (10, 32)               3232      |\n",
      "| dding)                                                        |\n",
      "\n",
      " artist_name_pl_emb_model (S  (10, 32)                 9206752   \n",
      " equential)                                                      \n",
      "||\n",
      "| string_lookup_5 (StringLook  (10, 375)              0         |\n",
      "| up)                                                           |\n",
      "|                                                               |\n",
      "| artist_name_pl_emb_layer (E  (10, 375, 32)          9206752   |\n",
      "| mbedding)                                                     |\n",
      "|                                                               |\n",
      "| artist_name_pl_1d (GlobalAv  (10, 32)               0         |\n",
      "| eragePooling1D)                                               |\n",
      "\n",
      " track_uri_pl_emb_model (Seq  (10, 32)                 72393376  \n",
      " uential)                                                        \n",
      "||\n",
      "| string_lookup_6 (StringLook  (10, 375)              0         |\n",
      "| up)                                                           |\n",
      "|                                                               |\n",
      "| track_uri_pl_emb_layer (Emb  (10, 375, 32)          72393376  |\n",
      "| edding)                                                       |\n",
      "|                                                               |\n",
      "| track_uri_1d (GlobalAverage  (10, 32)               0         |\n",
      "| Pooling1D)                                                    |\n",
      "\n",
      " track_name_pl_emb_model (Se  (10, 32)                 47480160  \n",
      " quential)                                                       \n",
      "||\n",
      "| track_name_pl_lookup (Strin  (10, 375)              0         |\n",
      "| gLookup)                                                      |\n",
      "|                                                               |\n",
      "| track_name_pl_emb_layer (Em  (10, 375, 32)          47480160  |\n",
      "| bedding)                                                      |\n",
      "|                                                               |\n",
      "| track_name_pl_1d (GlobalAve  (10, 32)               0         |\n",
      "| ragePooling1D)                                                |\n",
      "\n",
      " duration_ms_songs_pl_emb_mo  (10, 32)                 3232      \n",
      " del (Sequential)                                                \n",
      "||\n",
      "| discretization_10 (Discreti  (10, 375)              0         |\n",
      "| zation)                                                       |\n",
      "|                                                               |\n",
      "| duration_ms_songs_pl_emb_la  (10, 375, 32)          3232      |\n",
      "| yer (Embedding)                                               |\n",
      "|                                                               |\n",
      "| duration_ms_songs_pl_emb_la  (10, 32)               0         |\n",
      "| yer_pl_1d (GlobalAveragePoo                                   |\n",
      "| ling1D)                                                       |\n",
      "\n",
      " album_name_pl_emb_model (Se  (10, 32)                 18292032  \n",
      " quential)                                                       \n",
      "||\n",
      "| album_name_pl_lookup (Strin  (10, 375)              0         |\n",
      "| gLookup)                                                      |\n",
      "|                                                               |\n",
      "| album_name_pl_emb_layer (Em  (10, 375, 32)          18292032  |\n",
      "| bedding)                                                      |\n",
      "|                                                               |\n",
      "| album_name_pl_emb_layer_1d   (10, 32)               0         |\n",
      "| (GlobalAveragePooling1D)                                      |\n",
      "\n",
      " artist_pop_pl_emb_model (Se  (10, 32)                 352       \n",
      " quential)                                                       \n",
      "||\n",
      "| discretization_11 (Discreti  (10, 375)              0         |\n",
      "| zation)                                                       |\n",
      "|                                                               |\n",
      "| artist_pop_pl_emb_layer (Em  (10, 375, 32)          352       |\n",
      "| bedding)                                                      |\n",
      "|                                                               |\n",
      "| artist_pop_1d (GlobalAverag  (10, 32)               0         |\n",
      "| ePooling1D)                                                   |\n",
      "\n",
      " artists_followers_pl_emb_mo  (10, 32)                 352       \n",
      " del (Sequential)                                                \n",
      "||\n",
      "| discretization_12 (Discreti  (10, 375)              0         |\n",
      "| zation)                                                       |\n",
      "|                                                               |\n",
      "| artists_followers_pl_emb_la  (10, 375, 32)          352       |\n",
      "| yer (Embedding)                                               |\n",
      "|                                                               |\n",
      "| artists_followers_pl_1d (Gl  (10, 32)               0         |\n",
      "| obalAveragePooling1D)                                         |\n",
      "\n",
      " track_pop_pl_emb_model (Seq  (10, 32)                 352       \n",
      " uential)                                                        \n",
      "||\n",
      "| discretization_13 (Discreti  (10, 375)              0         |\n",
      "| zation)                                                       |\n",
      "|                                                               |\n",
      "| track_pop_pl_emb_layer (Emb  (10, 375, 32)          352       |\n",
      "| edding)                                                       |\n",
      "|                                                               |\n",
      "| track_pop_pl_1d (GlobalAver  (10, 32)               0         |\n",
      "| agePooling1D)                                                 |\n",
      "\n",
      " artist_genres_pl_emb_model   (10, 32)                 1260768   \n",
      " (Sequential)                                                    \n",
      "||\n",
      "| flatten_1 (Flatten)       (10, 375)                 0         |\n",
      "|                                                               |\n",
      "| string_lookup_7 (StringLook  (10, 375)              0         |\n",
      "| up)                                                           |\n",
      "|                                                               |\n",
      "| artist_genres_pl_emb_layer   (10, 375, 32)          1260768   |\n",
      "| (Embedding)                                                   |\n",
      "|                                                               |\n",
      "| artist_genres_pl_1d (Global  (10, 32)               0         |\n",
      "| AveragePooling1D)                                             |\n",
      "\n",
      " pl_cross_layer (Cross)      multiple                  5280      \n",
      "                                                                 \n",
      " pl_dense_layers (Sequential  (10, 32)                 32864     \n",
      " )                                                               \n",
      "||\n",
      "| dense_2 (Dense)           (10, 64)                  30784     |\n",
      "|                                                               |\n",
      "| dropout_1 (Dropout)       (10, 64)                  0         |\n",
      "|                                                               |\n",
      "| dense_3 (Dense)           (10, 32)                  2080      |\n",
      "|                                                               |\n",
      "| lambda_1 (Lambda)         (10, 32)                  0         |\n",
      "\n",
      "=================================================================\n",
      "Total params: 223,447,616\n",
      "Trainable params: 223,447,616\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "test_playlist_model.summary(expand_nested=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede145f9-1973-4261-8d98-c2429857b125",
   "metadata": {},
   "source": [
    "## Track (candidate) Tower "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f92c564e-e5e4-43b0-a011-fd4355555c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 32\n",
    "PROJECTION_DIM = 5\n",
    "SEED = 1234\n",
    "USE_CROSS_LAYER=True\n",
    "DROPOUT='False'\n",
    "DROPOUT_RATE='0.33'\n",
    "\n",
    "# candidate_test_instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9345cb95-f44f-4437-8ffa-d1670b30da15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Candidate_Track_Model(tf.keras.Model):\n",
    "    def __init__(self, layer_sizes, vocab_dict):\n",
    "        super().__init__()\n",
    "        \n",
    "        # ========================================\n",
    "        # Candidate features\n",
    "        # ========================================\n",
    "        \n",
    "        # Feature: artist_name_can\n",
    "        self.artist_name_can_text_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.TextVectorization(\n",
    "                    vocabulary=vocab_dict[\"artist_name_can\"],\n",
    "                    name=\"artist_name_can_txt_vectorizer\",\n",
    "                    ngrams=2,\n",
    "                ),\n",
    "                tf.keras.layers.Embedding(\n",
    "                    input_dim=len(vocab_dict[\"artist_name_can\"])+1,\n",
    "                    output_dim=EMBEDDING_DIM,\n",
    "                    mask_zero=False,\n",
    "                    name=\"artist_name_can_emb_layer\",\n",
    "                ),\n",
    "                tf.keras.layers.GlobalAveragePooling1D(name=\"artist_name_can_pooling\"),\n",
    "            ], name=\"artist_name_can_emb_model\"\n",
    "        )\n",
    "        \n",
    "        # Feature: track_name_can\n",
    "        self.track_name_can_text_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.TextVectorization(\n",
    "                    vocabulary=vocab_dict[\"track_name_can\"],\n",
    "                    name=\"track_name_can_txt_vectorizer\",\n",
    "                    ngrams=2,\n",
    "                ),\n",
    "                tf.keras.layers.Embedding(\n",
    "                    input_dim=len(vocab_dict[\"track_name_can\"])+1,\n",
    "                    output_dim=EMBEDDING_DIM,\n",
    "                    mask_zero=False,\n",
    "                    name=\"track_name_can_emb_layer\",\n",
    "                ),\n",
    "                tf.keras.layers.GlobalAveragePooling1D(name=\"track_name_can_pooling\"),\n",
    "            ], name=\"track_name_can_emb_model\"\n",
    "        )\n",
    "        \n",
    "        # Feature: album_name_can\n",
    "        self.album_name_can_text_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.TextVectorization(\n",
    "                    vocabulary=vocab_dict[\"album_name_can\"],\n",
    "                    name=\"album_name_can_txt_vectorizer\",\n",
    "                    ngrams=2,\n",
    "                ),\n",
    "                tf.keras.layers.Embedding(\n",
    "                    input_dim=len(vocab_dict[\"album_name_can\"])+1,\n",
    "                    output_dim=EMBEDDING_DIM,\n",
    "                    mask_zero=False,\n",
    "                    name=\"album_name_can_emb_layer\",\n",
    "                ),\n",
    "                tf.keras.layers.GlobalAveragePooling1D(name=\"album_name_can_pooling\"),\n",
    "            ], name=\"album_name_can_emb_model\"\n",
    "        )\n",
    "        \n",
    "        # Feature: artist_uri_can\n",
    "        self.artist_uri_can_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.Hashing(num_bins=200_000),\n",
    "                tf.keras.layers.Embedding(\n",
    "                    input_dim=200_000+1, \n",
    "                    output_dim=EMBEDDING_DIM,\n",
    "                    name=\"artist_uri_can_emb_layer\",\n",
    "                ),\n",
    "            ], name=\"artist_uri_can_emb_model\"\n",
    "        )\n",
    "        \n",
    "        # Feature: track_uri_can\n",
    "        self.track_uri_can_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.Hashing(num_bins=200_000),\n",
    "                tf.keras.layers.Embedding(\n",
    "                    input_dim=200_000+1, \n",
    "                    output_dim=EMBEDDING_DIM,\n",
    "                    name=\"track_uri_can_emb_layer\",\n",
    "                ),\n",
    "            ], name=\"track_uri_can_emb_model\"\n",
    "        )\n",
    "        \n",
    "        # Feature: album_uri_can\n",
    "        self.album_uri_can_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.Hashing(num_bins=200_000),\n",
    "                tf.keras.layers.Embedding(\n",
    "                    input_dim=200_000+1, \n",
    "                    output_dim=EMBEDDING_DIM,\n",
    "                    name=\"album_uri_can_emb_layer\",\n",
    "                ),\n",
    "            ], name=\"album_uri_can_emb_model\"\n",
    "        )\n",
    "        \n",
    "        # Feature: duration_ms_can\n",
    "        self.duration_ms_can_normalized = tf.keras.layers.Normalization(\n",
    "            mean=vocab_dict['avg_duration_ms_songs_pl'],\n",
    "            variance=vocab_dict['var_duration_ms_songs_pl'],\n",
    "            axis=None\n",
    "        )\n",
    "        \n",
    "        # Feature: track_pop_can\n",
    "        self.track_pop_can_normalized = tf.keras.layers.Normalization(\n",
    "            mean=vocab_dict['avg_track_pop'],\n",
    "            variance=vocab_dict['var_track_pop'],\n",
    "            axis=None\n",
    "        )\n",
    "        \n",
    "        # Feature: artist_pop_can\n",
    "        self.artist_pop_can_normalized = tf.keras.layers.Normalization(\n",
    "            mean=vocab_dict['avg_artist_pop'],\n",
    "            variance=vocab_dict['var_artist_pop'],\n",
    "            axis=None\n",
    "        )\n",
    "        \n",
    "        # Feature: artist_followers_can\n",
    "        self.artist_followers_can_normalized = tf.keras.layers.Normalization(\n",
    "            mean=vocab_dict['avg_artist_followers'],\n",
    "            variance=vocab_dict['var_artist_followers'],\n",
    "            axis=None\n",
    "        )\n",
    "        \n",
    "        # Feature: artist_genres_can\n",
    "        self.artist_genres_can_text_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.TextVectorization(\n",
    "                    vocabulary=vocab_dict[\"artist_genres_can\"],\n",
    "                    name=\"artist_genres_can_txt_vectorizer\",\n",
    "                    ngrams=2,\n",
    "                ),\n",
    "                tf.keras.layers.Embedding(\n",
    "                    input_dim=len(vocab_dict[\"artist_genres_can\"])+1,\n",
    "                    output_dim=EMBEDDING_DIM,\n",
    "                    mask_zero=False,\n",
    "                    name=\"artist_genres_can_emb_layer\",\n",
    "                ),\n",
    "                tf.keras.layers.GlobalAveragePooling1D(name=\"artist_genres_can_pooling\"),\n",
    "            ], name=\"artist_genres_can_emb_model\"\n",
    "        )\n",
    "        \n",
    "        # ========================================\n",
    "        # Dense & Cross Layers\n",
    "        # ========================================\n",
    "        \n",
    "        # Cross Layers\n",
    "        if USE_CROSS_LAYER:\n",
    "            self._cross_layer = tfrs.layers.dcn.Cross(\n",
    "                projection_dim=PROJECTION_DIM,\n",
    "                kernel_initializer=\"glorot_uniform\", \n",
    "                name=\"can_cross_layer\"\n",
    "            )\n",
    "        else:\n",
    "            self._cross_layer = None\n",
    "        \n",
    "        # Dense Layer\n",
    "        self.dense_layers = tf.keras.Sequential(name=\"candidate_dense_layers\")\n",
    "        initializer = tf.keras.initializers.GlorotUniform(seed=SEED)\n",
    "        \n",
    "        # Use the ReLU activation for all but the last layer.\n",
    "        for layer_size in layer_sizes[:-1]:\n",
    "            self.dense_layers.add(\n",
    "                tf.keras.layers.Dense(\n",
    "                    layer_size, \n",
    "                    activation=\"relu\", \n",
    "                    kernel_initializer=initializer,\n",
    "                )\n",
    "            )\n",
    "            if DROPOUT:\n",
    "                self.dense_layers.add(tf.keras.layers.Dropout(DROPOUT_RATE))\n",
    "                \n",
    "        # No activation for the last layer\n",
    "        for layer_size in layer_sizes[-1:]:\n",
    "            self.dense_layers.add(\n",
    "                tf.keras.layers.Dense(\n",
    "                    layer_size, \n",
    "                    kernel_initializer=initializer\n",
    "                )\n",
    "            )\n",
    "            \n",
    "    # ========================================\n",
    "    # Call Function\n",
    "    # ========================================\n",
    "            \n",
    "    def call(self, data):\n",
    "        \n",
    "        all_embs = tf.concat(\n",
    "            [\n",
    "                self.artist_name_can_text_embedding(data['artist_name_can']),  #TODO: removed `_can` from feature name\n",
    "                self.track_name_can_text_embedding(data['track_name_can']),  #TODO: removed `_can` from feature name\n",
    "                self.album_name_can_text_embedding(data['album_name_can']),  #TODO: removed `_can` from feature name\n",
    "                self.artist_uri_can_embedding(data['artist_uri_can']),  #TODO: removed `_can` from feature name\n",
    "                self.track_uri_can_embedding(data['track_uri_can']),  #TODO: removed `_can` from feature name\n",
    "                self.album_uri_can_embedding(data['album_uri_can']),  #TODO: removed `_can` from feature name\n",
    "                tf.reshape(self.duration_ms_can_normalized(data[\"duration_ms_can\"]), (-1, 1)),  #TODO: removed `_can` from feature name\n",
    "                tf.reshape(self.track_pop_can_normalized(data[\"track_pop_can\"]), (-1, 1)),  #TODO: removed `_can` from feature name\n",
    "                tf.reshape(self.artist_pop_can_normalized(data[\"artist_pop_can\"]), (-1, 1)),  #TODO: removed `_can` from feature name\n",
    "                tf.reshape(self.artist_followers_can_normalized(data[\"artist_followers_can\"]), (-1, 1)),  #TODO: removed `_can` from feature name\n",
    "                self.artist_genres_can_text_embedding(data['album_uri_can']),  #TODO: removed `_can` from feature name\n",
    "            ], axis=1\n",
    "        )\n",
    "        \n",
    "        \n",
    "        \n",
    "        # return self.dense_layers(all_embs)\n",
    "                # Build Cross Network\n",
    "        if self._cross_layer is not None:\n",
    "            cross_embs = self._cross_layer(all_embs)\n",
    "            return self.dense_layers(cross_embs)\n",
    "        else:\n",
    "            return self.dense_layers(all_embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "01b52944-7db2-4a41-95ff-602b97f35345",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_sizes=[64,32]\n",
    "\n",
    "test_can_track_model = Candidate_Track_Model(layer_sizes, vocab_dict_load)\n",
    "\n",
    "# can_result = test_can_track_model([candidate_test_instance])\n",
    "\n",
    "# print(f\"Shape of can_result: {can_result.shape}\")\n",
    "# can_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c848b2b1-97a4-4d69-96d3-6ff00452389d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b\"\\n\\xc8\\x04\\n1\\n\\x0ealbum_name_can\\x12\\x1f\\n\\x1d\\n\\x1bThe Sound of Everything Rmx\\n9\\n\\ralbum_uri_can\\x12(\\n&\\n$spotify:album:4a8tMD6qq6GUuUwNae38VI\\n \\n\\x14artist_followers_can\\x12\\x08\\x12\\x06\\n\\x04 \\x92\\x87H\\n}\\n\\x11artist_genres_can\\x12h\\nf\\nd'downtempo', 'electronica', 'funk', 'latin alternative', 'nu jazz', 'nu-cumbia', 'trip hop', 'world'\\n\\x1e\\n\\x0fartist_name_can\\x12\\x0b\\n\\t\\n\\x07Quantic\\n\\x1a\\n\\x0eartist_pop_can\\x12\\x08\\x12\\x06\\n\\x04\\x00\\x00\\x80B\\n;\\n\\x0eartist_uri_can\\x12)\\n'\\n%spotify:artist:5ZMwoAjeDtLJ0XRwRTgaK8\\n\\x1b\\n\\x0fduration_ms_can\\x12\\x08\\x12\\x06\\n\\x04@o\\x82H\\nK\\n\\x0etrack_name_can\\x129\\n7\\n5The Sound of Everything - Watch TV & Se\\xc3\\xb1orlobo Remix\\n\\x19\\n\\rtrack_pop_can\\x12\\x08\\x12\\x06\\n\\x04\\x00\\x00TB\\n9\\n\\rtrack_uri_can\\x12(\\n&\\n$spotify:track:27CDzo2P7Mf3dKoa76tNxb\", shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "for x in candidate_dataset.take(1):\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "02d1c46d-a143-4269-828d-04df1f9d1cc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[-0.09349287 -0.22176957 -0.14311549  0.04991766  0.08692671 -0.3928042\n",
      "  -0.18489382  0.18936606 -0.10074127  0.02399448  0.4489704   0.14458966\n",
      "  -0.15617758 -0.15572914 -0.209638   -0.21301563  0.56768996  0.08034034\n",
      "   0.12346448 -0.5299083   0.22090426 -0.539256    0.0303314   0.3994236\n",
      "  -0.45307884  0.02359112  0.14665318  0.03218898  0.05140447  0.10077867\n",
      "   0.06161986 -0.24914703]\n",
      " [ 0.02675009 -0.24460396 -0.09192912 -0.00456236  0.21518695 -0.01142278\n",
      "   0.08662941  0.04862015 -0.07425587  0.14195742  0.26314047 -0.07850688\n",
      "  -0.12067908  0.06303717  0.11477137 -0.12962018  0.01149705 -0.05550651\n",
      "   0.08546089 -0.20316233 -0.38543117 -0.38999683 -0.19894218  0.09914897\n",
      "  -0.23900056 -0.04658388 -0.21171199  0.07517096 -0.23067577  0.20067586\n",
      "   0.66321564  0.0349327 ]], shape=(2, 32), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#test with the source batched datset and candidate dataset\n",
    "batched_dataset = parsed_candidate_dataset.batch(2)\n",
    "for x in batched_dataset.take(1):\n",
    "    print(test_can_track_model(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "13a7e41a-88f2-4fd5-9d14-910f3f611dc1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"candidate__track__model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " artist_name_can_emb_model (  (None, 32)               9206752   \n",
      " Sequential)                                                     \n",
      "||\n",
      "| artist_name_can_txt_vectori  (None, None)           0         |\n",
      "| zer (TextVectorization)                                       |\n",
      "|                                                               |\n",
      "| artist_name_can_emb_layer (  (None, None, 32)       9206752   |\n",
      "| Embedding)                                                    |\n",
      "|                                                               |\n",
      "| artist_name_can_pooling (Gl  (None, 32)             0         |\n",
      "| obalAveragePooling1D)                                         |\n",
      "\n",
      " track_name_can_emb_model (S  (None, 32)               47480128  \n",
      " equential)                                                      \n",
      "||\n",
      "| track_name_can_txt_vectoriz  (None, None)           0         |\n",
      "| er (TextVectorization)                                        |\n",
      "|                                                               |\n",
      "| track_name_can_emb_layer (E  (None, None, 32)       47480128  |\n",
      "| mbedding)                                                     |\n",
      "|                                                               |\n",
      "| track_name_can_pooling (Glo  (None, 32)             0         |\n",
      "| balAveragePooling1D)                                          |\n",
      "\n",
      " album_name_can_emb_model (S  (None, 32)               18292032  \n",
      " equential)                                                      \n",
      "||\n",
      "| album_name_can_txt_vectoriz  (None, None)           0         |\n",
      "| er (TextVectorization)                                        |\n",
      "|                                                               |\n",
      "| album_name_can_emb_layer (E  (None, None, 32)       18292032  |\n",
      "| mbedding)                                                     |\n",
      "|                                                               |\n",
      "| album_name_can_pooling (Glo  (None, 32)             0         |\n",
      "| balAveragePooling1D)                                          |\n",
      "\n",
      " artist_uri_can_emb_model (S  (2, 32)                  6400032   \n",
      " equential)                                                      \n",
      "||\n",
      "| hashing (Hashing)         (2,)                      0         |\n",
      "|                                                               |\n",
      "| artist_uri_can_emb_layer (E  (2, 32)                6400032   |\n",
      "| mbedding)                                                     |\n",
      "\n",
      " track_uri_can_emb_model (Se  (2, 32)                  6400032   \n",
      " quential)                                                       \n",
      "||\n",
      "| hashing_1 (Hashing)       (2,)                      0         |\n",
      "|                                                               |\n",
      "| track_uri_can_emb_layer (Em  (2, 32)                6400032   |\n",
      "| bedding)                                                      |\n",
      "\n",
      " album_uri_can_emb_model (Se  (2, 32)                  6400032   \n",
      " quential)                                                       \n",
      "||\n",
      "| hashing_2 (Hashing)       (2,)                      0         |\n",
      "|                                                               |\n",
      "| album_uri_can_emb_layer (Em  (2, 32)                6400032   |\n",
      "| bedding)                                                      |\n",
      "\n",
      " normalization (Normalizatio  multiple                 0         \n",
      " n)                                                              \n",
      "                                                                 \n",
      " normalization_1 (Normalizat  multiple                 0         \n",
      " ion)                                                            \n",
      "                                                                 \n",
      " normalization_2 (Normalizat  multiple                 0         \n",
      " ion)                                                            \n",
      "                                                                 \n",
      " normalization_3 (Normalizat  multiple                 0         \n",
      " ion)                                                            \n",
      "                                                                 \n",
      " artist_genres_can_emb_model  (None, 32)               1260736   \n",
      "  (Sequential)                                                   \n",
      "||\n",
      "| artist_genres_can_txt_vecto  (None, None)           0         |\n",
      "| rizer (TextVectorization)                                     |\n",
      "|                                                               |\n",
      "| artist_genres_can_emb_layer  (None, None, 32)       1260736   |\n",
      "|  (Embedding)                                                  |\n",
      "|                                                               |\n",
      "| artist_genres_can_pooling (  (None, 32)             0         |\n",
      "| GlobalAveragePooling1D)                                       |\n",
      "\n",
      " can_cross_layer (Cross)     multiple                  2508      \n",
      "                                                                 \n",
      " candidate_dense_layers (Seq  (2, 32)                  16736     \n",
      " uential)                                                        \n",
      "||\n",
      "| dense_6 (Dense)           (2, 64)                   14656     |\n",
      "|                                                               |\n",
      "| dropout_2 (Dropout)       (2, 64)                   0         |\n",
      "|                                                               |\n",
      "| dense_7 (Dense)           (2, 32)                   2080      |\n",
      "\n",
      "=================================================================\n",
      "Total params: 95,458,988\n",
      "Trainable params: 95,458,988\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "test_can_track_model.summary(expand_nested=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4634b4-5fc8-4bb2-9257-6ab6d5651359",
   "metadata": {},
   "source": [
    "## Combined Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2edb083b-58d0-4568-93cf-0443c3bc0729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pl_can_concat = tf.concat([pl_result,can_result], axis=1)\n",
    "\n",
    "# print(f\"Shape of pl_can_concat: {pl_can_concat.shape[1]}\")\n",
    "# pl_can_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e03ddde4-dd1c-402d-a5ff-d7907b1ed3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile -a vertex_train/trainer/task.py\n",
    "\n",
    "class TheTwoTowers(tfrs.models.Model):\n",
    "\n",
    "    def __init__(self, layer_sizes, vocab_dict_load):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.query_tower = Playlist_Model(layer_sizes, vocab_dict_load)\n",
    "        \n",
    "        self.candidate_tower = Candidate_Track_Model(layer_sizes, vocab_dict_load)\n",
    "        \n",
    "        self.task = tfrs.tasks.Retrieval(\n",
    "            metrics=tfrs.metrics.FactorizedTopK(\n",
    "                candidates=parsed_candidate_dataset.batch(128).cache().map(self.candidate_tower)\n",
    "            )\n",
    "        )\n",
    "        \n",
    "    def compute_loss(self, data, training=False):\n",
    "        query_embeddings = self.query_tower(data)\n",
    "        candidate_embeddings = self.candidate_tower(data)\n",
    "\n",
    "        return self.task(\n",
    "            query_embeddings, \n",
    "            candidate_embeddings, \n",
    "            compute_metrics=not training\n",
    "        ) # turn off metrics to save time on training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "356a6481-42d2-4a75-a135-951e5ed40ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_sizes=[64,32]\n",
    "\n",
    "model = TheTwoTowers(layer_sizes, vocab_dict_load)\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adagrad(.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "383fd640-739d-477c-a452-3cc4934987db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#do the adapt\n",
    "model.query_tower.pl_name_text_embedding.layers[0].adapt(parsed_dataset_padded.map(lambda x: x['name']).batch(1000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "11401be6-0f5f-4454-997a-862a58df2a81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Playlist (query) Tower:\n",
      "0 pl_name_emb_model\n",
      "1 pl_collaborative_emb_model\n",
      "2 pl_track_uri_emb_model\n",
      "3 n_songs_pl_emb_model\n",
      "4 n_artists_pl_emb_model\n",
      "5 n_albums_pl_emb_model\n",
      "6 artist_name_pl_emb_model\n",
      "7 track_uri_pl_emb_model\n",
      "8 track_name_pl_emb_model\n",
      "9 duration_ms_songs_pl_emb_model\n",
      "10 album_name_pl_emb_model\n",
      "11 artist_pop_pl_emb_model\n",
      "12 artists_followers_pl_emb_model\n",
      "13 track_pop_pl_emb_model\n",
      "14 artist_genres_pl_emb_model\n",
      "15 pl_cross_layer\n",
      "16 pl_dense_layers\n"
     ]
    }
   ],
   "source": [
    "## Quick look at the layers\n",
    "print(\"Playlist (query) Tower:\")\n",
    "\n",
    "for i, l in enumerate(model.query_tower.layers):\n",
    "    print(i, l.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0d4fb669-7c98-448a-a2bf-1559b0a4d67e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Track (candidate) Tower:\n",
      "0 artist_name_can_emb_model\n",
      "1 track_name_can_emb_model\n",
      "2 album_name_can_emb_model\n",
      "3 artist_uri_can_emb_model\n",
      "4 track_uri_can_emb_model\n",
      "5 album_uri_can_emb_model\n",
      "6 normalization_4\n",
      "7 normalization_5\n",
      "8 normalization_6\n",
      "9 normalization_7\n",
      "10 artist_genres_can_emb_model\n",
      "11 can_cross_layer\n",
      "12 candidate_dense_layers\n"
     ]
    }
   ],
   "source": [
    "print(\"Track (candidate) Tower:\")\n",
    "for i, l in enumerate(model.candidate_tower.layers):\n",
    "    print(i, l.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425a17df-2e2a-41e5-a0c6-ac2abc2ad135",
   "metadata": {},
   "source": [
    "# Local Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "864281ee-f760-4b20-aacc-0b6f0e0ea5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "shuffle_train = parsed_dataset_padded.shuffle(10_000, seed=42, reshuffle_each_iteration=False)\n",
    "\n",
    "TRAIN = shuffle_train.batch(128)\n",
    "VALID = parsed_dataset_padded_valid.batch(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5e42f221-479c-4776-9e14-54f6a674cb12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset element_spec={'album_name_can': TensorSpec(shape=(None,), dtype=tf.string, name=None), 'album_name_seed_track': TensorSpec(shape=(None,), dtype=tf.string, name=None), 'album_uri_can': TensorSpec(shape=(None,), dtype=tf.string, name=None), 'album_uri_seed_track': TensorSpec(shape=(None,), dtype=tf.string, name=None), 'artist_followers_can': TensorSpec(shape=(None,), dtype=tf.float32, name=None), 'artist_followers_seed_track': TensorSpec(shape=(None,), dtype=tf.float32, name=None), 'artist_genres_can': TensorSpec(shape=(None,), dtype=tf.string, name=None), 'artist_genres_seed_track': TensorSpec(shape=(None,), dtype=tf.string, name=None), 'artist_name_can': TensorSpec(shape=(None,), dtype=tf.string, name=None), 'artist_name_seed_track': TensorSpec(shape=(None,), dtype=tf.string, name=None), 'artist_pop_can': TensorSpec(shape=(None,), dtype=tf.float32, name=None), 'artist_pop_seed_track': TensorSpec(shape=(None,), dtype=tf.float32, name=None), 'artist_uri_can': TensorSpec(shape=(None,), dtype=tf.string, name=None), 'artist_uri_seed_track': TensorSpec(shape=(None,), dtype=tf.string, name=None), 'collaborative': TensorSpec(shape=(None,), dtype=tf.string, name=None), 'description_pl': TensorSpec(shape=(None,), dtype=tf.string, name=None), 'duration_ms_can': TensorSpec(shape=(None,), dtype=tf.float32, name=None), 'duration_seed_track': TensorSpec(shape=(None,), dtype=tf.float32, name=None), 'n_songs_pl': TensorSpec(shape=(None,), dtype=tf.float32, name=None), 'name': TensorSpec(shape=(None,), dtype=tf.string, name=None), 'num_albums_pl': TensorSpec(shape=(None,), dtype=tf.float32, name=None), 'num_artists_pl': TensorSpec(shape=(None,), dtype=tf.float32, name=None), 'track_name_can': TensorSpec(shape=(None,), dtype=tf.string, name=None), 'track_name_seed_track': TensorSpec(shape=(None,), dtype=tf.string, name=None), 'track_pop_can': TensorSpec(shape=(None,), dtype=tf.float32, name=None), 'track_pop_seed_track': TensorSpec(shape=(None,), dtype=tf.float32, name=None), 'track_uri_can': TensorSpec(shape=(None,), dtype=tf.string, name=None), 'track_uri_seed_track': TensorSpec(shape=(None,), dtype=tf.string, name=None), 'track_name_pl': (TensorSpec(shape=(None, None, 375), dtype=tf.string, name=None),), 'artist_name_pl': (TensorSpec(shape=(None, None, 375), dtype=tf.string, name=None),), 'album_name_pl': (TensorSpec(shape=(None, None, 375), dtype=tf.string, name=None),), 'track_uri_pl': (TensorSpec(shape=(None, None, 375), dtype=tf.string, name=None),), 'duration_ms_songs_pl': (TensorSpec(shape=(None, None, 375), dtype=tf.float32, name=None),), 'artist_pop_pl': (TensorSpec(shape=(None, None, 375), dtype=tf.float32, name=None),), 'artists_followers_pl': (TensorSpec(shape=(None, None, 375), dtype=tf.float32, name=None),), 'track_pop_pl': (TensorSpec(shape=(None, None, 375), dtype=tf.float32, name=None),), 'artist_genres_pl': (TensorSpec(shape=(None, None, 375), dtype=tf.string, name=None),)}>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VALID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9b5d2320-28ac-4292-83fd-503191830179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    198/Unknown - 19s 74ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 635.7459 - regularization_loss: 0.0000e+00 - total_loss: 635.7459"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-22 00:58:22.544631: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node 'candidate__track__model_1/artist_name_can_emb_model/artist_name_can_emb_layer/embedding_lookup' defined at (most recent call last):\n    File \"/opt/conda/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n      \"__main__\", mod_spec)\n    File \"/opt/conda/lib/python3.7/runpy.py\", line 85, in _run_code\n      exec(code, run_globals)\n    File \"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/opt/conda/lib/python3.7/site-packages/traitlets/config/application.py\", line 978, in launch_instance\n      app.start()\n    File \"/opt/conda/lib/python3.7/site-packages/ipykernel/kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"/opt/conda/lib/python3.7/site-packages/tornado/platform/asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"/opt/conda/lib/python3.7/asyncio/base_events.py\", line 541, in run_forever\n      self._run_once()\n    File \"/opt/conda/lib/python3.7/asyncio/base_events.py\", line 1786, in _run_once\n      handle._run()\n    File \"/opt/conda/lib/python3.7/asyncio/events.py\", line 88, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/opt/conda/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"/opt/conda/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"/opt/conda/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"/opt/conda/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n      reply_content = await reply_content\n    File \"/opt/conda/lib/python3.7/site-packages/ipykernel/ipkernel.py\", line 387, in do_execute\n      cell_id=cell_id,\n    File \"/opt/conda/lib/python3.7/site-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n      raw_cell, store_history, silent, shell_futures, cell_id\n    File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3029, in _run_cell\n      return runner(coro)\n    File \"/opt/conda/lib/python3.7/site-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n      interactivity=interactivity, compiler=compiler, result=result)\n    File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3472, in run_ast_nodes\n      if (await self.run_code(code, result,  async_=asy)):\n    File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3552, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_22622/1346043644.py\", line 10, in <module>\n      epochs=NUM_EPOCHS,\n    File \"/opt/conda/lib/python3.7/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/opt/conda/lib/python3.7/site-packages/keras/engine/training.py\", line 1431, in fit\n      _use_cached_eval_dataset=True)\n    File \"/opt/conda/lib/python3.7/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/opt/conda/lib/python3.7/site-packages/keras/engine/training.py\", line 1716, in evaluate\n      tmp_logs = self.test_function(iterator)\n    File \"/opt/conda/lib/python3.7/site-packages/keras/engine/training.py\", line 1525, in test_function\n      return step_function(self, iterator)\n    File \"/opt/conda/lib/python3.7/site-packages/keras/engine/training.py\", line 1514, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/opt/conda/lib/python3.7/site-packages/keras/engine/training.py\", line 1507, in run_step\n      outputs = model.test_step(data)\n    File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow_recommenders/models/base.py\", line 88, in test_step\n      loss = self.compute_loss(inputs, training=False)\n    File \"/tmp/ipykernel_22622/3938437637.py\", line 20, in compute_loss\n      candidate_embeddings = self.candidate_tower(data)\n    File \"/opt/conda/lib/python3.7/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/opt/conda/lib/python3.7/site-packages/keras/engine/base_layer.py\", line 1096, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/opt/conda/lib/python3.7/site-packages/keras/utils/traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"/tmp/ipykernel_22622/4027247330.py\", line 190, in call\n      all_embs = tf.concat(\n    File \"/opt/conda/lib/python3.7/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/opt/conda/lib/python3.7/site-packages/keras/engine/base_layer.py\", line 1096, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/opt/conda/lib/python3.7/site-packages/keras/utils/traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"/opt/conda/lib/python3.7/site-packages/keras/engine/sequential.py\", line 374, in call\n      return super(Sequential, self).call(inputs, training=training, mask=mask)\n    File \"/opt/conda/lib/python3.7/site-packages/keras/engine/functional.py\", line 452, in call\n      inputs, training=training, mask=mask)\n    File \"/opt/conda/lib/python3.7/site-packages/keras/engine/functional.py\", line 589, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"/opt/conda/lib/python3.7/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/opt/conda/lib/python3.7/site-packages/keras/engine/base_layer.py\", line 1096, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/opt/conda/lib/python3.7/site-packages/keras/utils/traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"/opt/conda/lib/python3.7/site-packages/keras/layers/embeddings.py\", line 197, in call\n      out = tf.nn.embedding_lookup(self.embeddings, inputs)\nNode: 'candidate__track__model_1/artist_name_can_emb_model/artist_name_can_emb_layer/embedding_lookup'\nindices[3,0] = 287711 is not in [0, 287711)\n\t [[{{node candidate__track__model_1/artist_name_can_emb_model/artist_name_can_emb_layer/embedding_lookup}}]]\n\t [[retrieval/streaming/ReduceDataset]] [Op:__inference_test_function_10397]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_22622/4275992698.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mVALID\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# validation_freq=5,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;31m# callbacks=tensorboard_cb,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# verbose=1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 55\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node 'candidate__track__model_1/artist_name_can_emb_model/artist_name_can_emb_layer/embedding_lookup' defined at (most recent call last):\n    File \"/opt/conda/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n      \"__main__\", mod_spec)\n    File \"/opt/conda/lib/python3.7/runpy.py\", line 85, in _run_code\n      exec(code, run_globals)\n    File \"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/opt/conda/lib/python3.7/site-packages/traitlets/config/application.py\", line 978, in launch_instance\n      app.start()\n    File \"/opt/conda/lib/python3.7/site-packages/ipykernel/kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"/opt/conda/lib/python3.7/site-packages/tornado/platform/asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"/opt/conda/lib/python3.7/asyncio/base_events.py\", line 541, in run_forever\n      self._run_once()\n    File \"/opt/conda/lib/python3.7/asyncio/base_events.py\", line 1786, in _run_once\n      handle._run()\n    File \"/opt/conda/lib/python3.7/asyncio/events.py\", line 88, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/opt/conda/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"/opt/conda/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"/opt/conda/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"/opt/conda/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n      reply_content = await reply_content\n    File \"/opt/conda/lib/python3.7/site-packages/ipykernel/ipkernel.py\", line 387, in do_execute\n      cell_id=cell_id,\n    File \"/opt/conda/lib/python3.7/site-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n      raw_cell, store_history, silent, shell_futures, cell_id\n    File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3029, in _run_cell\n      return runner(coro)\n    File \"/opt/conda/lib/python3.7/site-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n      interactivity=interactivity, compiler=compiler, result=result)\n    File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3472, in run_ast_nodes\n      if (await self.run_code(code, result,  async_=asy)):\n    File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3552, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_22622/1346043644.py\", line 10, in <module>\n      epochs=NUM_EPOCHS,\n    File \"/opt/conda/lib/python3.7/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/opt/conda/lib/python3.7/site-packages/keras/engine/training.py\", line 1431, in fit\n      _use_cached_eval_dataset=True)\n    File \"/opt/conda/lib/python3.7/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/opt/conda/lib/python3.7/site-packages/keras/engine/training.py\", line 1716, in evaluate\n      tmp_logs = self.test_function(iterator)\n    File \"/opt/conda/lib/python3.7/site-packages/keras/engine/training.py\", line 1525, in test_function\n      return step_function(self, iterator)\n    File \"/opt/conda/lib/python3.7/site-packages/keras/engine/training.py\", line 1514, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/opt/conda/lib/python3.7/site-packages/keras/engine/training.py\", line 1507, in run_step\n      outputs = model.test_step(data)\n    File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow_recommenders/models/base.py\", line 88, in test_step\n      loss = self.compute_loss(inputs, training=False)\n    File \"/tmp/ipykernel_22622/3938437637.py\", line 20, in compute_loss\n      candidate_embeddings = self.candidate_tower(data)\n    File \"/opt/conda/lib/python3.7/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/opt/conda/lib/python3.7/site-packages/keras/engine/base_layer.py\", line 1096, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/opt/conda/lib/python3.7/site-packages/keras/utils/traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"/tmp/ipykernel_22622/4027247330.py\", line 190, in call\n      all_embs = tf.concat(\n    File \"/opt/conda/lib/python3.7/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/opt/conda/lib/python3.7/site-packages/keras/engine/base_layer.py\", line 1096, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/opt/conda/lib/python3.7/site-packages/keras/utils/traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"/opt/conda/lib/python3.7/site-packages/keras/engine/sequential.py\", line 374, in call\n      return super(Sequential, self).call(inputs, training=training, mask=mask)\n    File \"/opt/conda/lib/python3.7/site-packages/keras/engine/functional.py\", line 452, in call\n      inputs, training=training, mask=mask)\n    File \"/opt/conda/lib/python3.7/site-packages/keras/engine/functional.py\", line 589, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"/opt/conda/lib/python3.7/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/opt/conda/lib/python3.7/site-packages/keras/engine/base_layer.py\", line 1096, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/opt/conda/lib/python3.7/site-packages/keras/utils/traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"/opt/conda/lib/python3.7/site-packages/keras/layers/embeddings.py\", line 197, in call\n      out = tf.nn.embedding_lookup(self.embeddings, inputs)\nNode: 'candidate__track__model_1/artist_name_can_emb_model/artist_name_can_emb_layer/embedding_lookup'\nindices[3,0] = 287711 is not in [0, 287711)\n\t [[{{node candidate__track__model_1/artist_name_can_emb_model/artist_name_can_emb_layer/embedding_lookup}}]]\n\t [[retrieval/streaming/ReduceDataset]] [Op:__inference_test_function_10397]"
     ]
    }
   ],
   "source": [
    "import time    \n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "NUM_EPOCHS = 1\n",
    "layer_history = model.fit(\n",
    "    TRAIN,\n",
    "    validation_data=VALID,\n",
    "    # validation_freq=5,\n",
    "    epochs=NUM_EPOCHS,\n",
    "    # callbacks=tensorboard_cb,\n",
    "    # verbose=1\n",
    ")\n",
    "\n",
    "train_time = time.time() - start_time\n",
    "\n",
    "print(f\"Training for {NUM_EPOCHS} epoch, ran for: {train_time:.0} seconds\")\n",
    "accuracy = layer_history.history[\"factorized_top_k/top_100_categorical_accuracy\"][-1]\n",
    "print(f\"Top 100 categorical accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6a280c-998a-4d8f-801e-33a7baa8192c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Local Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef86789b-1a89-4944-8075-3c08eae90def",
   "metadata": {},
   "source": [
    "## Loading SavedModels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "id": "2d344efa-cf6f-42dc-a336-34d6f36c740c",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_tower_uri = 'gs://spotify-tfrs-dir/v2/run-20220920-210334/candidate_tower'\n",
    "loaded_candidate_model = tf.saved_model.load(candidate_tower_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6412c459-2203-42f3-b2e4-8da74ba01e64",
   "metadata": {},
   "source": [
    "### Candidate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "id": "c50049b0-5c34-49ff-8e94-d8569b0c5bc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['serving_default']\n"
     ]
    }
   ],
   "source": [
    "print(list(loaded_candidate_model.signatures.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "id": "3a667320-922d-4120-bdbf-9bec2c84ad43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'output_1': TensorSpec(shape=(None, 32), dtype=tf.float32, name='output_1')}\n"
     ]
    }
   ],
   "source": [
    "infer = loaded_candidate_model.signatures[\"serving_default\"]\n",
    "print(infer.structured_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "id": "39c74e86-8f4b-46f5-8309-30861a824975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_SignatureMap({'serving_default': <ConcreteFunction signature_wrapper(*, album_name_can, album_uri_can, artist_followers_can, artist_genres_can, artist_name_can, artist_pop_can, artist_uri_can, duration_ms_can, track_name_can, track_pop_can, track_uri_can) at 0x7F1A85BA2C10>})\n"
     ]
    }
   ],
   "source": [
    "loaded_candidate_model.signatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "id": "8e58f5f2-4c8a-4d51-8651-1d3956f13825",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output_1': TensorShape([None, 32])}"
      ]
     },
     "execution_count": 531,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict2 = loaded_candidate_model.signatures['serving_default']\n",
    "predict2.output_shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "id": "953a9df6-39c6-4399-832e-2b629371f8c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MapDataset element_spec={'album_name_can': TensorSpec(shape=(), dtype=tf.string, name=None), 'album_uri_can': TensorSpec(shape=(), dtype=tf.string, name=None), 'artist_followers_can': TensorSpec(shape=(), dtype=tf.float32, name=None), 'artist_genres_can': TensorSpec(shape=(), dtype=tf.string, name=None), 'artist_name_can': TensorSpec(shape=(), dtype=tf.string, name=None), 'artist_pop_can': TensorSpec(shape=(), dtype=tf.float32, name=None), 'artist_uri_can': TensorSpec(shape=(), dtype=tf.string, name=None), 'duration_ms_can': TensorSpec(shape=(), dtype=tf.float32, name=None), 'track_name_can': TensorSpec(shape=(), dtype=tf.string, name=None), 'track_pop_can': TensorSpec(shape=(), dtype=tf.float32, name=None), 'track_uri_can': TensorSpec(shape=(), dtype=tf.string, name=None)}>"
      ]
     },
     "execution_count": 533,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_candidate_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "id": "2680e5fd-63f4-4b44-8004-6a5020d28120",
   "metadata": {},
   "outputs": [],
   "source": [
    "embs_iter = parsed_candidate_dataset.batch(1).map(\n",
    "    lambda data: predict2(\n",
    "        artist_name_can = data[\"artist_name_can\"],\n",
    "        track_name_can = data['track_name_can'],\n",
    "        album_name_can = data['album_name_can'],\n",
    "        track_uri_can = data['track_uri_can'],\n",
    "        artist_uri_can = data['artist_uri_can'],\n",
    "        album_uri_can = data['album_uri_can'],\n",
    "        duration_ms_can = data['duration_ms_can'],\n",
    "        track_pop_can = data['track_pop_can'],\n",
    "        artist_pop_can = data['artist_pop_can'],\n",
    "        artist_followers_can = data['artist_followers_can'],\n",
    "        artist_genres_can = data['artist_genres_can']\n",
    "    )\n",
    ")\n",
    "\n",
    "embs = []\n",
    "for emb in embs_iter:\n",
    "    embs.append(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "id": "9094ab59-3c1c-4d7b-8a27-6f4bf1964816",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of embs: 166827\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'output_1': <tf.Tensor: shape=(1, 32), dtype=float32, numpy=\n",
       " array([[ 0.32745814,  0.15835902, -0.62232316, -0.18282643,  0.05425396,\n",
       "          0.08093273, -0.36878368, -0.6777717 , -0.28926852, -0.44578883,\n",
       "          0.03769037,  0.2013096 ,  0.01650199, -0.38547963, -0.2734376 ,\n",
       "          0.16123655,  0.12652676, -0.09455037,  0.3709236 , -0.04336764,\n",
       "         -0.20310198,  0.21418957, -0.44912496, -0.16798183, -0.21492694,\n",
       "          0.04033846,  0.14083184,  0.25597924,  0.20490994,  0.18837534,\n",
       "         -0.15261272,  0.06471566]], dtype=float32)>}"
      ]
     },
     "execution_count": 540,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Length of embs: {len(embs)}\")\n",
    "embs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "id": "48a0b07d-182b-46f6-af30-649a40930d94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of cleaned_embs: 166827\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.32745814,  0.15835902, -0.62232316, -0.18282643,  0.05425396,\n",
       "        0.08093273, -0.36878368, -0.6777717 , -0.28926852, -0.44578883,\n",
       "        0.03769037,  0.2013096 ,  0.01650199, -0.38547963, -0.2734376 ,\n",
       "        0.16123655,  0.12652676, -0.09455037,  0.3709236 , -0.04336764,\n",
       "       -0.20310198,  0.21418957, -0.44912496, -0.16798183, -0.21492694,\n",
       "        0.04033846,  0.14083184,  0.25597924,  0.20490994,  0.18837534,\n",
       "       -0.15261272,  0.06471566], dtype=float32)"
      ]
     },
     "execution_count": 541,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_embs = [x['output_1'].numpy()[0] for x in embs] #clean up the output\n",
    "\n",
    "print(f\"Length of cleaned_embs: {len(cleaned_embs)}\")\n",
    "cleaned_embs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "id": "27178db5-2a2c-4e86-9be2-97b2265ac114",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'spotify:track:6KhJeYLg1AimCQjH6ii1Al'"
      ]
     },
     "execution_count": 553,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clean product IDs\n",
    "track_uris = [x['track_uri_can'].numpy() for x in parsed_candidate_dataset]\n",
    "track_uris[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "id": "1df48335-27d6-4c6a-969a-2df1412ac466",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'spotify:track:6KhJeYLg1AimCQjH6ii1Al'"
      ]
     },
     "execution_count": 554,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "track_uris[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "id": "5b43b165-8992-4e9e-96a3-5753d860d661",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'spotify:track:6KhJeYLg1AimCQjH6ii1Al'"
      ]
     },
     "execution_count": 557,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "track_uris_cleaned = [str(z).replace(\"b'\",\"\").replace(\"'\",\"\") for z in track_uris]\n",
    "track_uris_cleaned[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "id": "3b6d3cf5-4a4d-4295-a371-d34bc0fef9e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of track_uris: 166827\n",
      "Length of track_uris_cleaned: 166827\n"
     ]
    }
   ],
   "source": [
    "print(f\"Length of track_uris: {len(track_uris)}\")\n",
    "print(f\"Length of track_uris_cleaned: {len(track_uris_cleaned)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a677bd2b-d8d6-4697-a1b0-62c68749f63d",
   "metadata": {},
   "source": [
    "### Write Index Json File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "id": "61178306-29e1-4d70-90b9-669e0a7a476a",
   "metadata": {},
   "outputs": [],
   "source": [
    "VERSION = 'local-v1'\n",
    "TIMESTAMP = '092022'\n",
    "\n",
    "embeddings_index_filename = f'candidate_embeddings_{VERSION}_{TIMESTAMP}.json'\n",
    "\n",
    "with open(f'{embeddings_index_filename}', 'w') as f:\n",
    "    for prod, emb in zip(track_uris_cleaned, cleaned_embs):\n",
    "        f.write('{\"id\":\"' + str(prod) + '\",')\n",
    "        f.write('\"embedding\":[' + \",\".join(str(x) for x in list(emb)) + \"]}\")\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "id": "1a772d6a-000b-468a-a7ad-5cacebf31889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# with open('candidate_embeddings_local-v1_092022.json', 'r') as f:\n",
    "#     data = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee5e34d-8bdf-46af-b39f-63f6d5caa695",
   "metadata": {},
   "source": [
    "### Query Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "id": "b3eb27dd-a85f-4970-be5b-2971607afdd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_tower_uri = 'gs://spotify-tfrs-dir/v2/run-20220920-210334/query_tower'\n",
    "loaded_query_model = tf.saved_model.load(query_tower_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "id": "4c85d98c-cb52-48e8-8c72-69d46a1b0c74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_SignatureMap({'serving_default': <ConcreteFunction signature_wrapper(*, album_name_can, album_name_pl, album_name_seed_track, album_uri_can, album_uri_seed_track, artist_followers_can, artist_followers_seed_track, artist_genres_can, artist_genres_pl, artist_genres_seed_track, artist_name_can, artist_name_pl, artist_name_seed_track, artist_pop_can, artist_pop_pl, artist_pop_seed_track, artist_uri_can, artist_uri_seed_track, artists_followers_pl, collaborative, description_pl, duration_ms_can, duration_ms_seed_pl, duration_ms_songs_pl, duration_seed_track, n_songs_pl, name, num_albums_pl, num_artists_pl, pid, pos_seed_track, track_name_can, track_name_pl, track_name_seed_track, track_pop_can, track_pop_pl, track_pop_seed_track, track_uri_can, track_uri_pl, track_uri_seed_track) at 0x7F1935582D50>})"
      ]
     },
     "execution_count": 566,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_query_model.signatures"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m96",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m96"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
