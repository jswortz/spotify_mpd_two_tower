{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d5383b2-071e-45b8-af2c-2421eb4f923a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Two - Tower Retreival Model\n",
    "\n",
    "### Key resources:\n",
    "* Many pages [here](https://www.tensorflow.org/recommenders/examples/deep_recommenders) include great techniques to build custom TFRS Models\n",
    "\n",
    "### Goals:\n",
    "* Show how to model off of most data types \n",
    "  * (String, Existing Embeddings (vectors), \n",
    "  * Floats (Normalized), \n",
    "  * Categorical with vocab, \n",
    "  * High Dim Categorical (Embed)\n",
    "* Leverage class templates to create custom 2 Tower Models quick/easy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d52e5f-a13b-46a7-849a-dc188b455a86",
   "metadata": {},
   "source": [
    "## SPOTIFY Create the tensorflow.io interface for the event and product table in Bigquery\n",
    "Best practices from Google are in this blog post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "312070aa-00ae-467b-8747-0fefac329474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !gsutil mb -l us-central1 gs://spotify-tfrecords-blog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de3122eb-564b-46f9-9a50-851d19301521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set variables\n",
    "SEED = 41781897\n",
    "PROJECT_ID = 'hybrid-vertex'\n",
    "DROP_FIELDS = ['modified_at', 'row_number', 'seed_playlist_tracks']\n",
    "TF_RECORDS_DIR = 'gs://spotify-tfrecords-blog'\n",
    "BATCH_SIZE = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c50a06-d69d-4d64-b254-26fb2a249fcf",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Quick counts on training data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5e1473-4a2d-4778-9f01-e2b76672a9f5",
   "metadata": {},
   "source": [
    "#### Quick counts on the training records for track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eaa56927-6722-43f9-88ac-ab638487164e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Query complete after 0.00s: 100%|██████████| 1/1 [00:00<00:00, 563.30query/s] \n",
      "Downloading: 100%|██████████| 1/1 [00:01<00:00,  1.02s/rows]\n"
     ]
    }
   ],
   "source": [
    "%%bigquery TOTAL_PLAYLISTS\n",
    "select count(1) from hybrid-vertex.spotify_train_3.train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "418e5d8d-5410-4bca-a168-002d27ed95ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65346428"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TOTAL_PLAYLISTS = TOTAL_PLAYLISTS.values[0][0]\n",
    "TOTAL_PLAYLISTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4710f67-de96-401b-8a0c-abb05dd62c40",
   "metadata": {},
   "source": [
    "### Set the tf.io pipelines function from bigquery\n",
    "\n",
    "[Great blog post here on it](https://towardsdatascience.com/how-to-read-bigquery-data-from-tensorflow-2-0-efficiently-9234b69165c8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e985c505-1712-4d63-bc25-75df479fc70d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m  WARNING: The scripts f2py, f2py3 and f2py3.7 are installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script tensorboard is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tfx-bsl 1.8.0 requires google-api-python-client<2,>=1.7.11, but you have google-api-python-client 2.49.0 which is incompatible.\n",
      "tfx-bsl 1.8.0 requires pyarrow<6,>=1, but you have pyarrow 8.0.0 which is incompatible.\n",
      "tensorflow-transform 1.8.0 requires pyarrow<6,>=1, but you have pyarrow 8.0.0 which is incompatible.\n",
      "tensorflow-transform 1.8.0 requires tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<2.9,>=1.15.5, but you have tensorflow 2.9.0rc2 which is incompatible.\n",
      "google-cloud-recommendations-ai 0.2.0 requires google-api-core[grpc]<2.0.0dev,>=1.22.2, but you have google-api-core 2.8.0 which is incompatible.\n",
      "apache-beam 2.39.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.5.1 which is incompatible.\n",
      "apache-beam 2.39.0 requires httplib2<0.20.0,>=0.8, but you have httplib2 0.20.4 which is incompatible.\n",
      "apache-beam 2.39.0 requires pyarrow<8.0.0,>=0.15.1, but you have pyarrow 8.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# !pip install tensorflow-recommenders -q --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4349b880-9ecb-4ec3-ad4e-a8c35356208e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow-io==0.15.0\n",
      "  Downloading tensorflow_io-0.15.0-cp37-cp37m-manylinux2010_x86_64.whl (22.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.3/22.3 MB\u001b[0m \u001b[31m65.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tensorflow<2.4.0,>=2.3.0 in /home/jupyter/.local/lib/python3.7/site-packages (from tensorflow-io==0.15.0) (2.3.1)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.4.0,>=2.3.0->tensorflow-io==0.15.0) (1.0.0)\n",
      "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /home/jupyter/.local/lib/python3.7/site-packages (from tensorflow<2.4.0,>=2.3.0->tensorflow-io==0.15.0) (2.10.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.4.0,>=2.3.0->tensorflow-io==0.15.0) (1.46.3)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.4.0,>=2.3.0->tensorflow-io==0.15.0) (1.14.1)\n",
      "Requirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.4.0,>=2.3.0->tensorflow-io==0.15.0) (0.37.1)\n",
      "Requirement already satisfied: numpy<1.19.0,>=1.16.0 in /home/jupyter/.local/lib/python3.7/site-packages (from tensorflow<2.4.0,>=2.3.0->tensorflow-io==0.15.0) (1.18.5)\n",
      "Requirement already satisfied: tensorboard<3,>=2.3.0 in /home/jupyter/.local/lib/python3.7/site-packages (from tensorflow<2.4.0,>=2.3.0->tensorflow-io==0.15.0) (2.9.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /home/jupyter/.local/lib/python3.7/site-packages (from tensorflow<2.4.0,>=2.3.0->tensorflow-io==0.15.0) (2.3.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /home/jupyter/.local/lib/python3.7/site-packages (from tensorflow<2.4.0,>=2.3.0->tensorflow-io==0.15.0) (3.19.4)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.4.0,>=2.3.0->tensorflow-io==0.15.0) (1.1.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.4.0,>=2.3.0->tensorflow-io==0.15.0) (3.3.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.4.0,>=2.3.0->tensorflow-io==0.15.0) (1.16.0)\n",
      "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.4.0,>=2.3.0->tensorflow-io==0.15.0) (1.1.2)\n",
      "Requirement already satisfied: google-pasta>=0.1.8 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.4.0,>=2.3.0->tensorflow-io==0.15.0) (0.2.0)\n",
      "Requirement already satisfied: gast==0.3.3 in /home/jupyter/.local/lib/python3.7/site-packages (from tensorflow<2.4.0,>=2.3.0->tensorflow-io==0.15.0) (0.3.3)\n",
      "Requirement already satisfied: astunparse==1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.4.0,>=2.3.0->tensorflow-io==0.15.0) (1.6.3)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard<3,>=2.3.0->tensorflow<2.4.0,>=2.3.0->tensorflow-io==0.15.0) (2.6.6)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<3,>=2.3.0->tensorflow<2.4.0,>=2.3.0->tensorflow-io==0.15.0) (2.27.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard<3,>=2.3.0->tensorflow<2.4.0,>=2.3.0->tensorflow-io==0.15.0) (0.4.6)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<3,>=2.3.0->tensorflow<2.4.0,>=2.3.0->tensorflow-io==0.15.0) (59.8.0)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard<3,>=2.3.0->tensorflow<2.4.0,>=2.3.0->tensorflow-io==0.15.0) (2.1.2)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<3,>=2.3.0->tensorflow<2.4.0,>=2.3.0->tensorflow-io==0.15.0) (1.8.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard<3,>=2.3.0->tensorflow<2.4.0,>=2.3.0->tensorflow-io==0.15.0) (3.3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<3,>=2.3.0->tensorflow<2.4.0,>=2.3.0->tensorflow-io==0.15.0) (0.6.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow<2.4.0,>=2.3.0->tensorflow-io==0.15.0) (0.2.7)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow<2.4.0,>=2.3.0->tensorflow-io==0.15.0) (4.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow<2.4.0,>=2.3.0->tensorflow-io==0.15.0) (5.0.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow<2.4.0,>=2.3.0->tensorflow-io==0.15.0) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow<2.4.0,>=2.3.0->tensorflow-io==0.15.0) (4.11.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow<2.4.0,>=2.3.0->tensorflow-io==0.15.0) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow<2.4.0,>=2.3.0->tensorflow-io==0.15.0) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow<2.4.0,>=2.3.0->tensorflow-io==0.15.0) (2022.5.18.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow<2.4.0,>=2.3.0->tensorflow-io==0.15.0) (2.0.12)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow<2.4.0,>=2.3.0->tensorflow-io==0.15.0) (3.8.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow<2.4.0,>=2.3.0->tensorflow-io==0.15.0) (4.2.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow<2.4.0,>=2.3.0->tensorflow-io==0.15.0) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow<2.4.0,>=2.3.0->tensorflow-io==0.15.0) (3.2.0)\n",
      "Installing collected packages: tensorflow-io\n",
      "  Attempting uninstall: tensorflow-io\n",
      "    Found existing installation: tensorflow-io 0.16.0\n",
      "    Uninstalling tensorflow-io-0.16.0:\n",
      "      Successfully uninstalled tensorflow-io-0.16.0\n",
      "Successfully installed tensorflow-io-0.15.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U tensorflow-io==0.15.0 --user "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7db86c9f-5454-4629-80ba-c0273fca117d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-30 17:29:37.656877: W tensorflow_io/core/kernels/audio_video_mp3_kernels.cc:271] libmp3lame.so.0 or lame functions are not available\n",
      "2022-06-30 17:29:37.657154: I tensorflow_io/core/kernels/cpu_check.cc:128] Your CPU supports instructions that this TensorFlow IO binary was not compiled to use: AVX2 FMA\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import dtypes\n",
    "from tensorflow_io.bigquery import BigQueryClient\n",
    "from tensorflow_io.bigquery import BigQueryReadSession\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") #do this b/c there's an info-level bug that can safely be ignored\n",
    "import json\n",
    "import tensorflow as tf\n",
    "# import tensorflow_recommenders as tfrs\n",
    "import datetime\n",
    "from tensorflow.python.lib.io import file_io\n",
    "from tensorflow.train import BytesList, Feature, FeatureList, Int64List, FloatList\n",
    "from tensorflow.train import SequenceExample, FeatureLists\n",
    "\n",
    "\n",
    "\n",
    "# def bq_to_tfdata(client, row_restriction, table_id, col_names, dataset, batch_size=BATCH_SIZE):\n",
    "#     TABLE_ID = table_id\n",
    "#     COL_NAMES = col_names\n",
    "#     DATASET = dataset\n",
    "#     bqsession = client.read_session(\n",
    "#         \"projects/\" + PROJECT_ID,\n",
    "#         PROJECT_ID, TABLE_ID, DATASET,\n",
    "#         COL_NAMES,\n",
    "#         requested_streams=2,\n",
    "#         row_restriction=row_restriction)\n",
    "#     dataset = bqsession.parallel_read_rows()\n",
    "#     return dataset.prefetch(1).shuffle(batch_size*10).batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e45e8d-3113-4528-9d71-8afdfd922d4f",
   "metadata": {},
   "source": [
    "## Get the song metadata\n",
    "\n",
    "To get a pipeline working we need the metadata for the table along with the table information. The following functions are helpers that give us the metadata into the proper types for `tf`\n",
    "\n",
    "\n",
    "For each table id, programatically get\n",
    "* Column names\n",
    "* Column types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd72e820-d1d7-48f9-902d-ad63be9b74fb",
   "metadata": {},
   "source": [
    "## Metadata dictionary to translate from BQ to tensorflow\n",
    "\n",
    "From the DDL we are going to get the types for use in a  to create a `BigQueryReadSession` from `tensorflow_io.bigquery` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "80b2013c-7896-4738-a174-79d9f2fbc188",
   "metadata": {},
   "outputs": [],
   "source": [
    "bq_2_tf_dict = {'name': {'mode': BigQueryClient.FieldMode.NULLABLE, 'output_type': dtypes.string},\n",
    " 'collaborative': {'mode': BigQueryClient.FieldMode.NULLABLE, 'output_type': dtypes.string},\n",
    "#  'pid': {'mode': BigQueryClient.FieldMode.NULLABLE, 'output_type': dtypes.int64},\n",
    "#  'description': {'mode': BigQueryClient.FieldMode.NULLABLE, 'output_type': dtypes.string},\n",
    "#  'duration_ms_playlist': {'mode': BigQueryClient.FieldMode.NULLABLE, 'output_type': dtypes.int64},\n",
    "#  'pid_pos_id': {'mode':BigQueryClient.FieldMode.NULLABLE, 'output_type': dtypes.string},\n",
    "#  'pos': {'mode':BigQueryClient.FieldMode.NULLABLE, 'output_type': dtypes.int64},\n",
    "#  'artist_name_seed': {'mode':BigQueryClient.FieldMode.NULLABLE, 'output_type': dtypes.string},\n",
    "#  'track_uri_seed': {'mode':BigQueryClient.FieldMode.NULLABLE, 'output_type': dtypes.string},\n",
    "#  'artist_uri_seed': {'mode':BigQueryClient.FieldMode.NULLABLE, 'output_type': dtypes.string},\n",
    "#  'track_name_seed': {'mode':BigQueryClient.FieldMode.NULLABLE, 'output_type': dtypes.string},\n",
    "#  'album_uri_seed': {'mode':BigQueryClient.FieldMode.NULLABLE, 'output_type': dtypes.string},\n",
    "#  'duration_ms_seed': {'mode':BigQueryClient.FieldMode.NULLABLE, 'output_type': dtypes.float64},\n",
    "#  'album_name_seed': {'mode':BigQueryClient.FieldMode.NULLABLE, 'output_type': dtypes.string},\n",
    "#  'track_pop_seed': {'mode':BigQueryClient.FieldMode.NULLABLE, 'output_type': dtypes.int64},\n",
    "#  'artist_pop_seed': {'mode':BigQueryClient.FieldMode.NULLABLE, 'output_type': dtypes.float64},\n",
    " 'artist_genres_pl': {'mode': BigQueryClient.FieldMode.REPEATED, 'output_type': dtypes.string},\n",
    "#  'artist_followers_seed': {'mode':BigQueryClient.FieldMode.NULLABLE, 'output_type': dtypes.float64},\n",
    "#  'pos_seed_track': {'mode':BigQueryClient.FieldMode.NULLABLE, 'output_type': dtypes.int64},\n",
    "#  'artist_name_seed_track': {'mode':BigQueryClient.FieldMode.NULLABLE, 'output_type': dtypes.string},\n",
    "#  'artist_uri_seed_track': {'mode':BigQueryClient.FieldMode.NULLABLE, 'output_type': dtypes.string},\n",
    "#  'track_name_seed_track': {'mode': BigQueryClient.FieldMode.NULLABLE, 'output_type': dtypes.string},\n",
    "#  'track_uri_seed_track': {'mode': BigQueryClient.FieldMode.NULLABLE, 'output_type': dtypes.string},\n",
    "#  'album_name_seed_track': {'mode': BigQueryClient.FieldMode.NULLABLE, 'output_type': dtypes.string},\n",
    "#  'album_uri_seed_track': {'mode': BigQueryClient.FieldMode.NULLABLE, 'output_type': dtypes.string},\n",
    "#  'duration_seed_track': {'mode': BigQueryClient.FieldMode.NULLABLE, 'output_type': dtypes.float64},\n",
    "#  'duration_ms_seed_pl': {'mode': BigQueryClient.FieldMode.NULLABLE, 'output_type': dtypes.float64},\n",
    "#  'n_songs': {'mode': BigQueryClient.FieldMode.NULLABLE, 'output_type': dtypes.int64},\n",
    "#  'num_artists': {'mode': BigQueryClient.FieldMode.NULLABLE, 'output_type': dtypes.int64},\n",
    "#  'num_albums': {'mode': BigQueryClient.FieldMode.NULLABLE, 'output_type': dtypes.int64},\n",
    "# 'pos_seed_pl': {'mode': BigQueryClient.FieldMode.REPEATED, 'output_type': dtypes.int64},\n",
    "# 'track_uri_seed_pl': {'mode': BigQueryClient.FieldMode.REPEATED, 'output_type': dtypes.string},\n",
    "# 'track_name_seed_pl': {'mode': BigQueryClient.FieldMode.REPEATED, 'output_type': dtypes.string},\n",
    "# 'duration_ms_seed_songs_pl': {'mode': BigQueryClient.FieldMode.REPEATED, 'output_type': dtypes.float64},\n",
    "# 'album_name_seed_pl': {'mode': BigQueryClient.FieldMode.REPEATED, 'output_type': dtypes.string},\n",
    "# 'artist_pop_seed_pl': {'mode': BigQueryClient.FieldMode.REPEATED, 'output_type': dtypes.float64},\n",
    "# 'artists_followers_seed_pl': {'mode': BigQueryClient.FieldMode.REPEATED, 'output_type': dtypes.float64},              \n",
    "# 'track_pop_seed_pl': {'mode': BigQueryClient.FieldMode.REPEATED, 'output_type': dtypes.int64},  \n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fa17d16d-a645-43af-bab2-950085f90ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = BigQueryClient()\n",
    "batch_size = 1\n",
    "bqsession = client.read_session(\n",
    "        \"projects/\" + PROJECT_ID,\n",
    "        PROJECT_ID, 'train_flatten', 'spotify_train_3',\n",
    "        bq_2_tf_dict,\n",
    "        requested_streams=2,)\n",
    "dataset = bqsession.parallel_read_rows()\n",
    "dataset = dataset.prefetch(1).shuffle(batch_size*10).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "13d812a6-9d55-4c4a-b687-896e2ae54caa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('artist_genres_pl', <tf.Tensor: shape=(1, 22), dtype=string, numpy=\n",
      "array([[b\"'bachata', 'latin'\",\n",
      "        b\"'latin', 'latin hip hop', 'reggaeton', 'trap latino'\",\n",
      "        b\"'latin', 'latin hip hop', 'reggaeton', 'reggaeton flow', 'trap latino'\",\n",
      "        b'',\n",
      "        b\"'latin', 'reggaeton', 'reggaeton colombiano', 'trap latino'\",\n",
      "        b\"'cubaton', 'latin', 'latin hip hop', 'latin pop'\",\n",
      "        b\"'latin', 'latin hip hop', 'reggaeton', 'reggaeton flow'\",\n",
      "        b\"'chicano rap', 'pop rap'\", b\"'dominican pop'\",\n",
      "        b\"'latin', 'latin hip hop', 'reggaeton', 'reggaeton flow', 'trap latino'\",\n",
      "        b\"'latin', 'latin hip hop', 'reggaeton', 'reggaeton flow', 'trap latino'\",\n",
      "        b\"'latin', 'latin hip hop', 'reggaeton', 'reggaeton flow', 'trap latino'\",\n",
      "        b\"'latin', 'modern salsa', 'salsa', 'salsa peruana', 'salsa puertorriquena', 'tropical'\",\n",
      "        b\"'latin', 'latin hip hop', 'reggaeton', 'trap latino'\",\n",
      "        b\"'bachata', 'latin', 'latin hip hop', 'latin pop'\",\n",
      "        b\"'latin', 'reggaeton', 'reggaeton colombiano', 'trap latino'\",\n",
      "        b\"'latin', 'latin hip hop', 'reggaeton', 'trap latino'\",\n",
      "        b\"'cubaton', 'latin', 'latin hip hop', 'latin pop'\",\n",
      "        b\"'bachata', 'latin'\",\n",
      "        b\"'bachata', 'bachata dominicana', 'dominican pop', 'latin', 'tropical'\",\n",
      "        b\"'bachata', 'bachata dominicana', 'dominican pop', 'latin', 'tropical'\",\n",
      "        b\"'bachata', 'bachata dominicana', 'dominican pop', 'latin', 'tropical'\"]],\n",
      "      dtype=object)>), ('collaborative', <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'false'], dtype=object)>), ('name', <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'Spanish Music'], dtype=object)>)])\n"
     ]
    }
   ],
   "source": [
    "for x in dataset.take(1):\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f532d8-d332-49c2-8fd2-43c09f79ebfa",
   "metadata": {},
   "source": [
    "### Confirm matching data and order for arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5fb71dff-f103-483f-a315-123b5665b769",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Query complete after 0.00s: 100%|██████████| 1/1 [00:00<00:00, 360.40query/s]                          \n",
      "Downloading: 100%|██████████| 1/1 [00:01<00:00,  1.14s/rows]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pid_pos_id</th>\n",
       "      <th>name</th>\n",
       "      <th>collaborative</th>\n",
       "      <th>pid</th>\n",
       "      <th>modified_at</th>\n",
       "      <th>num_tracks</th>\n",
       "      <th>num_albums</th>\n",
       "      <th>num_followers</th>\n",
       "      <th>num_edits</th>\n",
       "      <th>num_artists</th>\n",
       "      <th>...</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>album_name</th>\n",
       "      <th>pos_seed</th>\n",
       "      <th>pos_artist_name</th>\n",
       "      <th>track_uri_seed</th>\n",
       "      <th>artist_uri_seed</th>\n",
       "      <th>track_name_seed</th>\n",
       "      <th>album_uri_seed</th>\n",
       "      <th>duration_ms_seed</th>\n",
       "      <th>album_name_seed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11834-10</td>\n",
       "      <td>Throwback</td>\n",
       "      <td>false</td>\n",
       "      <td>11834</td>\n",
       "      <td>1499126400</td>\n",
       "      <td>75</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>51</td>\n",
       "      <td>...</td>\n",
       "      <td>170626.0</td>\n",
       "      <td>What Was I Thinking Of</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]</td>\n",
       "      <td>[Guns N' Roses, Big Sounds Band, The Bangles, ...</td>\n",
       "      <td>[spotify:track:7gXdAqJLCa5aYUeLVxosOz, spotify...</td>\n",
       "      <td>[spotify:artist:3qm84nBOXUEQ2vnTfUTTFC, spotif...</td>\n",
       "      <td>[Knockin' On Heaven's Door, Karma Chameleon (f...</td>\n",
       "      <td>[spotify:album:5NL0MCTSbQtO13G62ofWAf, spotify...</td>\n",
       "      <td>[336000.0, 237089.0, 204560.0, 238266.0, 14937...</td>\n",
       "      <td>[Use Your Illusion II, 80s Songs from the Big ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  pid_pos_id       name collaborative    pid  modified_at  num_tracks  \\\n",
       "0   11834-10  Throwback         false  11834   1499126400          75   \n",
       "\n",
       "   num_albums  num_followers  num_edits  num_artists  ... duration_ms  \\\n",
       "0          70              1          7           51  ...    170626.0   \n",
       "\n",
       "               album_name                        pos_seed  \\\n",
       "0  What Was I Thinking Of  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]   \n",
       "\n",
       "                                     pos_artist_name  \\\n",
       "0  [Guns N' Roses, Big Sounds Band, The Bangles, ...   \n",
       "\n",
       "                                      track_uri_seed  \\\n",
       "0  [spotify:track:7gXdAqJLCa5aYUeLVxosOz, spotify...   \n",
       "\n",
       "                                     artist_uri_seed  \\\n",
       "0  [spotify:artist:3qm84nBOXUEQ2vnTfUTTFC, spotif...   \n",
       "\n",
       "                                     track_name_seed  \\\n",
       "0  [Knockin' On Heaven's Door, Karma Chameleon (f...   \n",
       "\n",
       "                                      album_uri_seed  \\\n",
       "0  [spotify:album:5NL0MCTSbQtO13G62ofWAf, spotify...   \n",
       "\n",
       "                                    duration_ms_seed  \\\n",
       "0  [336000.0, 237089.0, 204560.0, 238266.0, 14937...   \n",
       "\n",
       "                                     album_name_seed  \n",
       "0  [Use Your Illusion II, 80s Songs from the Big ...  \n",
       "\n",
       "[1 rows x 29 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "select * from `hybrid-vertex.spotify_mpd.ordered_position_training` where pid_pos_id = '11834-10'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aae928c-beeb-4704-8b18-cd31bcb16d24",
   "metadata": {},
   "source": [
    "### Run adapts, and preprocess string lookups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f5dcb463-5b0d-45c8-8c7a-16de7a6601a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name\n",
      "collaborative\n",
      "pid\n",
      "description\n",
      "duration_ms_playlist\n",
      "pid_pos_id\n",
      "pos\n",
      "artist_name_seed\n",
      "track_uri_seed\n",
      "artist_uri_seed\n",
      "track_name_seed\n",
      "album_uri_seed\n",
      "duration_ms_seed\n",
      "album_name_seed\n",
      "track_pop_seed\n",
      "artist_pop_seed\n",
      "artist_genres_seed\n",
      "artist_followers_seed\n",
      "pos_seed_track\n",
      "artist_name_seed_track\n",
      "artist_uri_seed_track\n",
      "track_name_seed_track\n",
      "track_uri_seed_track\n",
      "album_name_seed_track\n",
      "album_uri_seed_track\n",
      "duration_seed_track\n",
      "duration_ms_seed_pl\n",
      "n_songs\n",
      "num_artists\n",
      "num_albums\n",
      "pos_seed_pl\n",
      "track_uri_seed_pl\n",
      "track_name_seed_pl\n",
      "duration_ms_seed_songs_pl\n",
      "album_name_seed_pl\n",
      "artist_pop_seed_pl\n",
      "artists_followers_seed_pl\n",
      "track_pop_seed_pl\n"
     ]
    }
   ],
   "source": [
    "for k in bq_2_tf_dict:\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b952922f-5e2c-405b-8b44-4513afb64c89",
   "metadata": {},
   "source": [
    "# Organize fields by transforms\n",
    "\n",
    "## Stringlookup get vocab \n",
    "- track_uri_seed\n",
    "- artist_uri_seed\n",
    "- album_uri_seed\n",
    "- artist_uri_seed_track\n",
    "- track_uri_seed_track\n",
    "- album_uri_seed_track\n",
    "- track_uri_seed_pl\n",
    "\n",
    "## TextVectorization (NLPish)\n",
    "- name\n",
    "- description\n",
    "- artist_name_seed\n",
    "- artist_name_seed_track\n",
    "- track_name_seed_track\n",
    "- album_name_seed_track\n",
    "- track_name_seed_pl\n",
    "- album_name_seed_pl\n",
    "- album_name_seed\n",
    "- artist_genres_seed\n",
    "\n",
    "## Rich features\n",
    "- collaborative\n",
    "- duration_ms_playlist\n",
    "- track_name_seed\n",
    "- track_pop_seed\n",
    "- artist_pop_seed\n",
    "- duration_seed_track\n",
    "#### --- playlist features\n",
    "- n_songs\n",
    "- num_artists\n",
    "- num_albums\n",
    "- duration_ms_seed_pl\n",
    "- artist_pop_seed_pl\n",
    "- artists_followers_seed_pl\n",
    "- track_pop_seed_pl\n",
    "- artist_followers_seed\n",
    "- duration_ms_seed_songs_pl\n",
    "- duration_ms_seed\n",
    "\n",
    "#not used\n",
    "#pid\n",
    "##Identifier pid_pos_id\n",
    "##POS id not used, infering order in dataset pos_seed_track\n",
    "##No POS pos_seed_pl\n",
    "#pos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1484e9-fe8b-459e-905d-a4284ba96ec6",
   "metadata": {},
   "source": [
    "#### Loop over values to find uniques to save to a vocab file\n",
    "\n",
    "We will save this in `gs://spotify-assets-blog`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "88cf1205-d5d8-4036-97a9-9943cafc241d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "\n",
    "# Construct a BigQuery client object.\n",
    "client = bigquery.Client()\n",
    "\n",
    "vocab_lookup_feats = [\n",
    "'track_uri_seed',\n",
    "'artist_uri_seed',\n",
    "'album_uri_seed',\n",
    "# 'artist_uri_seed_track',\n",
    "# 'track_uri_seed_track',\n",
    "# 'album_uri_seed_track',\n",
    "# 'track_uri_seed_pl', # ragged playlist\n",
    "]\n",
    "\n",
    "vocab_query = [f\"select distinct {field} from `hybrid-vertex.spotify_train_3.train_flatten`\" for field in vocab_lookup_feats]\n",
    "vocab_dict = {}\n",
    "for field, query in zip(vocab_lookup_feats, vocab_query):\n",
    "    data = client.query(query).result()\n",
    "    vocab_dict.update({field: list(d[0] for d in data)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "83a70287-19fa-470f-9fd3-89c23f4dd78d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "track_uri_seed counts: 2249561\n",
      "artist_uri_seed counts: 294110\n",
      "album_uri_seed counts: 730377\n"
     ]
    }
   ],
   "source": [
    "### quick counts\n",
    "\n",
    "for k in vocab_dict:\n",
    "    print(f\"{k} counts: {len(vocab_dict[k])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4097bce4-6cc1-4a2e-bb1d-660f3247c602",
   "metadata": {},
   "source": [
    "### Quick query to find the max length of repeated fields\n",
    "\n",
    "Find max counts to pad ragged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8cae6340-e09c-4aae-b02b-d81fe161baab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Query complete after 0.00s: 100%|██████████| 16/16 [00:00<00:00, 7311.13query/s]                       \n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.01rows/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f0_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>341</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   f0_\n",
       "0  341"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "with counts as (select \tpid_pos_id, count(distinct x) as distinct_counts from `hybrid-vertex.spotify_train_3.train_flatten` inner join UNNEST(track_uri_seed_pl) x group by 1)\n",
    "select max(counts.distinct_counts) from counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3155efa0-b4e9-40e1-930b-b30eb6783590",
   "metadata": {},
   "source": [
    "#### Recycling Below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ad63a5-184e-4b2c-9f20-5eab1b6e68f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function ragged_unique_collection.<locals>.<lambda> at 0x7fa7586677a0> and will run it as-is.\n",
      "Cause: could not parse the source code:\n",
      "\n",
      "    for x in ds.map(lambda x: x[field]).batch(1):\n",
      "\n",
      "This error may be avoided by creating the lambda in a standalone statement.\n",
      "\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function ragged_unique_collection.<locals>.<lambda> at 0x7fa7586677a0> and will run it as-is.\n",
      "Cause: could not parse the source code:\n",
      "\n",
      "    for x in ds.map(lambda x: x[field]).batch(1):\n",
      "\n",
      "This error may be avoided by creating the lambda in a standalone statement.\n",
      "\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_14418/2703635343.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtrack_uri_seed_pl_vocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mragged_unique_collection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'track_uri_seed_pl'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#unique values =\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_14418/2703635343.py\u001b[0m in \u001b[0;36mragged_unique_collection\u001b[0;34m(ds, field)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfield\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def ragged_unique_collection(ds, field):\n",
    "    data = np.array([''])\n",
    "    for x in ds.map(lambda x: x[field]).batch(1):\n",
    "        y = np.unique(np.concatenate(np.concatenate(x.numpy())))\n",
    "        data = np.concatenate([data, y])\n",
    "    data = np.unique(data)\n",
    "    return(data)\n",
    "\n",
    "track_uri_seed_pl_vocab = ragged_unique_collection(dataset, 'track_uri_seed_pl') #unique values = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e8c498-1d8d-4ca8-b738-99e712a62bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b55c77e-438a-433e-9fb4-fa15ea0da275",
   "metadata": {},
   "outputs": [],
   "source": [
    "hi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379ee8e5-d46a-4d62-94d2-5e2d60d6d3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil mb -l us-central1 gs://spotify-assets-blog"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a117d93f-7cfd-433b-84e0-f62955f98eb5",
   "metadata": {},
   "source": [
    "### Save the arrays to google storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710f3594-50ec-423a-87c1-9ffa8387f8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### save above arrays - use naming convention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d8590b-6679-49f5-9473-428663ac9ac9",
   "metadata": {},
   "source": [
    "### Text Vectorization section\n",
    "Loop over and save the layers to a bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c14332-b3cd-4de9-ab93-b0a2594ba43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vector_feats = ['name',\n",
    "'description',\n",
    "'artist_name_seed',\n",
    "'artist_name_seed_track',\n",
    "'track_name_seed_track',\n",
    "'album_name_seed_track',\n",
    "'track_name_seed_pl',\n",
    "'album_name_seed_pl',\n",
    "'album_name_seed',\n",
    "'artist_genres_seed',\n",
    "                    ]\n",
    "\n",
    "MAX_TOKENS = 100_000\n",
    "\n",
    "vectorizors = []\n",
    "\n",
    "def create_vectorizor_layers(ds, field, n_tokens, ngrams=3):\n",
    "    name = f\"{field}-{n_tokens}-{ngrams}\"\n",
    "    tv_layer = tf.keras.layers.TextVectorization(max_tokens=n_tokens, name=name, ngrams=ngrams)\n",
    "    return(tv_layer.adapt(ds.map(lambda x: x[field]).batch(1000))\n",
    "           \n",
    "for feat in text_vector_feats:\n",
    "           vectorizors.append(create_vectorizor_layers(dataset, feat, MAX_TOKENS)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "bf39a187-9c37-490f-b12a-c263050ff247",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in dataset.take(1):\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152d16ec-9ee3-45de-8786-cac1bd1c8cd2",
   "metadata": {},
   "source": [
    "### Create a function to process data to new ds using map - then write DS to gcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf1c5b6-1be6-4db7-b48b-d713559c355c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40d43ba5-38d8-4ba6-8ad6-cf1e7950e296",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_14418/2223016346.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfeat\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvocab_lookup_feats\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0munique_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_unique_np\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_14418/2223016346.py\u001b[0m in \u001b[0;36mget_unique_np\u001b[0;34m(ds, field)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_unique_np\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfield\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0munique\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfield\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    735\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# For Python 3 compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 736\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    771\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 772\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    773\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    757\u001b[0m             \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 758\u001b[0;31m             output_shapes=self._flat_output_shapes)\n\u001b[0m\u001b[1;32m    759\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36miterator_get_next\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   2606\u001b[0m         \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop_callbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"output_types\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2607\u001b[0;31m         \"output_shapes\", output_shapes)\n\u001b[0m\u001b[1;32m   2608\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ### Vocab to get string lookups\n",
    "# import numpy as np\n",
    "# import keras \n",
    "\n",
    "# vocab_lookup_feats = [\n",
    "# 'track_uri_seed',\n",
    "# 'artist_uri_seed',\n",
    "# 'album_uri_seed',\n",
    "# 'artist_uri_seed_track',\n",
    "# 'track_uri_seed_track',\n",
    "# 'album_uri_seed_track',\n",
    "# # 'track_uri_seed_pl', # ragged playlist\n",
    "# ]\n",
    "\n",
    "# def get_unique_np(ds, field) -> np.array:\n",
    "#     unique = np.unique(np.concatenate(list(ds.map(lambda x: x[field]).batch(1000))))\n",
    "#     return(unique)\n",
    "\n",
    "# unique_list = []\n",
    "# for feat in vocab_lookup_feats:\n",
    "#     unique_list.append(get_unique_np(dataset, feat))\n",
    "# text_vector_feats"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-9.m93",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-9:m93"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
