{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data prep\n",
    "\n",
    "## In this notebook we will load the songs from the zip file, and perform transformations to prepare the data for two-tower training\n",
    "Steps\n",
    "1. Create a bq dataset\n",
    "2. Load the million playlist data to Big Query\n",
    "3. Create pipelines to download audio and artist features for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set your variables for your project, region, and dataset name\n",
    "SOURCE_BUCKET = 'spotify-million-playlist-dataset'\n",
    "PROJECT_ID = 'hybrid-vertex'\n",
    "REGION = 'us-central1'\n",
    "BQ_DATASET = 'spotify_e2e_test'\n",
    "\n",
    "import time\n",
    "from google.cloud import bigquery\n",
    "\n",
    "bigquery_client = bigquery.Client(project=PROJECT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a bigquery dataset (one time operation)\n",
    "# # Construct a full Dataset object to send to the API.\n",
    "# dataset = bigquery.Dataset(f\"`{PROJECT_ID}.{BQ_DATASET}`\")\n",
    "\n",
    "# # TODO(developer): Specify the geographic location where the dataset should reside.\n",
    "# dataset.location = REGION\n",
    "\n",
    "# # Send the dataset to the API for creation, with an explicit timeout.\n",
    "# # Raises google.api_core.exceptions.Conflict if the Dataset already\n",
    "# # exists within the project.\n",
    "# dataset = bigquery_client.create_dataset(BQ_DATASET, timeout=30)  # Make an API request.\n",
    "# print(\"Created dataset {}.{}\".format(bigquery_client.project, dataset.dataset_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download the data \n",
    "(also `curl` from source, see readme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !gsutil cp gs://{SOURCE_BUCKET}/spotify_million_playlist_dataset.zip .\n",
    "# !unzip -n spotify_million_playlist_dataset.zip "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This step can take up to 30 minutes\n",
    "\n",
    "Iteration occurs over 1000 json files if you are using the full dataset\n",
    "\n",
    "This should give you a `playlists` bq data set with 1,076,000 rows (playlists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [31:09<00:00,  1.87s/it] \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import multiprocessing\n",
    "\n",
    "data_files = os.listdir('data')\n",
    "\n",
    "def load_data(filename: str):\n",
    "    with open(f'data/{filename}') as f:\n",
    "        json_dict = json.load(f)\n",
    "        df = pd.DataFrame(json_dict['playlists'])\n",
    "        df['tracks'] = df['tracks'].map(str)\n",
    "        #write to bq\n",
    "        return df\n",
    "        \n",
    "#make sure there is not already existing data in the playlists table\n",
    "#loops over json files - converts to pandas then upload/appends\n",
    "\n",
    "### add this if you want to limit to smaller number of playlists - this scales significantly later!\n",
    "n_playliststs_limit = None #add if you want to use in for loop: while counter <= n_playliststs_limit:\n",
    "total_files = len(data_files)\n",
    "count = 0\n",
    "batch_size = 15\n",
    "\n",
    "for filename in tqdm(data_files):\n",
    "    if count == 0 or (count-1) % batch_size == 0:\n",
    "        append_df = load_data(filename)\n",
    "        count += 1\n",
    "    if count % batch_size == 0 or count == total_files:\n",
    "        df = load_data(filename) \n",
    "        append_df = pd.concat([df, append_df])\n",
    "        count += 1\n",
    "        append_df.to_gbq(\n",
    "                destination_table=f'{BQ_DATASET}.playlists', \n",
    "                project_id=PROJECT_ID, # TODO: param\n",
    "                location='US', \n",
    "                progress_bar=False, \n",
    "                reauth=True, \n",
    "                if_exists='append'\n",
    "            ) \n",
    "    else:\n",
    "        df = load_data(filename) \n",
    "        append_df = pd.concat([df, append_df])\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the data is loaded but the playlists are nested as one large string that needs to be parsed - we will use json compatible functionality with BigQuery to address\n",
    "\n",
    "![](img/tracks-string.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import bigquery and run parameterized queries to shape the data\n",
    "\n",
    "This query formats the json strings to be read as Bigquery structs, to be manipulated in subsequent queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 148 ms, sys: 51.1 ms, total: 199 ms\n",
      "Wall time: 36.1 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<google.cloud.bigquery.table._EmptyRowIterator at 0x7f495488fe90>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "json_extract_query = f\"\"\"create or replace table `{PROJECT_ID}.{BQ_DATASET}.playlists_nested` as (\n",
    "with json_parsed as (SELECT * except(tracks), JSON_EXTRACT_ARRAY(tracks) as json_data FROM `{PROJECT_ID}.{BQ_DATASET}.playlists` )\n",
    "\n",
    "select json_parsed.* except(json_data),\n",
    "ARRAY(SELECT AS STRUCT\n",
    "JSON_EXTRACT_SCALAR(json_data, \"$.pos\") as pos, \n",
    "JSON_EXTRACT_SCALAR(json_data, \"$.artist_name\") as artist_name,\n",
    "JSON_EXTRACT_SCALAR(json_data, \"$.track_uri\") as track_uri,\n",
    "JSON_EXTRACT_SCALAR(json_data, \"$.artist_uri\") as artist_uri,\n",
    "JSON_EXTRACT_SCALAR(json_data, \"$.track_name\") as track_name,\n",
    "JSON_EXTRACT_SCALAR(json_data, \"$.album_uri\") as album_uri,\n",
    "JSON_EXTRACT_SCALAR(json_data, \"$.duration_ms\") as duration_ms,\n",
    "JSON_EXTRACT_SCALAR(json_data, \"$.album_name\") as album_name\n",
    "from json_parsed.json_data\n",
    ") as tracks,\n",
    "from json_parsed) \"\"\"\n",
    "\n",
    "bigquery_client.query(json_extract_query).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now `playlists_nested` has parsed the string data to a struct with arrays that will allow us to process the data much more easily\n",
    "\n",
    "![](img/playlists-nested.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Next we get the unique track features to put in a BQ table\n",
    "\n",
    "This table will then be used to call the Spotify API and enrich with additional data about each track and artist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 24.3 ms, sys: 29.4 ms, total: 53.7 ms\n",
      "Wall time: 10.9 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<google.cloud.bigquery.table._EmptyRowIterator at 0x7f4955f3c9d0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "unique_tracks_sql = f\"\"\"create or replace table `{PROJECT_ID}.{BQ_DATASET}.tracks_unique` as (\n",
    "SELECT distinct \n",
    "    track.track_uri,\n",
    "    track.album_uri,\n",
    "    track.artist_uri, \n",
    "FROM `{PROJECT_ID}.{BQ_DATASET}.playlists_nested`, UNNEST(tracks) as track)\n",
    "\"\"\"\n",
    "\n",
    "bigquery_client.query(unique_tracks_sql).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Core dataset loading complete\n",
    "\n",
    "We now have our unique id tables we will use for grabbing additional audio and artist features "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CVKyuWRezBGF"
   },
   "source": [
    "___________\n",
    "# Spotify API Feature Extraction\n",
    "\n",
    "___________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IgGWdClhze9D",
    "tags": []
   },
   "source": [
    "* Spotify Mlllion Playlist Dataset Challenge [Homepage](https://www.aicrowd.com/challenges/spotify-million-playlist-dataset-challenge)\n",
    "* [Spotify Web API docs](https://developer.spotify.com/documentation/web-api/reference/#/)\n",
    "\n",
    "**Community Examples**\n",
    "* [Extracting song lists](https://github.com/tojhe/recsys-spotify/blob/master/processing/songlist_extraction.py)\n",
    "* [construct audio features with Spotify API](https://github.com/tojhe/recsys-spotify/blob/master/processing/audio_features_construction.py)\n",
    "* [Using Spotify API](https://towardsdatascience.com/extracting-song-data-from-the-spotify-api-using-python-b1e79388d50)\n",
    "\n",
    "#### After reading through these, create a new Spotify App and get the customer id, secret\n",
    "![](img/spotify-dev-console.png)\n",
    "\n",
    "This example uses a local json credentials and if you are concerned over visibility to apikeys, please see [GCP Secret Manager](https://cloud.google.com/secret-manager)\n",
    "\n",
    "Below is an example if you were to add the json file to secret manager (keys: `secret`, `id`)\n",
    "\n",
    "```python\n",
    "from google.cloud import secretmanager\n",
    "\n",
    "###Note you copy/paste this from secret manager in console\n",
    "SECRET_VERSION = 'projects/934903580331/secrets/spotify-creds1/versions/1'\n",
    "\n",
    "sm_client = secretmanager.SecretManagerServiceClient()\n",
    "\n",
    "name = sm_client.secret_path(PROJECT_ID, SECRET_ID)\n",
    "\n",
    "response = client.access_secret_version(request={\"name\": SECRET_VERSION})   \n",
    "\n",
    "payload = json.loads(response.payload.data.decode(\"UTF-8\"))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "RRSdCB_HSBSl",
    "outputId": "b272ec83-d189-4a22-da3e-5ef650c574b7",
    "tags": []
   },
   "source": [
    "### Package Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip3 install -U spotipy google-cloud-storage google-cloud-aiplatform gcsfs --user -q\n",
    "# ! pip3 install --user kfp google-cloud-pipeline-components --upgrade -q\n",
    "# !pip3 install --user -q google-cloud-secret-manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "irhbKrY3fcG8",
    "outputId": "2008a7ca-303d-4943-e56b-a57040c9abd7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFP SDK version: 1.8.19\n",
      "google_cloud_pipeline_components version: 1.0.39\n",
      "aiplatform SDK version: 1.22.0\n"
     ]
    }
   ],
   "source": [
    "! python3 -c \"import kfp; print('KFP SDK version: {}'.format(kfp.__version__))\"\n",
    "! python3 -c \"import google_cloud_pipeline_components; print('google_cloud_pipeline_components version: {}'.format(google_cloud_pipeline_components.__version__))\"\n",
    "! python3 -c \"import google.cloud.aiplatform; print('aiplatform SDK version: {}'.format(google.cloud.aiplatform.__version__))\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jCbr2tLuzTJK"
   },
   "source": [
    "### Constants - setup to your config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7wwktfnCyzWO",
    "outputId": "6afe4a17-ebc2-40dc-d23a-1f447f394046"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PIPELINE_TAG: v10-spotify-feature-enrich\n"
     ]
    }
   ],
   "source": [
    "PROJECT_ID = 'hybrid-vertex' #update\n",
    "LOCATION = 'us-central1' \n",
    "\n",
    "BUCKET_NAME = 'matching-engine-content'\n",
    "# BUCKET_NAME = 'spotify-million-playlists'\n",
    "\n",
    "VERSION = 10\n",
    "PIPELINE_VERSION = f'v{VERSION}' # pipeline code\n",
    "PIPELINE_TAG = f'{PIPELINE_VERSION}-spotify-feature-enrich'\n",
    "print(\"PIPELINE_TAG:\", PIPELINE_TAG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Bucket if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !gsutil mb -l $LOCATION gs://$BUCKET_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "GqhJlsodR-xX"
   },
   "outputs": [],
   "source": [
    "import spotipy\n",
    "from spotipy.oauth2 import SpotifyClientCredentials\n",
    "\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "import time\n",
    "\n",
    "import gcsfs\n",
    "\n",
    "# GCP\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud import storage\n",
    "\n",
    "# Pipelines\n",
    "from typing import Any, Callable, Dict, NamedTuple, Optional, List\n",
    "from google_cloud_pipeline_components import aiplatform as gcc_aip\n",
    "from google_cloud_pipeline_components.types import artifact_types\n",
    "\n",
    "# Kubeflow SDK\n",
    "# TODO: fix these\n",
    "from kfp.v2 import dsl\n",
    "import kfp\n",
    "import kfp.v2.dsl\n",
    "from kfp.v2.google import client as pipelines_client\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output,\n",
    "                        OutputPath, component)\n",
    "\n",
    "\n",
    "aiplatform.init(\n",
    "    project=PROJECT_ID, location=LOCATION, staging_bucket=BUCKET_NAME\n",
    ")\n",
    "\n",
    "# Get spotify credentials\n",
    "# This file has id and secret stored as attributes\n",
    "\n",
    "creds = open('spotify-creds.json')\n",
    "spotify_creds = json.load(creds)\n",
    "creds.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IuJk9A5I9ZfF"
   },
   "source": [
    "### Clients & credentials\n",
    "\n",
    "Setup Vertex AI client for pipelines\n",
    "\n",
    "Spotify shoulld be stored in a json file with a your credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GCbHe1G_gICe",
    "outputId": "c3f7d046-4997-4b59-9bcc-44df8feba2ed"
   },
   "outputs": [],
   "source": [
    "# # Setup clients\n",
    "aiplatform.init(\n",
    "    project=PROJECT_ID, location=LOCATION, staging_bucket=BUCKET_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get spotify credentials\n",
    "# This file has id and secret stored as attributes\n",
    "\n",
    "\n",
    "###########################################\n",
    "### CAUTION THIS APPROACH WILL HAVE THE CREDENTIALS APPEAR IN THE CONSOLE - \n",
    "### USE SECRET MANAGER APPROACH IN EACH COMPONENT AS NEEDED (PROVIDED ABOVE)\n",
    "###########################################\n",
    "\n",
    "\n",
    "creds = open('spotify-creds.json')\n",
    "spotify_creds = json.load(creds)\n",
    "creds.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QbeNm6_whXMG"
   },
   "source": [
    "# Create Pipeline Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8m3D974DjTbE"
   },
   "source": [
    "### Audio Features\n",
    "\n",
    "[Link to artist API and related features we will pull](https://developer.spotify.com/documentation/web-api/reference/#/operations/get-audio-features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "86ZuQqElhZrS"
   },
   "outputs": [],
   "source": [
    "@kfp.v2.dsl.component(\n",
    "    base_image=\"python:3.9\",\n",
    "    packages_to_install=['fsspec', 'google-cloud-bigquery',\n",
    "                         'google-cloud-storage',\n",
    "                         'gcsfs',\n",
    "                         'spotipy','requests','db-dtypes',\n",
    "                         'numpy','pandas','pyarrow','absl-py', 'pandas-gbq==0.17.4',\n",
    "                        ])\n",
    "\n",
    "def call_spotify_api_audio(\n",
    "    project: str,\n",
    "    location: str,\n",
    "    client_id: str,\n",
    "    batch_size: int,\n",
    "    batches_to_store: int,\n",
    "    target_table: str,\n",
    "    client_secret: str,\n",
    "    unique_table: str,\n",
    "    sleep_param: float,\n",
    ") -> NamedTuple('Outputs', [('done_message', str),]):\n",
    "    print(f'pip install complete')\n",
    "    import os\n",
    "    import spotipy\n",
    "    from spotipy.oauth2 import SpotifyClientCredentials\n",
    "    import re\n",
    "    import warnings\n",
    "    warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "    import pandas as pd\n",
    "    import json\n",
    "    import time\n",
    "    from google.cloud import storage\n",
    "    import gcsfs\n",
    "    import numpy as np\n",
    "    from requests.exceptions import ReadTimeout, HTTPError, ConnectionError, RequestException\n",
    "    from absl import logging\n",
    "    from google.cloud import bigquery\n",
    "    import pandas_gbq\n",
    "\n",
    "    # print(f'package import complete')\n",
    "\n",
    "    logging.set_verbosity(logging.INFO)\n",
    "    logging.info(f'package import complete')\n",
    "\n",
    "    \n",
    "    bq_client = bigquery.Client(\n",
    "      project=project, location=location\n",
    "    )\n",
    "\n",
    "    logging.info(f'spotipy auth complete')\n",
    "    def spot_audio_features(uri, client_id, client_secret):\n",
    "\n",
    "        # Authenticate\n",
    "        client_credentials_manager = SpotifyClientCredentials(\n",
    "            client_id=client_id, \n",
    "            client_secret=client_secret\n",
    "        )\n",
    "        sp = spotipy.Spotify(\n",
    "            client_credentials_manager = client_credentials_manager, \n",
    "            requests_timeout=10, \n",
    "            retries=10 )\n",
    "        ############################################################################\n",
    "        # Create Track Audio Features DF\n",
    "        ############################################################################\n",
    "        \n",
    "        uri_stripped = [u.replace('spotify:track:', '') for u in uri] #fix the quotes \n",
    "        #getting track popularity\n",
    "        tracks = sp.tracks(uri_stripped)\n",
    "        #Audio features\n",
    "        a_feats = sp.audio_features(uri)\n",
    "        features = pd.json_normalize(a_feats)#.to_dict('list')\n",
    "        \n",
    "        features['track_pop'] = pd.json_normalize(tracks['tracks'])['popularity']\n",
    "        \n",
    "        features['track_uri'] = uri\n",
    "        return features\n",
    "\n",
    "    bq_client = bigquery.Client(\n",
    "      project=project, location='US'\n",
    "    )\n",
    "\n",
    "    query = f\"select distinct track_uri from `{unique_table}` LIMIT 233\" \n",
    "\n",
    "\n",
    "    #refactor\n",
    "    schema = [{'name':'danceability', 'type': 'FLOAT'},\n",
    "            {'name':'energy', 'type': 'FLOAT'},\n",
    "            {'name':'key', 'type': 'FLOAT'},\n",
    "            {'name':'loudness', 'type': 'FLOAT'},\n",
    "            {'name':'mode', 'type': 'INTEGER'},\n",
    "            {'name':'speechiness', 'type': 'FLOAT'},\n",
    "            {'name':'acousticness', 'type': 'FLOAT'},\n",
    "            {'name':'instrumentalness', 'type': 'FLOAT'},\n",
    "            {'name':'liveness', 'type': 'FLOAT'},\n",
    "            {'name':'valence', 'type': 'FLOAT'},\n",
    "            {'name':'followers', 'type': 'FLOAT'},\n",
    "            {'name':'tempo', 'type': 'FLOAT'},\n",
    "            {'name':'type', 'type': 'STRING'},\n",
    "            {'name':'id', 'type': 'STRING'},\n",
    "            {'name':'uri', 'type': 'STRING'},\n",
    "            {'name':'track_href', 'type': 'STRING'},\n",
    "            {'name':'analysis_url', 'type': 'STRING'},\n",
    "            {'name':'duration_ms_y', 'type': 'INTEGER'},\n",
    "            {'name':'time_signature', 'type': 'INTEGER'},\n",
    "            {'name':'track_uri', 'type': 'STRING'},\n",
    "    ]\n",
    "\n",
    "    tracks = bq_client.query(query).result().to_dataframe()\n",
    "    track_list = tracks.track_uri.to_list()\n",
    "    logging.info(f'finished downloading tracks')\n",
    "    uri_list_length = len(track_list)-1 #starting count at zero\n",
    "    inner_batch_count = 0 #avoiding calling the api on 0th iteration\n",
    "    \n",
    "    uri_batch = []\n",
    "    \n",
    "    for i, uri in enumerate(track_list):\n",
    "        uri_batch.append(uri)\n",
    "        if (len(uri_batch) == batch_size or uri_list_length == i) and i > 3: #grab a batch of 50 songs\n",
    "            logging.info(f\"appending final record for nth song at: {inner_batch_count} \\n i: {i} \\n uri_batch length: {len(uri_batch)}\")\n",
    "            ### Try catch block for function\n",
    "            try:\n",
    "                audio_featureDF = spot_audio_features(uri_batch, client_id, client_secret)\n",
    "                time.sleep(sleep_param)\n",
    "                uri_batch = []\n",
    "            except ReadTimeout:\n",
    "                logging.info(\"'Spotify timed out... trying again...'\")\n",
    "                audio_featureDF = spot_audio_features(uri_batch, client_id, client_secret)\n",
    "                uri_batch = []\n",
    "                time.sleep(sleep_param)\n",
    "            except HTTPError as err: #JW ADDED\n",
    "                logging.info(f\"HTTP Error: {err}\")\n",
    "            except spotipy.exceptions.SpotifyException as spotify_error: #jw_added\n",
    "                logging.info(f\"Spotify error: {spotify_error}\")\n",
    "            # Accumulate batches on the machine before writing to BQ\n",
    "            # if inner_batch_count <= batches_to_store or uri_list_length == i:\n",
    "            if inner_batch_count == 0:\n",
    "                appended_data = audio_featureDF\n",
    "                logging.info(f\"creating new appended data at IBC: {inner_batch_count} \\n i: {i}\")\n",
    "                inner_batch_count += 1\n",
    "            elif uri_list_length == i or inner_batch_count == batches_to_store: #send the batches to bq\n",
    "                appended_data = pd.concat([audio_featureDF, appended_data])\n",
    "                inner_batch_count = 0\n",
    "                try:\n",
    "                    appended_data.to_gbq(\n",
    "                        destination_table=target_table, \n",
    "                        project_id=f'{project}', \n",
    "                        location='US', \n",
    "                        table_schema=schema,\n",
    "                        progress_bar=False, \n",
    "                        reauth=False, \n",
    "                        if_exists='append'\n",
    "                        )\n",
    "                except pandas_gbq.gbq.InvalidSchema as invalid_schema:\n",
    "                    logging.info('invalid schema, skipping')\n",
    "                    pass\n",
    "                logging.info(f'{i+1} of {uri_list_length} complete!')\n",
    "                del appended_data\n",
    "            else:\n",
    "                appended_data = pd.concat([audio_featureDF, appended_data])\n",
    "                inner_batch_count += 1\n",
    "\n",
    "    logging.info(f'audio features appended')\n",
    "    return (\n",
    "          f'DONE',\n",
    "      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_hppPSwkh1x2"
   },
   "source": [
    "### Artists \n",
    "\n",
    "[Link to artist API and related features we will pull](https://developer.spotify.com/documentation/web-api/reference/#/operations/get-an-artist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "id": "JYbgsLrtxPHl"
   },
   "outputs": [],
   "source": [
    "### Artist tracks api call\n",
    "\n",
    "@kfp.v2.dsl.component(\n",
    "    base_image=\"python:3.9\",\n",
    "    packages_to_install=['fsspec',' google-cloud-bigquery',\n",
    "                         'google-cloud-storage',\n",
    "                         'gcsfs',\n",
    "                         'spotipy','requests','db-dtypes',\n",
    "                         'numpy','pandas','pyarrow','absl-py', 'pandas-gbq==0.17.4',\n",
    "                        'google-cloud-secret-manager'])\n",
    "\n",
    "def call_spotify_api_artist(\n",
    "    project: str,\n",
    "    location: str,\n",
    "    unique_table: str,\n",
    "    batch_size: int,\n",
    "    batches_to_store: int,\n",
    "    client_id: str,\n",
    "    client_secret: str,\n",
    "    sleep_param: float,\n",
    "    target_table: str,\n",
    ") -> NamedTuple('Outputs', [('done_message', str),]):\n",
    "    print(f'pip install complete')\n",
    "    import os\n",
    "    import spotipy\n",
    "    from spotipy.oauth2 import SpotifyClientCredentials\n",
    "    import re\n",
    "    import warnings\n",
    "    warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "    import pandas as pd\n",
    "    import json\n",
    "    import time\n",
    "    from google.cloud import storage\n",
    "    import gcsfs\n",
    "    import numpy as np\n",
    "    from requests.exceptions import ReadTimeout, HTTPError, ConnectionError, RequestException\n",
    "    from absl import logging\n",
    "    from google.cloud import bigquery\n",
    "    import pandas_gbq\n",
    "    pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "    logging.set_verbosity(logging.INFO)\n",
    "    logging.info(f'package import complete')\n",
    "\n",
    "    storage_client = storage.Client(\n",
    "        project=project\n",
    "    )\n",
    "    \n",
    "    logging.info(f'spotipy auth complete')\n",
    "    \n",
    "    def spot_track_features(uri, client_id, client_secret):\n",
    "\n",
    "        # Authenticate\n",
    "        client_credentials_manager = SpotifyClientCredentials(\n",
    "            client_id=client_id, \n",
    "            client_secret=client_secret\n",
    "        )\n",
    "        sp = spotipy.Spotify(\n",
    "            client_credentials_manager = client_credentials_manager, \n",
    "            requests_timeout=10, \n",
    "            retries=10 )\n",
    "\n",
    "        ############################################################################\n",
    "        # Create Track Audio Features DF\n",
    "        ############################################################################ \n",
    "\n",
    "        uri = [u.replace('\"', '') for u in uri] #fix the quotes \n",
    "\n",
    "        artists = sp.artists(uri)\n",
    "        features = pd.json_normalize(artists['artists'])\n",
    "        smaller_features = features[['genres', 'popularity', 'name', 'followers.total']]\n",
    "        smaller_features.columns = ['genres',  'popularity', 'name',  'followers']\n",
    "        smaller_features['artist_uri'] = uri\n",
    "        smaller_features['genres'] = smaller_features['genres'].map(lambda x: '_'.join(x))\n",
    "\n",
    "        return smaller_features\n",
    "        \n",
    "\n",
    "    bq_client = bigquery.Client(\n",
    "      project=project, location='US'\n",
    "    )\n",
    "\n",
    "\n",
    "    query = f\"select distinct artist_uri from `{unique_table}`\"\n",
    "    \n",
    "\n",
    "    schema = [{'name': 'popularity', 'type': 'INTEGER'},\n",
    "            {'name':'genres', 'type': 'STRING'},\n",
    "            {'name':'followers', 'type': 'INTEGER'},\n",
    "            {'name':'artist_uri', 'type': 'STRING'}\n",
    "    ]\n",
    "    uri_batch = []\n",
    "\n",
    "    ats = bq_client.query(query).result().to_dataframe()\n",
    "    artist_set = ats.artist_uri.to_list()\n",
    "    uri_list_length = len(artist_set)-1\n",
    "    logging.info(f'finished downloading tracks')\n",
    "    inner_batch_count = 0\n",
    "    for i, uri in enumerate(artist_set):\n",
    "        if (i % batch_size-1 == 0 or uri_list_length == i) and i != 0: #grab a batch of 50 songs\n",
    "            uri_batch.append(uri)\n",
    "            ### Try catch block for function\n",
    "            try:\n",
    "                artist_featureDF = spot_track_features(uri_batch, client_id, client_secret)\n",
    "                time.sleep(sleep_param)\n",
    "            except ReadTimeout:\n",
    "                logging.info(\"'Spotify timed out... trying again...'\")\n",
    "                artist_featureDF = spot_track_features(uri_batch, client_id, client_secret)\n",
    "                time.sleep(sleep_param)\n",
    "            except HTTPError as err: #JW ADDED\n",
    "                logging.info(f\"HTTP Error: {err}\")\n",
    "            except spotipy.exceptions.SpotifyException as spotify_error: #jw_added\n",
    "                logging.info(f\"Spotify error: {spotify_error}\")\n",
    "            # Accumulate batches on the machine before writing to BQ\n",
    "            if inner_batch_count <= batches_to_store or uri_list_length == i:\n",
    "                if inner_batch_count == 0:\n",
    "                    appended_data = artist_featureDF\n",
    "                else:\n",
    "                    appended_data = pd.concat([artist_featureDF, appended_data])\n",
    "                inner_batch_count += 1\n",
    "                uri_batch = []\n",
    "            else:\n",
    "                appended_data = pd.concat([artist_featureDF, appended_data])\n",
    "                try:\n",
    "                    appended_data.to_gbq(\n",
    "                        destination_table=target_table, \n",
    "                        project_id=f'{project}', \n",
    "                        location='US', \n",
    "                        table_schema=schema,\n",
    "                        progress_bar=False, \n",
    "                        reauth=False, \n",
    "                        if_exists='append'\n",
    "                        )\n",
    "                except pandas_gbq.gbq.InvalidSchema as invalid_schema:\n",
    "                    logging.info('invalid schema, skipping')\n",
    "                    pass\n",
    "                logging.info(f'{count} of {uri_list_length} complete!')\n",
    "                uri_batch = []\n",
    "                inner_batch_count = 0\n",
    "        else:\n",
    "            uri_batch.append(uri)\n",
    "    \n",
    "    logging.info(f'artist features appended')\n",
    "    return (\n",
    "          f'DONE',\n",
    "      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ma5qzpGkuYhD"
   },
   "source": [
    "## Build Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "id": "Buwtyt7rugt4"
   },
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "@kfp.v2.dsl.pipeline(\n",
    "  name=f'spotify-feature-enrichment-{PIPELINE_TAG}'.replace('_', '-')\n",
    ")\n",
    "def pipeline(\n",
    "    project: str,\n",
    "    location: str,\n",
    "    unique_table: str,\n",
    "    target_table_audio: str,\n",
    "    target_table_artist: str,\n",
    "    batch_size: int,\n",
    "    batches_to_store: int,\n",
    "    sleep_param: float,\n",
    "    spotify_id: str = spotify_creds['id'],\n",
    "    spotify_secret: str = spotify_creds['secret'],\n",
    "    ):\n",
    "\n",
    "\n",
    "    # call_spotify_api_artist_op = call_spotify_api_artist(\n",
    "    #     project=project,\n",
    "    #     location=location,\n",
    "    #     client_id=spotify_id,\n",
    "    #     client_secret=spotify_secret,\n",
    "    #     batch_size=batch_size,\n",
    "    #     sleep_param=sleep_param,\n",
    "    #     unique_table=unique_table,\n",
    "    #     target_table=target_table_artist,\n",
    "    #     batches_to_store=batches_to_store,\n",
    "    # ).set_display_name(\"Get Artist Features From Spotify API\")\n",
    "\n",
    "    call_spotify_api_audio_op = call_spotify_api_audio(\n",
    "        project=project,\n",
    "        location=location,\n",
    "        client_id=spotify_id,\n",
    "        client_secret=spotify_secret,\n",
    "        batch_size=batch_size,\n",
    "        sleep_param=sleep_param,\n",
    "        unique_table=unique_table,\n",
    "        target_table=target_table_audio,\n",
    "        batches_to_store=batches_to_store,\n",
    "    ).set_display_name(\"Get Track Audio Features From Spotify API\")#.after(call_spotify_api_artist_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the pipeline to json\n",
    "This can be stored on gcs as well for broader orchastration purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RoBsfT1IyIXd",
    "outputId": "50a65461-1c7c-44c0-9362-5524aaac21c4"
   },
   "outputs": [],
   "source": [
    "kfp.v2.compiler.Compiler().compile(\n",
    "  pipeline_func=pipeline, \n",
    "  package_path='custom_container_pipeline_spec.json',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the pipeline parameters\n",
    "\n",
    "Use a dictionary with the afforementioned types defined by your pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "id": "evUgOHllykr5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'project': 'hybrid-vertex',\n",
       " 'location': 'us-central1',\n",
       " 'unique_table': 'hybrid-vertex.spotify_e2e_test.tracks_unique',\n",
       " 'target_table_audio': 'hybrid-vertex.spotify_e2e_test.audio_features',\n",
       " 'target_table_artist': 'hybrid-vertex.spotify_e2e_test.artist_features',\n",
       " 'batch_size': 50,\n",
       " 'batches_to_store': 2,\n",
       " 'sleep_param': 0.05}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# jtotten-project #\n",
    "GCS_BUCKET = 'matching-engine-content'\n",
    "\n",
    "ideal_batch_size = 200_000\n",
    "bts = ideal_batch_size / 50\n",
    "\n",
    "PIPELINE_PARAMETERS = dict(\n",
    "    project = PROJECT_ID,\n",
    "    location = 'us-central1',\n",
    "    unique_table = f'{PROJECT_ID}.{BQ_DATASET}.tracks_unique', \n",
    "    target_table_audio = f'{PROJECT_ID}.{BQ_DATASET}.audio_features',\n",
    "    target_table_artist = f'{PROJECT_ID}.{BQ_DATASET}.artist_features',\n",
    "    batch_size = 50,\n",
    "    batches_to_store = 2, # int(bts),\n",
    "    sleep_param = 0.05,\n",
    ")\n",
    "\n",
    "PIPELINE_PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/934903580331/locations/us-central1/pipelineJobs/spotify-feature-enrichment-v10-spotify-feature-enrich-20230228012842\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/934903580331/locations/us-central1/pipelineJobs/spotify-feature-enrichment-v10-spotify-feature-enrich-20230228012842')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/spotify-feature-enrichment-v10-spotify-feature-enrich-20230228012842?project=934903580331\n"
     ]
    }
   ],
   "source": [
    "job = aiplatform.PipelineJob(display_name = f'spotify-feature-enrichment-{PIPELINE_TAG}'.replace('_', '-'),\n",
    "                             template_path = 'custom_container_pipeline_spec.json',\n",
    "                             pipeline_root = f'gs://{BUCKET_NAME}/{VERSION}',\n",
    "                             parameter_values = PIPELINE_PARAMETERS,\n",
    "                             project = PROJECT_ID,\n",
    "                             location = LOCATION,\n",
    "                              enable_caching=False)\n",
    "\n",
    "job.submit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline should look like this, it can take some time to run depending on your parameters\n",
    "\n",
    "![](img/feature-enrich-pipeline.png)\n",
    "\n",
    "#### [The next notebook](01-bq-data-prep.ipynb) will finish feature prep now that all the data is loaded in BQ"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Extract-spotify-features-pipeline-parallel-for.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-11.m103",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-11:m103"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
