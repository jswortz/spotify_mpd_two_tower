{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f523ce16-b93d-4107-8905-f3e07176174c",
   "metadata": {},
   "source": [
    "# Deploy Query Model to online endpoint with Monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211c8ed5-f34b-44f8-9028-b8b716165b08",
   "metadata": {},
   "source": [
    "### Steps in this notebook:\n",
    "\n",
    "* Deploy Query model to online prediction endpoint\n",
    "* Setup model monitoring for online prediction endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10943eed-f2af-4201-9e62-5f2ef91128cb",
   "metadata": {},
   "source": [
    "## Load env config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e8a1a95-6cd5-4d1f-94a0-d5762ebffaa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREFIX = ndr-v1\n"
     ]
    }
   ],
   "source": [
    "# naming convention for all cloud resources\n",
    "VERSION        = \"v1\"                  # TODO\n",
    "PREFIX         = f'ndr-{VERSION}'      # TODO\n",
    "\n",
    "print(f\"PREFIX = {PREFIX}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7964af95-f749-418f-a0b1-4012864aff9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PROJECT_ID               = \"hybrid-vertex\"\n",
      "PROJECT_NUM              = \"934903580331\"\n",
      "LOCATION                 = \"us-central1\"\n",
      "\n",
      "REGION                   = \"us-central1\"\n",
      "BQ_LOCATION              = \"US\"\n",
      "VPC_NETWORK_NAME         = \"ucaip-haystack-vpc-network\"\n",
      "\n",
      "VERTEX_SA                = \"934903580331-compute@developer.gserviceaccount.com\"\n",
      "\n",
      "PREFIX                   = \"ndr-v1\"\n",
      "VERSION                  = \"v1\"\n",
      "\n",
      "APP                      = \"sp\"\n",
      "MODEL_TYPE               = \"2tower\"\n",
      "FRAMEWORK                = \"tfrs\"\n",
      "DATA_VERSION             = \"v1\"\n",
      "TRACK_HISTORY            = \"5\"\n",
      "\n",
      "BUCKET_NAME              = \"ndr-v1-hybrid-vertex-bucket\"\n",
      "BUCKET_URI               = \"gs://ndr-v1-hybrid-vertex-bucket\"\n",
      "SOURCE_BUCKET            = \"spotify-million-playlist-dataset\"\n",
      "\n",
      "DATA_GCS_PREFIX          = \"data\"\n",
      "DATA_PATH                = \"gs://ndr-v1-hybrid-vertex-bucket/data\"\n",
      "VOCAB_SUBDIR             = \"vocabs\"\n",
      "VOCAB_FILENAME           = \"vocab_dict.pkl\"\n",
      "\n",
      "CANDIDATE_PREFIX         = \"candidates\"\n",
      "TRAIN_DIR_PREFIX         = \"train\"\n",
      "VALID_DIR_PREFIX         = \"valid\"\n",
      "\n",
      "VPC_NETWORK_FULL         = \"projects/934903580331/global/networks/ucaip-haystack-vpc-network\"\n",
      "\n",
      "BQ_DATASET               = \"spotify_e2e_test\"\n",
      "BQ_TABLE_TRAIN           = \"train_flatten_last_5\"\n",
      "BQ_TABLE_VALID           = \"train_flatten_valid_last_5\"\n",
      "BQ_TABLE_CANDIDATES      = \"candidates\"\n",
      "\n",
      "REPO_SRC                 = \"src\"\n",
      "PIPELINES_SUB_DIR        = \"feature_pipes\"\n",
      "\n",
      "REPOSITORY               = \"ndr-v1-spotify\"\n",
      "IMAGE_NAME               = \"train-v1\"\n",
      "REMOTE_IMAGE_NAME        = \"us-central1-docker.pkg.dev/hybrid-vertex/ndr-v1-spotify/train-v1\"\n",
      "DOCKERNAME               = \"tfrs\"\n",
      "\n",
      "SERVING_IMAGE_URI_CPU    = \"us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-11:latest\"\n",
      "SERVING_IMAGE_URI_GPU    = \"us-docker.pkg.dev/vertex-ai/prediction/tf2-gpu.2-11:latest\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# staging GCS\n",
    "GCP_PROJECTS             = !gcloud config get-value project\n",
    "PROJECT_ID               = GCP_PROJECTS[0]\n",
    "\n",
    "# GCS bucket and paths\n",
    "BUCKET_NAME              = f'{PREFIX}-{PROJECT_ID}-bucket'\n",
    "BUCKET_URI               = f'gs://{BUCKET_NAME}'\n",
    "\n",
    "config = !gsutil cat {BUCKET_URI}/config/notebook_env.py\n",
    "print(config.n)\n",
    "exec(config.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9233ca33-d18e-4bba-a016-67b2b691c83d",
   "metadata": {},
   "source": [
    "#### Edit these:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed0c55f4-97e4-4b22-9ed2-4b3e0149f5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "CREATE_NEW_ASSETS     = False # True | False\n",
    "ENABLE_XAI_MONITORING = False # True | False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05e0d3f3-3455-4cb1-88fe-cfd97382f5c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPERIMENT_NAME : tfrs-pipe-v1\n",
      "RUN_NAME        : run-20230919-173845\n",
      "RUN_DIR_PATH    : tfrs-pipe-v1/run-20230919-173845\n"
     ]
    }
   ],
   "source": [
    "# local-train-v1/run-20230919-150451/candidates/candidate_embeddings.json\n",
    "\n",
    "EXPERIMENT_NAME       = \"tfrs-pipe-v1\"         # local-train-v1\" \n",
    "RUN_NAME              = \"run-20230919-173845\"  # \"run-20230919-150451\"\n",
    "\n",
    "RUN_DIR_PATH = f'{EXPERIMENT_NAME}/{RUN_NAME}'\n",
    "\n",
    "print(f\"EXPERIMENT_NAME : {EXPERIMENT_NAME}\")\n",
    "print(f\"RUN_NAME        : {RUN_NAME}\")\n",
    "print(f\"RUN_DIR_PATH    : {RUN_DIR_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a6f278-8edf-4eb7-b355-d85f94bb044e",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "18bc2342-4c0b-43e0-ac8a-da94d007342e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "\n",
    "# google cloud SDKs\n",
    "from google.cloud import storage\n",
    "from google.cloud import aiplatform as vertex_ai\n",
    "from google.cloud.aiplatform import model_monitoring\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# this repo\n",
    "from src.two_tower_jt import test_instances as test_instances\n",
    "from util import feature_set_utils as feature_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "831bb4e2-7f81-4f3a-864b-333db47961ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_ai.init(project=PROJECT_ID, location=LOCATION)\n",
    "\n",
    "storage_client = storage.Client(project=PROJECT_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81c9a05-7794-40d7-bff2-0ed3cf5a07d7",
   "metadata": {},
   "source": [
    "# Deploy Query Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724761b6-697b-4716-bd60-6e95f0702f3e",
   "metadata": {},
   "source": [
    "## Register Query model to Vertex Model Registry\n",
    "\n",
    "**TODO:** parametrize new vs existing assets\n",
    "\n",
    "```\n",
    "model = vertex_ai.Model.list(filter=f\"display_name=bqml_fraud_classifier\")[-1]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42779994-fa65-4469-9d3e-e2edf691fe51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUERY_MODEL_DIR: gs://ndr-v1-hybrid-vertex-bucket/tfrs-pipe-v1/run-20230919-173845/model-dir/query_model\n"
     ]
    }
   ],
   "source": [
    "QUERY_MODEL_DIR = f\"{BUCKET_URI}/{RUN_DIR_PATH}/model-dir/query_model\"\n",
    "\n",
    "print(f\"QUERY_MODEL_DIR: {QUERY_MODEL_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17b220ea-8c86-4349-9d1f-efa83a8cfd15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://ndr-v1-hybrid-vertex-bucket/tfrs-pipe-v1/run-20230919-173845/model-dir/query_model/\n",
      "gs://ndr-v1-hybrid-vertex-bucket/tfrs-pipe-v1/run-20230919-173845/model-dir/query_model/fingerprint.pb\n",
      "gs://ndr-v1-hybrid-vertex-bucket/tfrs-pipe-v1/run-20230919-173845/model-dir/query_model/saved_model.pb\n",
      "gs://ndr-v1-hybrid-vertex-bucket/tfrs-pipe-v1/run-20230919-173845/model-dir/query_model/assets/\n",
      "gs://ndr-v1-hybrid-vertex-bucket/tfrs-pipe-v1/run-20230919-173845/model-dir/query_model/variables/\n"
     ]
    }
   ],
   "source": [
    "! gsutil ls $QUERY_MODEL_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12da6c87-988d-47a6-8b87-0644edd5a5c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "display_name         : query_model_tfrs_128dim_v1\n",
      "\n",
      "uploaded_query_model : <google.cloud.aiplatform.models.Model object at 0x7f79c97eee10> \n",
      "resource name: projects/934903580331/locations/us-central1/models/2404541769992634368\n"
     ]
    }
   ],
   "source": [
    "if CREATE_NEW_ASSETS == True:\n",
    "    \n",
    "    uploaded_query_model = vertex_ai.Model.upload(\n",
    "        display_name=f'query_model_{DISPLAY_NAME}',\n",
    "        artifact_uri=QUERY_MODEL_DIR,\n",
    "        serving_container_image_uri=SERVING_IMAGE_URI_CPU,\n",
    "        description=\"Top of the query tower, meant to return an embedding for each playlist instance\",\n",
    "        sync=True,\n",
    "    )\n",
    "else:\n",
    "    # use existing\n",
    "    uploaded_query_model = vertex_ai.Model('projects/934903580331/locations/us-central1/models/2404541769992634368@1')\n",
    "\n",
    "print(f\"display_name         : {uploaded_query_model.display_name}\\n\")\n",
    "print(f\"uploaded_query_model : {uploaded_query_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3be84c9-d6e4-4758-9848-52d80a9e9efd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# uploaded_query_model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35268707-4be6-43ed-a8de-e95e2fc4e046",
   "metadata": {},
   "source": [
    "## Deploy registered model to online endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0aa3fc4-2889-4de4-bb95-d7fe2b9905a2",
   "metadata": {},
   "source": [
    "**Create model endpoint**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0af496a-7ebd-4cc9-ae1a-b29e205c15ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "display_name : endpoint_tfrs_128dim_v1\n",
      "\n",
      "endpoint     : <google.cloud.aiplatform.models.Endpoint object at 0x7f79bb4abe90> \n",
      "resource name: projects/934903580331/locations/us-central1/endpoints/7270536031831588864\n"
     ]
    }
   ],
   "source": [
    "if CREATE_NEW_ASSETS == True:\n",
    "    \n",
    "    endpoint = vertex_ai.Endpoint.create(\n",
    "        display_name=f'endpoint_{DISPLAY_NAME}',\n",
    "        project=PROJECT_ID,\n",
    "        location=LOCATION,\n",
    "        sync=True,\n",
    "    )\n",
    "\n",
    "else:\n",
    "    endpoint = vertex_ai.Endpoint('projects/934903580331/locations/us-central1/endpoints/7270536031831588864')\n",
    "\n",
    "print(f\"display_name : {endpoint.display_name}\\n\")\n",
    "print(f\"endpoint     : {endpoint}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e93ad1-b812-491b-8ed2-b2c47fd3a5b9",
   "metadata": {},
   "source": [
    "**Deploy to endpoint**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "000fe6b3-095c-4473-971f-3e2d71e13f50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "display_name         : query-tower-endpoint-v1\n",
      "\n",
      "deployed_query_model : <google.cloud.aiplatform.models.Endpoint object at 0x7f79c97ee310> \n",
      "resource name: projects/934903580331/locations/us-central1/endpoints/7622942702673330176\n"
     ]
    }
   ],
   "source": [
    "if CREATE_NEW_ASSETS == True:\n",
    "    \n",
    "    deployed_query_model = uploaded_query_model.deploy(\n",
    "        endpoint=endpoint,\n",
    "        deployed_model_display_name=f'deployed_qmodel_{DISPLAY_NAME}',\n",
    "        machine_type=\"n1-standard-4\",\n",
    "        min_replica_count=1,\n",
    "        max_replica_count=2,\n",
    "        accelerator_type=None,\n",
    "        accelerator_count=0,\n",
    "        sync=True,\n",
    "    )\n",
    "\n",
    "else:\n",
    "    deployed_query_model = vertex_ai.Endpoint('projects/934903580331/locations/us-central1/endpoints/7622942702673330176')\n",
    "\n",
    "print(f\"display_name         : {deployed_query_model.display_name}\\n\")\n",
    "print(f\"deployed_query_model : {deployed_query_model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ea3d7a-7bae-46f0-8d3a-d45037db6410",
   "metadata": {},
   "source": [
    "#### list all model endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "121573c3-a437-4856-8802-11769ecec489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_of_model_endpoints = deployed_query_model.list()\n",
    "# list_of_model_endpoints[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1658136-11f3-4140-846d-2ed779d43f93",
   "metadata": {},
   "source": [
    "#### list all models on a single endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "50c79c85-f933-406c-825b-cec39777af39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_of_model_endpoints = deployed_query_model.list_models()\n",
    "# list_of_model_endpoints #[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c97c86-2552-4e7f-abbe-59751353d883",
   "metadata": {},
   "source": [
    "# Set Model Monitoring for Query Model Endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c624b7-eafa-4303-8709-12dd49082ba0",
   "metadata": {},
   "source": [
    "### Define and create a Model Monitoring job\n",
    "To set up either skew detection or drift detection, create a model deployment monitoring job.\n",
    "\n",
    "The job requires the following specifications:\n",
    "\n",
    "* `alert_config`: Configures how alerts are sent to the user. Right now only email alert is supported.\n",
    "* `schedule_config`: Configures model monitoring job scheduling interval in hours. This defines how often the monitoring jobs are triggered.\n",
    "* `logging_sampling_strategy`: Sample Strategy for logging.\n",
    "* `drift_config`: Configures drift thresholds per each feature to monitor.\n",
    "* `skew_config`: Configures skew thresholds per each feature to monitor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7f22f3-d83f-4b09-9a1a-d12eea0818d2",
   "metadata": {},
   "source": [
    "#### Define the alerting configuration\n",
    "\n",
    "The alerting configuration contains the mails to send alerts to. Also you can use the configuration to stream anomalies to Cloud Logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f1681284-b470-474c-b6a5-3943f73935f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<google.cloud.aiplatform.model_monitoring.alert.EmailAlertConfig at 0x7f4ae67c6190>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spotipy_secret_creds as creds\n",
    "\n",
    "USER_EMAILS = [creds.USER_EMAIL] #'recipient1@domain.com', 'recipient2@domain.com'\n",
    "alert_config = model_monitoring.EmailAlertConfig(USER_EMAILS, enable_logging=True)\n",
    "alert_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab3c600-b294-4962-8916-4c9566711ade",
   "metadata": {},
   "source": [
    "#### Define the schedule configuration\n",
    "\n",
    "The schedule configuration sets the hourly model monitoring job scheduling interval.\n",
    "\n",
    "> Sets the model monitoring job scheduling interval in hours. This defines how often the monitoring jobs are triggered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "37be31fb-379a-40ca-8932-4746c3c10cbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<google.cloud.aiplatform.model_monitoring.schedule.ScheduleConfig at 0x7f4b71ad9fd0>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MONITOR_INTERVAL = 1\n",
    "schedule_config = model_monitoring.ScheduleConfig(monitor_interval=MONITOR_INTERVAL)\n",
    "schedule_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e26d368-5d11-4b78-9b49-809f05d4980c",
   "metadata": {},
   "source": [
    "#### Define the logging sample strategy\n",
    "\n",
    "With the logging sample strategy, you configure how the model monitoring service randomly sample predictions to calculate monitoring metrics. The selected samples are logged to a BigQuery table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d70136de-ee26-4309-a0fe-bec7985a080c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<google.cloud.aiplatform.model_monitoring.sampling.RandomSampleConfig at 0x7f4ae65a3fd0>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SAMPLE_RATE = 0.8\n",
    "\n",
    "logging_sampling_strategy = model_monitoring.RandomSampleConfig(sample_rate=SAMPLE_RATE)\n",
    "logging_sampling_strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f4b98f-edef-4827-8fed-fbe5d875e7b7",
   "metadata": {},
   "source": [
    "#### Define the drift detection configuration\n",
    "\n",
    "With the drift detection configuration, you define the input features and the associated thresholds for monitoring feature distribution drift and (TODO) feature attribution drift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fcc8c0e5-8f3a-4a01-932d-0031778b3ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_dict = feature_utils.get_all_features(TRACK_HISTORY, ranker=False)\n",
    "# feature_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0408a464-e255-461a-9be6-d0321ee9bf5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = list(feature_dict.keys())\n",
    "# feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5783284c-c561-4098-bb38-13fe9a4f84ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drift_thresholds      : {'track_uri_can': 0.05, 'track_name_can': 0.05, 'artist_uri_can': 0.05, 'artist_name_can': 0.05, 'album_uri_can': 0.05, 'album_name_can': 0.05, 'duration_ms_can': 0.05, 'track_pop_can': 0.05, 'artist_pop_can': 0.05, 'artist_genres_can': 0.05, 'artist_followers_can': 0.05, 'track_danceability_can': 0.05, 'track_energy_can': 0.05, 'track_key_can': 0.05, 'track_loudness_can': 0.05, 'track_mode_can': 0.05, 'track_speechiness_can': 0.05, 'track_acousticness_can': 0.05, 'track_instrumentalness_can': 0.05, 'track_liveness_can': 0.05, 'track_valence_can': 0.05, 'track_tempo_can': 0.05, 'track_time_signature_can': 0.05, 'pl_name_src': 0.05, 'pl_collaborative_src': 0.05, 'pl_duration_ms_new': 0.05, 'num_pl_songs_new': 0.05, 'num_pl_artists_new': 0.05, 'num_pl_albums_new': 0.05, 'track_uri_pl': 0.05, 'track_name_pl': 0.05, 'artist_uri_pl': 0.05, 'artist_name_pl': 0.05, 'album_uri_pl': 0.05, 'album_name_pl': 0.05, 'artist_genres_pl': 0.05, 'duration_ms_songs_pl': 0.05, 'track_pop_pl': 0.05, 'artist_pop_pl': 0.05, 'artists_followers_pl': 0.05, 'track_danceability_pl': 0.05, 'track_energy_pl': 0.05, 'track_key_pl': 0.05, 'track_loudness_pl': 0.05, 'track_mode_pl': 0.05, 'track_speechiness_pl': 0.05, 'track_acousticness_pl': 0.05, 'track_instrumentalness_pl': 0.05, 'track_liveness_pl': 0.05, 'track_valence_pl': 0.05, 'track_tempo_pl': 0.05, 'track_time_signature_pl': 0.05}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DRIFT_THRESHOLD_VALUE = 0.05\n",
    "ATTRIBUTION_DRIFT_THRESHOLD_VALUE = 0.05\n",
    "\n",
    "# =========================== #\n",
    "##   Feature value drift     ##\n",
    "# =========================== #\n",
    "drift_thresholds = dict()\n",
    "\n",
    "for feature in feature_names:\n",
    "    if feature in drift_thresholds:\n",
    "        print(\"feature name already in dict\")\n",
    "    else:\n",
    "        drift_thresholds[feature] = DRIFT_THRESHOLD_VALUE\n",
    "        \n",
    "print(f\"drift_thresholds      : {drift_thresholds}\\n\")\n",
    "\n",
    "# =========================== #\n",
    "## Feature attribution drift ##\n",
    "# =========================== #\n",
    "# attr_drift_thresholds = dict()\n",
    "\n",
    "# for feature in feature_names:\n",
    "#     if feature in attr_drift_thresholds:\n",
    "#         print(\"feature name already in dict\")\n",
    "#     else:\n",
    "#         attr_drift_thresholds[feature] = ATTRIBUTION_DRIFT_THRESHOLD_VALUE\n",
    "\n",
    "# print(f\"attr_drift_thresholds : {attr_drift_thresholds}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8f339bdf-6129-454b-8580-ae858e35288f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<google.cloud.aiplatform.model_monitoring.objective.DriftDetectionConfig at 0x7f4ae4164390>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drift_config = model_monitoring.DriftDetectionConfig(\n",
    "    drift_thresholds=drift_thresholds,\n",
    "    # attribute_drift_thresholds=attr_drift_thresholds,\n",
    ")\n",
    "\n",
    "drift_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b6b468-35b4-49da-9c3c-1284cc222ed2",
   "metadata": {},
   "source": [
    "#### Define the skew detection configuration\n",
    "\n",
    "With the skew detection configuration, you define the input features and the associated thresholds for monitoring feature distribution skew and feature attribution skew."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "14fcf5a7-0bf0-4e01-8eac-7b7cd7879a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skew_thresholds      : {'track_uri_can': 0.05, 'track_name_can': 0.05, 'artist_uri_can': 0.05, 'artist_name_can': 0.05, 'album_uri_can': 0.05, 'album_name_can': 0.05, 'duration_ms_can': 0.05, 'track_pop_can': 0.05, 'artist_pop_can': 0.05, 'artist_genres_can': 0.05, 'artist_followers_can': 0.05, 'track_danceability_can': 0.05, 'track_energy_can': 0.05, 'track_key_can': 0.05, 'track_loudness_can': 0.05, 'track_mode_can': 0.05, 'track_speechiness_can': 0.05, 'track_acousticness_can': 0.05, 'track_instrumentalness_can': 0.05, 'track_liveness_can': 0.05, 'track_valence_can': 0.05, 'track_tempo_can': 0.05, 'track_time_signature_can': 0.05, 'pl_name_src': 0.05, 'pl_collaborative_src': 0.05, 'pl_duration_ms_new': 0.05, 'num_pl_songs_new': 0.05, 'num_pl_artists_new': 0.05, 'num_pl_albums_new': 0.05, 'track_uri_pl': 0.05, 'track_name_pl': 0.05, 'artist_uri_pl': 0.05, 'artist_name_pl': 0.05, 'album_uri_pl': 0.05, 'album_name_pl': 0.05, 'artist_genres_pl': 0.05, 'duration_ms_songs_pl': 0.05, 'track_pop_pl': 0.05, 'artist_pop_pl': 0.05, 'artists_followers_pl': 0.05, 'track_danceability_pl': 0.05, 'track_energy_pl': 0.05, 'track_key_pl': 0.05, 'track_loudness_pl': 0.05, 'track_mode_pl': 0.05, 'track_speechiness_pl': 0.05, 'track_acousticness_pl': 0.05, 'track_instrumentalness_pl': 0.05, 'track_liveness_pl': 0.05, 'track_valence_pl': 0.05, 'track_tempo_pl': 0.05, 'track_time_signature_pl': 0.05}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "SKEW_THRESHOLD_VALUE = 0.05\n",
    "ATTRIBUTION_SKEW_THRESHOLD_VALUE = 0.05\n",
    "\n",
    "# =========================== #\n",
    "##   Feature value skew      ##\n",
    "# =========================== #\n",
    "skew_thresholds = dict()\n",
    "\n",
    "for feature in feature_names:\n",
    "    if feature in skew_thresholds:\n",
    "        print(\"feature name already in dict\")\n",
    "    else:\n",
    "        skew_thresholds[feature] = SKEW_THRESHOLD_VALUE        \n",
    "print(f\"skew_thresholds      : {skew_thresholds}\\n\")\n",
    "\n",
    "# =========================== #\n",
    "## Feature attribution skew  ##\n",
    "# =========================== #\n",
    "# attr_skew_thresholds = dict()\n",
    "\n",
    "# for feature in feature_names:\n",
    "#     if feature in attr_skew_thresholds:\n",
    "#         print(\"feature name already in dict\")\n",
    "#     else:\n",
    "#         attr_skew_thresholds[feature] = ATTRIBUTION_SKEW_THRESHOLD_VALUE\n",
    "# print(f\"attr_skew_thresholds : {attr_skew_thresholds}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4776b603-11ea-4aad-bcd4-5660a2cf18e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<google.cloud.aiplatform.model_monitoring.objective.SkewDetectionConfig at 0x7f4ae416b310>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TRAIN_DATA_SOURCE_URI = f\"gs://{BUCKET_NAME}/data/{DATA_VERSION}/{TRAIN_DIR_PREFIX}/\"\n",
    "# TRAIN_DATA_FORMAT = \"tf-record\"\n",
    "\n",
    "TRAIN_DATA_SOURCE_URI = f\"bq://{PROJECT_ID}.{BQ_DATASET}.{BQ_TABLE_TRAIN}\"\n",
    "TRAIN_DATA_FORMAT = None\n",
    "\n",
    "if TRAIN_DATA_FORMAT:\n",
    "    skew_config = model_monitoring.SkewDetectionConfig(\n",
    "        data_source=TRAIN_DATA_SOURCE_URI,\n",
    "        data_format = TRAIN_DATA_FORMAT,\n",
    "        skew_thresholds=skew_thresholds,\n",
    "        # attribute_skew_thresholds=attribute_skew_thresholds,\n",
    "        # target_field=TARGET, # no target; embedding model\n",
    "    )\n",
    "else:\n",
    "    skew_config = model_monitoring.SkewDetectionConfig(\n",
    "        data_source=TRAIN_DATA_SOURCE_URI,\n",
    "        # data_format = TRAIN_DATA_FORMAT, # only used if source in GCS\n",
    "        skew_thresholds=skew_thresholds,\n",
    "        # attribute_skew_thresholds=attribute_skew_thresholds,\n",
    "        # target_field=TARGET, # no target; embedding model\n",
    "    )\n",
    "    \n",
    "skew_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952a86a9-df3e-4a2d-8042-9121917ee794",
   "metadata": {},
   "source": [
    "#### Define Explanation Config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ade45a-549d-4fc7-8b75-b8de5653d6e6",
   "metadata": {},
   "source": [
    "* If you are enabling skew detection, upload your training data or output of a [batch explanation job](https://cloud.google.com/vertex-ai/docs/explainable-ai/getting-explanations#batch) for your training dataset to `Cloud Storage` or `BigQuery`. Obtain the URI link to the data. For drift detection, training data or explanation baseline isn't required.\n",
    "\n",
    "* An imported custom-trained model must be [configured for Vertex Explainable AI](https://cloud.google.com/vertex-ai/docs/model-monitoring/monitor-explainable-ai#enable-feature-attribution-skew-or-drift-detection) when you create, import, or deploy the model.\n",
    "\n",
    "* [Configure your model](https://cloud.google.com/vertex-ai/docs/explainable-ai/configuring-explanations) to use Vertex Explainable AI when you create, import, or deploy the model. The `ExplanationSpec.ExplanationParameters` field must be populated for your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "12ec023f-c9ad-4011-a79a-bf7cae60d8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ENABLE_XAI_MONITORING:\n",
    "    explanation_config = model_monitoring.ExplanationConfig()\n",
    "else:\n",
    "    explanation_config = None\n",
    "    \n",
    "explanation_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afcee6e-cdbd-4f79-a196-560ced92c66c",
   "metadata": {},
   "source": [
    "### Create job config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "71d2766a-1910-44b1-bd92-de36355d1add",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<google.cloud.aiplatform.model_monitoring.objective.ObjectiveConfig at 0x7f4ae415bed0>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "objective_config = model_monitoring.ObjectiveConfig(\n",
    "    skew_detection_config=skew_config,\n",
    "    drift_detection_config=drift_config,\n",
    "    explanation_config=explanation_config,\n",
    ")\n",
    "\n",
    "objective_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2f453a-48ce-4542-b944-e3ab8551c106",
   "metadata": {},
   "source": [
    "## Create Model Monitoring Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4b9a1cb5-02f0-4b00-b3ac-e7c338ea6e1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JOB_DISPLAY_NAME: 2tower_ndr-v1_monitoring\n",
      "Creating ModelDeploymentMonitoringJob\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.jobs:Creating ModelDeploymentMonitoringJob\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelDeploymentMonitoringJob created. Resource name: projects/934903580331/locations/us-central1/modelDeploymentMonitoringJobs/5182827677073014784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.jobs:ModelDeploymentMonitoringJob created. Resource name: projects/934903580331/locations/us-central1/modelDeploymentMonitoringJobs/5182827677073014784\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To use this ModelDeploymentMonitoringJob in another session:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.jobs:To use this ModelDeploymentMonitoringJob in another session:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mdm_job = aiplatform.ModelDeploymentMonitoringJob('projects/934903580331/locations/us-central1/modelDeploymentMonitoringJobs/5182827677073014784')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.jobs:mdm_job = aiplatform.ModelDeploymentMonitoringJob('projects/934903580331/locations/us-central1/modelDeploymentMonitoringJobs/5182827677073014784')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View Model Deployment Monitoring Job:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/model-deployment-monitoring/5182827677073014784?project=934903580331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.jobs:View Model Deployment Monitoring Job:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/model-deployment-monitoring/5182827677073014784?project=934903580331\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<google.cloud.aiplatform.jobs.ModelDeploymentMonitoringJob object at 0x7f4ae415b5d0> \n",
       "resource name: projects/934903580331/locations/us-central1/modelDeploymentMonitoringJobs/5182827677073014784"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "JOB_DISPLAY_NAME = f\"{MODEL_TYPE}_{PREFIX}_monitoring\"\n",
    "print(f\"JOB_DISPLAY_NAME: {JOB_DISPLAY_NAME}\")\n",
    "\n",
    "monitoring_job = vertex_ai.ModelDeploymentMonitoringJob.create(\n",
    "    display_name=JOB_DISPLAY_NAME,\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    "    endpoint=deployed_query_model,\n",
    "    logging_sampling_strategy=logging_sampling_strategy,\n",
    "    schedule_config=schedule_config,\n",
    "    alert_config=alert_config,\n",
    "    objective_configs=objective_config,\n",
    ")\n",
    "\n",
    "monitoring_job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3197bbfc-377c-44f0-8c23-d009f3c4014e",
   "metadata": {},
   "source": [
    "Check the monitoring job state\n",
    "\n",
    "You can check the status of the model monitoring job using the state attribute of the job instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f1e687cb-718d-419e-a874-c9857703b1eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2tower_ndr-v1_monitoring'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "JOB_DISPLAY_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ac9dbac6-5239-4e0f-9588-eb16bc37b722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JobState.JOB_STATE_PENDING\n"
     ]
    }
   ],
   "source": [
    "jobs = monitoring_job.list(filter=f\"display_name={JOB_DISPLAY_NAME}\")\n",
    "job = jobs[0]\n",
    "print(job.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e707d9e4-3a01-41ab-9ebf-6135fa9c1e5b",
   "metadata": {},
   "source": [
    "**Receiving email alert**\n",
    "\n",
    "> After a minute or two, you should receive email at the address you configured above for `USER_EMAIL`. This email confirms successful deployment of your monitoring job."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632ba468-1156-4860-a43d-a60215e8302d",
   "metadata": {},
   "source": [
    "**Monitoring results in the Cloud Console**\n",
    "\n",
    "> After one hour, you can examine your model monitoring data from the Cloud Console.\n",
    "\n",
    "**See `Notes` at end of notebook for details on interpreting Model Monitoring results**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6e143b-df43-4bbb-b2be-967463b3d612",
   "metadata": {},
   "source": [
    "# Test endpoint deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "19318841-f6e1-42d9-9210-148bc6b98932",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRACK_HISTORY == '5':\n",
    "    TEST_INSTANCE = test_instances.TEST_INSTANCE_5\n",
    "elif TRACK_HISTORY == '15':\n",
    "    TEST_INSTANCE = test_instances.TEST_INSTANCE_15\n",
    "else:\n",
    "    TEST_INSTANCE = None\n",
    "    print(\"Track History length not supported\")\n",
    "    \n",
    "# TEST_INSTANCE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2b8faa-960a-49a1-afc8-a88a01597890",
   "metadata": {},
   "source": [
    "### Make prediction request"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3113da48-48a6-4492-a529-dee96ba1d8b3",
   "metadata": {},
   "source": [
    "test single prediction request and response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d279b029-dd5e-428f-876a-5fa617a9fcae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.19653273, -0.334559262, 0.0810001567, 0.493252, -0.160756573, 1.31879628, -1.39625704, 0.58631593, 0.817179263, 0.112577811, -2.640836, 1.14207935, 1.19983459, 0.237981498, 0.263604015, -0.570031762, -0.00874393526, -0.97826761, -0.774345636, -0.487044483, -0.461612493, 0.646577716, 1.2363373, -0.208847284, 0.0831734315, -1.67527008, 1.90116453, 0.482158244, 0.815588176, -0.17341572, -0.961700797, -0.64995718, -0.714651644, 0.491276503, -1.60751081, -0.169386595, 0.859483838, 1.11577523, 0.0723268613, 1.66885364, -1.38753307, 0.0597520955, 1.12370121, -0.538888931, -0.14825964, -0.371995538, 0.996796489, 0.496251285, -2.74364448, 1.41605437, -0.692977905, -0.379781693, -1.47451448, -0.77251333, 0.502322853, 1.52816653, 0.738132596, 2.72255349, 0.559346855, -2.50046325, -0.88221395, -0.242334023, 1.6774596, -0.668324947, -0.696153104, -0.243123859, 1.15069735, 2.32172489, 0.475567073, -0.996623933, -1.63570368, 0.556305885, 0.0126396995, 0.749343514, 1.10898, 0.614645839, -0.21073547, 0.54395479, 0.202814668, 0.0523695126, -0.88011992, 0.00579532702, -0.229624048, -0.338394374, 0.43990916, 0.779108524, -0.682462573, 0.345542669, 0.958746374, -0.698016644, -1.35358107, 1.22808468, -1.23964667, -1.09018505, -1.11755335, 0.340544254, 1.93542278, 2.13677955, -0.536869109, 0.773239, -0.806171179, 1.07895923, -0.847685277, 0.397387683, -0.1660593, 1.22591412, 0.00258986093, -0.0916697234, -2.24293494, 1.27113521, -0.491369426, -0.782439351, 0.470075876, -1.46270025, -0.512221158, -0.374952614, 0.239989579, 0.952725172, -1.56143165, 1.58141768, -1.20620728, -0.373179853, -0.207473084, -1.76514602, 1.1276499, -0.0149420993, 0.382134199, 0.0106226364]\n"
     ]
    }
   ],
   "source": [
    "response = deployed_query_model.predict(instances=[TEST_INSTANCE])\n",
    "\n",
    "prediction = response[0]\n",
    "\n",
    "# print the prediction for the first instance\n",
    "print(prediction[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06916856-115c-4f0f-8e0f-c52edee8ddc9",
   "metadata": {},
   "source": [
    "### Write (many) test instances to file\n",
    "\n",
    "> test endpoint monitoring with >= 1000 prediction requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9b63e4dc-5796-4692-8654-366a1d3e962f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRED_REQUEST_N : 50\n",
      "INTERVAL       : 25\n"
     ]
    }
   ],
   "source": [
    "PRED_REQUEST_N = 50\n",
    "INTERVAL       = PRED_REQUEST_N // 2\n",
    "SKIP_N         = INTERVAL\n",
    "\n",
    "print(f\"PRED_REQUEST_N : {PRED_REQUEST_N}\")\n",
    "print(f\"INTERVAL       : {INTERVAL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9e69b724-110e-47f4-a161-3ffeac3d6820",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "valid_files = []\n",
    "for blob in storage_client.list_blobs(f\"{BUCKET_NAME}\", prefix=f'data/{DATA_VERSION}/{VALID_DIR_PREFIX}/'):\n",
    "    if '.tfrecords' in blob.name:\n",
    "        valid_files.append(blob.public_url.replace(\"https://storage.googleapis.com/\", \"gs://\"))\n",
    "    \n",
    "valid = tf.data.TFRecordDataset(valid_files)\n",
    "\n",
    "valid_parsed = valid.map(feature_utils.parse_towers_tfrecord)\n",
    "# valid_parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e3337331-5dda-459b-a788-78ea5632dc94",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2600"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy\n",
    "\n",
    "subset_val = valid_parsed.skip(SKIP_N).take(PRED_REQUEST_N)\n",
    "\n",
    "list_of_dicts = []\n",
    "\n",
    "for tensor_dict in subset_val:\n",
    "    list_dict = {}\n",
    "    td_keys = tensor_dict.keys()\n",
    "    for k in td_keys:\n",
    "        \n",
    "        value = tensor_dict[k].numpy()\n",
    "        \n",
    "        if type(value) == bytes:\n",
    "\n",
    "            list_dict.update({k: value.decode()})\n",
    "        \n",
    "        elif type(value) == numpy.ndarray:\n",
    "            \n",
    "            if type(value[0]) != bytes:\n",
    "                list_dict.update({k: value.tolist()})\n",
    "            else:\n",
    "\n",
    "                tmp_list = []\n",
    "\n",
    "                for ele in value:\n",
    "                    tmp_list.append(ele.decode())\n",
    "\n",
    "                list_dict.update({k: tmp_list})\n",
    "                \n",
    "        elif type(value) == numpy.float32:\n",
    "            list_dict.update({k: value.item()})\n",
    "                \n",
    "        else:\n",
    "            list_dict.update({k: value})\n",
    "            \n",
    "        list_of_dicts.append(list_dict)\n",
    "    \n",
    "# list_dict\n",
    "len(list_of_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "af75be56-86ef-4122-9dfd-6ed60e4952b6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 prediciton requests..\n",
      "50 prediciton requests..\n",
      "75 prediciton requests..\n",
      "100 prediciton requests..\n",
      "125 prediciton requests..\n",
      "150 prediciton requests..\n",
      "175 prediciton requests..\n",
      "200 prediciton requests..\n",
      "225 prediciton requests..\n",
      "250 prediciton requests..\n",
      "275 prediciton requests..\n",
      "300 prediciton requests..\n",
      "325 prediciton requests..\n",
      "350 prediciton requests..\n",
      "375 prediciton requests..\n",
      "400 prediciton requests..\n",
      "425 prediciton requests..\n",
      "450 prediciton requests..\n",
      "475 prediciton requests..\n",
      "500 prediciton requests..\n",
      "525 prediciton requests..\n",
      "550 prediciton requests..\n",
      "575 prediciton requests..\n",
      "600 prediciton requests..\n",
      "625 prediciton requests..\n",
      "650 prediciton requests..\n",
      "675 prediciton requests..\n",
      "700 prediciton requests..\n",
      "725 prediciton requests..\n",
      "750 prediciton requests..\n",
      "775 prediciton requests..\n",
      "800 prediciton requests..\n",
      "825 prediciton requests..\n",
      "850 prediciton requests..\n",
      "875 prediciton requests..\n",
      "900 prediciton requests..\n",
      "925 prediciton requests..\n",
      "950 prediciton requests..\n",
      "975 prediciton requests..\n",
      "1000 prediciton requests..\n",
      "1025 prediciton requests..\n",
      "1050 prediciton requests..\n",
      "1075 prediciton requests..\n",
      "1100 prediciton requests..\n",
      "1125 prediciton requests..\n",
      "1150 prediciton requests..\n",
      "1175 prediciton requests..\n",
      "1200 prediciton requests..\n",
      "1225 prediciton requests..\n",
      "1250 prediciton requests..\n",
      "1275 prediciton requests..\n",
      "1300 prediciton requests..\n",
      "1325 prediciton requests..\n",
      "1350 prediciton requests..\n",
      "1375 prediciton requests..\n",
      "1400 prediciton requests..\n",
      "1425 prediciton requests..\n",
      "1450 prediciton requests..\n",
      "1475 prediciton requests..\n",
      "1500 prediciton requests..\n",
      "1525 prediciton requests..\n",
      "1550 prediciton requests..\n",
      "1575 prediciton requests..\n",
      "1600 prediciton requests..\n",
      "1625 prediciton requests..\n",
      "1650 prediciton requests..\n",
      "1675 prediciton requests..\n",
      "1700 prediciton requests..\n",
      "1725 prediciton requests..\n",
      "1750 prediciton requests..\n",
      "1775 prediciton requests..\n",
      "1800 prediciton requests..\n",
      "1825 prediciton requests..\n",
      "1850 prediciton requests..\n",
      "1875 prediciton requests..\n",
      "1900 prediciton requests..\n",
      "1925 prediciton requests..\n",
      "1950 prediciton requests..\n",
      "1975 prediciton requests..\n",
      "2000 prediciton requests..\n",
      "2025 prediciton requests..\n",
      "2050 prediciton requests..\n",
      "2075 prediciton requests..\n",
      "2100 prediciton requests..\n",
      "2125 prediciton requests..\n",
      "2150 prediciton requests..\n",
      "2175 prediciton requests..\n",
      "2200 prediciton requests..\n",
      "2225 prediciton requests..\n",
      "2250 prediciton requests..\n",
      "2275 prediciton requests..\n",
      "2300 prediciton requests..\n",
      "2325 prediciton requests..\n",
      "2350 prediciton requests..\n",
      "2375 prediciton requests..\n",
      "2400 prediciton requests..\n",
      "2425 prediciton requests..\n",
      "2450 prediciton requests..\n",
      "2475 prediciton requests..\n",
      "2500 prediciton requests..\n",
      "2525 prediciton requests..\n",
      "2550 prediciton requests..\n",
      "2575 prediciton requests..\n",
      "[-0.316191614, -0.736632228, -1.00103509, 0.936801553, 0.143262073, -0.413035601, 0.881459951, -0.968677163, -0.657993495, -1.60177445, 1.11540341, 0.534450293, 1.12290382, -0.79562974, -0.218611151, 0.216911197, 0.852999747, -1.70970297, -0.431490481, -0.796719253, -0.131098747, 0.678861737, 1.57433116, -1.71849513, -0.0131717063, -0.695032, 0.519422233, -1.51515913, -0.545648158, 0.00587499887, -1.16022086, 1.08450854, -0.109293938, 0.456809044, 0.0663500577, 0.415968299, -1.28065646, 0.211853147, -1.79086161, 0.36914134, -0.098710157, -2.06009436, -0.192913979, 0.135118887, -1.31829643, -0.238816991, 1.5937041, -0.967884064, 1.97131979, 0.786653876, -0.222255349, -0.301919788, 1.45176435, 0.0821444541, -1.35433638, 1.41081405, -1.17567897, 2.04352641, -0.430950701, 0.915213168, 0.124549598, -0.660427809, 0.242982119, 0.287387937, 0.876281738, -0.863683343, 0.536886334, 0.749792755, 0.23666884, -0.605295241, -0.859842956, -1.20190716, -1.00890052, -0.0171246603, -0.0018113181, -0.294076264, -0.197285056, 0.17216754, 1.38053346, 0.0969397649, 0.207888216, -0.716749966, 0.490655273, -0.159256935, 1.46190953, 0.30212304, 1.35271168, 0.50382483, 0.763873816, 0.786258, -0.980145335, 1.18392801, -0.724787176, -0.128117606, 1.12640297, -0.395618558, -0.49375689, -1.45199978, -0.349447608, 0.601396322, 0.48471114, 0.337583274, -0.9782933, -0.0502888262, -0.644129455, 2.3293643, 0.708969831, -0.434916556, -0.998240054, -1.35022128, 0.642638624, 0.315309107, 1.40024364, 1.52585, 1.30187464, 0.575985253, -1.29062474, 0.379185259, -0.185181409, -2.22602654, -0.370215207, -0.292384863, 0.133326396, -1.48911917, 0.847954273, 1.29093385, 1.93435824, -0.0492398106]\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "\n",
    "for test in list_of_dicts:\n",
    "    response = deployed_query_model.predict(instances=[test])\n",
    "    \n",
    "    if count > 0 and count % INTERVAL == 0:\n",
    "        print(f\"{count} prediciton requests..\")\n",
    "        \n",
    "    count += 1\n",
    "    \n",
    "prediction = response[0]\n",
    "# print the prediction for the first instance\n",
    "print(prediction[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4fe2e2-ff4f-4135-8e02-894c406975ab",
   "metadata": {},
   "source": [
    "### Save test instances to pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7a59f282-7b36-4741-b3a4-7aa08441a209",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "\n",
    "LOCAL_INSTANCE_FILE = 'test_instance_list.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "230b7816-94ba-432f-a54d-d246d5315bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "filehandler = open(LOCAL_INSTANCE_FILE, 'wb')\n",
    "pkl.dump(list_of_dicts, filehandler)\n",
    "filehandler.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1d65169c-a9ae-4537-b6ee-38c4cdddc568",
   "metadata": {},
   "outputs": [],
   "source": [
    "filehandler = open(LOCAL_INSTANCE_FILE, 'rb')\n",
    "LIST_OF_INSTANCES = pkl.load(filehandler)\n",
    "filehandler.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3a54b6c2-2a68-4e7e-b4fc-5bef6903c871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LIST_OF_INSTANCES[200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "dcb203a9-8e92-431d-9a70-6ba922e11a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://ndr-v1-hybrid-vertex-bucket/endpoint-tests/test_instance_list.pkl\n"
     ]
    }
   ],
   "source": [
    "ENDPOINT_TEST_SUBDIR = \"endpoint-tests\"\n",
    "\n",
    "!gsutil -q cp $LOCAL_INSTANCE_FILE $BUCKET_URI/$ENDPOINT_TEST_SUBDIR/$LOCAL_INSTANCE_FILE\n",
    "\n",
    "!gsutil ls $BUCKET_URI/$ENDPOINT_TEST_SUBDIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615e0e55-c935-419d-ba08-12b31fd9dc24",
   "metadata": {},
   "source": [
    "## Create skewed online query traffic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "703c3579-dcc0-4884-9370-cadbada868f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERATE_NEW_STATS       = False # True | False\n",
    "SKEW_FEATURES_STATS_FILE = 'skew_feat_stats.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "08ee60e2-6574-4835-a197-0718574d3e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_durations : 16084755.0\n",
      "std_durations : 15129075.0\n",
      "mean_num_songs : 68.29652404785156\n",
      "std_num_songs  : 63.434242248535156\n",
      "mean_num_artists : 32.46001052856445\n",
      "std_num_artists  : 27.37798309326172\n",
      "mean_num_albums : 42.67263412475586\n",
      "std_num_albums  : 35.765804290771484\n"
     ]
    }
   ],
   "source": [
    "if GENERATE_NEW_STATS:\n",
    "    \n",
    "    valid_files = []\n",
    "    for blob in storage_client.list_blobs(f\"{BUCKET_NAME}\", prefix=f'data/{DATA_VERSION}/{VALID_DIR_PREFIX}/'):\n",
    "        if '.tfrecords' in blob.name:\n",
    "            valid_files.append(blob.public_url.replace(\"https://storage.googleapis.com/\", \"gs://\"))\n",
    "\n",
    "    valid = tf.data.TFRecordDataset(valid_files)\n",
    "    valid_parsed = valid.map(feature_utils.parse_towers_tfrecord)\n",
    "    \n",
    "    PRED_REQUEST_N = 5000\n",
    "    valid_parsed = valid_parsed.take(PRED_REQUEST_N)\n",
    "    \n",
    "    # feature\n",
    "    start_time = time.time()\n",
    "\n",
    "    durations = np.concatenate(list(valid_parsed.map(lambda x: x[\"pl_duration_ms_new\"]).batch(100)))\n",
    "    mean_durations = durations.mean()\n",
    "    std_durations = durations.std()\n",
    "\n",
    "    num_songs = np.concatenate(list(valid_parsed.map(lambda x: x[\"num_pl_songs_new\"]).batch(100)))\n",
    "    mean_num_songs = num_songs.mean()\n",
    "    std_num_songs = num_songs.std()\n",
    "\n",
    "    num_artists = np.concatenate(list(valid_parsed.map(lambda x: x[\"num_pl_artists_new\"]).batch(100)))\n",
    "    mean_num_artists = num_artists.mean()\n",
    "    std_num_artists = num_artists.std()\n",
    "\n",
    "    num_albums = np.concatenate(list(valid_parsed.map(lambda x: x[\"num_pl_albums_new\"]).batch(100)))\n",
    "    mean_num_albums = num_albums.mean()\n",
    "    std_num_albums = num_albums.std()\n",
    "    \n",
    "    end_time = time.time()\n",
    "    elapsed_time = int((end_time - start_time) / 60)\n",
    "    print(f\"elapsed_time: {elapsed_time}\")\n",
    "    \n",
    "    SKEW_FEATURES = {\n",
    "        'pl_duration_ms_new': (mean_durations, std_durations),\n",
    "        'num_pl_songs_new': (mean_num_songs, std_num_songs),\n",
    "        'num_pl_artists_new': (mean_num_artists, std_num_artists),\n",
    "        'num_pl_albums_new': (mean_num_albums, std_num_albums),\n",
    "    }\n",
    "    \n",
    "else:\n",
    "    \n",
    "    filehandler = open(SKEW_FEATURES_STATS_FILE, 'rb')\n",
    "    SKEW_FEATURES = pkl.load(filehandler)\n",
    "    filehandler.close()\n",
    "    \n",
    "    mean_durations, std_durations = SKEW_FEATURES['pl_duration_ms_new']\n",
    "    mean_num_songs, std_num_songs = SKEW_FEATURES['num_pl_songs_new']\n",
    "    mean_num_artists, std_num_artists = SKEW_FEATURES['num_pl_artists_new']\n",
    "    mean_num_albums, std_num_albums = SKEW_FEATURES['num_pl_albums_new']\n",
    "\n",
    "print(f\"mean_durations : {mean_durations}\")\n",
    "print(f\"std_durations : {std_durations}\")\n",
    "\n",
    "print(f\"mean_num_songs : {mean_num_songs}\")\n",
    "print(f\"std_num_songs  : {std_num_songs}\")\n",
    "\n",
    "print(f\"mean_num_artists : {mean_num_artists}\")\n",
    "print(f\"std_num_artists  : {std_num_artists}\")\n",
    "\n",
    "print(f\"mean_num_albums : {mean_num_albums}\")\n",
    "print(f\"std_num_albums  : {std_num_albums}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7cf6da2a-f183-4e4e-83d8-193fff2b3d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def monitoring_test(endpoint, instances, skew_feat_stat, start=2, end=4):\n",
    "    \n",
    "    mean_durations, std_durations = skew_feat_stat['pl_duration_ms_new']\n",
    "    mean_num_songs, std_num_songs = skew_feat_stat['num_pl_songs_new']\n",
    "    mean_num_artists, std_num_artists = skew_feat_stat['num_pl_artists_new']\n",
    "    mean_num_albums, std_num_albums = skew_feat_stat['num_pl_albums_new']\n",
    "    print(f\"std_durations   : {round(std_durations, 0)}\")\n",
    "    print(f\"std_num_songs   : {round(std_num_songs, 0)}\")\n",
    "    print(f\"std_num_artists : {round(std_num_artists, 0)}\")\n",
    "    print(f\"std_num_albums  : {round(std_num_albums, 0)}\\n\")\n",
    "    \n",
    "    total_preds = 0\n",
    "    \n",
    "    for multiplier in range(start, end+1):\n",
    "\n",
    "        print(f\"multiplier: {multiplier}\")\n",
    "\n",
    "        pred_count = 0\n",
    "\n",
    "        for example in instances:\n",
    "            list_dict = {}\n",
    "\n",
    "            example['pl_duration_ms_new'] = round(std_durations * multiplier, 0)\n",
    "            example['num_pl_songs_new'] = round(std_num_songs * multiplier, 0)\n",
    "            example['num_pl_artists_new'] = round(std_num_artists * multiplier, 0)\n",
    "            example['num_pl_albums_new'] = round(std_num_albums * multiplier, 0)\n",
    "            # list_of_skewed_instances.append(example)\n",
    "\n",
    "            response = endpoint.predict(instances=[example])\n",
    "\n",
    "            if pred_count > 0 and pred_count % 250 == 0:\n",
    "                print(f\"pred_count: {pred_count}\")\n",
    "\n",
    "            pred_count += 1\n",
    "            total_preds += 1\n",
    "\n",
    "        print(f\"sent {pred_count} pred requests with {multiplier}X multiplier\")\n",
    "    \n",
    "    print(f\"sent {total_preds} total pred requests\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b9d5326c-fc06-49cc-a7f9-546f878a4f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "monitoring_test(\n",
    "    endpoint=deployed_query_model, \n",
    "    instances=LIST_OF_INSTANCES,\n",
    "    skew_feat_stat=SKEW_FEATURES,\n",
    "    start=2, \n",
    "    end=8\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bd5a1d-4066-475b-acca-e0e32c2c07dc",
   "metadata": {},
   "source": [
    "# (Optional): Clean-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c943d3ad-91aa-479d-8375-02886d57ca6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# monitoring_job.pause()\n",
    "# monitoring_job.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d944ae-5be9-46b0-b842-2e3581d5a20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deployed_query_model.undeploy_all()\n",
    "# deployed_query_model.delete()\n",
    "# uploaded_query_model.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e378f6d-8bd1-4093-ad8b-0f40b1862a20",
   "metadata": {},
   "source": [
    "# Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6905f32-d1f1-429a-9465-a23c6bfc0e97",
   "metadata": {},
   "source": [
    "## Model Monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe643c8c-6fd8-4908-ac14-61db520275cd",
   "metadata": {},
   "source": [
    "### Cloud storage layout\n",
    "\n",
    "> Notice the following components in these Cloud Storage paths:\n",
    "\n",
    "* **cloud-ai-platform-** .. - This is a bucket created for you and assigned to capture your service's prediction data. Each monitoring job you create will trigger creation of a new folder in this bucket.\n",
    "* **`model_monitoring|instance_schemas`/job-** .. - This is your unique monitoring job number, which you can see above in both the response to your job creation requesst and the email notification.\n",
    "* **instance_schemas/job-** ../analysis - This is the monitoring jobs understanding and encoding of your training data's schema (field names, types, etc.).\n",
    "* **instance_schemas/job-** ../predict - This is the first prediction made to your model after the current monitoring job was enabled.\n",
    "* **model_monitoring/job-** ../serving - This folder is used to record data relevant to drift calculations. It contains measurement summaries for every hour your model serves traffic.\n",
    "* **model_monitoring/job-** ../training - This folder is used to record data relevant to training-serving skew calculations. It contains an ongoing summary of prediction data relative to training data.\n",
    "* **model_monitoring/job-** ../feature_attribution_score - This folder is used to record data relevant to feature attribution calculations. It contains an ongoing summary of feature attribution scores relative to training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff97cacb-e7e6-460e-bc03-abf25e72290f",
   "metadata": {},
   "source": [
    "### Interpret your results\n",
    "\n",
    "Vertex AI Model Monitoring detects an anomaly when the threshold set for a feature is exceeded. The following cells give you a sense of the alerting and reporting experience after model monitoring anomalies have been detected.\n",
    "\n",
    "Vertex AI Model Monitoring automatically notifies you of detected anomalies through email, but you can also [set up alerts through Cloud Logging](https://cloud.google.com/vertex-ai/docs/model-monitoring/using-model-monitoring#monitor-job)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376728a1-380e-4e44-b14e-66e20a1b9082",
   "metadata": {},
   "source": [
    "### Learn more about model monitoring\n",
    "\n",
    "**Congratulations!** You've now learned what model monitoring is, how to configure and enable it, and how to find and interpret the results. Check out the following resources to learn more about model monitoring and ML Ops.\n",
    "\n",
    "- [TensorFlow Data Validation](https://www.tensorflow.org/tfx/guide/tfdv)\n",
    "- [Data Understanding, Validation, and Monitoring At Scale](https://blog.tensorflow.org/2018/09/introducing-tensorflow-data-validation.html)\n",
    "- [Vertex Product Documentation](https://cloud.google.com/vertex-ai)\n",
    "- [Vertex AI Model Monitoring Reference Docs](https://cloud.google.com/vertex-ai/docs/reference)\n",
    "- [Vertex AI Model Monitoring blog article](https://cloud.google.com/blog/topics/developers-practitioners/monitor-models-training-serving-skew-vertex-ai)\n",
    "- [Explainable AI Whitepaper](https://storage.googleapis.com/cloud-ai-whitepapers/AI%20Explainability%20Whitepaper.pdf)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-10.m103",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-10:m103"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
