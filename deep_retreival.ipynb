{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d5383b2-071e-45b8-af2c-2421eb4f923a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Two - Tower Retreival Model\n",
    "\n",
    "### Key resources:\n",
    "* Many pages [here](https://www.tensorflow.org/recommenders/examples/deep_recommenders) include great techniques to build custom TFRS Models\n",
    "\n",
    "### Goals:\n",
    "* Show how to model off of most data types \n",
    "  * (String, Existing Embeddings (vectors), \n",
    "  * Floats (Normalized), \n",
    "  * Categorical with vocab, \n",
    "  * High Dim Categorical (Embed)\n",
    "* Leverage class templates to create custom 2 Tower Models quick/easy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d52e5f-a13b-46a7-849a-dc188b455a86",
   "metadata": {},
   "source": [
    "## SPOTIFY Create the tensorflow.io interface for the event and product table in Bigquery\n",
    "Best practices from Google are in this blog post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de3122eb-564b-46f9-9a50-851d19301521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set variables\n",
    "DROPOUT = False\n",
    "DROPOUT_RATE = 0.2\n",
    "EMBEDDING_DIM = 64\n",
    "MAX_TOKENS = 100_000\n",
    "BATCH_SIZE = 256\n",
    "ARCH = [128, 64]\n",
    "NUM_EPOCHS = 1\n",
    "SEED = 41781897\n",
    "PROJECT_ID = 'jtotten-project'\n",
    "DROP_FIELDS = ['pid', 'track_uri', 'artist_uri', 'album_uri']\n",
    "N_RECORDS_PER_TFRECORD_FILE = 300 * 50 #100ish mb  \n",
    "TF_RECORDS_DIR = 'gs://spotify-tfrecords'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c50a06-d69d-4d64-b254-26fb2a249fcf",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Quick counts on training data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5e1473-4a2d-4778-9f01-e2b76672a9f5",
   "metadata": {},
   "source": [
    "#### Quick counts on the training records for track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eaa56927-6722-43f9-88ac-ab638487164e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Query complete after 0.00s: 100%|██████████| 1/1 [00:00<00:00, 1309.08query/s]\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.34rows/s]\n"
     ]
    }
   ],
   "source": [
    "%%bigquery TOTAL_PLAYLISTS\n",
    "select count(1) from jtotten-project.spotify_mpd.playlists_track_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "418e5d8d-5410-4bca-a168-002d27ed95ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1032000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TOTAL_PLAYLISTS = TOTAL_PLAYLISTS.values[0][0]\n",
    "TOTAL_PLAYLISTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3ee0aa-1c6d-47f0-a3a8-236c7a072ee6",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Same with playlist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031039fa-decd-4396-a9ad-89f300a01f25",
   "metadata": {},
   "source": [
    "#### Quick counts (this time playlists) on the training records for track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c56750aa-be5c-4f4f-8b5b-4b8277e2a838",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Query complete after 0.00s: 100%|██████████| 1/1 [00:00<00:00, 1371.14query/s]\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.30rows/s]\n"
     ]
    }
   ],
   "source": [
    "%%bigquery TOTAL_TRACKS\n",
    "select count(1) from jtotten-project.spotify_mpd.track_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ccacb43d-c04c-4935-9e1f-06d51983e02f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2261490"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TOTAL_TRACKS = TOTAL_TRACKS.values[0][0]\n",
    "TOTAL_TRACKS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4710f67-de96-401b-8a0c-abb05dd62c40",
   "metadata": {},
   "source": [
    "### Set the tf.io pipelines function from bigquery\n",
    "\n",
    "[Great blog post here on it](https://towardsdatascience.com/how-to-read-bigquery-data-from-tensorflow-2-0-efficiently-9234b69165c8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7db86c9f-5454-4629-80ba-c0273fca117d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import dtypes\n",
    "from tensorflow_io.bigquery import BigQueryClient\n",
    "from tensorflow_io.bigquery import BigQueryReadSession\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") #do this b/c there's an info-level bug that can safely be ignored\n",
    "import json\n",
    "import tensorflow as tf\n",
    "import tensorflow_recommenders as tfrs\n",
    "import datetime\n",
    "from tensorflow.python.lib.io import file_io\n",
    "from tensorflow.train import BytesList, Feature, FeatureList, Int64List\n",
    "from tensorflow.train import SequenceExample, FeatureLists\n",
    "\n",
    "\n",
    "\n",
    "def bq_to_tfdata(client, row_restriction, table_id, col_names, col_types, dataset, batch_size=BATCH_SIZE):\n",
    "    TABLE_ID = table_id\n",
    "    COL_NAMES = col_names\n",
    "    COL_TYPES = col_types\n",
    "    DATASET = dataset\n",
    "    bqsession = client.read_session(\n",
    "        \"projects/\" + PROJECT_ID,\n",
    "        PROJECT_ID, TABLE_ID, DATASET,\n",
    "        COL_NAMES, COL_TYPES,\n",
    "        requested_streams=2,\n",
    "        row_restriction=row_restriction)\n",
    "    dataset = bqsession.parallel_read_rows()\n",
    "    return dataset.prefetch(1).shuffle(batch_size*10).batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e45e8d-3113-4528-9d71-8afdfd922d4f",
   "metadata": {},
   "source": [
    "## Get the song metadata\n",
    "\n",
    "To get a pipeline working we need the metadata for the table along with the table information. The following functions are helpers that give us the metadata into the proper types for `tf`\n",
    "\n",
    "\n",
    "For each table id, programatically get\n",
    "* Column names\n",
    "* Column types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "abcf9098-fd4c-4d00-9e84-8658a620b848",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Query complete after 0.00s: 100%|██████████| 1/1 [00:00<00:00, 821.29query/s]                          \n",
      "Downloading: 100%|██████████| 2/2 [00:00<00:00,  4.07rows/s]\n"
     ]
    }
   ],
   "source": [
    "%%bigquery schema\n",
    "SELECT * FROM jtotten-project.spotify_mpd.INFORMATION_SCHEMA.TABLES\n",
    "where table_name in ('track_audio', 'playlists_track_string');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "402d9cd0-a837-418a-aa7d-8ba8350c6940",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>table_catalog</th>\n",
       "      <th>table_schema</th>\n",
       "      <th>table_name</th>\n",
       "      <th>table_type</th>\n",
       "      <th>is_insertable_into</th>\n",
       "      <th>is_typed</th>\n",
       "      <th>creation_time</th>\n",
       "      <th>base_table_catalog</th>\n",
       "      <th>base_table_schema</th>\n",
       "      <th>base_table_name</th>\n",
       "      <th>snapshot_time_ms</th>\n",
       "      <th>ddl</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>jtotten-project</td>\n",
       "      <td>spotify_mpd</td>\n",
       "      <td>track_audio</td>\n",
       "      <td>BASE TABLE</td>\n",
       "      <td>YES</td>\n",
       "      <td>NO</td>\n",
       "      <td>2022-04-06 17:46:25.801000+00:00</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaT</td>\n",
       "      <td>CREATE TABLE `jtotten-project.spotify_mpd.trac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>jtotten-project</td>\n",
       "      <td>spotify_mpd</td>\n",
       "      <td>playlists_track_string</td>\n",
       "      <td>BASE TABLE</td>\n",
       "      <td>YES</td>\n",
       "      <td>NO</td>\n",
       "      <td>2022-04-22 22:50:46.601000+00:00</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaT</td>\n",
       "      <td>CREATE TABLE `jtotten-project.spotify_mpd.play...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     table_catalog table_schema              table_name  table_type  \\\n",
       "0  jtotten-project  spotify_mpd             track_audio  BASE TABLE   \n",
       "1  jtotten-project  spotify_mpd  playlists_track_string  BASE TABLE   \n",
       "\n",
       "  is_insertable_into is_typed                    creation_time  \\\n",
       "0                YES       NO 2022-04-06 17:46:25.801000+00:00   \n",
       "1                YES       NO 2022-04-22 22:50:46.601000+00:00   \n",
       "\n",
       "  base_table_catalog base_table_schema base_table_name snapshot_time_ms  \\\n",
       "0               None              None            None              NaT   \n",
       "1               None              None            None              NaT   \n",
       "\n",
       "                                                 ddl  \n",
       "0  CREATE TABLE `jtotten-project.spotify_mpd.trac...  \n",
       "1  CREATE TABLE `jtotten-project.spotify_mpd.play...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema # we will get the fields out of the ddl field"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd72e820-d1d7-48f9-902d-ad63be9b74fb",
   "metadata": {},
   "source": [
    "## Helper functions to pull metadata from ddl statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09e90599-b0e7-41e6-a6e7-99d6668ead55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert string type representation to tf data types\n",
    "\n",
    "def conv_dtype_to_tf(dtype_str):\n",
    "    if dtype_str == 'FLOAT64':\n",
    "        return dtypes.float64\n",
    "    elif dtype_str == 'INT64':\n",
    "        return dtypes.int64\n",
    "    else: \n",
    "        return dtypes.string\n",
    "        \n",
    "def get_metadata_from_ddl(ddl, drop_field=None):\n",
    "    fields = []\n",
    "    types = []\n",
    "    ddl = ddl.values[0]\n",
    "    for line in ddl.splitlines():\n",
    "        if line[:1] == ' ': #only pull indented lines for the fields\n",
    "            # drop the comma\n",
    "            line = line.replace(',','')\n",
    "            space_delim = line.split(' ')\n",
    "            if space_delim[2] in drop_field:\n",
    "                pass\n",
    "            else:\n",
    "                fields.append(space_delim[2])\n",
    "                types.append(conv_dtype_to_tf(space_delim[3]))\n",
    "    return fields, types\n",
    "\n",
    "\n",
    "track_audio_fields, track_audio_types = get_metadata_from_ddl(schema.ddl[schema.table_name == 'track_audio'], DROP_FIELDS)\n",
    "playlist_fields, playlist_types = get_metadata_from_ddl(schema.ddl[schema.table_name == 'playlists_track_string'], DROP_FIELDS) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4fd06cfb-66c6-4a18-9e5d-22352317da83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name : <dtype: 'string'>\n",
      "collaborative : <dtype: 'string'>\n",
      "modified_at : <dtype: 'int64'>\n",
      "num_tracks : <dtype: 'int64'>\n",
      "num_albums : <dtype: 'int64'>\n",
      "num_followers : <dtype: 'int64'>\n",
      "tracks : <dtype: 'string'>\n",
      "num_edits : <dtype: 'int64'>\n",
      "duration_ms : <dtype: 'int64'>\n",
      "num_artists : <dtype: 'int64'>\n",
      "description : <dtype: 'string'>\n"
     ]
    }
   ],
   "source": [
    "# Quick check on data\n",
    "for a, b in zip(playlist_fields, playlist_types):\n",
    "    print(a +\" : \" + str(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c4667a86-574c-4484-a331-8c9c7ae6fbf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "artist_name : <dtype: 'string'>\n",
      "track_name : <dtype: 'string'>\n",
      "album_name : <dtype: 'string'>\n",
      "name : <dtype: 'string'>\n",
      "danceability : <dtype: 'float64'>\n",
      "energy : <dtype: 'float64'>\n",
      "key : <dtype: 'float64'>\n",
      "loudness : <dtype: 'float64'>\n",
      "mode : <dtype: 'float64'>\n",
      "speechiness : <dtype: 'float64'>\n",
      "acousticness : <dtype: 'float64'>\n",
      "instrumentalness : <dtype: 'float64'>\n",
      "liveness : <dtype: 'float64'>\n",
      "valence : <dtype: 'float64'>\n",
      "tempo : <dtype: 'float64'>\n",
      "type : <dtype: 'string'>\n",
      "id : <dtype: 'string'>\n",
      "uri : <dtype: 'string'>\n",
      "track_href : <dtype: 'string'>\n",
      "analysis_url : <dtype: 'string'>\n",
      "time_signature : <dtype: 'float64'>\n",
      "artist_pop : <dtype: 'int64'>\n",
      "track_pop : <dtype: 'string'>\n",
      "genres : <dtype: 'string'>\n",
      "duration_ms : <dtype: 'int64'>\n"
     ]
    }
   ],
   "source": [
    "# Quick check on data\n",
    "for a, b in zip(track_audio_fields, track_audio_types):\n",
    "    print(a +\" : \" + str(b))\n",
    "    \n",
    "DROP_TRACK_AUDIO_FIELDS = ['pid', 'track_uri', 'artist_uri', 'album_uri']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b94ebf1-0080-4cd5-be46-c72af9d3a0e5",
   "metadata": {},
   "source": [
    "### Now the helper functions are set. Below tf.data pipelines are created from bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "311dd38e-94c3-4bef-b4b0-e051ba529530",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-24 20:02:37.141644: W tensorflow_io/core/kernels/audio_video_mp3_kernels.cc:271] libmp3lame.so.0 or lame functions are not available\n",
      "2022-04-24 20:02:37.142160: I tensorflow_io/core/kernels/cpu_check.cc:128] Your CPU supports instructions that this TensorFlow IO binary was not compiled to use: AVX2 FMA\n",
      "2022-04-24 20:02:38.514837: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-24 20:02:38.515586: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-24 20:02:38.525382: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-24 20:02:38.526144: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-24 20:02:38.526829: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-24 20:02:38.527492: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-24 20:02:38.529644: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-04-24 20:02:38.699524: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-24 20:02:38.700274: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-24 20:02:38.700939: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-24 20:02:38.701569: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-24 20:02:38.702230: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-24 20:02:38.702903: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-24 20:02:39.668079: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-24 20:02:39.668877: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-24 20:02:39.669576: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-24 20:02:39.670299: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-24 20:02:39.670935: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-24 20:02:39.671580: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13793 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
      "2022-04-24 20:02:39.672169: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-24 20:02:39.672794: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13793 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "track_train_pipeline = bq_to_tfdata(BigQueryClient(), row_restriction=None, table_id = 'track_audio'\n",
    "                                    , col_names=track_audio_fields, col_types=track_audio_types, dataset='spotify_mpd', batch_size=1) #we will change to BATCH_SIZE after we test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e49e5cd5-7f73-419d-9c09-8495e09decab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-24 20:02:40.307367: E tensorflow/core/framework/dataset.cc:577] UNIMPLEMENTED: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2022-04-24 20:02:40.307412: E tensorflow/core/framework/dataset.cc:581] UNIMPLEMENTED: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2022-04-24 20:02:40.307824: E tensorflow/core/framework/dataset.cc:577] UNIMPLEMENTED: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2022-04-24 20:02:40.307846: E tensorflow/core/framework/dataset.cc:581] UNIMPLEMENTED: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('acousticness', <tf.Tensor: shape=(1,), dtype=float64, numpy=array([0.0288])>), ('album_name', <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'Dim Mak Records New Noise, Vol. 3'], dtype=object)>), ('analysis_url', <tf.Tensor: shape=(1,), dtype=string, numpy=\n",
      "array([b'https://api.spotify.com/v1/audio-analysis/6L16Zmb3Wtk1jwj1vuWne4'],\n",
      "      dtype=object)>), ('artist_name', <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'Comic Strips'], dtype=object)>), ('artist_pop', <tf.Tensor: shape=(1,), dtype=int64, numpy=array([-1])>), ('danceability', <tf.Tensor: shape=(1,), dtype=float64, numpy=array([0.83])>), ('duration_ms', <tf.Tensor: shape=(1,), dtype=int64, numpy=array([288634])>), ('energy', <tf.Tensor: shape=(1,), dtype=float64, numpy=array([0.513])>), ('genres', <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'unknown'], dtype=object)>), ('id', <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'6L16Zmb3Wtk1jwj1vuWne4'], dtype=object)>), ('instrumentalness', <tf.Tensor: shape=(1,), dtype=float64, numpy=array([0.317])>), ('key', <tf.Tensor: shape=(1,), dtype=float64, numpy=array([0.])>), ('liveness', <tf.Tensor: shape=(1,), dtype=float64, numpy=array([0.656])>), ('loudness', <tf.Tensor: shape=(1,), dtype=float64, numpy=array([-6.894])>), ('mode', <tf.Tensor: shape=(1,), dtype=float64, numpy=array([0.])>), ('name', <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'March 2013'], dtype=object)>), ('speechiness', <tf.Tensor: shape=(1,), dtype=float64, numpy=array([0.0622])>), ('tempo', <tf.Tensor: shape=(1,), dtype=float64, numpy=array([129.999])>), ('time_signature', <tf.Tensor: shape=(1,), dtype=float64, numpy=array([4.])>), ('track_href', <tf.Tensor: shape=(1,), dtype=string, numpy=\n",
      "array([b'https://api.spotify.com/v1/tracks/6L16Zmb3Wtk1jwj1vuWne4'],\n",
      "      dtype=object)>), ('track_name', <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'Start of the Night'], dtype=object)>), ('track_pop', <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'0'], dtype=object)>), ('type', <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'audio_features'], dtype=object)>), ('uri', <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'spotify:track:6L16Zmb3Wtk1jwj1vuWne4'], dtype=object)>), ('valence', <tf.Tensor: shape=(1,), dtype=float64, numpy=array([0.328])>)])\n"
     ]
    }
   ],
   "source": [
    "### Validate we are getting records\n",
    "\n",
    "for line in track_train_pipeline.take(1):\n",
    "    print(line) #should come out based on batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc539f35-02a5-4010-b07d-02a77f66f4ce",
   "metadata": {},
   "source": [
    "### For the song audio data, we are set and will use this pipeline in training - there's no need to pre-process as there are no nested elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e3c14a9-a269-4bc5-a198-5270ac0faa92",
   "metadata": {},
   "outputs": [],
   "source": [
    "playlist_types[-1] = dtypes.string #try manually setting the dtype for the tracks nested column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4fb806df-3f54-4785-a063-9a416e487ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-24 20:02:41.482012: E tensorflow/core/framework/dataset.cc:577] UNIMPLEMENTED: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2022-04-24 20:02:41.482069: E tensorflow/core/framework/dataset.cc:581] UNIMPLEMENTED: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2022-04-24 20:02:41.482422: E tensorflow/core/framework/dataset.cc:577] UNIMPLEMENTED: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2022-04-24 20:02:41.482462: E tensorflow/core/framework/dataset.cc:581] UNIMPLEMENTED: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('collaborative', <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'false'], dtype=object)>), ('description', <tf.Tensor: shape=(1,), dtype=string, numpy=array([b''], dtype=object)>), ('duration_ms', <tf.Tensor: shape=(1,), dtype=int64, numpy=array([4203113])>), ('modified_at', <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1499990400])>), ('name', <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'Span'], dtype=object)>), ('num_albums', <tf.Tensor: shape=(1,), dtype=int64, numpy=array([10])>), ('num_artists', <tf.Tensor: shape=(1,), dtype=int64, numpy=array([6])>), ('num_edits', <tf.Tensor: shape=(1,), dtype=int64, numpy=array([3])>), ('num_followers', <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>), ('num_tracks', <tf.Tensor: shape=(1,), dtype=int64, numpy=array([18])>), ('tracks', <tf.Tensor: shape=(1,), dtype=string, numpy=\n",
      "array([b\"[{'pos': 0, 'artist_name': 'Carlos Vives', 'track_uri': 'spotify:track:1KbX1VuFIgsRa9JjnQtRYA', 'artist_uri': 'spotify:artist:4vhNDa5ycK0ST968ek7kRr', 'track_name': 'Bailar Contigo', 'album_uri': 'spotify:album:5aYaYsDnVXSvVTP7KTHmSl', 'duration_ms': 252120, 'album_name': 'Coraz\\xc3\\xb3n Profundo (Versi\\xc3\\xb3n Deluxe)'}, {'pos': 1, 'artist_name': 'Sebastian Yatra', 'track_uri': 'spotify:track:5J1c3M4EldCfNxXwrwt8mT', 'artist_uri': 'spotify:artist:07YUOmWljBTXwIseAUd9TW', 'track_name': 'Traicionera', 'album_uri': 'spotify:album:1TnP5HDR6fxVZBIGZq6DgO', 'duration_ms': 228466, 'album_name': 'Traicionera'}, {'pos': 2, 'artist_name': 'Romeo Santos', 'track_uri': 'spotify:track:6r46lnXFbE9fr2d3KNaGbe', 'artist_uri': 'spotify:artist:5lwmRuXgjX8xIwlnauTZIP', 'track_name': 'Imitadora', 'album_uri': 'spotify:album:6bm9EpUNvQ9xMglBJGRmgS', 'duration_ms': 234828, 'album_name': 'Golden'}, {'pos': 3, 'artist_name': 'Romeo Santos', 'track_uri': 'spotify:track:6BdAwMv1YorLfkBWlE493X', 'artist_uri': 'spotify:artist:5lwmRuXgjX8xIwlnauTZIP', 'track_name': 'H\\xc3\\xa9roe Favorito', 'album_uri': 'spotify:album:6bm9EpUNvQ9xMglBJGRmgS', 'duration_ms': 239823, 'album_name': 'Golden'}, {'pos': 4, 'artist_name': 'Romeo Santos', 'track_uri': 'spotify:track:5PycBIeabfvX3n9ILG7Vrv', 'artist_uri': 'spotify:artist:5lwmRuXgjX8xIwlnauTZIP', 'track_name': 'Propuesta Indecente', 'album_uri': 'spotify:album:17HsiXfqKUPoTP6Y5ebs1L', 'duration_ms': 235133, 'album_name': 'F\\xc3\\xb3rmula, Vol. 2 (Deluxe Edition)'}, {'pos': 5, 'artist_name': 'Romeo Santos', 'track_uri': 'spotify:track:6I86RF3odBlcuZA9Vfjzeq', 'artist_uri': 'spotify:artist:5lwmRuXgjX8xIwlnauTZIP', 'track_name': 'Eres M\\xc3\\xada', 'album_uri': 'spotify:album:17HsiXfqKUPoTP6Y5ebs1L', 'duration_ms': 250640, 'album_name': 'F\\xc3\\xb3rmula, Vol. 2 (Deluxe Edition)'}, {'pos': 6, 'artist_name': 'Romeo Santos', 'track_uri': 'spotify:track:1iLv1ieT9BZ3qsti9yTCnG', 'artist_uri': 'spotify:artist:5lwmRuXgjX8xIwlnauTZIP', 'track_name': 'Odio', 'album_uri': 'spotify:album:17HsiXfqKUPoTP6Y5ebs1L', 'duration_ms': 225000, 'album_name': 'F\\xc3\\xb3rmula, Vol. 2 (Deluxe Edition)'}, {'pos': 7, 'artist_name': 'Carlos Vives', 'track_uri': 'spotify:track:0sXvAOmXgjR2QUqLK1MltU', 'artist_uri': 'spotify:artist:4vhNDa5ycK0ST968ek7kRr', 'track_name': 'La Bicicleta', 'album_uri': 'spotify:album:6bUxh58rYTL67FS8dyTKMN', 'duration_ms': 227706, 'album_name': 'El Dorado'}, {'pos': 8, 'artist_name': 'Romeo Santos', 'track_uri': 'spotify:track:6bcdiJvYNX115yvbh3wASz', 'artist_uri': 'spotify:artist:5lwmRuXgjX8xIwlnauTZIP', 'track_name': 'Cancioncitas de Amor', 'album_uri': 'spotify:album:17HsiXfqKUPoTP6Y5ebs1L', 'duration_ms': 237093, 'album_name': 'F\\xc3\\xb3rmula, Vol. 2 (Deluxe Edition)'}, {'pos': 9, 'artist_name': 'Romeo Santos', 'track_uri': 'spotify:track:3DKWF8is9hzp84aSxnhlag', 'artist_uri': 'spotify:artist:5lwmRuXgjX8xIwlnauTZIP', 'track_name': 'Necio', 'album_uri': 'spotify:album:17HsiXfqKUPoTP6Y5ebs1L', 'duration_ms': 264266, 'album_name': 'F\\xc3\\xb3rmula, Vol. 2 (Deluxe Edition)'}, {'pos': 10, 'artist_name': 'Luis Fonsi', 'track_uri': 'spotify:track:4aWmUDTfIPGksMNLV2rQP2', 'artist_uri': 'spotify:artist:4V8Sr092TqfHkfAA5fXXqG', 'track_name': 'Despacito (Featuring Daddy Yankee)', 'album_uri': 'spotify:album:1HV4uCbhCicfl07dm2WvU0', 'duration_ms': 228200, 'album_name': 'Despacito (Featuring Daddy Yankee)'}, {'pos': 11, 'artist_name': 'Romeo Santos', 'track_uri': 'spotify:track:6CKlQHQIxhWtq7MnEJ6QCz', 'artist_uri': 'spotify:artist:5lwmRuXgjX8xIwlnauTZIP', 'track_name': 'Hilito', 'album_uri': 'spotify:album:17HsiXfqKUPoTP6Y5ebs1L', 'duration_ms': 234760, 'album_name': 'F\\xc3\\xb3rmula, Vol. 2 (Deluxe Edition)'}, {'pos': 12, 'artist_name': 'Romeo Santos', 'track_uri': 'spotify:track:0uLwWjN1KFO678WNJcZDkw', 'artist_uri': 'spotify:artist:5lwmRuXgjX8xIwlnauTZIP', 'track_name': 'Ll\\xc3\\xa9vame Contigo', 'album_uri': 'spotify:album:6a3RDPcFamZvFCi8VeXWkK', 'duration_ms': 226400, 'album_name': 'F\\xc3\\xb3rmula Vol. 1 (Deluxe Edition)'}, {'pos': 13, 'artist_name': 'Romeo Santos', 'track_uri': 'spotify:track:4EbAftNM732UGLF8gmIIsX', 'artist_uri': 'spotify:artist:5lwmRuXgjX8xIwlnauTZIP', 'track_name': 'Mi Santa', 'album_uri': 'spotify:album:6a3RDPcFamZvFCi8VeXWkK', 'duration_ms': 231226, 'album_name': 'F\\xc3\\xb3rmula Vol. 1 (Deluxe Edition)'}, {'pos': 14, 'artist_name': 'Romeo Santos', 'track_uri': 'spotify:track:6rXvMpWJbTyhSrVWye7jPE', 'artist_uri': 'spotify:artist:5lwmRuXgjX8xIwlnauTZIP', 'track_name': 'Inocente', 'album_uri': 'spotify:album:17HsiXfqKUPoTP6Y5ebs1L', 'duration_ms': 236293, 'album_name': 'F\\xc3\\xb3rmula, Vol. 2 (Deluxe Edition)'}, {'pos': 15, 'artist_name': 'Prince Royce', 'track_uri': 'spotify:track:6cJLfIqwh0tCKRjYM3WpZ5', 'artist_uri': 'spotify:artist:3MHaV05u0io8fQbZ2XPtlC', 'track_name': 'Darte un Beso', 'album_uri': 'spotify:album:4o8ZyBzwPxPVc2bqNG5Xfe', 'duration_ms': 206933, 'album_name': 'Soy el Mismo'}, {'pos': 16, 'artist_name': 'Enrique Iglesias', 'track_uri': 'spotify:track:32lm3769IRfcnrQV11LO4E', 'artist_uri': 'spotify:artist:7qG3b048QCHVRO5Pv1T5lw', 'track_name': 'Bailando - Spanish Version', 'album_uri': 'spotify:album:2kZkiVn1m00XcgaWlLb2LD', 'duration_ms': 243413, 'album_name': 'SEX AND LOVE'}, {'pos': 17, 'artist_name': 'Enrique Iglesias', 'track_uri': 'spotify:track:6YZdkObH88npeKrrkb8Ggf', 'artist_uri': 'spotify:artist:7qG3b048QCHVRO5Pv1T5lw', 'track_name': 'DUELE EL CORAZON', 'album_uri': 'spotify:album:5Ouuxga807CPAs81lSloBJ', 'duration_ms': 200813, 'album_name': 'DUELE EL CORAZON'}]\"],\n",
      "      dtype=object)>)])\n"
     ]
    }
   ],
   "source": [
    "## Validate playlist data\n",
    "playlist_train_pipeline = bq_to_tfdata(BigQueryClient(), row_restriction=None, table_id = 'playlists_track_string'\n",
    "                                    , col_names=playlist_fields\n",
    "                                       , col_types=playlist_types\n",
    "                                       , dataset='spotify_mpd', batch_size=1)\n",
    "for line in playlist_train_pipeline.take(1):\n",
    "    print(line) #should come out based on batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae20b3a5-a553-405d-b2f5-d37af35fdc8a",
   "metadata": {},
   "source": [
    "## In pulling one record it looks like it's properly parsing a tf record"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aae928c-beeb-4704-8b18-cd31bcb16d24",
   "metadata": {},
   "source": [
    "# do some data wranglging on the text data\n",
    "# tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "for _ in playlist_train_pipeline.map(lambda x: tf.io.parse_sequence_example(tf.io.serialize_tensor(x['tracks'][0]), sequence_features=feature_description, context_features=context_features, name='tracks')).take(1):\n",
    "    tensor = _\n",
    "    print(_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7132c7a5-5f76-429b-a1aa-47663f20080b",
   "metadata": {},
   "source": [
    "Since the data is stored in a text dictionary we will eagerly execute, grab the values and do a string `eval`\n",
    "`eval(\"{'pos': 0, 'artist_name': 'King Crimson', 'track_uri': 'spotify:track:173gp7NIXqk0MEo8K7Av4a', 'artist_uri': 'spotify:artist:7M1FPw29m5FbicYzS2xdpi', 'track_name': '21st Century Schizoid Man', 'album_uri': 'spotify:album:0ga8Q4tTXaFf9q3LvT8hrC', 'duration_ms': 657517, 'album_name': 'Radical Action To Unseat the Hold of Monkey Mind (Live)'}\")`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4abb83-f280-4108-998a-e19689841f8f",
   "metadata": {},
   "source": [
    "## This funcion parses the playlist data and breaks down the nested fields to be conformant with `SequenceExample`\n",
    "The 'flat' features come along as `context_features` in `SequenceExample`\n",
    "There is one more helper function to parse the example and write it to the destination `gs://` path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "82765dec-1d18-49b5-9a9f-81cbfff56a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def get_tensor_from_tracks(tensor):\n",
    "    key_list = ['pos', 'artist_name', 'track_uri', 'artist_uri', 'track_name', 'album_uri', 'duration_ms', 'album_name']\n",
    "    y = {}\n",
    "    \n",
    "    \n",
    "    tracks = tensor[\"tracks\"][0]\n",
    "    tracks = tracks.numpy()\n",
    "\n",
    "    tracks = eval(tracks)\n",
    "\n",
    "    for _ in key_list:\n",
    "        y[_] = []\n",
    "\n",
    "    for track in tracks:\n",
    "        y['pos'].append(track['pos'])\n",
    "        y['artist_name'].append(track['artist_name'].encode('utf8'))\n",
    "        y['artist_uri'].append(track['artist_uri'].encode('utf8'))\n",
    "        y['track_name'].append(track['track_name'].encode('utf8'))\n",
    "        y['album_uri'].append(track['album_uri'].encode('utf8'))\n",
    "        y['duration_ms'].append(track['duration_ms'])\n",
    "        y['album_name'].append(track['album_name'].encode('utf8'))\n",
    "        y['track_uri'].append(track['track_uri'].encode('utf8'))\n",
    "        \n",
    "\n",
    "\n",
    "    # set list types\n",
    "    pos = Int64List(value=y['pos'])\n",
    "    artist_name = BytesList(value=y['artist_name'])\n",
    "    artist_uri = BytesList(value=y['artist_uri'])\n",
    "    track_name = BytesList(value=y['track_name'])\n",
    "    album_uri = BytesList(value=y['album_uri'])\n",
    "    duration_ms = Int64List(value=y['duration_ms'])\n",
    "    album_name = BytesList(value=y['album_name'])\n",
    "    track_uri = BytesList(value=y['track_uri'])\n",
    "\n",
    "    \n",
    "    sample_dict = {\n",
    "    \"name\" : Feature(bytes_list=BytesList(value=tensor['name'].numpy())),\n",
    "    \"collaborative\" : Feature(bytes_list=BytesList(value=tensor['collaborative'].numpy())),\n",
    "    \"modified_at\" : Feature(int64_list=Int64List(value=tensor['modified_at'].numpy())),\n",
    "    \"num_tracks\" : Feature(int64_list=Int64List(value=tensor['num_tracks'].numpy())),\n",
    "    \"num_albums\" : Feature(int64_list=Int64List(value=tensor['num_albums'].numpy())),\n",
    "    \"num_followers\" : Feature(int64_list=Int64List(value=tensor['num_followers'].numpy())),\n",
    "    \"num_edits\" : Feature(int64_list=Int64List(value=tensor['num_edits'].numpy())),\n",
    "    \"duration_ms\" : Feature(int64_list=Int64List(value=tensor['duration_ms'].numpy())),\n",
    "    \"num_artists\" : Feature(int64_list=Int64List(value=tensor['num_artists'].numpy())),\n",
    "    \"description\" : Feature(bytes_list=BytesList(value=tensor['description'].numpy()))\n",
    "    }\n",
    "\n",
    "    # combine feature list\n",
    "\n",
    "    fl = FeatureList(feature=[Feature(int64_list=pos), \n",
    "                              Feature(bytes_list=artist_name),\n",
    "                              Feature(bytes_list=artist_uri),\n",
    "                              Feature(bytes_list=track_name),\n",
    "                              Feature(bytes_list=album_uri),\n",
    "                              Feature(int64_list=duration_ms),\n",
    "                              Feature(bytes_list=album_name),\n",
    "                              Feature(bytes_list=track_uri)\n",
    "                             ])\n",
    "\n",
    "    #create the sequence\n",
    "    seq = SequenceExample(context=tf.train.Features(feature=sample_dict),\n",
    "                          feature_lists=FeatureLists(feature_list={\n",
    "        \"tracks\": fl\n",
    "    }))\n",
    "    \n",
    "    return seq\n",
    "\n",
    "\n",
    "def write_a_tfrec(lns, n_records_per_file, file_counter, subfolder):\n",
    "    #next write to a tfrecord\n",
    "    with tf.io.TFRecordWriter(\n",
    "        TF_RECORDS_DIR + \"/\" + subfolder +\"/file_%.2i-%i.tfrec\" % (n_records_per_file, file_counter)\n",
    "    ) as writer:\n",
    "        for example in lns:\n",
    "            writer.write(example.SerializeToString())\n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2eb503-d2b4-424b-8f48-7b3187b5fc4a",
   "metadata": {},
   "source": [
    "## Now iterate over the pipeline\n",
    "Creating files with batches of `N_RECORDS_PER_TFRECORD_FILE`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2a4fc6-cdb7-4e81-abe0-d4df4d890b98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-24 20:02:41.862578: E tensorflow/core/framework/dataset.cc:577] UNIMPLEMENTED: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2022-04-24 20:02:41.862619: E tensorflow/core/framework/dataset.cc:581] UNIMPLEMENTED: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2022-04-24 20:02:41.863001: E tensorflow/core/framework/dataset.cc:577] UNIMPLEMENTED: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2022-04-24 20:02:41.863051: E tensorflow/core/framework/dataset.cc:581] UNIMPLEMENTED: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n"
     ]
    }
   ],
   "source": [
    "tf.config.run_functions_eagerly(True)\n",
    "# using datetime module\n",
    "\n",
    "# ct stores current time\n",
    "ct = str(datetime.datetime.now()).replace(\" \",\":\")\n",
    "\n",
    "records = []\n",
    "\n",
    "for i, line in enumerate(playlist_train_pipeline):\n",
    "    sequence_example = get_tensor_from_tracks(line) #should come out based on batch size\n",
    "    file_count = i // N_RECORDS_PER_TFRECORD_FILE ## iterates when floor division increments for i over NRPTF\n",
    "    if i % N_RECORDS_PER_TFRECORD_FILE == 0 or i == TOTAL_PLAYLISTS-1: #write-a-file and reset the batch\n",
    "        write_a_tfrec(records, n_records_per_file=N_RECORDS_PER_TFRECORD_FILE, subfolder=ct, file_counter = file_count)\n",
    "        records = []\n",
    "    else:\n",
    "        records.append(sequence_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe0b41d-9959-4601-9094-57cf01890fbb",
   "metadata": {},
   "source": [
    "# Model Draft Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc57822a-235f-4459-a43c-34762964b823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile -a vertex_train/trainer/task.py\n",
    "\n",
    "class PlaylistsModel(tf.keras.Model):\n",
    "    def __init__(self, layer_sizes, adapt_data):\n",
    "        super().__init__()\n",
    "        \n",
    "        #start with lookups on low cardnality categorical items\n",
    "        colab_vocab = tf.constant(['true','false'], name='colab_vocab', dtype='string')\n",
    "        \n",
    "        self.colab = tf.keras.Sequential([\n",
    "            tf.keras.layers.StringLookup(\n",
    "                vocabulary=colab_vocab, mask_token=None, name=\"colab_lookup\", output_mode='count')\n",
    "        ], name=\"colab\")\n",
    "        \n",
    "        #create text vectorizors to be fed to an embedding layer\n",
    "        self.artist_vectorizor = tf.keras.layers.TextVectorization(\n",
    "            max_tokens=MAX_TOKENS, name=\"artist_tv\", ngrams=2)\n",
    "        \n",
    "        self.album_vectorizor = tf.keras.layers.TextVectorization(\n",
    "            max_tokens=MAX_TOKENS, name=\"album_tv\", ngrams=2)\n",
    "        \n",
    "        self.description_vectorizor = tf.keras.layers.TextVectorization(\n",
    "            max_tokens=MAX_TOKENS, name=\"album_tv\", ngrams=2)\n",
    "        \n",
    "        self.query_embedding = tf.keras.Sequential([\n",
    "            self.album_vectorizor,\n",
    "            tf.keras.layers.Embedding(MAX_TOKENS+1, EMBEDDING_DIM , mask_zero=True, name=\"album_emb\"),\n",
    "            tf.keras.layers.GlobalAveragePooling1D()\n",
    "        ], name=\"album_embedding_model\")\n",
    "        \n",
    "        self.artist_embedding = tf.keras.Sequential([\n",
    "            self.artist_vectorizor,\n",
    "            tf.keras.layers.Embedding(MAX_TOKENS+1, EMBEDDING_DIM , mask_zero=True, name=\"artist_emb\"),\n",
    "            tf.keras.layers.GlobalAveragePooling1D()\n",
    "        ], name=\"artist_embedding\")\n",
    "        \n",
    "        ###############\n",
    "        ### adapt stuff\n",
    "        ###############\n",
    "        \n",
    "        self.artist_vectorizor.adapt(adapt_data.map(lambda x: x['artist_name']))\n",
    "        self.album_vectorizor.adapt(adapt_data.map(lambda x: x['album_name'])) \n",
    "        \n",
    "        # Then construct the layers.\n",
    "        self.dense_layers = tf.keras.Sequential(name=\"dense_layers_query\")\n",
    "        \n",
    "        initializer = tf.keras.initializers.GlorotUniform(seed=SEED)\n",
    "        # Use the ReLU activation for all but the last layer.\n",
    "        for layer_size in layer_sizes[:-1]:\n",
    "            self.dense_layers.add(tf.keras.layers.Dense(layer_size, activation=\"relu\", kernel_initializer=initializer))\n",
    "            if DROPOUT:\n",
    "                self.dense_layers.add(tf.keras.layers.Dropout(DROPOUT_RATE))\n",
    "        # No activation for the last layer\n",
    "        for layer_size in layer_sizes[-1:]:\n",
    "            self.dense_layers.add(tf.keras.layers.Dense(layer_size, kernel_initializer=initializer))\n",
    "        ### ADDING L2 NORM AT THE END\n",
    "        self.dense_layers.add(tf.keras.layers.Lambda(lambda x: tf.nn.l2_normalize(x, 1, epsilon=1e-12, name=\"normalize_dense\")))\n",
    "\n",
    "\n",
    "    def call(self, data):    \n",
    "        all_embs = tf.concat(\n",
    "                [\n",
    "                    self.album_embedding(data['album_name']),\n",
    "                    self.artist_embedding(data['artist_name']),\n",
    "                    self.colab(data['collaborative']),\n",
    "                    self.description_embedding(data['description'])\n",
    "                ], axis=1)\n",
    "        return self.dense_layers(all_embs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9821506a-054e-4ad2-8530-bde59fb5e5d6",
   "metadata": {},
   "source": [
    "## Use the example output to think of how you process your features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f83a334-4008-44a8-84f6-f2ef827200db",
   "metadata": {},
   "source": [
    "```\n",
    "OrderedDict([('album_name', <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'The Helm'], dtype=object)>), ('artist_name', <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'Carrot Green'], dtype=object)>), ('collaborative', <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'false'], dtype=object)>), ('description', <tf.Tensor: shape=(1,), dtype=string, numpy=array([b''], dtype=object)>), ('duration_ms', <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'358500'], dtype=object)>), ('modified_at', <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1505692800])>), ('name', <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'FeSTa'], dtype=object)>), ('num_albums', <tf.Tensor: shape=(1,), dtype=int64, numpy=array([82])>), ('num_artists', <tf.Tensor: shape=(1,), dtype=int64, numpy=array([66])>), ('num_edits', <tf.Tensor: shape=(1,), dtype=int64, numpy=array([48])>), ('num_followers', <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>), ('num_tracks', <tf.Tensor: shape=(1,), dtype=int64, numpy=array([85])>), ('pos', <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'45'], dtype=object)>), ('track_name', <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'The Helm - Carrot Green Remix'], dtype=object)>)])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6a663c63-58be-43bf-a573-e986cce3e9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "id": "8b37b159-34ce-4ef0-88ab-e9cc1ae7d03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data[0].keys()#originally got the values from this\n",
    "feature_description = {'pos': tf.io.RaggedFeature(tf.int64), \n",
    "                     'artist_name':  tf.io.RaggedFeature(tf.string), \n",
    "                     'track_uri':  tf.io.RaggedFeature(tf.string), \n",
    "                     'artist_uri': tf.io.RaggedFeature(tf.string), \n",
    "                     'track_name': tf.io.RaggedFeature(tf.string), \n",
    "                     'album_uri': tf.io.RaggedFeature(tf.string),\n",
    "                     'duration_ms': tf.io.RaggedFeature(tf.int64), \n",
    "                     'album_name': tf.io.RaggedFeature(tf.string)\n",
    "                    }\n",
    "context_features = {\"name\" : tf.io.FixedLenFeature(dtype=tf.string, shape=(1)),\n",
    "                    \"collaborative\" : tf.io.FixedLenFeature(dtype=tf.string, shape=(1)),\n",
    "                    \"modified_at\" : tf.io.FixedLenFeature(dtype=tf.int64, shape=(1)),\n",
    "                    \"num_tracks\" : tf.io.FixedLenFeature(dtype=tf.int64, shape=(1)),\n",
    "                    \"num_albums\" : tf.io.FixedLenFeature(dtype=tf.int64, shape=(1)),\n",
    "                    \"num_followers\" :tf.io.FixedLenFeature(dtype=tf.int64, shape=(1)),\n",
    "                    \"num_edits\" :tf.io.FixedLenFeature(dtype=tf.int64, shape=(1)),\n",
    "                    \"duration_ms\" : tf.io.FixedLenFeature(dtype=tf.int64, shape=(1)),\n",
    "                    \"num_artists\" : tf.io.FixedLenFeature(dtype=tf.int64, shape=(1)),\n",
    "                    \"description\" : tf.io.FixedLenFeature(dtype=tf.string, shape=(1))\n",
    "                   }"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m91",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m91"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
