{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9d8757f6-8f36-4b0d-8c37-ba31a1773f94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install google-cloud-aiplatform[prediction]>=1.16.0 fastapi nvtabular git+https://github.com/NVIDIA-Merlin/models.git --user"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201e92f9-980b-43e9-8c76-b2aa02260e96",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Buidling a custom Vertex AI endpoint for Merlin Query Tower\n",
    "\n",
    "**IMPORTANT** Make sure you are running this notebook in a DLVM (e.g. tensorflow enterprise 2.8) to build the image\n",
    "\n",
    "________\n",
    "**This will not work in the training container**\n",
    "________\n",
    "\n",
    "Your output should look like this - you are going to use the query model endpoint to create a custom container\n",
    "\n",
    "![](img/merlin-bucket.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cbe8a06-ed15-40ac-bc2f-d387bb406301",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "\n",
    "PROJECT = 'wortz-project-352116'  # <--- TODO: CHANGE THIS\n",
    "LOCATION = 'us-central1'\n",
    "REGION = 'us-central1'\n",
    "# path = 'gs://two-tower-models' #TODO change to your model directory\n",
    "BUCKET = 'gs://spotify-jsw-mpd-2023'\n",
    "REPOSITORY = 'merlin-spotify-cpr'\n",
    "ARTIFACT_URI = f'{BUCKET}'\n",
    "MODEL_DIR = f'{BUCKET}/query_model_merlin'\n",
    "PREFIX = 'merlin-spotify'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0da3551-62ee-4216-86b0-6b0289b709dd",
   "metadata": {},
   "source": [
    "### Run one time to locally copy the workflow for the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d89e9cd-654d-44e4-a076-739d95c8cd93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !mkdir workflow\n",
    "# !gsutil cp -r $BUCKET/workflow workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d045aab2-d485-4bf1-9319-b7afc5cccb34",
   "metadata": {},
   "source": [
    "### Set up repo and configure Docker (one-time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e72acda9-ae76-4ce9-8600-71238d545f57",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mERROR:\u001b[0m (gcloud.beta.artifacts.repositories.create) ALREADY_EXISTS: the repository already exists\n"
     ]
    }
   ],
   "source": [
    "# Create the repo if needed for the artifacts\n",
    "\n",
    "! gcloud beta artifacts repositories create {REPOSITORY} \\\n",
    "    --repository-format=docker \\\n",
    "    --location=$REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1858a130-739b-4752-bf02-d0e8356a0829",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;33mWARNING:\u001b[0m Your config file at [/home/jupyter/.docker/config.json] contains these credential helper entries:\n",
      "\n",
      "{\n",
      "  \"credHelpers\": {\n",
      "    \"gcr.io\": \"gcloud\",\n",
      "    \"us.gcr.io\": \"gcloud\",\n",
      "    \"eu.gcr.io\": \"gcloud\",\n",
      "    \"asia.gcr.io\": \"gcloud\",\n",
      "    \"staging-k8s.gcr.io\": \"gcloud\",\n",
      "    \"marketplace.gcr.io\": \"gcloud\",\n",
      "    \"us-central1-docker.pkg.dev\": \"gcloud\"\n",
      "  }\n",
      "}\n",
      "Adding credentials for: us-central1-docker.pkg.dev\n",
      "gcloud credential helpers already registered correctly.\n"
     ]
    }
   ],
   "source": [
    "! gcloud auth configure-docker {REGION}-docker.pkg.dev --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7bf429ed-e23c-47ac-93da-8a70c4e3e92a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! rm -rf app\n",
    "! mkdir app"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd404ace-87cc-4614-9bd0-cdd8f060ba51",
   "metadata": {},
   "source": [
    "### Dependency file\n",
    "The first few are for the server handling traffic\n",
    "Nvtabular was downgraded for this example, it may not be necessary in future versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4af962a7-3196-4b9a-9c36-3f35fd525b5b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing app/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile app/requirements.txt\n",
    "uvicorn[standard]==0.15.0\n",
    "gunicorn==20.1.0\n",
    "fastapi==0.68.1\n",
    "google-cloud-aiplatform\n",
    "merlin-models\n",
    "nvtabular\n",
    "gcsfs\n",
    "google-cloud-storage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e608f690-037f-4607-87cd-5ba72bd3dd16",
   "metadata": {},
   "source": [
    "### Predictor module and class\n",
    "This is an adaptation of the CPR examples\n",
    "\n",
    "This was locally tested and created shortly after model creation in the [tensorflow-predict](tensorflow-predict.ipynb) notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e53bfc90-6ab4-40f0-9c06-53a8620eea07",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing app/dataset_to_tensors.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app/dataset_to_tensors.py\n",
    "\n",
    "try:\n",
    "    import cudf\n",
    "except ImportError:\n",
    "    cudf = None\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from typing import Dict\n",
    "from merlin.io import Dataset\n",
    "import itertools\n",
    "\n",
    "\n",
    "def cupy_array_to_tensor(array):\n",
    "    return tf.experimental.dlpack.from_dlpack(array.reshape(-1, 1).toDlpack())\n",
    "\n",
    "def numpy_array_to_tensor(array):\n",
    "    return tf.convert_to_tensor(array.reshape(-1, 1))\n",
    "\n",
    "def cudf_series_to_tensor(col) -> tf.Tensor:\n",
    "    \"Convert a cudf.Series to a TensorFlow Tensor with DLPack\"\n",
    "    if isinstance(col.dtype, cudf.ListDtype):\n",
    "        values = col.list.leaves.values\n",
    "        offsets = col.list._column.offsets.values\n",
    "        row_lengths = offsets[1:] - offsets[:-1]\n",
    "        return cupy_array_to_tensor(values), cupy_array_to_tensor(row_lengths)\n",
    "    else:\n",
    "        return cupy_array_to_tensor(col.values)\n",
    "\n",
    "def pandas_series_to_tensor(col) -> tf.Tensor:\n",
    "    if len(col) and pd.api.types.is_list_like(col.values[0]):\n",
    "        values = pd.Series(itertools.chain(*col)).values\n",
    "        row_lengths = col.map(len).values\n",
    "        return numpy_array_to_tensor(values), numpy_array_to_tensor(row_lengths)\n",
    "    else:\n",
    "        return numpy_array_to_tensor(col.values)\n",
    "        \n",
    "    \n",
    "def dataset_to_tensors(dataset: Dataset) -> Dict[str, tf.Tensor]:\n",
    "    \"\"\"Convert a DataFrame to Dict of Tensors\"\"\"\n",
    "    df = dataset.to_ddf().compute()\n",
    "    if isinstance(df, pd.DataFrame):\n",
    "        col_to_tensor = pandas_series_to_tensor\n",
    "    else:\n",
    "        col_to_tensor = cudf_series_to_tensor\n",
    "    return {\n",
    "        column: col_to_tensor(df[column])\n",
    "        for column in df.columns\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "badee042-ed21-406a-9cb5-e60d726d38a1",
   "metadata": {},
   "source": [
    "#### Get the workflow if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "37dc50cb-9dd7-4d43-ab2f-2aea66d9cba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘workflow’: File exists\n",
      "Copying gs://spotify-jsw-mpd-2023/workflow/2t-spotify-workflow/metadata.json...\n",
      "Copying gs://spotify-jsw-mpd-2023/workflow/2t-spotify-workflow/workflow.pkl...  \n",
      "Copying gs://spotify-jsw-mpd-2023/workflow/2t-spotify-workflow/categories/unique.album_name_pl.parquet...\n",
      "Copying gs://spotify-jsw-mpd-2023/workflow/2t-spotify-workflow/categories/unique.artist_genres_can.parquet...\n",
      "/ [4 files][ 16.8 MiB/ 16.8 MiB]                                                \n",
      "==> NOTE: You are performing a sequence of gsutil operations that may\n",
      "run significantly faster if you instead use gsutil -m cp ... Please\n",
      "see the -m section under \"gsutil help options\" for further information\n",
      "about when gsutil -m can be advantageous.\n",
      "\n",
      "Copying gs://spotify-jsw-mpd-2023/workflow/2t-spotify-workflow/categories/unique.artist_genres_pl.parquet...\n",
      "Copying gs://spotify-jsw-mpd-2023/workflow/2t-spotify-workflow/categories/unique.artist_name_can.parquet...\n",
      "Copying gs://spotify-jsw-mpd-2023/workflow/2t-spotify-workflow/categories/unique.artist_name_pl.parquet...\n",
      "Copying gs://spotify-jsw-mpd-2023/workflow/2t-spotify-workflow/categories/unique.pid.parquet...\n",
      "Copying gs://spotify-jsw-mpd-2023/workflow/2t-spotify-workflow/categories/unique.pl_collaborative_src.parquet...\n",
      "Copying gs://spotify-jsw-mpd-2023/workflow/2t-spotify-workflow/categories/unique.pl_name_src.parquet...\n",
      "Copying gs://spotify-jsw-mpd-2023/workflow/2t-spotify-workflow/categories/unique.track_name_can.parquet...\n",
      "Copying gs://spotify-jsw-mpd-2023/workflow/2t-spotify-workflow/categories/unique.track_name_pl.parquet...\n",
      "Copying gs://spotify-jsw-mpd-2023/workflow/2t-spotify-workflow/categories/unique.track_uri_can.parquet...\n",
      "Copying gs://spotify-jsw-mpd-2023/workflow/2t-spotify-workflow/categories/unique.track_uri_pl.parquet...\n",
      "| [14 files][283.5 MiB/283.5 MiB]                                               \n",
      "Operation completed over 14 objects/283.5 MiB.                                   \n"
     ]
    }
   ],
   "source": [
    "!mkdir workflow\n",
    "!gsutil cp -r gs://spotify-jsw-mpd-2023/workflow/2t-spotify-workflow/* workflow/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3d80c477-d403-4d39-9220-98d90511a180",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing app/predictor.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app/predictor.py\n",
    "# These are helper functions that ensure the dictionary input is in a certain order and types are preserved\n",
    "# this is to get scalar values to appear first in the dict to not confuse pandas with lists https://github.com/pandas-dev/pandas/issues/46092\n",
    "import nvtabular as nvt\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from typing import Dict\n",
    "\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import logging\n",
    "from merlin.models.tf.loader import Loader\n",
    "from dataset_to_tensors import *\n",
    "from merlin.table.tensor_table import TensorTable\n",
    "from merlin.table.tensorflow_column import TensorflowColumn\n",
    "\n",
    "from merlin.table.conversions import convert_col, df_from_tensor_table, tensor_table_from_df\n",
    "from merlin.table import TensorTable, df_from_tensor_table\n",
    "from merlin.core.dispatch import df_from_dict, dict_from_df\n",
    "\n",
    "reordered_keys = [\n",
    "    'pl_collaborative_src', \n",
    "    'album_name_pl', \n",
    "    'artist_genres_pl', \n",
    "    'artist_name_pl', \n",
    "    'artist_pop_can', \n",
    "    'num_pl_songs_new', \n",
    "    'pl_name_src', \n",
    "    'num_pl_albums_new', \n",
    "    'num_pl_artists_new', \n",
    "    'track_name_pl', \n",
    "    'track_pop_can',\n",
    "    'artist_pop_can',\n",
    "    'artist_followers_can'\n",
    "    'pl_duration_ms_new', \n",
    "    'pid', \n",
    "    'track_uri_pl'\n",
    "]\n",
    "\n",
    "float_num_fix = ['n_songs_pl','num_albums_pl','num_artists_pl','duration_ms_seed_pl']\n",
    "float_list_fix = ['track_pop_pl', 'duration_ms_songs_pl']\n",
    "    \n",
    "def fix_list_num_dtypes(num_list):\n",
    "    \"this fixes lists of ints to list of floats converted in json input\"\n",
    "    return [float(x) for x in num_list]\n",
    "\n",
    "def fix_num_dtypes(num):\n",
    "    \"this fixes ints and casts to floats\"\n",
    "    return float(num)\n",
    "\n",
    "def fix_types(k, v):\n",
    "    if k in float_num_fix:\n",
    "        return fix_num_dtypes(v)\n",
    "    if k in float_list_fix:\n",
    "        return fix_list_num_dtypes(v)\n",
    "    else:\n",
    "        return v\n",
    "\n",
    "def create_pandas_instance(inputs):\n",
    "    \"\"\"\n",
    "    Helper function to reorder the input to have a sclar first for pandas\n",
    "    And fix the types converted when data is imported by fastAPI\n",
    "    \"\"\"\n",
    "    if type(inputs) == list:\n",
    "        header = inputs[0]\n",
    "        reordered_header_dict = {k: fix_types(k,header[k]) for k in reordered_keys}\n",
    "        pandas_instance = pd.DataFrame.from_dict(reordered_header_dict, orient='index').T\n",
    "        if len(inputs) > 1:\n",
    "            for ti in inputs[1:]:\n",
    "                reordered_dict = {k: fix_types(k,ti[k]) for k in reordered_keys}\n",
    "                pandas_instance = pandas_instance.append(pd.DataFrame.from_dict(reordered_dict, orient='index').T)\n",
    "    else:\n",
    "        reordered_dict = {k: fix_types(k,inputs[k]) for k in reordered_keys}\n",
    "        pandas_instance = pd.DataFrame.from_dict(reordered_dict, orient='index').T\n",
    "    return pandas_instance\n",
    "\n",
    "\n",
    "def table_to_tensors(table: TensorTable, schema=None) -> Dict[str, tf.Tensor]:\n",
    "    \n",
    "    for col in table.columns:\n",
    "        if schema and not schema[col].is_ragged:\n",
    "            values = table[col].values\n",
    "            offsets = table[col].offsets\n",
    "            if offsets is None:\n",
    "                continue\n",
    "\n",
    "            row_lengths = offsets[1:] - offsets[:-1]\n",
    "            if not all(row_lengths == row_lengths[0]):\n",
    "                raise ValueError(\n",
    "                        f\"ColumnSchema for list column '{col}' describes a fixed size list. \"\n",
    "                        \"Found a ragged list output. If this dataframe contains a ragged list, \"\n",
    "                        \"Please check the 'schema' has a column shape defined to reflect this. \"\n",
    "                    )\n",
    "            values_list = values.reshape((len(row_lengths), int(row_lengths[0])) + values.shape[1:])\n",
    "            column_type =type(table[col])\n",
    "            table[col] = column_type(values_list)\n",
    "\n",
    "    \n",
    "    # convert columns from array to TensorFlow types\n",
    "    for column in table.columns:\n",
    "        table[column] = convert_col(table[column], TensorflowColumn)\n",
    "\n",
    "    # convert TensorTable to dictionary\n",
    "    return table.to_dict()\n",
    "    \n",
    "\n",
    "\n",
    "class Predictor():\n",
    "    \"\"\"Interface of the Predictor class for Custom Prediction Routines.\n",
    "    The Predictor is responsible for the ML logic for processing a prediction request.\n",
    "    Specifically, the Predictor must define:\n",
    "    (1) How to load all model artifacts used during prediction into memory.\n",
    "    (2) The logic that should be executed at predict time.\n",
    "    When using the default PredictionHandler, the Predictor will be invoked as follows:\n",
    "      predictor.postprocess(predictor.predict(predictor.preprocess(prediction_input)))\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        return\n",
    "    \n",
    "    def load(self, artifacts_uri):\n",
    "        \"\"\"Loads the model artifact.\n",
    "        Args:\n",
    "            artifacts_uri (str):\n",
    "                Required. The value of the environment variable AIP_STORAGE_URI.\n",
    "        \"\"\"\n",
    "        logging.info(\"loading model and workflow\")\n",
    "        start_init = time.process_time()\n",
    "        \n",
    "        #test_bucket = 'gs://jt-merlin-scaling'\n",
    "        test_bucket = '/workspace/google'\n",
    "        self.model = tf.keras.models.load_model(os.path.join(artifacts_uri, \"query-tower\"))\n",
    "        # self.workflow = nvt.Workflow.load(os.path.join(artifacts_uri, \"workflow/2t-spotify-workflow\")) # TODO: parameterize\n",
    "        self.workflow = nvt.Workflow.load(os.path.join(test_bucket, \"train_transformed\"))\n",
    "        # self.workflow = nvt.Workflow.load('gs://jt-merlin-scaling/nvt-last5-v1full/nvt-analyzed') # TODO: parametrize\n",
    "        self.workflow = self.workflow.remove_inputs(\n",
    "            [\n",
    "                'track_pop_can', \n",
    "                'track_uri_can', \n",
    "                'duration_ms_can', \n",
    "                'track_name_can', \n",
    "                'artist_name_can',\n",
    "                'album_name_can',\n",
    "                'album_uri_can',\n",
    "                'artist_followers_can', \n",
    "                'artist_genres_can',\n",
    "                'artist_name_can', \n",
    "                'artist_pop_can',\n",
    "                'artist_pop_pl',\n",
    "                'artist_uri_can', \n",
    "                'artists_followers_pl'\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.loader = None # will load this after first load\n",
    "        self.n_rows = 0\n",
    "        #logging.info(f\"loading took {time.process_time() - start_init} seconds\")\n",
    "        print(f\"loading took {time.process_time() - start_init} seconds\")\n",
    "        \n",
    "        return self\n",
    "        \n",
    "    def predict(self, prediction_input):\n",
    "        \"\"\"Preprocesses the prediction input before doing the prediction.\n",
    "        Args:\n",
    "            prediction_input (Any):\n",
    "                Required. The prediction input that needs to be preprocessed.\n",
    "        Returns:\n",
    "            The preprocessed prediction input.\n",
    "        \"\"\"\n",
    "        # handle different input types, can take a dict or list of dicts\n",
    "        self.n_rows = len(prediction_input)\n",
    "        start = time.process_time()\n",
    "        cudf_instance = cudf.from_pandas(create_pandas_instance(prediction_input))\n",
    "        print(f\"Pandas conversion took {time.process_time() - start} seconds\")\n",
    "        start = time.process_time()\n",
    "        transformed_instance = self.workflow.transform(cudf_instance)\n",
    "        print(f\"Workflow transformation took {time.process_time() - start} seconds\")\n",
    "\n",
    "    # def predict(self, instances):\n",
    "        \"\"\"Performs prediction.\n",
    "        Args:\n",
    "            instances (Any):\n",
    "                Required. The instance(s) used for performing prediction.\n",
    "        Returns:\n",
    "            Prediction results.\n",
    "        \"\"\"  \n",
    "        start = time.process_time()\n",
    "\n",
    "        table = TensorTable.from_df(transformed_instance)\n",
    "        batch = table_to_tensors(table, schema=user_schema)\n",
    "        print(f\"converting to dict_tensors took {time.process_time() - start} seconds\")\n",
    "        start = time.process_time()\n",
    "        output = self.model(batch)\n",
    "        print(f\"Generating query embeddings took {time.process_time() - start} seconds\")\n",
    "        return transformed_instance, output, batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2b9a8771-cae3-494d-a56b-060616889463",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing app/main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app/main.py\n",
    "from fastapi import FastAPI, Request\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import logging\n",
    "\n",
    "\n",
    "from google.cloud import storage\n",
    "from predictor import Predictor\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "predictor_instance = Predictor()\n",
    "loaded_predictor = predictor_instance.load(artifacts_uri = os.environ['AIP_STORAGE_URI'])\n",
    "\n",
    "@app.get(os.environ['AIP_HEALTH_ROUTE'], status_code=200)\n",
    "def health():\n",
    "    return {}\n",
    "\n",
    "\n",
    "@app.post(os.environ['AIP_PREDICT_ROUTE'])\n",
    "async def predict(request: Request):\n",
    "    body = await request.json()\n",
    "    instances = body[\"instances\"]\n",
    "    outputs = loaded_predictor.predict(instances)\n",
    "\n",
    "    return {\"predictions\": outputs[1].numpy().tolist()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2dfde697-cec3-470c-9c6d-3be275ce6b55",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing app/prestart.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile app/prestart.sh\n",
    "#!/bin/bash\n",
    "export PORT=$AIP_HTTP_PORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "970ed68e-37f6-406c-9615-3aab9570daeb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#make it a package\n",
    "!touch app/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0cb65bef-d269-4f80-a40b-e0b43f844c2a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile Dockerfile\n",
    "\n",
    "FROM nvcr.io/nvidia/merlin/merlin-tensorflow:23.04\n",
    "WORKDIR /app \n",
    "\n",
    "COPY ./app/requirements.txt /requirements.txt\n",
    "RUN pip install -r /requirements.txt\n",
    "#DEBUG CHANGES!!\n",
    "RUN mkdir /workflow\n",
    "# RUN mkdir /docker_model\n",
    "# ADD local_model /docker_model\n",
    "ADD workflow /workflow\n",
    "#END DEBUG!\n",
    "\n",
    "\n",
    "COPY ./app /app\n",
    "EXPOSE 80\n",
    "    \n",
    "CMD [\"sh\", \"-c\", \"uvicorn main:app --host 0.0.0.0 --port $AIP_HTTP_PORT\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "22af2064-0460-4dd5-b352-180f33fbdd1c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 30 file(s) totalling 566.9 MiB before compression.\n",
      "^C\n",
      "\n",
      "\n",
      "Command killed by keyboard interrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "SERVER_IMAGE = \"merlin-prediction-cpr\"  # @param {type:\"string\"} \n",
    "REMOTE_IMAGE_NAME=f\"{REGION}-docker.pkg.dev/{PROJECT}/{REPOSITORY}/{SERVER_IMAGE}\"\n",
    "\n",
    "# !docker build -t $REMOTE_IMAGE_NAME .\n",
    "!gcloud builds submit -t $REMOTE_IMAGE_NAME ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3db156d-4675-43cc-ac8f-305d5fc79230",
   "metadata": {},
   "source": [
    "#### If you are debugging, be sure to set `-d` detached flag off and run the commands in console"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4b2c6d-add5-4753-9069-8499b09ba5c6",
   "metadata": {},
   "source": [
    "us-central1-docker.pkg.dev/wortz-project-352116/merlin-spotify-cpr/merlin-prediction-cp### Copy/paste if you want to run from console for testing\n",
    "```python\n",
    "docker run --gpus all -p 80:8080 \\\n",
    "            --name=merlin-prediction-cpr \\\n",
    "            -e AIP_HTTP_PORT=8080 \\\n",
    "            -e AIP_HEALTH_ROUTE=/health \\\n",
    "            -e AIP_PREDICT_ROUTE=/predict \\\n",
    "            -e AIP_STORAGE_URI=gs://spotify-jsw-mpd-2023 \\\n",
    "            us-central1-docker.pkg.dev/wortz-project-352116/merlin-spotify-cpr/merlin-prediction-cpr\n",
    "```\n",
    "\n",
    "##### No GPU:\n",
    "```python\n",
    "docker run -p 80:8080 \\\n",
    "            --name=merlin-prediction-cpr \\\n",
    "            -e AIP_HTTP_PORT=8080 \\\n",
    "            -e AIP_HEALTH_ROUTE=/health \\\n",
    "            -e AIP_PREDICT_ROUTE=/predict \\\n",
    "            -e AIP_STORAGE_URI=gs://spotify-jsw-mpd-2023 \\\n",
    "            us-central1-docker.pkg.dev/wortz-project-352116/merlin-spotify-cpr/merlin-prediction-cpr\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c53532f-5b5a-4f70-9172-27347571c812",
   "metadata": {},
   "source": [
    "#### Test the health route"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ee770509-65a7-45cb-b860-faa73142cefc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Error response from daemon: You cannot remove a running container 0ec8bd5a4f81fcdb74398d428b1f0777c2c4afed8608d38c1ed6fb038824a967. Stop the container before attempting removal or force remove\n"
     ]
    }
   ],
   "source": [
    "!docker kill merlin-prediction-cpr\n",
    "!docker rm merlin-prediction-cpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "ae9f6a29-7e69-4bf6-bb22-926578ff1181",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}"
     ]
    }
   ],
   "source": [
    "! curl localhost/health"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "fae134d5-92da-48b5-afdb-7a330c88ace0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ground truth candidate:\n",
    "    # 'album_uri_can': 'spotify:album:5l83t3mbVgCrIe1VU9uJZR', \n",
    "    # 'artist_name_can': 'Russ', \n",
    "    # 'track_name_can': 'We Just Havent Met Yet', \n",
    "## TODO - we have to overload with candidate data because of the workflow transform, add overloaded values in the predictor\n",
    "TEST_INSTANCE = {\n",
    "  \"pl_collaborative_src\": \"false\",\n",
    "  \"album_name_pl\": [\"Kind Of Blue (Legacy Edition)\", \"Duke Ellington \\u0026 John Coltrane\", \"The Genius Of Charlie Parker #2: April In Paris\", \"Good Night, And Good Luck\", \"Question and Answer\"],\n",
    "  \"artist_genres_pl\": [\"[\\u0027cool jazz\\u0027, \\u0027hard bop\\u0027, \\u0027jazz\\u0027, \\u0027jazz fusion\\u0027, \\u0027jazz trumpet\\u0027, \\u0027uk contemporary jazz\\u0027]\", \"[\\u0027african-american classical\\u0027, \\u0027big band\\u0027, \\u0027harlem renaissance\\u0027, \\u0027jazz\\u0027, \\u0027jazz piano\\u0027, \\u0027swing\\u0027]\", \"[\\u0027bebop\\u0027, \\u0027big band\\u0027, \\u0027cool jazz\\u0027, \\u0027jazz\\u0027, \\u0027jazz saxophone\\u0027]\", \"[\\u0027contemporary vocal jazz\\u0027, \\u0027vocal jazz\\u0027]\", \"[\\u0027avant-garde jazz\\u0027, \\u0027contemporary jazz\\u0027, \\u0027contemporary post-bop\\u0027, \\u0027jazz\\u0027, \\u0027jazz fusion\\u0027, \\u0027jazz guitar\\u0027]\"],\n",
    "  \"artist_name_pl\": [\"Miles Davis\", \"Duke Ellington\", \"Charlie Parker\", \"Dianne Reeves\", \"Pat Metheny\"],\n",
    "  \"artist_pop_can\": \"53.0\",\n",
    "  \"num_pl_songs_new\": \"12.0\",\n",
    "  \"pl_name_src\": \"Jazz Standards\",\n",
    "  \"num_pl_albums_new\": \"11.0\",\n",
    "  \"num_pl_artists_new\": \"7.0\",\n",
    "  \"track_name_pl\": [\"Blue in Green\", \"In A Sentimental Mood\", \"April In Paris\", \"How High The Moon\", \"All the Things You Are\"],\n",
    "  \"track_pop_can\": \"22.0\",\n",
    "  \"artist_pop_can_1\": \"53.0\",\n",
    "  \"artist_followers_can\": \"313024.0\",\n",
    "  \"pl_duration_ms_new\": \"2968084.0\",\n",
    "  \"pid\": \"476475\",\n",
    "  \"track_uri_pl\": [\"spotify:track:0aWMVrwxPNYkKmFthzmpRi\", \"spotify:track:0PrGgNDwfJPNXADJYROvBw\", \"spotify:track:5rPMbUxXRXvWu89k0n6Sxj\", \"spotify:track:3DRL2sPYVbx87ArfP2TBqD\", \"spotify:track:7rYSSGZShi5Zgde60MQAMx\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "4002745d-cbcc-469a-91a2-f4ec14d41e45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 'Lit Tunes ',\n",
       " 'false',\n",
       " 237506.0,\n",
       " ['Russ', 'Jeremih', 'Khalid', 'BeyoncÃ©', 'William Singe'],\n",
       " ['spotify:track:4cxMGhkinTocPSVVKWIw0d',\n",
       "  'spotify:track:1wNEBPo3nsbGCZRryI832I',\n",
       "  'spotify:track:152lZdxL1OR0ZMW6KquMif',\n",
       "  'spotify:track:2f4IuijXLxYOeBncS60GUD',\n",
       "  'spotify:track:4Lj8paMFwyKTGfILLELVxt'],\n",
       " ['Losin Control', 'Paradise', 'Location', 'Crazy In Love - Remix', 'Pony'],\n",
       " [\"There's Really A Wolf\",\n",
       "  'Late Nights: The Album',\n",
       "  'American Teen',\n",
       "  'Crazy In Love',\n",
       "  'Pony']]"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test with an ordered list input (no dictionary)\n",
    "\n",
    "TEST_INSTANCE_LIST = [TEST_INSTANCE[k] for k in TEST_INSTANCE.keys()]\n",
    "TEST_INSTANCE_LIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "acde41b9-5025-4d57-80a9-b9f0213c2e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"instances\": [{\"pid\": 1, \"pl_name_src\": \"Lit Tunes \", \"pl_collaborative_src\": \"false\", \"pl_duration_ms_new\": 237506.0, \"artist_name_pl\": [\"Russ\", \"Jeremih\", \"Khalid\", \"Beyonc\\u00c3\\u00a9\", \"William Singe\"], \"track_uri_pl\": [\"spotify:track:4cxMGhkinTocPSVVKWIw0d\", \"spotify:track:1wNEBPo3nsbGCZRryI832I\", \"spotify:track:152lZdxL1OR0ZMW6KquMif\", \"spotify:track:2f4IuijXLxYOeBncS60GUD\", \"spotify:track:4Lj8paMFwyKTGfILLELVxt\"], \"track_name_pl\": [\"Losin Control\", \"Paradise\", \"Location\", \"Crazy In Love - Remix\", \"Pony\"], \"album_name_pl\": [\"There's Really A Wolf\", \"Late Nights: The Album\", \"American Teen\", \"Crazy In Love\", \"Pony\"]}]}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "json_instance = json.dumps({\"instances\": [TEST_INSTANCE]})\n",
    "print(json_instance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caea4b39-7a0c-4417-afc9-72681f24a910",
   "metadata": {},
   "source": [
    "### Test the predict route"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "68c98f55-2f05-497d-b129-86b3c33a935c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.94 ms, sys: 2.24 ms, total: 5.18 ms\n",
      "Wall time: 427 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'predictions': [[0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   2.197338342666626,\n",
       "   0.0,\n",
       "   0.6343151926994324,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.8681452870368958,\n",
       "   0.082596555352211,\n",
       "   0.06858403980731964,\n",
       "   0.8690858483314514,\n",
       "   0.6754541397094727,\n",
       "   0.04301293566823006,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.11446070671081543,\n",
       "   3.286738872528076,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   2.3397533893585205,\n",
       "   0.0,\n",
       "   1.9221959114074707,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.4394824206829071,\n",
       "   0.0,\n",
       "   0.8329581618309021,\n",
       "   0.0,\n",
       "   0.4763728380203247,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   1.2817498445510864,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   2.144164800643921,\n",
       "   0.0,\n",
       "   0.715079665184021,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.4205787777900696,\n",
       "   0.0,\n",
       "   0.9302131533622742,\n",
       "   1.045517086982727,\n",
       "   0.0,\n",
       "   0.4166524410247803,\n",
       "   0.0,\n",
       "   1.2028098106384277,\n",
       "   0.0,\n",
       "   0.03395405411720276,\n",
       "   0.4649958908557892,\n",
       "   2.633600950241089,\n",
       "   0.8353680372238159,\n",
       "   0.0,\n",
       "   0.6172369122505188,\n",
       "   0.014025628566741943,\n",
       "   0.04429948329925537,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   2.771533966064453,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   1.7489553689956665,\n",
       "   0.0,\n",
       "   1.3139915466308594,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.37883666157722473,\n",
       "   0.31884539127349854,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.041849423199892044,\n",
       "   1.2340717315673828,\n",
       "   0.8302457332611084,\n",
       "   0.45156005024909973,\n",
       "   0.36637669801712036,\n",
       "   0.18876822292804718,\n",
       "   1.4170316457748413,\n",
       "   0.5635349154472351,\n",
       "   1.227432370185852,\n",
       "   0.0,\n",
       "   1.833836555480957,\n",
       "   0.6638538241386414,\n",
       "   0.4477311670780182,\n",
       "   0.0,\n",
       "   2.194976329803467,\n",
       "   0.8841541409492493,\n",
       "   2.037673234939575,\n",
       "   3.3430445194244385,\n",
       "   3.2102982997894287,\n",
       "   0.0,\n",
       "   1.2679942846298218,\n",
       "   2.5830116271972656,\n",
       "   0.6817995309829712,\n",
       "   1.1135557889938354,\n",
       "   0.0,\n",
       "   0.5490128397941589,\n",
       "   0.878645658493042,\n",
       "   0.9193580746650696,\n",
       "   0.042682550847530365,\n",
       "   0.0,\n",
       "   1.2845640182495117,\n",
       "   0.16097167134284973,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   1.092848300933838,\n",
       "   0.32491201162338257,\n",
       "   0.0,\n",
       "   0.9054877758026123,\n",
       "   0.7531888484954834,\n",
       "   1.8477554321289062,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.5464975833892822,\n",
       "   2.130723714828491,\n",
       "   0.627360463142395,\n",
       "   1.4092442989349365,\n",
       "   1.9500364065170288]]}"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "requests.post('http://localhost/predict', data=json_instance).json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b95e4a-2bf2-4659-9dbe-435571bd6b13",
   "metadata": {},
   "source": [
    "## Stop the images if they are running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "e52cf577-dcb3-4534-87a1-5acb460d9302",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merlin-prediction-cpr\n",
      "merlin-prediction-cpr\n"
     ]
    }
   ],
   "source": [
    "! docker stop $SERVER_IMAGE\n",
    "! docker rm $SERVER_IMAGE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd45707-2c87-4148-bf69-25feb40a74a3",
   "metadata": {},
   "source": [
    "### Push the container once ready and testing is complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "b9cf27ed-0c42-4cee-8116-44962bb57c13",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default tag: latest\n",
      "The push refers to repository [us-central1-docker.pkg.dev/wortz-project-352116/merlin-spotify-cpr/merlin-prediction-cpr]\n",
      "\n",
      "\u001b[1Bba9615c9: Preparing \n",
      "\u001b[1B018f1149: Preparing \n",
      "\u001b[1B0f3c1a79: Preparing \n",
      "\u001b[1B0a88f4b4: Preparing \n",
      "\u001b[1B625d9539: Preparing \n",
      "\u001b[1Bbd45f47a: Preparing \n",
      "\u001b[1B959e4900: Preparing \n",
      "\u001b[1Bed4c0b88: Preparing \n",
      "\u001b[1B435e7569: Preparing \n",
      "\u001b[1B6c9c8a80: Preparing \n",
      "\u001b[1B7ee6b1e6: Preparing \n",
      "\u001b[1B14522996: Preparing \n",
      "\u001b[1B02fe8fdf: Preparing \n",
      "\u001b[1Bf1d8a22e: Preparing \n",
      "\u001b[1B7f811af3: Preparing \n",
      "\u001b[1Bfa2a6f77: Preparing \n",
      "\u001b[1B99f75613: Preparing \n",
      "\u001b[1Bf314ccd3: Preparing \n",
      "\u001b[1Bbe9e41cc: Preparing \n",
      "\u001b[1B042e77d8: Preparing \n",
      "\u001b[1Bb1adf810: Preparing \n",
      "\u001b[1B5d6e3dc6: Preparing \n",
      "\u001b[1B5dfe406a: Preparing \n",
      "\u001b[1B3f5c8bb1: Preparing \n",
      "\u001b[1B24cd08b1: Preparing \n",
      "\u001b[1Bf287408a: Preparing \n",
      "\u001b[1B5fc4d7d6: Preparing \n",
      "\u001b[1Ba50a1b1f: Preparing \n",
      "\u001b[1B13e97a8a: Preparing \n",
      "\u001b[1Bad3d21ac: Preparing \n",
      "\u001b[1B09a62824: Preparing \n",
      "\u001b[1B9a9dae8e: Preparing \n",
      "\u001b[1Bb713171c: Preparing \n",
      "\u001b[1B1bd92e98: Preparing \n",
      "\u001b[1B3a6772a2: Preparing \n",
      "\u001b[1Bce57a041: Preparing \n",
      "\u001b[1Bd4b0d7a5: Preparing \n",
      "\u001b[1Bed92250c: Preparing \n",
      "\u001b[1B2ec9aaf4: Preparing \n",
      "\u001b[1B48aaa07e: Preparing \n",
      "\u001b[1B84509167: Preparing \n",
      "\u001b[1B0670a88a: Preparing \n",
      "\u001b[1Bc734a213: Preparing \n",
      "\u001b[1Be719fe00: Preparing \n",
      "\u001b[1B048a0911: Preparing \n",
      "\u001b[41Bd45f47a: Waiting g \n",
      "\u001b[1B1162b441: Preparing \n",
      "\u001b[42B59e4900: Waiting g \n",
      "\u001b[1B2f0c2802: Preparing \n",
      "\u001b[43Bd4c0b88: Waiting g \n",
      "\u001b[43B35e7569: Waiting g \n",
      "\u001b[1B81f9e517: Preparing \n",
      "\u001b[1B9c031281: Preparing \n",
      "\u001b[45Bc9c8a80: Waiting g \n",
      "\u001b[45Bee6b1e6: Waiting g \n",
      "\u001b[45B4522996: Waiting g \n",
      "\u001b[1B95e6a4e6: Preparing \n",
      "\u001b[46B2fe8fdf: Waiting g \n",
      "\u001b[46B1d8a22e: Waiting g \n",
      "\u001b[25Be57a041: Waiting g \n",
      "\u001b[47Bf811af3: Waiting g \n",
      "\u001b[47Ba2a6f77: Waiting g \n",
      "\u001b[27B4b0d7a5: Waiting g \n",
      "\u001b[27Bd92250c: Waiting g \n",
      "\u001b[49B9f75613: Waiting g \n",
      "\u001b[28Bec9aaf4: Waiting g \n",
      "\u001b[28B8aaa07e: Waiting g \n",
      "\u001b[1Ba581cc22: Preparing \n",
      "\u001b[29B4509167: Waiting g \n",
      "\u001b[26B48a0911: Waiting g \n",
      "\u001b[52B42e77d8: Waiting g \n",
      "\u001b[31B670a88a: Waiting g \n",
      "\u001b[28Bdfc83c2: Waiting g \n",
      "\u001b[32B734a213: Waiting g \n",
      "\u001b[1Bbf18a086: Preparing \n",
      "\u001b[33B719fe00: Waiting g \n",
      "\u001b[1B71e1a97b: Preparing \n",
      "\u001b[32B162b441: Waiting g \n",
      "\u001b[27Bc031281: Waiting g \n",
      "\u001b[33B32f1f3c: Waiting g \n",
      "\u001b[1Bd1f53982: Preparing \n",
      "\u001b[1B662a799d: Preparing \n",
      "\u001b[35Bf0c2802: Waiting g \n",
      "\u001b[1B365b2088: Preparing \n",
      "\u001b[1Ba552b9cc: Preparing \n",
      "\u001b[1B510b97a5: Preparing \n",
      "\u001b[1B3182f917: Preparing \n",
      "\u001b[1Bc93bdb37: Layer already exists 6kB3A\u001b[2K\u001b[88A\u001b[2K\u001b[75A\u001b[2K\u001b[70A\u001b[2K\u001b[66A\u001b[2K\u001b[63A\u001b[2K\u001b[57A\u001b[2K\u001b[51A\u001b[2K\u001b[44A\u001b[2K\u001b[38A\u001b[2K\u001b[34A\u001b[2K\u001b[27A\u001b[2K\u001b[20A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2Klatest: digest: sha256:bec47ae58e9de003620f86da20961dc47b01e8300974508c31751ac7b3593c50 size: 18792\n"
     ]
    }
   ],
   "source": [
    "# ### push the container to registry\n",
    "!docker push $REMOTE_IMAGE_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a9a707-6965-4b6f-a3fe-7312dfddbf24",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Deploy to Vertex AI\n",
    "\n",
    "After the serving metadata is set below, the model is properly abstracted for use on Vertex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "b166b499-b407-4e3e-99f3-da3f3031a147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Model\n",
      "Create Model backing LRO: projects/679926387543/locations/us-central1/models/3019815282756550656/operations/5879471097606307840\n",
      "Model created. Resource name: projects/679926387543/locations/us-central1/models/3019815282756550656@1\n",
      "To use this Model in another session:\n",
      "model = aiplatform.Model('projects/679926387543/locations/us-central1/models/3019815282756550656@1')\n"
     ]
    }
   ],
   "source": [
    "MODEL_DISPLAY_NAME = \"Merlin Spotify Query Tower Model - Workflow Local V2\"\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "ARTIFACT_URI = BUCKET\n",
    "\n",
    "model = aiplatform.Model.upload(\n",
    "        display_name=MODEL_DISPLAY_NAME,\n",
    "        artifact_uri=ARTIFACT_URI,\n",
    "        serving_container_image_uri=REMOTE_IMAGE_NAME,\n",
    "        serving_container_predict_route='/predict',\n",
    "        serving_container_health_route='/health',\n",
    "        serving_container_command=[\"sh\", \"-c\", \"uvicorn main:app --host 0.0.0.0 --port $AIP_HTTP_PORT\"],\n",
    "        serving_container_args=[\"--gpus all\"],\n",
    "        sync=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "9e80231d-0430-478e-989a-4802adca17ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Endpoint\n",
      "Create Endpoint backing LRO: projects/679926387543/locations/us-central1/endpoints/7469831310459011072/operations/7505270563087056896\n",
      "Endpoint created. Resource name: projects/679926387543/locations/us-central1/endpoints/7469831310459011072\n",
      "To use this Endpoint in another session:\n",
      "endpoint = aiplatform.Endpoint('projects/679926387543/locations/us-central1/endpoints/7469831310459011072')\n",
      "Deploying model to Endpoint : projects/679926387543/locations/us-central1/endpoints/7469831310459011072\n",
      "Deploy Endpoint model backing LRO: projects/679926387543/locations/us-central1/endpoints/7469831310459011072/operations/1230630382253113344\n",
      "Endpoint model deployed. Resource name: projects/679926387543/locations/us-central1/endpoints/7469831310459011072\n"
     ]
    }
   ],
   "source": [
    "endpoint = model.deploy(machine_type=\"n1-standard-8\",\n",
    "                        accelerator_type=\"NVIDIA_TESLA_T4\",\n",
    "                        accelerator_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "b8b8fa58-c587-4b5d-9958-8ed904e0c162",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(predictions=[[0.0, 0.0, 0.0, 2.197338342666626, 0.0, 0.6343151926994324, 0.0, 0.0, 0.8681452870368958, 0.082596555352211, 0.06858403980731964, 0.8690858483314514, 0.6754541397094727, 0.04301293566823006, 0.0, 0.0, 0.0, 0.1144607067108154, 3.286738872528076, 0.0, 0.0, 2.339753389358521, 0.0, 1.922195911407471, 0.0, 0.0, 0.4394824206829071, 0.0, 0.8329581618309021, 0.0, 0.4763728380203247, 0.0, 0.0, 1.281749844551086, 0.0, 0.0, 2.144164800643921, 0.0, 0.715079665184021, 0.0, 0.0, 0.4205787777900696, 0.0, 0.9302131533622742, 1.045517086982727, 0.0, 0.4166524410247803, 0.0, 1.202809810638428, 0.0, 0.03395405411720276, 0.4649958908557892, 2.633600950241089, 0.8353680372238159, 0.0, 0.6172369122505188, 0.01402562856674194, 0.04429948329925537, 0.0, 0.0, 2.771533966064453, 0.0, 0.0, 0.0, 0.0, 1.748955368995667, 0.0, 1.313991546630859, 0.0, 0.0, 0.0, 0.3788366615772247, 0.3188453912734985, 0.0, 0.0, 0.04184942319989204, 1.234071731567383, 0.8302457332611084, 0.4515600502490997, 0.3663766980171204, 0.1887682229280472, 1.417031645774841, 0.5635349154472351, 1.227432370185852, 0.0, 1.833836555480957, 0.6638538241386414, 0.4477311670780182, 0.0, 2.194976329803467, 0.8841541409492493, 2.037673234939575, 3.343044519424438, 3.210298299789429, 0.0, 1.267994284629822, 2.583011627197266, 0.6817995309829712, 1.113555788993835, 0.0, 0.5490128397941589, 0.878645658493042, 0.9193580746650696, 0.04268255084753036, 0.0, 1.284564018249512, 0.1609716713428497, 0.0, 0.0, 1.092848300933838, 0.3249120116233826, 0.0, 0.9054877758026123, 0.7531888484954834, 1.847755432128906, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5464975833892822, 2.130723714828491, 0.627360463142395, 1.409244298934937, 1.950036406517029]], deployed_model_id='1920646702608416768', model_version_id='1', model_resource_name='projects/679926387543/locations/us-central1/models/3019815282756550656', explanations=None)"
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#make a prediction\n",
    "\n",
    "endpoint.predict(instances=[TEST_INSTANCE_LIST])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856c6edc-5687-47ea-871b-ac0b4bee1339",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Finished - now go on to the next notebook to create a [matching engine notebook](03-matching-engine.ipynb) and test out the first end to end recommendation\n",
    "\n",
    "Be sure to use the output of the endpoint logs above to save the endpoint for use in the matching engine notebook\n",
    "\n",
    "e.g.:\n",
    "\n",
    "```python\n",
    "INFO:google.cloud.aiplatform.models:To use this Endpoint in another session:\n",
    "endpoint = aiplatform.Endpoint('projects/934903580331/locations/us-central1/endpoints/494907775848022016')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e38e8d-9053-4c9d-bb54-8d6ea261e5ff",
   "metadata": {},
   "source": [
    "### Bonus content - Batch Predictions\n",
    "\n",
    "![](img/batch-predict-output.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "a3598c97-477a-4bd7-8b4f-bbb82a9b1d1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating BatchPredictionJob\n",
      "BatchPredictionJob created. Resource name: projects/679926387543/locations/us-central1/batchPredictionJobs/2698619761297719296\n",
      "To use this BatchPredictionJob in another session:\n",
      "bpj = aiplatform.BatchPredictionJob('projects/679926387543/locations/us-central1/batchPredictionJobs/2698619761297719296')\n",
      "View Batch Prediction Job:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/batch-predictions/2698619761297719296?project=679926387543\n",
      "BatchPredictionJob projects/679926387543/locations/us-central1/batchPredictionJobs/2698619761297719296 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "BatchPredictionJob projects/679926387543/locations/us-central1/batchPredictionJobs/2698619761297719296 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "BatchPredictionJob projects/679926387543/locations/us-central1/batchPredictionJobs/2698619761297719296 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "BatchPredictionJob projects/679926387543/locations/us-central1/batchPredictionJobs/2698619761297719296 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "BatchPredictionJob projects/679926387543/locations/us-central1/batchPredictionJobs/2698619761297719296 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "BatchPredictionJob projects/679926387543/locations/us-central1/batchPredictionJobs/2698619761297719296 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "BatchPredictionJob projects/679926387543/locations/us-central1/batchPredictionJobs/2698619761297719296 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "BatchPredictionJob projects/679926387543/locations/us-central1/batchPredictionJobs/2698619761297719296 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "BatchPredictionJob projects/679926387543/locations/us-central1/batchPredictionJobs/2698619761297719296 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "BatchPredictionJob projects/679926387543/locations/us-central1/batchPredictionJobs/2698619761297719296 current state:\n",
      "JobState.JOB_STATE_SUCCEEDED\n",
      "BatchPredictionJob run completed. Resource name: projects/679926387543/locations/us-central1/batchPredictionJobs/2698619761297719296\n"
     ]
    }
   ],
   "source": [
    "batch_prediction_job = model.batch_predict(\n",
    "    job_display_name=f\"batch predictions for merlin query tower\",\n",
    "    machine_type=\"n1-standard-4\",\n",
    "    accelerator_type=\"NVIDIA_TESLA_T4\",\n",
    "    accelerator_count=1,\n",
    "    #bigquery as input and output\n",
    "    instances_format=\"bigquery\",\n",
    "    predictions_format=\"bigquery\",\n",
    "    bigquery_source=f'bq://{PROJECT}.spotify_e2e_test.validtion_batch_prediction_test',\n",
    "    bigquery_destination_prefix=f'bq://{PROJECT}.spotify_e2e_test.batch_validation_results',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc8835a-6d97-4458-9922-6dc8a7611939",
   "metadata": {},
   "source": [
    "### Timing analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "d09216e2-08fa-4519-b1f2-d2b9c04149ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "514 ms ± 11.9 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "endpoint.predict(instances=[TEST_INSTANCE])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "79cd6b35-6b3d-492b-9b8e-9f378098a60b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "489 ms ± 11.1 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "endpoint.predict(instances=[TEST_INSTANCE, TEST_INSTANCE])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f9cca042-9d79-4b66-835b-05935101babf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Undeploying Endpoint model: projects/934903580331/locations/us-central1/endpoints/4736674102226452480\n",
      "Undeploy Endpoint model backing LRO: projects/934903580331/locations/us-central1/endpoints/4736674102226452480/operations/6606761755496415232\n",
      "Endpoint model undeployed. Resource name: projects/934903580331/locations/us-central1/endpoints/4736674102226452480\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<google.cloud.aiplatform.models.Endpoint object at 0x7f0d1434ad10> \n",
       "resource name: projects/934903580331/locations/us-central1/endpoints/4736674102226452480"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "endpoint.undeploy_all()"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-11.m108",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-11:m108"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
