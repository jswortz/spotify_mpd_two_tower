{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8757f6-8f36-4b0d-8c37-ba31a1773f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install google-cloud-aiplatform[prediction]>=1.16.0 fastapi nvtabular git+https://github.com/NVIDIA-Merlin/models.git --user"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201e92f9-980b-43e9-8c76-b2aa02260e96",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Sklearn with Pandas - Custom Prediction Routine to get Merlin Model predictions\n",
    "\n",
    "Your output should look like this - you are going to use the query model endpoint to create a CPR Endpoing\n",
    "\n",
    "![](img/merlin-bucket.png)\n",
    "\n",
    "This is similar to [the other notebook](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/ml_ops/stage6/get_started_with_cpr.ipynb) except we will be using pandas and bigquery\n",
    "\n",
    "Topics covered\n",
    "* Training sklearn locally, deploying to endpoint\n",
    "* Saving data as CSV and doing batch predict from GCS\n",
    "* Loading data to BQ, using BQ magics\n",
    "* Running a batch prediction from BQ to BQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b65e26b-31d7-459e-8f80-92e2c206fc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !gsutil mb -l us-central1 gs://wortz-project-bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5075f2f7-c075-4e56-a188-4adfee578a2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !echo y | docker container prune\n",
    "# !echo y | docker image prune"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d045aab2-d485-4bf1-9319-b7afc5cccb34",
   "metadata": {},
   "source": [
    "### Set up repo and configure Docker (one-time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e72acda9-ae76-4ce9-8600-71238d545f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mERROR:\u001b[0m (gcloud.beta.artifacts.repositories.create) Error parsing [repository].\n",
      "The [repository] resource is not properly specified.\n",
      "Failed to find attribute [location]. The attribute can be set in the following ways: \n",
      "- provide the argument `--location` on the command line\n",
      "- set the property `artifacts/location`\n"
     ]
    }
   ],
   "source": [
    "# Create the repo if needed for the artifacts\n",
    "\n",
    "! gcloud beta artifacts repositories create {REPOSITORY} \\\n",
    "    --repository-format=docker \\\n",
    "    --location=$REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1858a130-739b-4752-bf02-d0e8356a0829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;33mWARNING:\u001b[0m Your config file at [/home/jupyter/.docker/config.json] contains these credential helper entries:\n",
      "\n",
      "{\n",
      "  \"credHelpers\": {\n",
      "    \"gcr.io\": \"gcloud\",\n",
      "    \"us.gcr.io\": \"gcloud\",\n",
      "    \"eu.gcr.io\": \"gcloud\",\n",
      "    \"asia.gcr.io\": \"gcloud\",\n",
      "    \"staging-k8s.gcr.io\": \"gcloud\",\n",
      "    \"marketplace.gcr.io\": \"gcloud\"\n",
      "  }\n",
      "}\n",
      "Adding credentials for: us-central1-docker.pkg.dev\n",
      "Docker configuration file updated.\n"
     ]
    }
   ],
   "source": [
    "! gcloud auth configure-docker {REGION}-docker.pkg.dev --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9cbe8a06-ed15-40ac-bc2f-d387bb406301",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "\n",
    "PROJECT = 'hybrid-vertex'  # <--- TODO: CHANGE THIS\n",
    "REGION = 'us-central1' \n",
    "BUCKET = 'gs://spotify-beam-v3'\n",
    "REPOSITORY = 'merlin-spotify-cpr'\n",
    "ARTIFACT_URI = f'{BUCKET}/merlin-processed'\n",
    "MODEL_DIR = f'{BUCKET}/merlin-processed/query_model_merlin'\n",
    "PREFIX = 'merlin-spotify'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2831cdd6-8d18-40aa-88ba-50889c3624a0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# New section - preprocessor creation.\n",
    "\n",
    "In this section we will create a pipeline object that stores a standard scaler \n",
    "using the `PipeLine` class is important as it provides a lot of flexibility and conforms to sklearn's framework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c749d4-18ca-46a0-91f9-432dbf1220e3",
   "metadata": {
    "tags": []
   },
   "source": [
    "appapp## Create a generic sklearn container that returns instances\n",
    "\n",
    "https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/ml_ops/stage6/get_started_with_cpr.ipynb\n",
    "\n",
    "**highly recommend reviewing this notebook first as it breaks down the custom predictor interface**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7bf429ed-e23c-47ac-93da-8a70c4e3e92a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! rm -rf app\n",
    "! mkdir app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4af962a7-3196-4b9a-9c36-3f35fd525b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing app/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile app/requirements.txt\n",
    "uvicorn[standard]==0.15.0\n",
    "gunicorn==20.1.0\n",
    "fastapi==0.68.1\n",
    "git+https://github.com/NVIDIA-Merlin/models.git\n",
    "nvtabular\n",
    "gcsfs\n",
    "google-cloud-storage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0c1627-edf8-4879-b1eb-10f28e19cb7a",
   "metadata": {},
   "source": [
    "### CPR Template from here https://cloud.google.com/vertex-ai/docs/predictions/custom-prediction-routines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d80c477-d403-4d39-9220-98d90511a180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing app/predictor.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app/predictor.py\n",
    "\n",
    "import nvtabular as nvt\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import merlin.models.tf as mm\n",
    "from nvtabular.loader.tf_utils import configure_tensorflow\n",
    "configure_tensorflow()\n",
    "import tensorflow as tf\n",
    "\n",
    "class Predictor():\n",
    "    \"\"\"Interface of the Predictor class for Custom Prediction Routines.\n",
    "    The Predictor is responsible for the ML logic for processing a prediction request.\n",
    "    Specifically, the Predictor must define:\n",
    "    (1) How to load all model artifacts used during prediction into memory.\n",
    "    (2) The logic that should be executed at predict time.\n",
    "    When using the default PredictionHandler, the Predictor will be invoked as follows:\n",
    "      predictor.postprocess(predictor.predict(predictor.preprocess(prediction_input)))\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        return\n",
    "    \n",
    "    def load(self, artifacts_uri):\n",
    "        \"\"\"Loads the model artifact.\n",
    "        Args:\n",
    "            artifacts_uri (str):\n",
    "                Required. The value of the environment variable AIP_STORAGE_URI.\n",
    "        \"\"\"\n",
    "        self._model = tf.keras.models.load_model(os.path.join(artifacts_uri, \"query_model_merlin\" ))\n",
    "        self._workflow = nvt.Workflow.load(os.path.join(artifacts_uri, \"workflow/2t-spotify-workflow\"))\n",
    "        self._workflow.remove_inputs(['track_pop_can', 'track_uri_can', 'duration_ms_can', \n",
    "                                      'track_name_can', 'artist_name_can','album_name_can',\n",
    "                                      'album_uri_can','artist_followers_can', 'artist_genres_can',\n",
    "                                      'artist_name_can', 'artist_pop_can','artist_pop_pl','artist_uri_can', \n",
    "                                      'artists_followers_pl'])  \n",
    "        return self\n",
    "        \n",
    "    def preprocess(self, prediction_input):\n",
    "        \"\"\"Preprocesses the prediction input before doing the prediction.\n",
    "        Args:\n",
    "            prediction_input (Any):\n",
    "                Required. The prediction input that needs to be preprocessed.\n",
    "        Returns:\n",
    "            The preprocessed prediction input.\n",
    "        \"\"\"\n",
    "        #handle different input types, can take a dict or list of dicts\n",
    "        if type(prediction_input) == list:\n",
    "            pandas_instance = pd.DataFrame.from_dict(prediction_input[0], orient='index').T\n",
    "            if len(prediction_input) > 1:\n",
    "                for ti in prediction_input[0:]:\n",
    "                    pandas_instance = pandas_instance.append(pd.DataFrame.from_dict(ti, orient='index').T)\n",
    "        if type(prediction_input) == dict:\n",
    "            pandas_instance = pd.DataFrame.from_dict(prediction_input, orient='index').T\n",
    "        else:\n",
    "            raise Exception(\"Data must be provided as a dict or list of dicts\")\n",
    "\n",
    "        transformed_inputs = nvt.Dataset(pandas_instance)\n",
    "        transformed_instance = self._workflow.transform(transformed_inputs)\n",
    "        return transformed_instance\n",
    "\n",
    "    def predict(self, instances):\n",
    "        \"\"\"Performs prediction.\n",
    "        Args:\n",
    "            instances (Any):\n",
    "                Required. The instance(s) used for performing prediction.\n",
    "        Returns:\n",
    "            Prediction results.\n",
    "        \"\"\"  \n",
    "        \n",
    "        loader = mm.Loader(instances, batch_size=instances.num_rows, shuffle=False)\n",
    "        batch =next(iter(loader))\n",
    "        return self._model(batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b9a8771-cae3-494d-a56b-060616889463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing app/main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app/main.py\n",
    "from fastapi import FastAPI, Request\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from google.cloud import storage\n",
    "from predictor import Predictor\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "predictor_instance = Predictor()\n",
    "loaded_predictor = predictor_instance.load(artifacts_uri = os.environ['AIP_STORAGE_URI'])\n",
    "\n",
    "@app.get(os.environ['AIP_HEALTH_ROUTE'], status_code=200)\n",
    "def health():\n",
    "    return {}\n",
    "\n",
    "\n",
    "@app.post(os.environ['AIP_PREDICT_ROUTE'])\n",
    "async def predict(request: Request):\n",
    "    body = await request.json()\n",
    "\n",
    "    instances = body[\"instances\"]\n",
    "    # inputs = np.asarray(instances)\n",
    "    preprocessed_inputs = loaded_predictor.preprocess(instances)\n",
    "    outputs = loaded_predictor.predict(preprocessed_inputs)\n",
    "\n",
    "    return {\"predictions\": outputs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2dfde697-cec3-470c-9c6d-3be275ce6b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing app/prestart.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile app/prestart.sh\n",
    "#!/bin/bash\n",
    "export PORT=$AIP_HTTP_PORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "970ed68e-37f6-406c-9615-3aab9570daeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make it a package\n",
    "!touch app/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0cb65bef-d269-4f80-a40b-e0b43f844c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile Dockerfile\n",
    "\n",
    "FROM nvcr.io/nvidia/merlin/merlin-tensorflow:latest\n",
    "WORKDIR /app \n",
    "\n",
    "\n",
    "COPY ./app/requirements.txt /requirements.txt\n",
    "RUN pip install -r /requirements.txt\n",
    "\n",
    "COPY ./app /app\n",
    "EXPOSE 80\n",
    "\n",
    "CMD [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8080\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "22af2064-0460-4dd5-b352-180f33fbdd1c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  83.13MB\n",
      "Step 1/7 : FROM nvcr.io/nvidia/merlin/merlin-tensorflow:latest\n",
      "manifest for nvcr.io/nvidia/merlin/merlin-tensorflow:latest not found: manifest unknown: manifest unknown\n"
     ]
    }
   ],
   "source": [
    "SERVER_IMAGE = \"merlin-prediction-cpr\"  # @param {type:\"string\"} \n",
    "REMOTE_IMAGE_NAME=f\"{REGION}-docker.pkg.dev/{PROJECT}/{REPOSITORY}/{SERVER_IMAGE}\"\n",
    "\n",
    "!docker build -t $REMOTE_IMAGE_NAME ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3db156d-4675-43cc-ac8f-305d5fc79230",
   "metadata": {},
   "source": [
    "#### If you are debugging, be sure to set `-d` detached flag off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3f67e5fc-51f0-4d45-80d5-9bc441b11d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merlin-prediction-cpr us-central1-docker.pkg.dev/hybrid-vertex/merlin-spotify-cpr/merlin-prediction-cpr gs://spotify-beam-v3/merlin-processed\n"
     ]
    }
   ],
   "source": [
    "print(SERVER_IMAGE, REMOTE_IMAGE_NAME, ARTIFACT_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4b2c6d-add5-4753-9069-8499b09ba5c6",
   "metadata": {},
   "source": [
    "### Copy/paste if you want to run from console for testing\n",
    "```python\n",
    "docker run --gpus all -p 80:8080 \\\n",
    "            --name=merlin-prediction-cpr \\\n",
    "            -e AIP_HTTP_PORT=8080 \\\n",
    "            -e AIP_HEALTH_ROUTE=/health \\\n",
    "            -e AIP_PREDICT_ROUTE=/predict \\\n",
    "            -e AIP_STORAGE_URI=gs://spotify-beam-v3/merlin-processed \\\n",
    "            us-central1-docker.pkg.dev/hybrid-vertex/merlin-spotify-cpr/merlin-prediction-cpr\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e52cf577-dcb3-4534-87a1-5acb460d9302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merlin-prediction-cpr\n",
      "merlin-prediction-cpr\n",
      "\n",
      "==================================\n",
      "== Triton Inference Server Base ==\n",
      "==================================\n",
      "\n",
      "NVIDIA Release 22.08 (build 42766143)\n",
      "\n",
      "Copyright (c) 2018-2022, NVIDIA CORPORATION & AFFILIATES.  All rights reserved.\n",
      "\n",
      "Various files include modifications (c) NVIDIA CORPORATION & AFFILIATES.  All rights reserved.\n",
      "\n",
      "This container image and its contents are governed by the NVIDIA Deep Learning Container License.\n",
      "By pulling and using the container, you accept the terms and conditions of this license:\n",
      "https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license\n",
      "\n",
      "NOTE: CUDA Forward Compatibility mode ENABLED.\n",
      "  Using CUDA 11.7 driver version 515.65.01 with kernel driver version 510.47.03.\n",
      "  See https://docs.nvidia.com/deploy/cuda-compatibility/ for details.\n",
      "\n",
      "2022-10-25 14:12:21.352290: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-25 14:12:21.354556: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-25 14:12:21.356373: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-25 14:12:21.404185: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-25 14:12:21.405297: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-25 14:12:21.407144: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-25 14:12:21.408955: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-25 14:12:22.930948: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-25 14:12:22.933049: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-25 14:12:22.934903: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-25 14:12:22.936637: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7680 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
      "2022-10-25 14:12:34.487790: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 421813152 exceeds 10% of free system memory.\n",
      "2022-10-25 14:12:34.506249: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 713987200 exceeds 10% of free system memory.\n",
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.12) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
      "/usr/local/lib/python3.8/dist-packages/merlin/schema/tags.py:148: UserWarning: Compound tags like Tags.USER_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.USER: 'user'>, <Tags.ID: 'id'>].\n",
      "  warnings.warn(\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "/usr/local/lib/python3.8/dist-packages/nvtabular/workflow/workflow.py:320: UserWarning: Loading workflow generated with nvtabular version 1.5.0+2.g97abf4ea3 - but we are running nvtabular 1.6.0+1.gc0636dead. This might cause issues\n",
      "  warnings.warn(\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/uvicorn\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/click/core.py\", line 1130, in __call__\n",
      "    return self.main(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/click/core.py\", line 1055, in main\n",
      "    rv = self.invoke(ctx)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/click/core.py\", line 1404, in invoke\n",
      "    return ctx.invoke(self.callback, **ctx.params)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/click/core.py\", line 760, in invoke\n",
      "    return __callback(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/uvicorn/main.py\", line 425, in main\n",
      "    run(app, **kwargs)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/uvicorn/main.py\", line 447, in run\n",
      "    server.run()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/uvicorn/server.py\", line 68, in run\n",
      "    return asyncio.run(self.serve(sockets=sockets))\n",
      "  File \"/usr/lib/python3.8/asyncio/runners.py\", line 44, in run\n",
      "    return loop.run_until_complete(main)\n",
      "  File \"uvloop/loop.pyx\", line 1517, in uvloop.loop.Loop.run_until_complete\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/uvicorn/server.py\", line 76, in serve\n",
      "    config.load()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/uvicorn/config.py\", line 448, in load\n",
      "    self.loaded_app = import_from_string(self.app)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/uvicorn/importer.py\", line 21, in import_from_string\n",
      "    module = importlib.import_module(module_str)\n",
      "  File \"/usr/lib/python3.8/importlib/__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 1014, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 671, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 848, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"/app/./main.py\", line 13, in <module>\n",
      "    loaded_predictor = predictor_instance.load(artifacts_uri = os.environ['AIP_STORAGE_URI'])\n",
      "  File \"/app/./predictor.py\", line 31, in load\n",
      "    self._workflow.remove_inputs(['track_pop_can', 'track_uri_can', 'duration_ms_can', \n",
      "  File \"/usr/local/lib/python3.8/dist-packages/nvtabular/workflow/workflow.py\", line 156, in remove_inputs\n",
      "    self.graph.remove_inputs(input_cols)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/merlin/dag/graph.py\", line 153, in remove_inputs\n",
      "    output_columns_to_remove = node.remove_inputs(columns_to_remove)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/merlin/dag/node.py\", line 397, in remove_inputs\n",
      "    removed_outputs = _derived_output_cols(input_cols, self.column_mapping)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/merlin/dag/node.py\", line 456, in column_mapping\n",
      "    selector = self.selector or ColumnSelector(self.input_schema.column_names)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/merlin/dag/selector.py\", line 151, in __bool__\n",
      "    return bool(self.all or self._names or self.subgroups or self.tags)\n",
      "AttributeError: 'ColumnSelector' object has no attribute 'all'\n"
     ]
    }
   ],
   "source": [
    "! docker stop $SERVER_IMAGE\n",
    "! docker rm $SERVER_IMAGE\n",
    "! docker run --gpus all -p 80:8080 \\\n",
    "            --name=$SERVER_IMAGE \\\n",
    "            -e AIP_HTTP_PORT=8080 \\\n",
    "            -e AIP_HEALTH_ROUTE=/health \\\n",
    "            -e AIP_PREDICT_ROUTE=/predict \\\n",
    "            -e AIP_STORAGE_URI=$ARTIFACT_URI \\\n",
    "            $REMOTE_IMAGE_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9f6a29-7e69-4bf6-bb22-926578ff1181",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! curl localhost/health"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae134d5-92da-48b5-afdb-7a330c88ace0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ground truth candidate:\n",
    "    # 'album_uri_can': 'spotify:album:5l83t3mbVgCrIe1VU9uJZR', \n",
    "    # 'artist_name_can': 'Russ', \n",
    "    # 'track_name_can': 'We Just Havent Met Yet', \n",
    "\n",
    "TEST_INSTANCE = {'collaborative': 'false',\n",
    "                 'album_name_pl': [\"There's Really A Wolf\", 'Late Nights: The Album',\n",
    "                       'American Teen', 'Crazy In Love', 'Pony'], \n",
    "                 'artist_genres_pl': [\"'hawaiian hip hop', 'rap'\",\n",
    "                       \"'chicago rap', 'dance pop', 'pop', 'pop rap', 'r&b', 'southern hip hop', 'trap', 'urban contemporary'\",\n",
    "                       \"'pop', 'pop r&b'\", \"'dance pop', 'pop', 'r&b'\",\n",
    "                       \"'chill r&b', 'pop', 'pop r&b', 'r&b', 'urban contemporary'\"], \n",
    "                 'artist_name_pl': ['Russ', 'Jeremih', 'Khalid', 'Beyonc\\xc3\\xa9',\n",
    "                       'William Singe'], \n",
    "                 'description_pl': '', \n",
    "                 'n_songs_pl': 8.0, \n",
    "                 'name': 'Lit Tunes ', \n",
    "                 'num_albums_pl': 8.0, \n",
    "                 'num_artists_pl': 8.0, \n",
    "                 'track_name_pl': ['Losin Control', 'Paradise', 'Location',\n",
    "                       'Crazy In Love - Remix', 'Pony'], \n",
    "                 'duration_ms_seed_pl': 51023.1,\n",
    "                 'pid': 1,\n",
    "                 'track_uri_pl': ['spotify:track:4cxMGhkinTocPSVVKWIw0d',\n",
    "                       'spotify:track:1wNEBPo3nsbGCZRryI832I',\n",
    "                       'spotify:track:152lZdxL1OR0ZMW6KquMif',\n",
    "                       'spotify:track:2f4IuijXLxYOeBncS60GUD',\n",
    "                       'spotify:track:4Lj8paMFwyKTGfILLELVxt']\n",
    "                     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acde41b9-5025-4d57-80a9-b9f0213c2e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "json_instance = json.dumps({\"instances\": TEST_INSTANCE})\n",
    "print(json_instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9659cb-5e93-4ecf-8553-9058934e090e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile instances.json\n",
    "{\"instances\": {\"collaborative\": \"false\", \"album_name_pl\": [\"There's Really A Wolf\", \"Late Nights: The Album\", \"American Teen\", \"Crazy In Love\", \"Pony\"], \"artist_genres_pl\": [\"'hawaiian hip hop', 'rap'\", \"'chicago rap', 'dance pop', 'pop', 'pop rap', 'r&b', 'southern hip hop', 'trap', 'urban contemporary'\", \"'pop', 'pop r&b'\", \"'dance pop', 'pop', 'r&b'\", \"'chill r&b', 'pop', 'pop r&b', 'r&b', 'urban contemporary'\"], \"artist_name_pl\": [\"Russ\", \"Jeremih\", \"Khalid\", \"Beyonc\\u00c3\\u00a9\", \"William Singe\"], \"description_pl\": \"\", \"n_songs_pl\": 8.0, \"name\": \"Lit Tunes \", \"num_albums_pl\": 8.0, \"num_artists_pl\": 8.0, \"track_name_pl\": [\"Losin Control\", \"Paradise\", \"Location\", \"Crazy In Love - Remix\", \"Pony\"], \"duration_ms_seed_pl\": 51023.1, \"pid\": 1, \"track_uri_pl\": [\"spotify:track:4cxMGhkinTocPSVVKWIw0d\", \"spotify:track:1wNEBPo3nsbGCZRryI832I\", \"spotify:track:152lZdxL1OR0ZMW6KquMif\", \"spotify:track:2f4IuijXLxYOeBncS60GUD\", \"spotify:track:4Lj8paMFwyKTGfILLELVxt\"]}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae4376b-01af-4ad9-a649-254a777ba186",
   "metadata": {},
   "outputs": [],
   "source": [
    "! echo $json_instance > json_instance.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ca1d02-548d-4d5d-a09e-9a5a6ddf7f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "! curl -X POST \\\n",
    "      -d @instances.json \\\n",
    "      -H \"Content-Type: application/json; charset=utf-8\" \\\n",
    "      localhost/predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd45707-2c87-4148-bf69-25feb40a74a3",
   "metadata": {},
   "source": [
    "### Cloud build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efda6bd-b348-455d-807c-c3a8c6ba3082",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!gcloud builds submit --region={REGION} --tag=$REMOTE_IMAGE_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6f4d47-b610-45d7-a916-282efe2fe315",
   "metadata": {},
   "source": [
    "### Create test instances \n",
    "Random selection from BQ validation table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cf27ed-0c42-4cee-8116-44962bb57c13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ### push the container to registry\n",
    "# !docker push $REMOTE_IMAGE_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a9a707-6965-4b6f-a3fe-7312dfddbf24",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Deploy to Vertex AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b166b499-b407-4e3e-99f3-da3f3031a147",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DISPLAY_NAME = \"Merlin Spotify Query Tower Model\"\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "model = aiplatform.Model.upload(\n",
    "    display_name=MODEL_DISPLAY_NAME,\n",
    "    artifact_uri=ARTIFACT_URI,\n",
    "    serving_container_image_uri=REMOTE_IMAGE_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e80231d-0430-478e-989a-4802adca17ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = model.deploy(machine_type=\"n1-standard-4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b8fa58-c587-4b5d-9958-8ed904e0c162",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint.predict(instances=[TEST_INSTANCE])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8450d789-86ba-499e-8ff6-de88fc0797d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m98",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m98"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
