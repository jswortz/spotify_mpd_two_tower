{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fcc7a97-2f93-4b74-b30c-e341d374cd60",
   "metadata": {},
   "source": [
    "fixed_text_layer adapts vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a7b981-d83b-4d9e-bc73-4f119baa5da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/{PIPELINES_SUB_DIR}/adapt_fixed_text_layer_vocab.py\n",
    "\n",
    "import kfp\n",
    "from typing import Any, Callable, Dict, NamedTuple, Optional, List\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output,\n",
    "                        OutputPath, component, Metrics)\n",
    "\n",
    "@kfp.v2.dsl.component(\n",
    "    base_image=\"python:3.9\",\n",
    "    packages_to_install=[\n",
    "        # 'google-cloud-aiplatform==1.18.1',\n",
    "        'google-cloud-storage',\n",
    "        'tensorflow==2.10.1',\n",
    "    ],\n",
    ")\n",
    "def adapt_fixed_text_layer_vocab(\n",
    "    project: str,\n",
    "    location: str,\n",
    "    version: str,\n",
    "    data_dir_bucket_name: str,\n",
    "    data_dir_path_prefix: str,\n",
    "    train_output_gcs_bucket: str,\n",
    "    experiment_name: str,\n",
    "    experiment_run: str,\n",
    "    max_playlist_length: int,\n",
    "    max_tokens: int,\n",
    "    ngrams: int,\n",
    "    feature_name: str,\n",
    "    generate_new_vocab: bool,\n",
    "    # feat_type: str,\n",
    ") -> NamedTuple('Outputs', [\n",
    "    ('vocab_gcs_uri', str),\n",
    "    # ('feature_name', str),\n",
    "]):\n",
    "\n",
    "    \"\"\"\n",
    "    custom pipeline component to adapt the `pl_name_src` layer\n",
    "    writes vocab to pickled dict in GCS\n",
    "    dict combined with other layer vocabs and used in Two Tower training\n",
    "    \"\"\"\n",
    "    \n",
    "    # import packages\n",
    "    import os\n",
    "    import logging\n",
    "    import pickle as pkl\n",
    "    import time\n",
    "    \n",
    "    from google.cloud import storage\n",
    "    \n",
    "    import tensorflow as tf\n",
    "    \n",
    "    storage_client = storage.Client(project=project)\n",
    "    \n",
    "    logging.info(f\"feature_name: {feature_name}\")\n",
    "    \n",
    "    # ===================================================\n",
    "    # helper function\n",
    "    # ===================================================\n",
    "    \n",
    "    def download_blob(bucket_name, source_gcs_obj, local_filename):\n",
    "        \"\"\"Uploads a file to the bucket.\"\"\"\n",
    "        # storage_client = storage.Client(project=project_number)\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "        blob = bucket.blob(source_gcs_obj)\n",
    "        blob.download_to_filename(local_filename)\n",
    "\n",
    "        filehandler = open(f'{local_filename}', 'rb')\n",
    "        loaded_dict = pkl.load(filehandler)\n",
    "        filehandler.close()\n",
    "\n",
    "        logging.info(f\"File {local_filename} downloaded from gs://{bucket_name}/{source_gcs_obj}\")\n",
    "\n",
    "        return loaded_dict\n",
    "    \n",
    "    # ===================================================\n",
    "    # set feature vars\n",
    "    # ===================================================\n",
    "    MAX_PLAYLIST_LENGTH = max_playlist_length\n",
    "    logging.info(f\"MAX_PLAYLIST_LENGTH: {MAX_PLAYLIST_LENGTH}\")\n",
    "    \n",
    "    FEATURES_PREFIX = f'{experiment_name}/{experiment_run}/features'\n",
    "    logging.info(f\"FEATURES_PREFIX: {FEATURES_PREFIX}\")\n",
    "    \n",
    "    all_features_dict = {}\n",
    "    \n",
    "    # ===================================================\n",
    "    # load pickled Candidate features\n",
    "    # ===================================================\n",
    "    \n",
    "    # candidate features\n",
    "    CAND_FEAT_FILENAME = 'candidate_feats_dict.pkl'\n",
    "    CAND_FEAT_GCS_OBJ = f'{FEATURES_PREFIX}/{CAND_FEAT_FILENAME}'\n",
    "    LOADED_CANDIDATE_DICT = f'loaded_{CAND_FEAT_FILENAME}'\n",
    "    \n",
    "    loaded_candidate_features_dict = download_blob(\n",
    "        train_output_gcs_bucket,\n",
    "        CAND_FEAT_GCS_OBJ,\n",
    "        LOADED_CANDIDATE_DICT\n",
    "    )\n",
    "    \n",
    "    all_features_dict.update(loaded_candidate_features_dict)\n",
    "    logging.info(f\"all_features_dict: {all_features_dict}\")\n",
    "\n",
    "    # ===================================================\n",
    "    # load pickled Query features\n",
    "    # ===================================================\n",
    "\n",
    "    # query features\n",
    "    QUERY_FEAT_FILENAME = 'query_feats_dict.pkl'\n",
    "    QUERY_FEAT_GCS_OBJ = f'{FEATURES_PREFIX}/{QUERY_FEAT_FILENAME}'\n",
    "    LOADED_QUERY_DICT = f'loaded_{QUERY_FEAT_FILENAME}'\n",
    "    \n",
    "    loaded_query_features_dict = download_blob(\n",
    "        train_output_gcs_bucket,\n",
    "        QUERY_FEAT_GCS_OBJ,\n",
    "        LOADED_QUERY_DICT\n",
    "    )\n",
    "    \n",
    "    all_features_dict.update(loaded_query_features_dict)\n",
    "    logging.info(f\"all_features_dict: {all_features_dict}\")\n",
    "    \n",
    "    # ===================================================\n",
    "    # tfrecord parser\n",
    "    # ===================================================\n",
    "    \n",
    "    # parsing function\n",
    "    def parse_tfrecord(example):\n",
    "        \"\"\"\n",
    "        Reads a serialized example from GCS and converts to tfrecord\n",
    "        \"\"\"\n",
    "        # example = tf.io.parse_single_example(\n",
    "        example = tf.io.parse_example(\n",
    "            example,\n",
    "            # feats\n",
    "            features=all_features_dict\n",
    "        )\n",
    "        return example\n",
    "    \n",
    "    if generate_new_vocab:\n",
    "        logging.info(f\"Generating new vocab file...\")\n",
    "        \n",
    "        # list blobs (tfrecords)\n",
    "        train_files = []\n",
    "        for blob in storage_client.list_blobs(f'{data_dir_bucket_name}', prefix=f'{data_dir_path_prefix}'):\n",
    "            if '.tfrecords' in blob.name:\n",
    "                train_files.append(blob.public_url.replace(\"https://storage.googleapis.com/\", \"gs://\"))\n",
    "\n",
    "        logging.info(f\"TFRecord file count: {len(train_files)}\")\n",
    "\n",
    "        # ===================================================\n",
    "        # create TF dataset\n",
    "        # ===================================================\n",
    "        logging.info(f\"Creating TFRecordDataset...\")\n",
    "        train_dataset = tf.data.TFRecordDataset(train_files)\n",
    "        train_parsed = train_dataset.map(parse_tfrecord)\n",
    "\n",
    "        # ===================================================\n",
    "        # adapt layer for feature\n",
    "        # ===================================================\n",
    "        start = time.time()\n",
    "        text_layer = tf.keras.layers.TextVectorization(\n",
    "            max_tokens=max_tokens,\n",
    "            ngrams=ngrams\n",
    "        )\n",
    "        text_layer.adapt(train_parsed.map(lambda x: x[f'{feature_name}']))\n",
    "        end = time.time()\n",
    "\n",
    "        logging.info(f'Layer adapt elapsed time: {round((end - start), 2)} seconds')\n",
    "\n",
    "        # ===================================================\n",
    "        # write vocab to pickled dict --> gcs\n",
    "        # ===================================================\n",
    "        logging.info(f\"Writting pickled dict to GCS...\")\n",
    "\n",
    "        VOCAB_LOCAL_FILE = f'{feature_name}_vocab_dict.pkl'\n",
    "        VOCAB_GCS_OBJ = f'{experiment_name}/{experiment_run}/vocab-staging/{VOCAB_LOCAL_FILE}' # destination folder prefix and blob name\n",
    "        VOCAB_DICT = {f'{feature_name}' : text_layer.get_vocabulary(),}\n",
    "\n",
    "        logging.info(f\"VOCAB_LOCAL_FILE: {VOCAB_LOCAL_FILE}\")\n",
    "        logging.info(f\"VOCAB_GCS_OBJ: {VOCAB_GCS_OBJ}\")\n",
    "\n",
    "        # pickle\n",
    "        filehandler = open(f'{VOCAB_LOCAL_FILE}', 'wb')\n",
    "        pkl.dump(VOCAB_DICT, filehandler)\n",
    "        filehandler.close()\n",
    "\n",
    "        # upload to GCS\n",
    "        bucket_client = storage_client.bucket(train_output_gcs_bucket)\n",
    "        blob = bucket_client.blob(VOCAB_GCS_OBJ)\n",
    "        blob.upload_from_filename(VOCAB_LOCAL_FILE)\n",
    "\n",
    "        vocab_uri = f'gs://{train_output_gcs_bucket}/{VOCAB_GCS_OBJ}'\n",
    "\n",
    "        logging.info(f\"File {VOCAB_LOCAL_FILE} uploaded to {vocab_uri}\")\n",
    "        \n",
    "    else:\n",
    "        logging.info(f\"Using existing vocab file...\")\n",
    "        \n",
    "        vocab_uri = 'gs://two-tower-models/vocabs/vocab_dict.pkl'\n",
    "        logging.info(f\"Using vocab file: {vocab_uri}\")\n",
    "    \n",
    "    return(\n",
    "        vocab_uri,\n",
    "        # feature_name,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c92e62-5657-49dc-b32a-73f45d089456",
   "metadata": {},
   "source": [
    "ragged adapts vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63ee930-011a-4923-8979-72be282afda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/{PIPELINES_SUB_DIR}/adapt_ragged_text_layer_vocab.py\n",
    "\n",
    "import kfp\n",
    "from typing import Any, Callable, Dict, NamedTuple, Optional, List\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output,\n",
    "                        OutputPath, component, Metrics)\n",
    "\n",
    "@kfp.v2.dsl.component(\n",
    "    base_image=\"python:3.9\",\n",
    "    packages_to_install=[\n",
    "        # 'google-cloud-aiplatform==1.18.1',\n",
    "        'google-cloud-storage',\n",
    "        'tensorflow==2.10.1',\n",
    "    ],\n",
    ")\n",
    "def adapt_ragged_text_layer_vocab(\n",
    "    project: str,\n",
    "    location: str,\n",
    "    version: str,\n",
    "    data_dir_bucket_name: str,\n",
    "    data_dir_path_prefix: str,\n",
    "    train_output_gcs_bucket: str,\n",
    "    experiment_name: str,\n",
    "    experiment_run: str,\n",
    "    max_playlist_length: int,\n",
    "    max_tokens: int,\n",
    "    ngrams: int,\n",
    "    feature_name: str,\n",
    "    generate_new_vocab: bool,\n",
    "    # feat_type: str,\n",
    ") -> NamedTuple('Outputs', [\n",
    "    ('vocab_gcs_uri', str),\n",
    "    # ('feature_name', str),\n",
    "]):\n",
    "\n",
    "    \"\"\"\n",
    "    custom pipeline component to adapt the `pl_name_src` layer\n",
    "    writes vocab to pickled dict in GCS\n",
    "    dict combined with other layer vocabs and used in Two Tower training\n",
    "    \"\"\"\n",
    "    \n",
    "    # import packages\n",
    "    import os\n",
    "    import logging\n",
    "    import pickle as pkl\n",
    "    import time\n",
    "    \n",
    "    from google.cloud import storage\n",
    "    \n",
    "    import tensorflow as tf\n",
    "    \n",
    "    storage_client = storage.Client(project=project)\n",
    "    \n",
    "    logging.info(f\"feature_name: {feature_name}\")\n",
    "    # logging.info(f\"feat_type: {feat_type}\")\n",
    "    \n",
    "    # ===================================================\n",
    "    # set feature vars\n",
    "    # ===================================================\n",
    "    MAX_PLAYLIST_LENGTH = max_playlist_length\n",
    "    logging.info(f\"MAX_PLAYLIST_LENGTH: {MAX_PLAYLIST_LENGTH}\")\n",
    "    \n",
    "    FEATURES_PREFIX = f'{experiment_name}/{experiment_run}/features'\n",
    "    logging.info(f\"FEATURES_PREFIX: {FEATURES_PREFIX}\")\n",
    "    \n",
    "    all_features_dict = {}\n",
    "    \n",
    "    # ===================================================\n",
    "    # load pickled Candidate features\n",
    "    # ===================================================\n",
    "    \n",
    "    # candidate features\n",
    "    CAND_FEAT_FILENAME = 'candidate_feats_dict.pkl'\n",
    "    CAND_FEAT_GCS_OBJ = f'{FEATURES_PREFIX}/{CAND_FEAT_FILENAME}'\n",
    "    LOADED_CANDIDATE_DICT = f'loaded_{CAND_FEAT_FILENAME}'\n",
    "    logging.info(f\"CAND_FEAT_FILENAME: {CAND_FEAT_FILENAME}; CAND_FEAT_GCS_OBJ:{CAND_FEAT_GCS_OBJ}; LOADED_CANDIDATE_DICT: {LOADED_CANDIDATE_DICT}\")\n",
    "    \n",
    "    # os.system(f'gsutil cp gs://{train_output_gcs_bucket}/{CAND_FEAT_GCS_OBJ} {LOADED_CANDIDATE_DICT}')\n",
    "    bucket = storage_client.bucket(train_output_gcs_bucket)\n",
    "    blob = bucket.blob(CAND_FEAT_GCS_OBJ)\n",
    "    blob.download_to_filename(LOADED_CANDIDATE_DICT)\n",
    "    \n",
    "    filehandler = open(f'{LOADED_CANDIDATE_DICT}', 'rb')\n",
    "    loaded_candidate_features_dict = pkl.load(filehandler)\n",
    "    filehandler.close()\n",
    "    logging.info(f\"loaded_candidate_features_dict: {loaded_candidate_features_dict}\")\n",
    "    \n",
    "    all_features_dict.update(loaded_candidate_features_dict)\n",
    "    logging.info(f\"all_features_dict: {all_features_dict}\")\n",
    "\n",
    "    # ===================================================\n",
    "    # load pickled Query features\n",
    "    # ===================================================\n",
    "\n",
    "    # query features\n",
    "    QUERY_FEAT_FILENAME = 'query_feats_dict.pkl'\n",
    "    QUERY_FEAT_GCS_OBJ = f'{FEATURES_PREFIX}/{QUERY_FEAT_FILENAME}'\n",
    "    LOADED_QUERY_DICT = f'loaded_{QUERY_FEAT_FILENAME}'\n",
    "    logging.info(f\"QUERY_FEAT_FILENAME: {QUERY_FEAT_FILENAME}; QUERY_FEAT_GCS_OBJ:{QUERY_FEAT_GCS_OBJ}; LOADED_QUERY_DICT: {LOADED_QUERY_DICT}\")\n",
    "    \n",
    "    # os.system(f'gsutil cp gs://{train_output_gcs_bucket}/{QUERY_FEATURES_GCS_OBJ} {LOADED_QUERY_DICT}')\n",
    "    bucket = storage_client.bucket(train_output_gcs_bucket)\n",
    "    blob = bucket.blob(QUERY_FEAT_GCS_OBJ)\n",
    "    blob.download_to_filename(LOADED_QUERY_DICT)\n",
    "    \n",
    "    filehandler = open(f'{LOADED_QUERY_DICT}', 'rb')\n",
    "    loaded_query_features_dict = pkl.load(filehandler)\n",
    "    filehandler.close()\n",
    "    logging.info(f\"loaded_query_features_dict: {loaded_query_features_dict}\")\n",
    "    \n",
    "    all_features_dict.update(loaded_query_features_dict)\n",
    "    logging.info(f\"all_features_dict: {all_features_dict}\")\n",
    "    \n",
    "    # ===================================================\n",
    "    # tfrecord parser\n",
    "    # ===================================================\n",
    "    \n",
    "    # parsing function\n",
    "    def parse_tfrecord(example):\n",
    "        \"\"\"\n",
    "        Reads a serialized example from GCS and converts to tfrecord\n",
    "        \"\"\"\n",
    "        # example = tf.io.parse_single_example(\n",
    "        example = tf.io.parse_example(\n",
    "            example,\n",
    "            # feats\n",
    "            features=all_features_dict\n",
    "        )\n",
    "        return example\n",
    "    \n",
    "    \n",
    "    if generate_new_vocab:\n",
    "        logging.info(f\"Generating new vocab file...\")\n",
    "    \n",
    "        # list blobs (tfrecords)\n",
    "        train_files = []\n",
    "        for blob in storage_client.list_blobs(f'{data_dir_bucket_name}', prefix=f'{data_dir_path_prefix}'):\n",
    "            if '.tfrecords' in blob.name:\n",
    "                train_files.append(blob.public_url.replace(\"https://storage.googleapis.com/\", \"gs://\"))\n",
    "\n",
    "        logging.info(f\"TFRecord file count: {len(train_files)}\")\n",
    "\n",
    "        # ===================================================\n",
    "        # create TF dataset\n",
    "        # ===================================================\n",
    "        logging.info(f\"Creating TFRecordDataset...\")\n",
    "        train_dataset = tf.data.TFRecordDataset(train_files)\n",
    "        train_parsed = train_dataset.map(parse_tfrecord)\n",
    "\n",
    "        # ===================================================\n",
    "        # adapt layer for feature\n",
    "        # ===================================================\n",
    "\n",
    "        start = time.time()\n",
    "        text_layer = tf.keras.layers.TextVectorization(\n",
    "            max_tokens=max_tokens,\n",
    "            ngrams=ngrams\n",
    "        )\n",
    "        text_layer.adapt(train_parsed.map(lambda x: tf.reshape(x[f'{feature_name}'], [-1, MAX_PLAYLIST_LENGTH, 1])))\n",
    "        end = time.time()\n",
    "\n",
    "        logging.info(f'Layer adapt elapsed time: {round((end - start), 2)} seconds')\n",
    "\n",
    "        # ===================================================\n",
    "        # write vocab to pickled dict --> gcs\n",
    "        # ===================================================\n",
    "        logging.info(f\"Writting pickled dict to GCS...\")\n",
    "\n",
    "        VOCAB_LOCAL_FILE = f'{feature_name}_vocab_dict.pkl'\n",
    "        VOCAB_GCS_OBJ = f'{experiment_name}/{experiment_run}/vocab-staging/{VOCAB_LOCAL_FILE}' # destination folder prefix and blob name\n",
    "        VOCAB_DICT = {f'{feature_name}' : text_layer.get_vocabulary(),}\n",
    "\n",
    "        logging.info(f\"VOCAB_LOCAL_FILE: {VOCAB_LOCAL_FILE}\")\n",
    "        logging.info(f\"VOCAB_GCS_OBJ: {VOCAB_GCS_OBJ}\")\n",
    "\n",
    "        # pickle\n",
    "        filehandler = open(f'{VOCAB_LOCAL_FILE}', 'wb')\n",
    "        pkl.dump(VOCAB_DICT, filehandler)\n",
    "        filehandler.close()\n",
    "\n",
    "        # upload to GCS\n",
    "        bucket_client = storage_client.bucket(train_output_gcs_bucket)\n",
    "        blob = bucket_client.blob(VOCAB_GCS_OBJ)\n",
    "        blob.upload_from_filename(VOCAB_LOCAL_FILE)\n",
    "\n",
    "        vocab_uri = f'gs://{train_output_gcs_bucket}/{VOCAB_GCS_OBJ}'\n",
    "\n",
    "        logging.info(f\"File {VOCAB_LOCAL_FILE} uploaded to {vocab_uri}\")\n",
    "        \n",
    "    else:\n",
    "        logging.info(f\"Using existing vocab files...\")\n",
    "        vocab_uri = 'gs://two-tower-models/vocabs/vocab_dict.pkl'\n",
    "        logging.info(f\"Using vocab file: {vocab_uri}\")\n",
    "    \n",
    "    return(\n",
    "        vocab_uri,\n",
    "        # feature_name,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e24902-655a-4c07-a0f9-3c7d33d2f121",
   "metadata": {},
   "outputs": [],
   "source": [
    "create master vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed36774c-ec68-4742-bef3-67cd2e79c1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/{PIPELINES_SUB_DIR}/create_master_vocab.py\n",
    "\n",
    "import kfp\n",
    "from typing import Any, Callable, Dict, NamedTuple, Optional, List\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output,\n",
    "                        OutputPath, component, Metrics)\n",
    "\n",
    "@kfp.v2.dsl.component(\n",
    "    base_image=\"python:3.9\",\n",
    "    packages_to_install=[\n",
    "        # 'google-cloud-aiplatform==1.18.1',\n",
    "        'google-cloud-storage',\n",
    "        'numpy',\n",
    "        # 'tensorflow==2.8.3',\n",
    "    ],\n",
    ")\n",
    "def create_master_vocab(\n",
    "    project: str,\n",
    "    location: str,\n",
    "    version: str,\n",
    "    train_output_gcs_bucket: str,\n",
    "    experiment_name: str,\n",
    "    experiment_run: str,\n",
    "    vocab_uri_1: str,\n",
    "    vocab_uri_2: str,\n",
    "    vocab_uri_3: str,\n",
    "    vocab_uri_4: str,\n",
    "    vocab_uri_5: str,\n",
    "    vocab_uri_6: str,\n",
    "    vocab_uri_7: str,\n",
    "    vocab_uri_8: str,\n",
    "    vocab_uri_9: str,\n",
    "    generate_new_vocab: bool,\n",
    ") -> NamedTuple('Outputs', [\n",
    "    ('master_vocab_gcs_uri', str),\n",
    "    ('experiment_name', str),\n",
    "    ('experiment_run', str),\n",
    "]):\n",
    "    \n",
    "    \"\"\"\n",
    "    combine layer dictionaires to master dictionary\n",
    "    master dictionary passed to train job for layer vocabs\n",
    "    \"\"\"\n",
    "    \n",
    "    # import packages\n",
    "    import os\n",
    "    import logging\n",
    "    import pickle as pkl\n",
    "    import time\n",
    "    import numpy as np\n",
    "    \n",
    "    from google.cloud import storage\n",
    "    \n",
    "    # setup clients\n",
    "    storage_client = storage.Client()\n",
    "    \n",
    "    if generate_new_vocab:\n",
    "        \n",
    "        logging.info(f\"Generating new vocab master file...\")\n",
    "        # ===================================================\n",
    "        # Create list of all layer vocab dict uris\n",
    "        # ===================================================\n",
    "\n",
    "        vocab_dict_uris = [\n",
    "            vocab_uri_1, vocab_uri_2, \n",
    "            vocab_uri_3, vocab_uri_4, \n",
    "            vocab_uri_5, vocab_uri_6, \n",
    "            vocab_uri_7, vocab_uri_8, \n",
    "            vocab_uri_9, \n",
    "        ]\n",
    "        logging.info(f\"count of vocab_dict_uris: {len(vocab_dict_uris)}\")\n",
    "        logging.info(f\"vocab_dict_uris: {vocab_dict_uris}\")\n",
    "\n",
    "        # ===================================================\n",
    "        # load pickled dicts\n",
    "        # ===================================================\n",
    "\n",
    "        loaded_pickle_list = []\n",
    "        for i, pickled_dict in enumerate(vocab_dict_uris):\n",
    "\n",
    "            with open(f\"v{i}_vocab_pre_load\", 'wb') as local_vocab_file:\n",
    "                storage_client.download_blob_to_file(pickled_dict, local_vocab_file)\n",
    "\n",
    "            with open(f\"v{i}_vocab_pre_load\", 'rb') as pickle_file:\n",
    "                loaded_vocab_dict = pkl.load(pickle_file)\n",
    "\n",
    "            loaded_pickle_list.append(loaded_vocab_dict)\n",
    "\n",
    "        # ===================================================\n",
    "        # create master vocab dict\n",
    "        # ===================================================\n",
    "        master_dict = {}\n",
    "        for loaded_dict in loaded_pickle_list:\n",
    "            master_dict.update(loaded_dict)\n",
    "\n",
    "        # ===================================================\n",
    "        # Upload master to GCS\n",
    "        # ===================================================\n",
    "        MASTER_VOCAB_LOCAL_FILE = f'vocab_dict.pkl'\n",
    "        MASTER_VOCAB_GCS_OBJ = f'{experiment_name}/{experiment_run}/{MASTER_VOCAB_LOCAL_FILE}' # destination folder prefix and blob name\n",
    "\n",
    "        # pickle\n",
    "        filehandler = open(f'{MASTER_VOCAB_LOCAL_FILE}', 'wb')\n",
    "        pkl.dump(master_dict, filehandler)\n",
    "        filehandler.close()\n",
    "\n",
    "        # upload to GCS\n",
    "        bucket_client = storage_client.bucket(train_output_gcs_bucket)\n",
    "        blob = bucket_client.blob(MASTER_VOCAB_GCS_OBJ)\n",
    "        blob.upload_from_filename(MASTER_VOCAB_LOCAL_FILE)\n",
    "\n",
    "        master_vocab_uri = f'gs://{train_output_gcs_bucket}/{MASTER_VOCAB_GCS_OBJ}'\n",
    "\n",
    "        logging.info(f\"File {MASTER_VOCAB_LOCAL_FILE} uploaded to {master_vocab_uri}\")\n",
    "        \n",
    "    else:\n",
    "        logging.info(f\"Using existing vocab file...\")\n",
    "        master_vocab_uri = 'gs://two-tower-models/vocabs/vocab_dict.pkl'\n",
    "        logging.info(f\"Using vocab file: {master_vocab_uri}\")\n",
    "    \n",
    "    return(\n",
    "        master_vocab_uri,\n",
    "        experiment_name,\n",
    "        experiment_run\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa25e6b4-ad36-48c6-98dd-eda7b86aca0b",
   "metadata": {},
   "source": [
    "## pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2cfd48-ff19-4f48-ad08-4fcee3e6b3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.train_pipes import build_custom_image, train_custom_model, create_tensorboard, generate_candidates, \\\n",
    "                            create_ann_index, create_brute_force_index, create_ann_index_endpoint_vpc, \\\n",
    "                            create_brute_index_endpoint_vpc, deploy_ann_index, deploy_brute_index, \\\n",
    "                            adapt_ragged_text_layer_vocab, adapt_fixed_text_layer_vocab, create_master_vocab, \\\n",
    "                            test_model_index_endpoint_v5\n",
    "\n",
    "from src.train_pipes import pipeline_config as cfg\n",
    "\n",
    "@kfp.v2.dsl.pipeline(\n",
    "    name=f'{PIPELINE_NAME}'.replace('_', '-')\n",
    ")\n",
    "def pipeline(\n",
    "    project: str,\n",
    "    project_number: str,\n",
    "    location: str,\n",
    "    service_account: str,\n",
    "    model_version: str,\n",
    "    pipeline_version: str,\n",
    "    train_image_uri: str,\n",
    "    train_output_gcs_bucket: str,\n",
    "    gcs_train_script_path: str,\n",
    "    model_display_name: str,\n",
    "    train_dockerfile_name: str,\n",
    "    train_dir: str,\n",
    "    train_dir_prefix: str,\n",
    "    valid_dir: str,\n",
    "    valid_dir_prefix: str,\n",
    "    candidate_file_dir: str,\n",
    "    candidate_files_prefix: str,\n",
    "    # tensorboard_resource_name: str,\n",
    "    experiment_name: str,\n",
    "    experiment_run: str,\n",
    "    register_model_flag: str,\n",
    "    vpc_network_name: str,\n",
    "    generate_new_vocab: bool,\n",
    "    max_playlist_length: int,\n",
    "    max_tokens: int,\n",
    "    ngrams: int,\n",
    "):\n",
    "    \n",
    "    from kfp.v2.components import importer_node\n",
    "    from google_cloud_pipeline_components.types import artifact_types\n",
    "    \n",
    "    # ========================================================================\n",
    "    # Build Custom Train Image\n",
    "    # ========================================================================\n",
    "    \n",
    "    # build_custom_train_image_op = (\n",
    "    #     build_custom_train_image.build_custom_train_image(\n",
    "    #         project=project,\n",
    "    #         gcs_train_script_path=gcs_train_script_path,\n",
    "    #         training_image_uri=train_image_uri,\n",
    "    #         train_dockerfile_name=train_dockerfile_name,\n",
    "    #     )\n",
    "    #     .set_display_name(\"Build custom train image\")\n",
    "    #     .set_caching_options(False)\n",
    "    # )\n",
    "    \n",
    "    # ========================================================================\n",
    "    # Conditional: Upload models to Vertex model registry\n",
    "    # ========================================================================\n",
    "    # with kfp.v2.dsl.Condition(generate_new_vocab == 'True', name=\"Generate New Vocab\"):\n",
    "        \n",
    "        # here\n",
    "    # pl_name_src\n",
    "    adapt_pl_name_src_op = (\n",
    "        adapt_fixed_text_layer_vocab.adapt_fixed_text_layer_vocab(\n",
    "            project=project,\n",
    "            location=location,\n",
    "            version=model_version,\n",
    "            data_dir_bucket_name=train_dir,\n",
    "            data_dir_path_prefix=train_dir_prefix,\n",
    "            train_output_gcs_bucket=train_output_gcs_bucket,\n",
    "            experiment_name=experiment_name,\n",
    "            experiment_run=experiment_run,\n",
    "            max_playlist_length=max_playlist_length,\n",
    "            max_tokens=max_tokens,\n",
    "            ngrams=ngrams,\n",
    "            feature_name='pl_name_src',\n",
    "            generate_new_vocab=generate_new_vocab,\n",
    "        )\n",
    "        .set_display_name(f\"adapt: pl_name_src\")\n",
    "        .set_caching_options(True)\n",
    "        .set_cpu_limit('96')\n",
    "        .set_memory_limit('624G')\n",
    "    )\n",
    "    # track_name_can\n",
    "    adapt_track_name_can_op = (\n",
    "        adapt_fixed_text_layer_vocab.adapt_fixed_text_layer_vocab(\n",
    "            project=project,\n",
    "            location=location,\n",
    "            version=model_version,\n",
    "            data_dir_bucket_name=train_dir,\n",
    "            data_dir_path_prefix=train_dir_prefix,\n",
    "            train_output_gcs_bucket=train_output_gcs_bucket,\n",
    "            experiment_name=experiment_name,\n",
    "            experiment_run=experiment_run,\n",
    "            max_playlist_length=max_playlist_length,\n",
    "            max_tokens=max_tokens,\n",
    "            ngrams=ngrams,\n",
    "            feature_name='track_name_can',\n",
    "            generate_new_vocab=generate_new_vocab,\n",
    "        )\n",
    "        .set_display_name(f\"adapt: track_name_can\")\n",
    "        .set_caching_options(True)\n",
    "        .set_cpu_limit(cfg.CPU_LIMIT)\n",
    "        .set_memory_limit(cfg.MEMORY_LIMIT)\n",
    "    )\n",
    "    # artist_name_can\n",
    "    adapt_artist_name_can_op = (\n",
    "        adapt_fixed_text_layer_vocab.adapt_fixed_text_layer_vocab(\n",
    "            project=project,\n",
    "            location=location,\n",
    "            version=model_version,\n",
    "            data_dir_bucket_name=train_dir,\n",
    "            data_dir_path_prefix=train_dir_prefix,\n",
    "            train_output_gcs_bucket=train_output_gcs_bucket,\n",
    "            experiment_name=experiment_name,\n",
    "            experiment_run=experiment_run,\n",
    "            max_playlist_length=max_playlist_length,\n",
    "            max_tokens=max_tokens,\n",
    "            ngrams=ngrams,\n",
    "            feature_name='artist_name_can',\n",
    "            generate_new_vocab=generate_new_vocab,\n",
    "        )\n",
    "        .set_display_name(f\"adapt: artist_name_can\")\n",
    "        .set_caching_options(True)\n",
    "        .set_cpu_limit(cfg.CPU_LIMIT)\n",
    "        .set_memory_limit(cfg.MEMORY_LIMIT)\n",
    "    )\n",
    "\n",
    "    # album_name_can\n",
    "    adapt_album_name_can_op = (\n",
    "        adapt_fixed_text_layer_vocab.adapt_fixed_text_layer_vocab(\n",
    "            project=project,\n",
    "            location=location,\n",
    "            version=model_version,\n",
    "            data_dir_bucket_name=train_dir,\n",
    "            data_dir_path_prefix=train_dir_prefix,\n",
    "            train_output_gcs_bucket=train_output_gcs_bucket,\n",
    "            experiment_name=experiment_name,\n",
    "            experiment_run=experiment_run,\n",
    "            max_playlist_length=max_playlist_length,\n",
    "            max_tokens=max_tokens,\n",
    "            ngrams=ngrams,\n",
    "            feature_name='album_name_can',\n",
    "            generate_new_vocab=generate_new_vocab,\n",
    "        )\n",
    "        .set_display_name(f\"adapt: album_name_can\")\n",
    "        .set_caching_options(True)\n",
    "        .set_cpu_limit(cfg.CPU_LIMIT)\n",
    "        .set_memory_limit(cfg.MEMORY_LIMIT)\n",
    "    )\n",
    "    # artist_genres_can\n",
    "    adapt_artist_genres_can_op = (\n",
    "        adapt_fixed_text_layer_vocab.adapt_fixed_text_layer_vocab(\n",
    "            project=project,\n",
    "            location=location,\n",
    "            version=model_version,\n",
    "            data_dir_bucket_name=train_dir,\n",
    "            data_dir_path_prefix=train_dir_prefix,\n",
    "            train_output_gcs_bucket=train_output_gcs_bucket,\n",
    "            experiment_name=experiment_name,\n",
    "            experiment_run=experiment_run,\n",
    "            max_playlist_length=max_playlist_length,\n",
    "            max_tokens=max_tokens,\n",
    "            ngrams=ngrams,\n",
    "            feature_name='artist_genres_can',\n",
    "            generate_new_vocab=generate_new_vocab,\n",
    "        )\n",
    "        .set_display_name(f\"adapt: artist_genres_can\")\n",
    "        .set_caching_options(True)\n",
    "        .set_cpu_limit(cfg.CPU_LIMIT)\n",
    "        .set_memory_limit(cfg.MEMORY_LIMIT)\n",
    "    )\n",
    "    # raggeds\n",
    "\n",
    "    # track_name_pl\n",
    "    adapt_track_name_pl_features_op = (\n",
    "        adapt_ragged_text_layer_vocab.adapt_ragged_text_layer_vocab(\n",
    "            project=project,\n",
    "            location=location,\n",
    "            version=model_version,\n",
    "            data_dir_bucket_name=train_dir,\n",
    "            data_dir_path_prefix=train_dir_prefix,\n",
    "            train_output_gcs_bucket=train_output_gcs_bucket,\n",
    "            experiment_name=experiment_name,\n",
    "            experiment_run=experiment_run,\n",
    "            max_playlist_length=max_playlist_length,\n",
    "            max_tokens=max_tokens,\n",
    "            ngrams=ngrams,\n",
    "            feature_name='track_name_pl',\n",
    "            generate_new_vocab=generate_new_vocab,\n",
    "        )\n",
    "        .set_display_name(f\"adapt: track_name_pl\")\n",
    "        .set_caching_options(True)\n",
    "        .set_cpu_limit(cfg.CPU_LIMIT)\n",
    "        .set_memory_limit(cfg.MEMORY_LIMIT)\n",
    "    )\n",
    "    # artist_name_pl\n",
    "    adapt_artist_name_pl_op = (\n",
    "        adapt_ragged_text_layer_vocab.adapt_ragged_text_layer_vocab(\n",
    "            project=project,\n",
    "            location=location,\n",
    "            version=model_version,\n",
    "            data_dir_bucket_name=train_dir,\n",
    "            data_dir_path_prefix=train_dir_prefix,\n",
    "            train_output_gcs_bucket=train_output_gcs_bucket,\n",
    "            experiment_name=experiment_name,\n",
    "            experiment_run=experiment_run,\n",
    "            max_playlist_length=max_playlist_length,\n",
    "            max_tokens=max_tokens,\n",
    "            ngrams=ngrams,\n",
    "            feature_name='artist_name_pl',\n",
    "            generate_new_vocab=generate_new_vocab,\n",
    "        )\n",
    "        .set_display_name(f\"adapt: artist_name_pl\")\n",
    "        .set_caching_options(True)\n",
    "        .set_cpu_limit(cfg.CPU_LIMIT)\n",
    "        .set_memory_limit(cfg.MEMORY_LIMIT)\n",
    "    )\n",
    "\n",
    "    # album_name_pl\n",
    "    adapt_album_name_pl_op = (\n",
    "        adapt_ragged_text_layer_vocab.adapt_ragged_text_layer_vocab(\n",
    "            project=project,\n",
    "            location=location,\n",
    "            version=model_version,\n",
    "            data_dir_bucket_name=train_dir,\n",
    "            data_dir_path_prefix=train_dir_prefix,\n",
    "            train_output_gcs_bucket=train_output_gcs_bucket,\n",
    "            experiment_name=experiment_name,\n",
    "            experiment_run=experiment_run,\n",
    "            max_playlist_length=max_playlist_length,\n",
    "            max_tokens=max_tokens,\n",
    "            ngrams=ngrams,\n",
    "            feature_name='album_name_pl',\n",
    "            generate_new_vocab=generate_new_vocab,\n",
    "        )\n",
    "        .set_display_name(f\"adapt: album_name_pl\")\n",
    "        .set_caching_options(True)\n",
    "        .set_cpu_limit(cfg.CPU_LIMIT)\n",
    "        .set_memory_limit(cfg.MEMORY_LIMIT)\n",
    "    )\n",
    "\n",
    "    # artist_genres_pl\n",
    "    adapt_artist_genres_pl_op = (\n",
    "        adapt_ragged_text_layer_vocab.adapt_ragged_text_layer_vocab(\n",
    "            project=project,\n",
    "            location=location,\n",
    "            version=model_version,\n",
    "            data_dir_bucket_name=train_dir,\n",
    "            data_dir_path_prefix=train_dir_prefix,\n",
    "            train_output_gcs_bucket=train_output_gcs_bucket,\n",
    "            experiment_name=experiment_name,\n",
    "            experiment_run=experiment_run,\n",
    "            max_playlist_length=max_playlist_length,\n",
    "            max_tokens=max_tokens,\n",
    "            ngrams=ngrams,\n",
    "            feature_name='artist_genres_pl',\n",
    "            generate_new_vocab=generate_new_vocab,\n",
    "        )\n",
    "        .set_display_name(f\"adapt: artist_genres_pl\")\n",
    "        .set_caching_options(True)\n",
    "        .set_cpu_limit(cfg.CPU_LIMIT)\n",
    "        .set_memory_limit(cfg.MEMORY_LIMIT)\n",
    "    )\n",
    "\n",
    "    # ====================================================\n",
    "    # Aggregate all Dicts\n",
    "    # ====================================================\n",
    "\n",
    "    create_master_vocab_op = (\n",
    "        create_master_vocab.create_master_vocab(\n",
    "            project=project,\n",
    "            location=location,\n",
    "            version=model_version,\n",
    "            train_output_gcs_bucket=train_output_gcs_bucket,\n",
    "            experiment_name=experiment_name,\n",
    "            experiment_run=experiment_run,\n",
    "            vocab_uri_1=adapt_pl_name_src_op.outputs['vocab_gcs_uri'], \n",
    "            vocab_uri_2=adapt_track_name_can_op.outputs['vocab_gcs_uri'], \n",
    "            vocab_uri_3=adapt_artist_name_can_op.outputs['vocab_gcs_uri'], \n",
    "            vocab_uri_4=adapt_album_name_can_op.outputs['vocab_gcs_uri'], \n",
    "            vocab_uri_5=adapt_artist_genres_can_op.outputs['vocab_gcs_uri'], \n",
    "            vocab_uri_6=adapt_track_name_pl_features_op.outputs['vocab_gcs_uri'], \n",
    "            vocab_uri_7=adapt_artist_name_pl_op.outputs['vocab_gcs_uri'], \n",
    "            vocab_uri_8=adapt_album_name_pl_op.outputs['vocab_gcs_uri'], \n",
    "            vocab_uri_9=adapt_artist_genres_pl_op.outputs['vocab_gcs_uri'],\n",
    "            generate_new_vocab=generate_new_vocab,\n",
    "        )\n",
    "        # .after(fixed_for_loop_op).after(ragged_for_loop_op)\n",
    "        .set_display_name(\"create master vocab\")\n",
    "        .set_caching_options(True)\n",
    "        .set_cpu_limit(cfg.CPU_LIMIT)\n",
    "        .set_memory_limit(cfg.MEMORY_LIMIT)\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-10.m103",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-10:m103"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
