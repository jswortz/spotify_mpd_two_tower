{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5adf6255-487d-4cb4-947f-0fa4d76f153b",
   "metadata": {},
   "source": [
    "## Start Vocab Generation for Stringlookups\n",
    "This section deals with getting string lookup vocabs quickly using bq\n",
    "\n",
    "*DO NOT RUN THIS IF YOU HAVE ALREADY GENERATED YOUR VOCABS*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64f92717-e7c3-4df4-8d85-f777a786f582",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# len(vocab_dict_load[\"unique_pids\"])\n",
    "\n",
    "avg_duration_ms_seed_pl = 13000151.68\n",
    "var_duration_ms_seed_pl = 133092900971233.58\n",
    "vocab_dict_load['avg_duration_ms_seed_pl']=avg_duration_ms_seed_pl\n",
    "vocab_dict_load['var_duration_ms_seed_pl']=var_duration_ms_seed_pl\n",
    "\n",
    "avg_n_songs_pl = 55.21\n",
    "var_n_songs_pl = 2317.54\n",
    "vocab_dict_load['avg_n_songs_pl']=avg_n_songs_pl\n",
    "vocab_dict_load['var_n_songs_pl']=var_n_songs_pl\n",
    "\n",
    "avg_n_artists_pl = 30.56\n",
    "var_n_artists_pl = 769.26\n",
    "vocab_dict_load['avg_n_artists_pl']=avg_n_artists_pl\n",
    "vocab_dict_load['var_n_artists_pl']=var_n_artists_pl\n",
    "\n",
    "avg_n_albums_pl = 40.25\n",
    "var_n_albums_pl = 1305.54\n",
    "vocab_dict_load['avg_n_albums_pl']=avg_n_albums_pl\n",
    "vocab_dict_load['var_n_albums_pl']=var_n_albums_pl\n",
    "\n",
    "avg_artist_pop = 16.08\n",
    "var_artist_pop = 300.64\n",
    "vocab_dict_load['avg_artist_pop']=avg_artist_pop\n",
    "vocab_dict_load['var_artist_pop']=var_artist_pop\n",
    "\n",
    "avg_duration_ms_songs_pl = 234823.14\n",
    "var_duration_ms_songs_pl = 5558806228.41\n",
    "vocab_dict_load['avg_duration_ms_songs_pl']=avg_duration_ms_songs_pl\n",
    "vocab_dict_load['var_duration_ms_songs_pl']=var_duration_ms_songs_pl\n",
    "\n",
    "avg_artist_followers = 43337.77\n",
    "var_artist_followers = 377777790193.57\n",
    "vocab_dict_load['avg_artist_followers']=avg_artist_followers\n",
    "vocab_dict_load['var_artist_followers']=var_artist_followers\n",
    "\n",
    "avg_track_pop = 10.85\n",
    "var_track_pop = 202.18\n",
    "vocab_dict_load['avg_track_pop']=avg_track_pop\n",
    "vocab_dict_load['var_track_pop']=var_track_pop\n",
    "# vocab_dict_load['unique_pids_string']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e278a9b1-5a50-455e-9e24-47fc9ef9d5e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['name', 'artist_name_can', 'track_uri_can', 'artist_uri_can', 'track_name_can', 'album_uri_can', 'album_name_can', 'artist_genres_can', 'unique_pids', 'artist_name_seed_track', 'artist_uri_seed_track', 'track_name_seed_track', 'track_uri_seed_track', 'album_name_seed_track', 'album_uri_seed_track', 'artist_genres_seed_track', 'description_pl', 'artist_name_pl', 'track_uri_pl', 'track_name_pl', 'album_name_pl', 'artist_genres_pl', 'min_duration_ms_seed_pl', 'max_duration_ms_seed_pl', 'min_n_songs_pl', 'max_n_songs_pl', 'min_n_artists_pl', 'max_n_artists_pl', 'min_n_albums_pl', 'max_n_albums_pl', 'min_artist_pop', 'max_artist_pop', 'min_duration_ms_songs_pl', 'max_duration_ms_songs_pl', 'min_artist_followers', 'max_artist_followers', 'min_track_pop', 'max_track_pop', 'avg_duration_ms_seed_pl', 'var_duration_ms_seed_pl', 'avg_n_songs_pl', 'var_n_songs_pl', 'avg_n_artists_pl', 'var_n_artists_pl', 'avg_n_albums_pl', 'var_n_albums_pl', 'avg_artist_pop', 'var_artist_pop', 'avg_duration_ms_songs_pl', 'var_duration_ms_songs_pl', 'avg_artist_followers', 'var_artist_followers', 'avg_track_pop', 'var_track_pop', '20000_tokens', '50000_tokens', '100000_tokens', '250000_tokens'])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_dict_load.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d2cd9c-2bc6-4d63-9b96-4c2a5861affd",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Regenerate a new vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2961d21e-6941-4274-9511-d4caa01748c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_vocabs = ['min_duration_ms_seed_pl', 'max_duration_ms_seed_pl', 'min_n_songs_pl', 'max_n_songs_pl', 'min_n_artists_pl', 'max_n_artists_pl', 'min_n_albums_pl', 'max_n_albums_pl', 'min_artist_pop', 'max_artist_pop', 'min_duration_ms_songs_pl', 'max_duration_ms_songs_pl', 'min_artist_followers', 'max_artist_followers', 'min_track_pop', 'max_track_pop', 'avg_duration_ms_seed_pl', 'var_duration_ms_seed_pl', 'avg_n_songs_pl', 'var_n_songs_pl', 'avg_n_artists_pl', 'var_n_artists_pl', 'avg_n_albums_pl', 'var_n_albums_pl', 'avg_artist_pop', 'var_artist_pop', 'avg_duration_ms_songs_pl', 'var_duration_ms_songs_pl', 'avg_artist_followers', 'var_artist_followers', 'avg_track_pop', 'var_track_pop', '20000_tokens', '50000_tokens', '100000_tokens', '250000_tokens']\n",
    "#dropping the string based ones as they are generated below using BQ and or adapted\n",
    "\n",
    "new_vocab_dict = {}\n",
    "\n",
    "for key in keep_vocabs:\n",
    "    new_vocab_dict[key] = vocab_dict_load[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c16d2f-5418-4734-b359-24c4ccd72f4e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Fix the string lookups as select distinct from BQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "276b698d-85ac-456d-9cf7-01fae30bc307",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Query complete after 0.00s: 100%|██████████| 1/1 [00:00<00:00, 715.51query/s] \n",
      "Downloading: 100%|██████████| 287740/287740 [00:00<00:00, 348262.55rows/s]\n"
     ]
    }
   ],
   "source": [
    "%%bigquery artist_name_distinct\n",
    "SELECT distinct artist_name FROM `hybrid-vertex.mdp_eda_test.unique_artists_features` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "25a5fb0a-85fe-4357-9baf-8650cd9322be",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_vocab_dict['artist_name_can'] = list(artist_name_distinct['artist_name'].map(lambda x: str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "26f708a3-a51d-4011-805e-196ad8fcabee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Query complete after 0.00s: 100%|██████████| 1/1 [00:00<00:00, 830.39query/s] \n",
      "Downloading: 100%|██████████| 295860/295860 [00:00<00:00, 402580.82rows/s]\n"
     ]
    }
   ],
   "source": [
    "%%bigquery artist_uri_distinct\n",
    "SELECT distinct artist_uri FROM `hybrid-vertex.mdp_eda_test.unique_artists_features` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1e9d1ca6-20c3-45dd-90ea-b6cac6b0f6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_vocab_dict['artist_uri_can'] = list(artist_uri_distinct['artist_uri'].map(lambda x: str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9a78e9bb-478b-4b1d-a405-732c4ca6bbbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Query complete after 0.00s: 100%|██████████| 1/1 [00:00<00:00, 907.66query/s] \n",
      "Downloading: 100%|██████████| 2262292/2262292 [00:01<00:00, 1245589.53rows/s]\n"
     ]
    }
   ],
   "source": [
    "%%bigquery track_uri_distinct\n",
    "SELECT track_uri FROM `hybrid-vertex.mdp_eda_test.unique_track_features`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "61217f89-755e-47fc-a8ea-9b155e54a116",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_vocab_dict['track_uri_can'] = list(track_uri_distinct['track_uri'].map(lambda x: str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c6b54e98-cac0-487a-b1e4-fbb83817c9cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Query complete after 0.00s: 100%|██████████| 1/1 [00:00<00:00, 838.19query/s] \n",
      "Downloading: 100%|██████████| 1483760/1483760 [00:01<00:00, 994229.30rows/s] \n"
     ]
    }
   ],
   "source": [
    "%%bigquery track_name_distinct\n",
    "SELECT distinct tracks.track_name FROM `hybrid-vertex.mdp_eda_test.playlists_nested` a, UNNEST(a.tracks) as tracks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "53f6b68e-9215-4396-8fba-6c83076a4511",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_vocab_dict['track_name_can'] = list(track_name_distinct['track_name'].map(lambda x: str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "44650618-b4bd-47c2-bfd9-d8cc70e615ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Query complete after 0.00s: 100%|██████████| 4/4 [00:00<00:00, 2098.46query/s]                        \n",
      "Downloading: 100%|██████████| 571629/571629 [00:01<00:00, 559467.53rows/s]\n"
     ]
    }
   ],
   "source": [
    "%%bigquery album_name_distinct\n",
    "SELECT distinct tracks.album_name FROM `hybrid-vertex.mdp_eda_test.playlists_nested` a, UNNEST(a.tracks) as tracks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f68515b9-d733-432f-92b5-494f44cbadf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_vocab_dict['album_name_can'] = list(album_name_distinct['album_name'].map(lambda x: str(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4668a7-eb20-49c1-ae56-32e67458c1e8",
   "metadata": {
    "tags": []
   },
   "source": [
    "### This section is used to adapt the data over multiple max-token configs\n",
    "Only run this once then save the adapted vocabularies to gcs so the model trains faster and adapts no lonter are needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "535d660b-7cff-4ab2-a321-7f1e0fce5d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(max_tokens) -> TheTwoTowers: \n",
    "    layer_sizes=[64,32]\n",
    "    global MAX_TOKENS #set global for the init \n",
    "    MAX_TOKENS = max_tokens\n",
    "    model = tt.TheTwoTowers(layer_sizes, vocab_dict_load)\n",
    "\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adagrad(0.01))\n",
    "    mt_confirm = model.query_tower.pl_name_text_embedding.layers[0].get_config()['max_tokens'] #confirm the setting takes place\n",
    "    assert mt_confirm == max_tokens\n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0d40b3-516e-46dc-bf20-f94abc11df8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test it out\n",
    "model = create_model(22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d0cb989b-2493-451b-9102-eabb15f7f130",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#validate max tokens are set properly in the layer config\n",
    "model.query_tower.pl_name_text_embedding.layers[0].get_config()['max_tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e3417621-5266-4023-9bf7-cf02bbe92053",
   "metadata": {},
   "outputs": [],
   "source": [
    "#in place function to adapt the text vectorizors. This will set the vocabulary in the layer as a list of tokens\n",
    "def adapt_text_vectorizors(model) -> None:\n",
    "    #do the adapts\n",
    "    model.query_tower.pl_name_text_embedding.layers[0].adapt(parsed_dataset_padded.map(lambda x: x['name']).batch(10_000))\n",
    "    print(\"Adapts for name complete\")\n",
    "    model.candidate_tower.artist_name_can_text_embedding.layers[0].adapt(parsed_candidate_dataset.map(lambda x: x['artist_name_can']).batch(10_000))\n",
    "    print(\"Adapts for artist_name_can complete\")\n",
    "    model.candidate_tower.track_name_can_text_embedding.layers[0].adapt(parsed_candidate_dataset.map(lambda x: x['track_name_can']).batch(10_000))\n",
    "    print(\"Adapts for track_name_can complete\")\n",
    "    model.candidate_tower.album_name_can_text_embedding.layers[0].adapt(parsed_candidate_dataset.map(lambda x: x['album_name_can']).batch(10_000))\n",
    "    print(\"Adapts for album_name_can complete\")\n",
    "    model.candidate_tower.artist_genres_can_text_embedding.layers[0].adapt(parsed_candidate_dataset.map(lambda x: x['artist_genres_can']).batch(10_000))\n",
    "    print(\"Adapts for artist_genres_can complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0698f8d4-6ca2-429e-b598-b7b3dc048794",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_vocab_dict(model) -> dict:\n",
    "    #get the dictionaries from the adapted layers\n",
    "    pl_name_text_vocab = model.query_tower.pl_name_text_embedding.layers[0].get_vocabulary()\n",
    "    artist_name_can_vocab = model.candidate_tower.artist_name_can_text_embedding.layers[0].get_vocabulary()\n",
    "    album_name_can_vocab = model.candidate_tower.album_name_can_text_embedding.layers[0].get_vocabulary()\n",
    "    artist_genres_can_vocab = model.candidate_tower.artist_genres_can_text_embedding.layers[0].get_vocabulary()\n",
    "    track_name_can_vocab = model.candidate_tower.track_name_can_text_embedding.layers[0].get_vocabulary()\n",
    "    #create a new dictionary for the adapted vocabularies\n",
    "    vocab_dict = {}\n",
    "    vocab_dict[f\"{MAX_TOKENS}_tokens\"] = {'name':[], 'artist_name_can': [], 'album_name_can': [], 'artist_genres_can': []}\n",
    "    vocab_dict[f\"{MAX_TOKENS}_tokens\"].update({\"name\": pl_name_text_vocab})\n",
    "    vocab_dict[f\"{MAX_TOKENS}_tokens\"].update({\"artist_name_can\": artist_name_can_vocab})\n",
    "    vocab_dict[f\"{MAX_TOKENS}_tokens\"].update({\"album_name_can\": album_name_can_vocab})\n",
    "    vocab_dict[f\"{MAX_TOKENS}_tokens\"].update({\"artist_genres_can\": artist_genres_can_vocab})\n",
    "    vocab_dict[f\"{MAX_TOKENS}_tokens\"].update({\"track_name_can\": track_name_can_vocab})\n",
    "    return(vocab_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6ea091-5888-43bc-a7d7-e1136f87a15b",
   "metadata": {},
   "source": [
    "#### Now call all of the functions in a loop of desired n_tokens \n",
    "This is all to compute only one time and subsequently leverage the vocabularies generated\n",
    "\n",
    "Note it may be good practice to `.pop` undesired keys from the dictionary to preserve memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c928cb4f-b7b7-407e-9a22-c6a8e494afac",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_token_list = [20_000, 50_000, 100_000, 250_000]\n",
    "\n",
    "def adapt_loop(mt) -> dict:\n",
    "    model = create_model(max_tokens=mt)\n",
    "    adapt_text_vectorizors(model)\n",
    "    vocab_dict = update_vocab_dict(model)\n",
    "    return(vocab_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ea3474-ce9e-4c95-b396-3a7b41bac7ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adapts for name complete\n",
      "Adapts for artist_name_can complete\n",
      "Adapts for track_name_can complete\n",
      "Adapts for album_name_can complete\n",
      "Adapts for artist_genres_can complete\n",
      "Adapts for name complete\n",
      "Adapts for artist_name_can complete\n",
      "Adapts for track_name_can complete\n",
      "Adapts for album_name_can complete\n",
      "Adapts for artist_genres_can complete\n",
      "Adapts for track_name_can complete\n",
      "Adapts for album_name_can complete\n",
      "Adapts for artist_genres_can complete\n"
     ]
    }
   ],
   "source": [
    "#run thru the loop and do the adapts, re-creating the model each time - research todo - subsetting larger adapts??\n",
    "for max_token in max_token_list:\n",
    "    incremental_vocab = adapt_loop(max_token)\n",
    "    vocab_dict_load.update(incremental_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9018ca9e-049c-4d60-b9a6-dcc7262f9edc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['name', 'artist_name_can', 'track_uri_can', 'artist_uri_can', 'track_name_can', 'album_uri_can', 'album_name_can', 'artist_genres_can', 'unique_pids', 'artist_name_seed_track', 'artist_uri_seed_track', 'track_name_seed_track', 'track_uri_seed_track', 'album_name_seed_track', 'album_uri_seed_track', 'artist_genres_seed_track', 'description_pl', 'artist_name_pl', 'track_uri_pl', 'track_name_pl', 'album_name_pl', 'artist_genres_pl', 'min_duration_ms_seed_pl', 'max_duration_ms_seed_pl', 'min_n_songs_pl', 'max_n_songs_pl', 'min_n_artists_pl', 'max_n_artists_pl', 'min_n_albums_pl', 'max_n_albums_pl', 'min_artist_pop', 'max_artist_pop', 'min_duration_ms_songs_pl', 'max_duration_ms_songs_pl', 'min_artist_followers', 'max_artist_followers', 'min_track_pop', 'max_track_pop', 'avg_duration_ms_seed_pl', 'var_duration_ms_seed_pl', 'avg_n_songs_pl', 'var_n_songs_pl', 'avg_n_artists_pl', 'var_n_artists_pl', 'avg_n_albums_pl', 'var_n_albums_pl', 'avg_artist_pop', 'var_artist_pop', 'avg_duration_ms_songs_pl', 'var_duration_ms_songs_pl', 'avg_artist_followers', 'var_artist_followers', 'avg_track_pop', 'var_track_pop', '20000_tokens', '50000_tokens', '100000_tokens', '250000_tokens'])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_dict_load.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a412e4-4f79-4cd9-aa5c-639cc2aa9b30",
   "metadata": {},
   "source": [
    "#### Upload new adapts for use in training\n",
    "\n",
    "Notice the new keys `20000_tokens`, `50000_tokens`, `100000_tokens`, `250000_tokens` tokens. Each one of these dictionaries inside the original dictionary are available and should be used as vocabularies based on the number of tokens. Note each used `ngrams=2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9c2d2137-48b1-41a4-a54a-696dd9b4039d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '[UNK]', 'the', 'of', 'a']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_dict_load[f\"{MAX_TOKENS}_tokens\"][\"album_name_can\"][:5] #example access and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "833b81d5-e2d3-4318-a7ad-a8bd63ea1582",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Optional - update vocab files after adapts\n",
    "\n",
    "BUCKET_NAME = 'spotify-v1'\n",
    "FILE_PATH = 'vocabs/v2_string_vocabs'\n",
    "FILE_NAME = 'string_vocabs_v1_20220924-tokens21.pkl'\n",
    "\n",
    "client = storage.Client()\n",
    "\n",
    "with open(f'{FILE_NAME}', 'wb') as pickle_file:\n",
    "    pkl.dump(vocab_dict_load, pickle_file, protocol=pkl.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9a4268-6b5a-4f70-b7af-bc790c108b15",
   "metadata": {},
   "source": [
    "##### Upload the new vocabulary to gcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e4be6e3a-3fab-4876-bb42-3187b304129b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "string_vocabs_v1_20220924-tokens.pkl uploaded to gs://vocabs/v2_string_vocabs/string_vocabs_v1_20220924-tokens.pkl/vocabs/v2_string_vocabs\n"
     ]
    }
   ],
   "source": [
    "bucket = client.bucket(BUCKET_NAME)\n",
    "blob = bucket.blob(FILE_PATH + \"/\" + FILE_NAME)\n",
    "blob.upload_from_filename(FILE_NAME)\n",
    "\n",
    "print(f\"{FILE_NAME} uploaded to gs://{blob.name}/{FILE_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0659d4-5d6c-4852-be2a-7304a20ef472",
   "metadata": {},
   "source": [
    "#### End of Adapt Section"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44c54f4-f9f3-44c7-afe2-1402a8924a19",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Download and save the vocabs to gcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f39037f6-7b6b-48d9-99ea-cd798188c826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Optional - update vocab files after adapts\n",
    "\n",
    "# BUCKET_NAME = 'spotify-v1'\n",
    "# FILE_PATH = 'vocabs/v2_string_vocabs'\n",
    "# FILE_NAME = 'string_vocabs_v1_20220924-tokens22.pkl'\n",
    "\n",
    "# client = storage.Client()\n",
    "\n",
    "# with open(f'{FILE_NAME}', 'wb') as pickle_file:\n",
    "#     pkl.dump(vocab_dict_load, pickle_file, protocol=pkl.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c96e7fcf-a43e-4d53-a934-1b32cba59c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "string_vocabs_v1_20220924-tokens22.pkl uploaded to gs://vocabs/v2_string_vocabs/string_vocabs_v1_20220924-tokens22.pkl/vocabs/v2_string_vocabs\n"
     ]
    }
   ],
   "source": [
    "# bucket = client.bucket(BUCKET_NAME)\n",
    "# blob = bucket.blob(FILE_PATH + \"/\" + FILE_NAME)\n",
    "# blob.upload_from_filename(FILE_NAME)\n",
    "\n",
    "# print(f\"{FILE_NAME} uploaded to gs://{blob.name}/{FILE_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a26212d-e1d3-4ba9-a7df-3cec349cbdaf",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "### End of vocab fix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2fa2e6-a5b7-4e37-8ec1-73d3eec1ff3a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c1f07eda-65ea-42e7-ad70-a099ee69b30b",
   "metadata": {},
   "source": [
    "### Test instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a8d7347c-7e9f-4f67-a030-90b6cdd092f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# playlist_test_instancet = {\n",
    "#     'name': np.asarray([b'Best Christmas']),\n",
    "#     'collaborative': np.asarray([b'false']),\n",
    "#     'pid': np.asarray([173671]),\n",
    "#     'description_pl': np.asarray([b'test description']),\n",
    "#     'duration_ms_seed_pl': np.asarray([5458995.]),\n",
    "#     'n_songs_pl': np.asarray([58.]),\n",
    "#     'num_artists_pl': np.asarray([19.]),\n",
    "#     'num_albums_pl': np.asarray([27.]),\n",
    "#     'artist_name_pl': np.asarray([[b'Juan Luis Guerra 4.40', b'Prince Royce', b'Luis Vargas']]),\n",
    "#     'track_uri_pl': np.asarray([[b'spotify:track:1g0IBPZTRP7VYkctJ4Qafg',b'spotify:track:43wUzbYxEFoXugYkgTzMWp']]),\n",
    "#     'track_name_pl': np.asarray([[b'Lover Come Back', b'White Lightning', b'Shake Me Down']]),\n",
    "#     'duration_ms_songs_pl': np.asarray([[245888., 195709., 283906., 271475., 300373., 275173., 236145.,]]),\n",
    "#     'album_name_pl': np.asarray([[b'Silsulim', b'Sara Shara', b'Muzika Vesheket', b'Ba La Lirkod']]),\n",
    "#     'artist_pop_pl': np.asarray([[81., 81., 70., 66., 66., 66., 46., 87.]]),\n",
    "#     'artists_followers_pl': np.asarray([[3.556710e+05, 8.200000e+02, 1.510000e+02, 1.098080e+05,]]),\n",
    "#     'artist_genres_pl': np.asarray([[b\"'israeli pop', 'jewish pop'\", b\"'israeli pop', 'jewish pop'\",]]),\n",
    "#     'track_pop_pl': np.asarray([[70, 77, 50, 44, 30, 28, 15, 26, 15, 18, 46, 38,]])\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4df79201-d250-451a-9543-0004bf227050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# candidate_test_instance = {\n",
    "#     'artist_name': np.asarray([b'Hellogoodbye']),\n",
    "#     'track_name': np.asarray([b'When We First Met']),\n",
    "#     'album_name': np.asarray([b'Would It Kill You?']),\n",
    "#     'track_uri': np.asarray([b'Ba La Lirkod']),\n",
    "#     'artist_uri': np.asarray([b'spotify:artist:6GH0NzpthMGxu1mcfAkOde']),\n",
    "#     'album_uri': np.asarray([b'spotify:album:4dHXV7pJs6d8N9ACAMzhIw']),\n",
    "#     'duration_ms': np.asarray([154813.0]),\n",
    "#     'track_pop': np.asarray([45.0]),\n",
    "#     'artist_pop': np.asarray([51.0]),\n",
    "#     'artist_followers':np.asarray([205331.0]),\n",
    "#     'artist_genres': np.asarray([b\"'neon pop punk', 'pop punk'\"]),\n",
    "#     # 'test': np.asarray([b'test'])\n",
    "# }\n",
    "\n",
    "# # candidate_test_instance = {\n",
    "# #     'artist_name': np.asarray([b'Hellogoodbye']),\n",
    "# #     'track_name': np.asarray([b'When We First Met']),\n",
    "# #     'album_name': np.asarray([b'Would It Kill You?']),\n",
    "# #     'track_uri': np.asarray([b'Ba La Lirkod']),\n",
    "# #     'artist_uri': np.asarray([b'spotify:artist:6GH0NzpthMGxu1mcfAkOde']),\n",
    "# #     'album_uri': np.asarray([b'spotify:album:4dHXV7pJs6d8N9ACAMzhIw']),\n",
    "# #     'duration_ms': np.asarray([154813.0]),\n",
    "# #     'track_pop': np.asarray([45.0]),\n",
    "# #     'artist_pop': np.asarray([51.0]),\n",
    "# #     'artist_followers':np.asarray([205331.0]),\n",
    "# #     'artist_genres': np.asarray([b\"'neon pop punk', 'pop punk'\"]),\n",
    "# #     # 'test': np.asarray([b'test'])\n",
    "# # }\n",
    "# # pprint(can_test_instance)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m96",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m96"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
