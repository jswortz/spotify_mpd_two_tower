{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "152d43dc-e7c4-4085-b214-b576e1b4a94e",
   "metadata": {},
   "source": [
    "# Training pipeline for TFRS  2tower model \n",
    "\n",
    "When completed you should have a pipeline that looks like this:\n",
    "\n",
    "![](img/train-pipeline-sp-e2e.png)\n",
    "\n",
    "#### Setps performed\n",
    "1. Create custom components for training and parallel vocabulary adapts\n",
    "2. Save master vocabulary and add a managed tensorboard to monitor training\n",
    "3. Create pipeline with blend of custom and built-in components\n",
    "4. Export/Import models to registry, deploy to endpoints\n",
    "5. Create Matching Engine Endpoint as well as ANN, Brute Force indexes to test recall/latency tradeoff\n",
    "6. Perform final tests on entire deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36940f5c-4a38-48c4-bf85-09e1a8ddf60b",
   "metadata": {},
   "source": [
    "### pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80e83208-972c-42d1-b79e-38e603d08461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip3 install --upgrade --user -q google-cloud-aiplatform\n",
    "# ! pip3 install --upgrade --user -q google-cloud-storage\n",
    "# ! pip3 install --upgrade --user -q kfp\n",
    "# ! pip3 install --upgrade --user -q google-cloud-pipeline-components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffea60e3-5cbe-43b6-bcea-9d83c3e162ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFP SDK version: 1.8.20\n",
      "google_cloud_pipeline_components version: 1.0.42\n",
      "aiplatform SDK version: 1.26.1\n"
     ]
    }
   ],
   "source": [
    "! python3 -c \"import kfp; print('KFP SDK version: {}'.format(kfp.__version__))\"\n",
    "! python3 -c \"import google_cloud_pipeline_components; print('google_cloud_pipeline_components version: {}'.format(google_cloud_pipeline_components.__version__))\"\n",
    "! python3 -c \"import google.cloud.aiplatform; print('aiplatform SDK version: {}'.format(google.cloud.aiplatform.__version__))\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f67ba9-2c8b-413e-8a4c-f4b0cd73f7bb",
   "metadata": {},
   "source": [
    "## Load env config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cec2257a-f493-42c7-9429-7543af864ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREFIX = ndr-v1\n"
     ]
    }
   ],
   "source": [
    "# naming convention for all cloud resources\n",
    "VERSION        = \"v1\"                  # TODO\n",
    "PREFIX         = f'ndr-{VERSION}'      # TODO\n",
    "\n",
    "print(f\"PREFIX = {PREFIX}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a06fbe24-6018-4d6b-8f81-42516b880696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PROJECT_ID               = \"hybrid-vertex\"\n",
      "PROJECT_NUM              = \"934903580331\"\n",
      "LOCATION                 = \"us-central1\"\n",
      "\n",
      "REGION                   = \"us-central1\"\n",
      "BQ_LOCATION              = \"US\"\n",
      "VPC_NETWORK_NAME         = \"ucaip-haystack-vpc-network\"\n",
      "\n",
      "VERTEX_SA                = \"934903580331-compute@developer.gserviceaccount.com\"\n",
      "\n",
      "PREFIX                   = \"ndr-v1\"\n",
      "VERSION                  = \"v1\"\n",
      "\n",
      "APP                      = \"sp\"\n",
      "MODEL_TYPE               = \"2tower\"\n",
      "FRAMEWORK                = \"tfrs\"\n",
      "DATA_VERSION             = \"v1\"\n",
      "TRACK_HISTORY            = \"5\"\n",
      "\n",
      "BUCKET_NAME              = \"ndr-v1-hybrid-vertex-bucket\"\n",
      "BUCKET_URI               = \"gs://ndr-v1-hybrid-vertex-bucket\"\n",
      "SOURCE_BUCKET            = \"spotify-million-playlist-dataset\"\n",
      "\n",
      "DATA_GCS_PREFIX          = \"data\"\n",
      "DATA_PATH                = \"gs://ndr-v1-hybrid-vertex-bucket/data\"\n",
      "VOCAB_SUBDIR             = \"vocabs\"\n",
      "VOCAB_FILENAME           = \"vocab_dict.pkl\"\n",
      "\n",
      "CANDIDATE_PREFIX         = \"candidates\"\n",
      "TRAIN_DIR_PREFIX         = \"train\"\n",
      "VALID_DIR_PREFIX         = \"valid\"\n",
      "\n",
      "VPC_NETWORK_FULL         = \"projects/934903580331/global/networks/ucaip-haystack-vpc-network\"\n",
      "\n",
      "BQ_DATASET               = \"spotify_e2e_test\"\n",
      "BQ_TABLE_TRAIN           = \"train_flatten_last_5\"\n",
      "BQ_TABLE_VALID           = \"train_flatten_valid_last_5\"\n",
      "BQ_TABLE_CANDIDATES      = \"candidates\"\n",
      "\n",
      "REPO_SRC                 = \"src\"\n",
      "PIPELINES_SUB_DIR        = \"feature_pipes\"\n",
      "\n",
      "REPOSITORY               = \"ndr-v1-spotify\"\n",
      "IMAGE_NAME               = \"train-v1\"\n",
      "REMOTE_IMAGE_NAME        = \"us-central1-docker.pkg.dev/hybrid-vertex/ndr-v1-spotify/train-v1\"\n",
      "DOCKERNAME               = \"tfrs\"\n",
      "\n",
      "SERVING_IMAGE_URI_CPU    = \"us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-11:latest\"\n",
      "SERVING_IMAGE_URI_GPU    = \"us-docker.pkg.dev/vertex-ai/prediction/tf2-gpu.2-11:latest\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# staging GCS\n",
    "GCP_PROJECTS             = !gcloud config get-value project\n",
    "PROJECT_ID               = GCP_PROJECTS[0]\n",
    "\n",
    "# GCS bucket and paths\n",
    "BUCKET_NAME              = f'{PREFIX}-{PROJECT_ID}-bucket'\n",
    "BUCKET_URI               = f'gs://{BUCKET_NAME}'\n",
    "\n",
    "config = !gsutil cat {BUCKET_URI}/config/notebook_env.py\n",
    "print(config.n)\n",
    "exec(config.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79aa47fd-8aa6-4126-b167-bfc678c94e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68a4fb97-bed6-4533-92d3-dbc79e134f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "from time import time\n",
    "import pandas as pd\n",
    "import time\n",
    "from pprint import pprint\n",
    "import pickle as pkl\n",
    "\n",
    "import logging\n",
    "logging.disable(logging.WARNING)\n",
    "\n",
    "from google.cloud import aiplatform as vertex_ai\n",
    "from google.cloud import storage\n",
    "\n",
    "# Pipelines\n",
    "from typing import Any, Callable, Dict, NamedTuple, Optional, List\n",
    "from google_cloud_pipeline_components import aiplatform as gcc_aip\n",
    "from google_cloud_pipeline_components.types import artifact_types\n",
    "\n",
    "# Kubeflow SDK\n",
    "# TODO: fix these\n",
    "from kfp.v2 import dsl\n",
    "import kfp\n",
    "import kfp.v2.dsl\n",
    "from kfp.v2.google import client as pipelines_client\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output,\n",
    "                        OutputPath, component)\n",
    "\n",
    "# import modules\n",
    "from util import feature_set_utils as feature_utils\n",
    "from util import test_instances\n",
    "\n",
    "storage_client = storage.Client(project=PROJECT_ID)\n",
    "\n",
    "vertex_ai.init(project=PROJECT_ID,location=REGION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b0bdf3f-6834-4ae0-ab0e-18e64e58031e",
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINE_VERSION = 'pipe-v2'       # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd32b85b-b44a-4e17-9c23-98fad8efe064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL_ROOT_NAME: sp-2tower-tfrs-v1-pipe-v2\n"
     ]
    }
   ],
   "source": [
    "MODEL_ROOT_NAME = f'{APP}-{MODEL_TYPE}-{FRAMEWORK}-{VERSION}-{PIPELINE_VERSION}'\n",
    "print(f\"MODEL_ROOT_NAME: {MODEL_ROOT_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c889eb1-b663-4491-8bbf-4247d87b1762",
   "metadata": {},
   "source": [
    "# Pipeline Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3868029b-b1ce-45a9-aade-ebdfec8f110f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa1292ae-b51d-42bc-83c5-996e827c2633",
   "metadata": {},
   "outputs": [],
   "source": [
    "REPO_SRC = 'src'\n",
    "PIPELINES_SUB_DIR = 'train_pipes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6893f359-64de-458e-a964-d42ce581fd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -rf {REPO_SRC}/{PIPELINES_SUB_DIR}\n",
    "! mkdir {REPO_SRC}/{PIPELINES_SUB_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40fc5199-4cda-4d2f-8fb3-633618f405cc",
   "metadata": {},
   "source": [
    "## Create Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8ab9fab-7a7e-4d7b-b241-19f4d39eb266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/train_pipes/create_tensorboard.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_SRC}/{PIPELINES_SUB_DIR}/create_tensorboard.py\n",
    "\n",
    "import kfp\n",
    "from typing import Any, Callable, Dict, NamedTuple, Optional, List\n",
    "from kfp.v2.dsl import (\n",
    "    Artifact, Dataset, Input, InputPath, \n",
    "    Model, Output, OutputPath, component, Metrics\n",
    ")\n",
    "@kfp.v2.dsl.component(\n",
    "    base_image='python:3.9',\n",
    "    packages_to_install=[\n",
    "        'google-cloud-aiplatform==1.26.1',\n",
    "        'numpy',\n",
    "        'google-cloud-storage',\n",
    "    ],\n",
    "    # output_component_file=\"./pipelines/train_custom_model.yaml\",\n",
    ")\n",
    "def create_tensorboard(\n",
    "    project: str,\n",
    "    location: str,\n",
    "    model_version: str,\n",
    "    pipeline_version: str,\n",
    "    model_name: str, \n",
    "    experiment_name: str,\n",
    "    experiment_run: str,\n",
    ") -> NamedTuple('Outputs', [\n",
    "    ('tensorboard_resource_name', str),\n",
    "    ('tensorboard_display_name', str),\n",
    "]):\n",
    "    \n",
    "    import logging\n",
    "    from google.cloud import aiplatform as vertex_ai\n",
    "    from google.cloud import storage\n",
    "    \n",
    "    vertex_ai.init(\n",
    "        project=project,\n",
    "        location=location,\n",
    "        # experiment=experiment_name,\n",
    "    )\n",
    "    \n",
    "    logging.info(f'experiment_name: {experiment_name}')\n",
    "    \n",
    "    # # create new TB instance\n",
    "    TENSORBOARD_DISPLAY_NAME=f\"{experiment_name}-v1\"\n",
    "    tensorboard = vertex_ai.Tensorboard.create(display_name=TENSORBOARD_DISPLAY_NAME, project=project, location=location)\n",
    "    TB_RESOURCE_NAME = tensorboard.resource_name\n",
    "    \n",
    "    logging.info(f'TENSORBOARD_DISPLAY_NAME: {TENSORBOARD_DISPLAY_NAME}')\n",
    "    logging.info(f'TB_RESOURCE_NAME: {TB_RESOURCE_NAME}')\n",
    "    \n",
    "    return (\n",
    "        f'{TB_RESOURCE_NAME}',\n",
    "        f'{TENSORBOARD_DISPLAY_NAME}',\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8843984-b680-4c46-a456-b55291643e14",
   "metadata": {},
   "source": [
    "## Custom train job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc26d281-a72f-4825-a5f8-33ff35d2fb03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/train_pipes/train_custom_model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_SRC}/{PIPELINES_SUB_DIR}/train_custom_model.py\n",
    "\n",
    "import kfp\n",
    "from typing import Any, Callable, Dict, NamedTuple, Optional, List\n",
    "from kfp.v2.dsl import (\n",
    "    Artifact, Dataset, Input, InputPath, \n",
    "    Model, Output, OutputPath, component, Metrics\n",
    ")\n",
    "@kfp.v2.dsl.component(\n",
    "    base_image='python:3.9',\n",
    "    packages_to_install=[\n",
    "        'google-cloud-aiplatform==1.26.1',\n",
    "        # 'tensorflow==2.9.2',\n",
    "        # 'tensorflow-recommenders==0.7.0',\n",
    "        'numpy',\n",
    "        'google-cloud-storage',\n",
    "    ],\n",
    "    # output_component_file=\"./pipelines/train_custom_model.yaml\",\n",
    ")\n",
    "def train_custom_model(\n",
    "    project: str,\n",
    "    location: str,\n",
    "    model_version: str,\n",
    "    pipeline_version: str,\n",
    "    model_name: str, \n",
    "    worker_pool_specs: dict,\n",
    "    # vocab_dict_uri: str, \n",
    "    train_output_gcs_bucket: str,                         # change to workdir?\n",
    "    training_image_uri: str,\n",
    "    tensorboard_resource_name: str,\n",
    "    service_account: str,\n",
    "    experiment_name: str,\n",
    "    experiment_run: str,\n",
    "    generate_new_vocab: bool,\n",
    ") -> NamedTuple('Outputs', [\n",
    "    ('job_dict_uri', str),\n",
    "    ('query_tower_dir_uri', str),\n",
    "    ('candidate_tower_dir_uri', str),\n",
    "    ('experiment_run_dir', str),\n",
    "]):\n",
    "    \n",
    "    import logging\n",
    "    import numpy as np\n",
    "    import pickle as pkl\n",
    "    \n",
    "    from google.cloud import aiplatform as vertex_ai\n",
    "    # import google.cloud.aiplatform_v1beta1 as aip_beta\n",
    "    from google.cloud import storage\n",
    "    \n",
    "    vertex_ai.init(\n",
    "        project=project,\n",
    "        location=location,\n",
    "        experiment=experiment_name,\n",
    "    )\n",
    "    \n",
    "    storage_client = storage.Client(project=project)\n",
    "    \n",
    "    JOB_NAME = f'train-{model_name}'\n",
    "    logging.info(f'JOB_NAME: {JOB_NAME}')\n",
    "    \n",
    "    BASE_OUTPUT_DIR = f'gs://{train_output_gcs_bucket}/{experiment_name}/{experiment_run}'\n",
    "    logging.info(f'BASE_OUTPUT_DIR: {BASE_OUTPUT_DIR}')\n",
    "    \n",
    "    # logging.info(f'vocab_dict_uri: {vocab_dict_uri}')\n",
    "    \n",
    "    logging.info(f'tensorboard_resource_name: {tensorboard_resource_name}')\n",
    "    logging.info(f'service_account: {service_account}')\n",
    "    logging.info(f'worker_pool_specs: {worker_pool_specs}')\n",
    "    \n",
    "    # ====================================================\n",
    "    # Launch Vertex job\n",
    "    # ====================================================\n",
    "    \n",
    "    worker_pool_specs[0]['container_spec']['args'].append(f'--tb_resource_name={tensorboard_resource_name}')\n",
    "    \n",
    "    if generate_new_vocab == 'True':\n",
    "        worker_pool_specs[0]['container_spec']['args'].append(f'--new_vocab')\n",
    "  \n",
    "    job = vertex_ai.CustomJob(\n",
    "        display_name=JOB_NAME,\n",
    "        worker_pool_specs=worker_pool_specs,\n",
    "        base_output_dir=BASE_OUTPUT_DIR,\n",
    "        staging_bucket=f\"{BASE_OUTPUT_DIR}/staging\",\n",
    "    )\n",
    "    \n",
    "    logging.info(f'Submitting train job to Vertex AI...')\n",
    "    \n",
    "    job.run(\n",
    "        tensorboard=tensorboard_resource_name,\n",
    "        service_account=f'{service_account}',\n",
    "        restart_job_on_worker_restart=False,\n",
    "        enable_web_access=True,\n",
    "        sync=False,\n",
    "    )\n",
    "        \n",
    "    # wait for job to complete\n",
    "    job.wait()\n",
    "    \n",
    "    # ====================================================\n",
    "    # Save job details\n",
    "    # ====================================================\n",
    "    \n",
    "    train_job_dict = job.to_dict()\n",
    "    logging.info(f'train_job_dict: {train_job_dict}')\n",
    "    \n",
    "    # pkl dict to GCS\n",
    "    logging.info(f\"Write pickled dict to GCS...\")\n",
    "    TRAIN_DICT_LOCAL = f'train_job_dict.pkl'\n",
    "    TRAIN_DICT_GCS_OBJ = f'{experiment_name}/{experiment_run}/{TRAIN_DICT_LOCAL}' # destination folder prefix and blob name\n",
    "    \n",
    "    logging.info(f\"TRAIN_DICT_LOCAL: {TRAIN_DICT_LOCAL}\")\n",
    "    logging.info(f\"TRAIN_DICT_GCS_OBJ: {TRAIN_DICT_GCS_OBJ}\")\n",
    "\n",
    "    # pickle\n",
    "    filehandler = open(f'{TRAIN_DICT_LOCAL}', 'wb')\n",
    "    pkl.dump(train_job_dict, filehandler)\n",
    "    filehandler.close()\n",
    "    \n",
    "    # upload to GCS\n",
    "    bucket_client = storage_client.bucket(train_output_gcs_bucket)\n",
    "    blob = bucket_client.blob(TRAIN_DICT_GCS_OBJ)\n",
    "    blob.upload_from_filename(TRAIN_DICT_LOCAL)\n",
    "    \n",
    "    job_dict_uri = f'gs://{train_output_gcs_bucket}/{TRAIN_DICT_GCS_OBJ}'\n",
    "    logging.info(f\"{TRAIN_DICT_LOCAL} uploaded to {job_dict_uri}\")\n",
    "    \n",
    "    # ====================================================\n",
    "    # Model and index artifact uris\n",
    "    # ====================================================\n",
    "    EXPERIMENT_RUN_DIR = f\"gs://{train_output_gcs_bucket}/{experiment_name}/{experiment_run}\"\n",
    "    query_tower_dir_uri = f\"{EXPERIMENT_RUN_DIR}/model-dir/query_model\" \n",
    "    candidate_tower_dir_uri = f\"{EXPERIMENT_RUN_DIR}/model-dir/candidate_model\"\n",
    "    # candidate_index_dir_uri = f\"gs://{output_dir_gcs_bucket_name}/{experiment_name}/{experiment_run}/candidate_model\"\n",
    "    \n",
    "    logging.info(f'query_tower_dir_uri: {query_tower_dir_uri}')\n",
    "    logging.info(f'candidate_tower_dir_uri: {candidate_tower_dir_uri}')\n",
    "    # logging.info(f'candidate_index_dir_uri: {candidate_index_dir_uri}')\n",
    "    \n",
    "    return (\n",
    "        f'{job_dict_uri}',\n",
    "        f'{query_tower_dir_uri}',\n",
    "        f'{candidate_tower_dir_uri}',\n",
    "        f'{EXPERIMENT_RUN_DIR}',\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098b5708-4536-4d8a-a4d4-7aaa76112a47",
   "metadata": {},
   "source": [
    "## Generate Candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf697751-337f-4757-a5b4-fedc2246cc66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/train_pipes/generate_candidates.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_SRC}/{PIPELINES_SUB_DIR}/generate_candidates.py\n",
    "\n",
    "import kfp\n",
    "from typing import Any, Callable, Dict, NamedTuple, Optional, List\n",
    "from kfp.v2.dsl import (\n",
    "    Artifact, Dataset, Input, InputPath, \n",
    "    Model, Output, OutputPath, component, Metrics\n",
    ")\n",
    "@kfp.v2.dsl.component(\n",
    "    base_image=\"python:3.9\",\n",
    "    packages_to_install=[\n",
    "        'google-cloud-aiplatform==1.26.1',\n",
    "        'tensorflow==2.11.0',\n",
    "        'tensorflow-recommenders==0.7.2',\n",
    "        'numpy',\n",
    "        # 'google-cloud-storage',\n",
    "    ],\n",
    ")\n",
    "def generate_candidates(\n",
    "    project: str,\n",
    "    location: str,\n",
    "    version: str, \n",
    "    # emb_index_gcs_uri: str,\n",
    "    candidate_tower_dir_uri: str,\n",
    "    candidate_file_dir_bucket: str,\n",
    "    candidate_file_dir_prefix: str,\n",
    "    train_output_gcs_bucket: str,\n",
    "    experiment_name: str,\n",
    "    experiment_run: str,\n",
    "    experiment_run_dir: str,\n",
    ") -> NamedTuple('Outputs', [\n",
    "    ('emb_index_gcs_uri', str),\n",
    "    # ('emb_index_artifact', Artifact),\n",
    "]):\n",
    "    import logging\n",
    "    import json\n",
    "    import pickle as pkl\n",
    "    from pprint import pprint\n",
    "    import time\n",
    "    import numpy as np\n",
    "\n",
    "    import os\n",
    "\n",
    "    # os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "\n",
    "    import tensorflow as tf\n",
    "    import tensorflow_recommenders as tfrs\n",
    "\n",
    "    from google.cloud import storage\n",
    "    from google.cloud.storage.bucket import Bucket\n",
    "    from google.cloud.storage.blob import Blob\n",
    "\n",
    "    import google.cloud.aiplatform as vertex_ai\n",
    "    \n",
    "    # set clients\n",
    "    vertex_ai.init(\n",
    "        project=project,\n",
    "        location=location,\n",
    "    )\n",
    "    storage_client = storage.Client(project=project)\n",
    "\n",
    "    # tf.Data confg\n",
    "    options = tf.data.Options()\n",
    "    options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.DATA\n",
    "    \n",
    "    # ====================================================\n",
    "    # Load trained candidate tower\n",
    "    # ====================================================\n",
    "    logging.info(f\"candidate_tower_dir_uri: {candidate_tower_dir_uri}\")\n",
    "    \n",
    "    loaded_candidate_model = tf.saved_model.load(candidate_tower_dir_uri)\n",
    "    logging.info(f\"loaded_candidate_model.signatures: {loaded_candidate_model.signatures}\")\n",
    "    \n",
    "    candidate_predictor = loaded_candidate_model.signatures[\"serving_default\"]\n",
    "    logging.info(f\"structured_outputs: {candidate_predictor.structured_outputs}\")\n",
    "    \n",
    "    # ===================================================\n",
    "    # set feature vars\n",
    "    # ===================================================\n",
    "    FEATURES_PREFIX = f'{experiment_name}/{experiment_run}/features'\n",
    "    logging.info(f\"FEATURES_PREFIX: {FEATURES_PREFIX}\")\n",
    "    \n",
    "    def download_blob(bucket_name, source_gcs_obj, local_filename):\n",
    "        \"\"\"Uploads a file to the bucket.\"\"\"\n",
    "        # storage_client = storage.Client(project=project_number)\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "        blob = bucket.blob(source_gcs_obj)\n",
    "        blob.download_to_filename(local_filename)\n",
    "        \n",
    "        filehandler = open(f'{local_filename}', 'rb')\n",
    "        loaded_dict = pkl.load(filehandler)\n",
    "        filehandler.close()\n",
    "        \n",
    "        logging.info(f\"File {local_filename} downloaded from gs://{bucket_name}/{source_gcs_obj}\")\n",
    "        \n",
    "        return loaded_dict\n",
    "    \n",
    "    # ===================================================\n",
    "    # load pickled Candidate features\n",
    "    # ===================================================\n",
    "    \n",
    "    # candidate features\n",
    "    CAND_FEAT_FILENAME = 'candidate_feats_dict.pkl'\n",
    "    CAND_FEAT_GCS_OBJ = f'{FEATURES_PREFIX}/{CAND_FEAT_FILENAME}'\n",
    "    LOADED_CANDIDATE_DICT = f'loaded_{CAND_FEAT_FILENAME}'\n",
    "    \n",
    "    loaded_candidate_features_dict = download_blob(\n",
    "        train_output_gcs_bucket,\n",
    "        CAND_FEAT_GCS_OBJ,\n",
    "        LOADED_CANDIDATE_DICT\n",
    "    )\n",
    "    \n",
    "    # ====================================================\n",
    "    # Features and Helper Functions\n",
    "    # ====================================================\n",
    "    \n",
    "    def parse_candidate_tfrecord_fn(example):\n",
    "        \"\"\"\n",
    "        Reads candidate serialized examples from gcs and converts to tfrecord\n",
    "        \"\"\"\n",
    "        # example = tf.io.parse_single_example(\n",
    "        example = tf.io.parse_example(\n",
    "            example, \n",
    "            features=loaded_candidate_features_dict\n",
    "        )\n",
    "        return example\n",
    "\n",
    "    def full_parse(data):\n",
    "        # used for interleave - takes tensors and returns a tf.dataset\n",
    "        data = tf.data.TFRecordDataset(data)\n",
    "        return data\n",
    "    \n",
    "    # ====================================================\n",
    "    # Create Candidate Dataset\n",
    "    # ====================================================\n",
    "\n",
    "    candidate_files = []\n",
    "    for blob in storage_client.list_blobs(f\"{candidate_file_dir_bucket}\", prefix=f'{candidate_file_dir_prefix}/'):\n",
    "        if '.tfrecords' in blob.name:\n",
    "            candidate_files.append(blob.public_url.replace(\"https://storage.googleapis.com/\", \"gs://\"))\n",
    "\n",
    "    candidate_dataset = tf.data.Dataset.from_tensor_slices(candidate_files)\n",
    "\n",
    "    parsed_candidate_dataset = candidate_dataset.interleave(\n",
    "        # lambda x: tf.data.TFRecordDataset(x),\n",
    "        full_parse,\n",
    "        cycle_length=tf.data.AUTOTUNE, \n",
    "        num_parallel_calls=tf.data.AUTOTUNE,\n",
    "        deterministic=False\n",
    "    ).map(parse_candidate_tfrecord_fn, num_parallel_calls=tf.data.AUTOTUNE).with_options(options)\n",
    "\n",
    "    parsed_candidate_dataset = parsed_candidate_dataset.cache() #400 MB on machine mem\n",
    "    \n",
    "    # ====================================================\n",
    "    # Generate embedding vectors for each candidate\n",
    "    # ====================================================\n",
    "    logging.info(\"Starting candidate dataset mapping...\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    embs_iter = parsed_candidate_dataset.batch(10000).map(\n",
    "        lambda data: (\n",
    "            data[\"track_uri_can\"],\n",
    "            loaded_candidate_model(data)\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    embs = []\n",
    "    for emb in embs_iter:\n",
    "        embs.append(emb)\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = int((end_time - start_time) / 60)\n",
    "    logging.info(f\"elapsed_time   : {elapsed_time}\")\n",
    "    logging.info(f\"Length of embs : {len(embs)}\")\n",
    "    logging.info(f\"embeddings[0]  : {embs[0]}\")\n",
    "    \n",
    "    # ====================================================\n",
    "    # prep Track IDs and Vectors for JSON\n",
    "    # ====================================================\n",
    "    logging.info(\"Cleaning embeddings and track IDs...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # cleaned_embs = [x['output_1'].numpy()[0] for x in embs] #clean up the output\n",
    "    \n",
    "    cleaned_embs = []\n",
    "    track_uris = []\n",
    "    \n",
    "    for ids , embedding in embs:\n",
    "        cleaned_embs.extend(embedding.numpy())\n",
    "        track_uris.extend(ids.numpy())\n",
    "    \n",
    "    end_time = time.time()\n",
    "    elapsed_time = int((end_time - start_time) / 60)\n",
    "    logging.info(f\"elapsed_time           : {elapsed_time}\")\n",
    "    logging.info(f\"Length of cleaned_embs : {len(cleaned_embs)}\")\n",
    "    logging.info(f\"Length of track_uris: {len(track_uris)}\")\n",
    "    \n",
    "    track_uris_decoded = [z.decode(\"utf-8\") for z in track_uris]\n",
    "    logging.info(f\"Length of track_uris decoded: {len(track_uris_decoded)}\")\n",
    "    logging.info(f\"track_uris_decoded[0]       : {track_uris_decoded[0]}\")\n",
    "    \n",
    "    # check for bad records\n",
    "    bad_records = []\n",
    "\n",
    "    for i, emb in enumerate(cleaned_embs):\n",
    "        bool_emb = np.isnan(emb)\n",
    "        for val in bool_emb:\n",
    "            if val:\n",
    "                bad_records.append(i)\n",
    "\n",
    "    bad_record_filter = np.unique(bad_records)\n",
    "\n",
    "    logging.info(f\"bad_records: {len(bad_records)}\")\n",
    "    logging.info(f\"bad_record_filter: {len(bad_record_filter)}\")\n",
    "    \n",
    "    # ZIP together\n",
    "    logging.info(\"Zipping IDs and vectors ...\")\n",
    "    \n",
    "    track_uris_valid = []\n",
    "    emb_valid = []\n",
    "\n",
    "    for i, pair in enumerate(zip(track_uris_decoded, cleaned_embs)):\n",
    "        if i in bad_record_filter:\n",
    "            pass\n",
    "        else:\n",
    "            t_uri, embed = pair\n",
    "            track_uris_valid.append(t_uri)\n",
    "            emb_valid.append(embed)\n",
    "            \n",
    "    logging.info(f\"track_uris_valid[0]: {track_uris_valid[0]}\")\n",
    "    logging.info(f\"bad_records: {len(bad_records)}\")\n",
    "            \n",
    "    # ====================================================\n",
    "    # writting JSON file to GCS\n",
    "    # ====================================================\n",
    "    TIMESTAMP = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    embeddings_index_filename = f'candidate_embs.json'\n",
    "\n",
    "    with open(f'{embeddings_index_filename}', 'w') as f:\n",
    "        for prod, emb in zip(track_uris_valid, emb_valid):\n",
    "            f.write('{\"id\":\"' + str(prod) + '\",')\n",
    "            f.write('\"embedding\":[' + \",\".join(str(x) for x in list(emb)) + \"]}\")\n",
    "            f.write(\"\\n\")\n",
    "            \n",
    "    # write to GCS\n",
    "    INDEX_GCS_URI = f'{experiment_run_dir}/candidate-embeddings-{TIMESTAMP}'\n",
    "    logging.info(f\"INDEX_GCS_URI: {INDEX_GCS_URI}\")\n",
    "\n",
    "    DESTINATION_BLOB_NAME = embeddings_index_filename\n",
    "    SOURCE_FILE_NAME = embeddings_index_filename\n",
    "\n",
    "    logging.info(f\"DESTINATION_BLOB_NAME: {DESTINATION_BLOB_NAME}\")\n",
    "    logging.info(f\"SOURCE_FILE_NAME: {SOURCE_FILE_NAME}\")\n",
    "    \n",
    "    blob = Blob.from_string(os.path.join(INDEX_GCS_URI, DESTINATION_BLOB_NAME))\n",
    "    blob.bucket._client = storage_client\n",
    "    blob.upload_from_filename(SOURCE_FILE_NAME)\n",
    "    \n",
    "    return (\n",
    "        f'{INDEX_GCS_URI}',\n",
    "        # f'{INDEX_GCS_URI}',\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a839b3b-207f-4b65-a2f6-607a56dc7181",
   "metadata": {},
   "source": [
    "## Create ANN Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b7a65b44-3ad4-4168-af40-67289000a434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/train_pipes/create_ann_index.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_SRC}/{PIPELINES_SUB_DIR}/create_ann_index.py\n",
    "\n",
    "import kfp\n",
    "from typing import Any, Callable, Dict, NamedTuple, Optional, List\n",
    "from kfp.v2.dsl import (\n",
    "    Artifact, Dataset, Input, InputPath, \n",
    "    Model, Output, OutputPath, component, Metrics\n",
    ")\n",
    "@kfp.v2.dsl.component(\n",
    "    base_image=\"python:3.9\",\n",
    "    packages_to_install=[\n",
    "        'google-cloud-aiplatform==1.26.1',\n",
    "        'google-api-core==2.11.0'\n",
    "        # 'google-cloud-storage',\n",
    "    ],\n",
    ")\n",
    "def create_ann_index(\n",
    "    project: str,\n",
    "    location: str,\n",
    "    version: str, \n",
    "    vpc_network_name: str,\n",
    "    emb_index_gcs_uri: str,\n",
    "    dimensions: int,\n",
    "    ann_index_display_name: str,\n",
    "    approximate_neighbors_count: int,\n",
    "    distance_measure_type: str,\n",
    "    leaf_node_embedding_count: int,\n",
    "    leaf_nodes_to_search_percent: int, \n",
    "    ann_index_description: str,\n",
    "    # ann_index_labels: Dict, \n",
    ") -> NamedTuple('Outputs', [\n",
    "    ('ann_index_resource_uri', str),\n",
    "    ('ann_index', Artifact),\n",
    "]):\n",
    "    import logging\n",
    "    from google.cloud import aiplatform as vertex_ai\n",
    "    from datetime import datetime\n",
    "    import time\n",
    "\n",
    "    vertex_ai.init(\n",
    "        project=project,\n",
    "        location=location,\n",
    "    )\n",
    "    \n",
    "    VERSION = version.replace('_', '-')\n",
    "    \n",
    "    TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    \n",
    "    ENDPOINT = \"{}-aiplatform.googleapis.com\".format(location)\n",
    "    NETWORK_NAME = vpc_network_name\n",
    "    INDEX_DIR_GCS = emb_index_gcs_uri\n",
    "    PARENT = \"projects/{}/locations/{}\".format(project, location)\n",
    "\n",
    "    logging.info(f\"ENDPOINT: {ENDPOINT}\")\n",
    "    logging.info(f\"project: {project}\")\n",
    "    logging.info(f\"location: {location}\")\n",
    "    logging.info(f\"INDEX_DIR_GCS: {INDEX_DIR_GCS}\")\n",
    "    \n",
    "    display_name = f'{ann_index_display_name}-{VERSION}'\n",
    "    \n",
    "    logging.info(f\"display_name: {display_name}\")\n",
    "    \n",
    "    # ==============================================================================\n",
    "    # Create Index \n",
    "    # ==============================================================================\n",
    "\n",
    "    start = time.time()\n",
    "        \n",
    "    tree_ah_index = vertex_ai.MatchingEngineIndex.create_tree_ah_index(\n",
    "        display_name=display_name,\n",
    "        contents_delta_uri=f'{emb_index_gcs_uri}', # emb_index_gcs_uri,\n",
    "        dimensions=dimensions,\n",
    "        approximate_neighbors_count=approximate_neighbors_count,\n",
    "        distance_measure_type=distance_measure_type,\n",
    "        leaf_node_embedding_count=leaf_node_embedding_count,\n",
    "        leaf_nodes_to_search_percent=leaf_nodes_to_search_percent,\n",
    "        description=ann_index_description,\n",
    "        # labels=ann_index_labels,\n",
    "        sync=True,\n",
    "    )\n",
    "\n",
    "    end = time.time()\n",
    "    elapsed_time = round((end - start), 2)\n",
    "    logging.info(f'Elapsed time creating index: {elapsed_time} seconds\\n')\n",
    "    \n",
    "    ann_index_resource_uri = tree_ah_index.resource_name\n",
    "    logging.info(\"ann_index_resource_uri:\", ann_index_resource_uri) \n",
    "\n",
    "    return (\n",
    "      f'{ann_index_resource_uri}',\n",
    "      tree_ah_index,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b163cf-9bd7-4814-b1d4-eadc7554cf30",
   "metadata": {},
   "source": [
    "## Create brute force index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "758b11c7-0595-4787-a238-11bcbf3378ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/train_pipes/create_brute_force_index.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_SRC}/{PIPELINES_SUB_DIR}/create_brute_force_index.py\n",
    "\n",
    "import kfp\n",
    "from typing import Any, Callable, Dict, NamedTuple, Optional, List\n",
    "from kfp.v2.dsl import (\n",
    "    Artifact, Dataset, Input, InputPath, \n",
    "    Model, Output, OutputPath, component, Metrics\n",
    ")\n",
    "@kfp.v2.dsl.component(\n",
    "    base_image=\"python:3.9\",\n",
    "    packages_to_install=[\n",
    "        'google-cloud-aiplatform==1.26.1',\n",
    "        'google-api-core==2.11.0',\n",
    "        # 'google-cloud-storage',\n",
    "    ],\n",
    ")\n",
    "def create_brute_force_index(\n",
    "    project: str,\n",
    "    location: str,\n",
    "    version: str,\n",
    "    vpc_network_name: str,\n",
    "    emb_index_gcs_uri: str,\n",
    "    dimensions: int,\n",
    "    brute_force_index_display_name: str,\n",
    "    approximate_neighbors_count: int,\n",
    "    distance_measure_type: str,\n",
    "    brute_force_index_description: str,\n",
    "    # brute_force_index_labels: Dict,\n",
    ") -> NamedTuple('Outputs', [\n",
    "    ('brute_force_index_resource_uri', str),\n",
    "    ('brute_force_index', Artifact),\n",
    "]):\n",
    "\n",
    "    import logging\n",
    "    from google.cloud import aiplatform as vertex_ai\n",
    "    from datetime import datetime\n",
    "    import time\n",
    "\n",
    "    vertex_ai.init(\n",
    "        project=project,\n",
    "        location=location,\n",
    "    )\n",
    "    \n",
    "    TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    VERSION = version.replace('_', '-')\n",
    "    \n",
    "    ENDPOINT = \"{}-aiplatform.googleapis.com\".format(location)\n",
    "    NETWORK_NAME = vpc_network_name\n",
    "    INDEX_DIR_GCS = emb_index_gcs_uri\n",
    "    PARENT = \"projects/{}/locations/{}\".format(project, location)\n",
    "\n",
    "    logging.info(\"ENDPOINT: {}\".format(ENDPOINT))\n",
    "    logging.info(\"PROJECT_ID: {}\".format(project))\n",
    "    logging.info(\"REGION: {}\".format(location))\n",
    "    \n",
    "    display_name = f'{brute_force_index_display_name}_{VERSION}'\n",
    "    \n",
    "    logging.info(f\"display_name: {display_name}\")\n",
    "    \n",
    "    # ==============================================================================\n",
    "    # Create Index \n",
    "    # ==============================================================================\n",
    "\n",
    "    start = time.time()\n",
    "    \n",
    "    brute_force_index = vertex_ai.MatchingEngineIndex.create_brute_force_index(\n",
    "        display_name=display_name,\n",
    "        contents_delta_uri=f'{emb_index_gcs_uri}', # emb_index_gcs_uri,\n",
    "        dimensions=dimensions,\n",
    "        # approximate_neighbors_count=approximate_neighbors_count,\n",
    "        distance_measure_type=distance_measure_type,\n",
    "        description=brute_force_index_description,\n",
    "        # labels=brute_force_index_labels,\n",
    "        sync=True,\n",
    "    )\n",
    "    brute_force_index_resource_uri = brute_force_index.resource_name\n",
    "    print(\"brute_force_index_resource_uri:\",brute_force_index_resource_uri) \n",
    "\n",
    "    return (\n",
    "      f'{brute_force_index_resource_uri}',\n",
    "      brute_force_index,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c28ae77-5ce7-465c-ab5b-57d8ec8b3880",
   "metadata": {},
   "source": [
    "## Create ANN index endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fa1f7c39-6fd8-43c0-ab6a-21fabcd57da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/train_pipes/create_ann_index_endpoint_vpc.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_SRC}/{PIPELINES_SUB_DIR}/create_ann_index_endpoint_vpc.py\n",
    "\n",
    "import kfp\n",
    "from typing import Any, Callable, Dict, NamedTuple, Optional, List\n",
    "from kfp.v2.dsl import (\n",
    "    Artifact, Dataset, Input, InputPath, \n",
    "    Model, Output, OutputPath, component, Metrics\n",
    ")\n",
    "@kfp.v2.dsl.component(\n",
    "    base_image=\"python:3.9\",\n",
    "    packages_to_install=[\n",
    "        'google-cloud-aiplatform==1.26.1',\n",
    "        'google-api-core==2.11.0',\n",
    "    ],\n",
    ")\n",
    "def create_ann_index_endpoint_vpc(\n",
    "    ann_index_artifact: Input[Artifact],\n",
    "    project: str,\n",
    "    project_number: str,\n",
    "    location: str,\n",
    "    version: str,\n",
    "    vpc_network_name: str,\n",
    "    ann_index_endpoint_display_name: str,\n",
    "    ann_index_endpoint_description: str,\n",
    "    ann_index_resource_uri: str,\n",
    ") -> NamedTuple('Outputs', [\n",
    "    ('vpc_network_resource_uri', str),\n",
    "    ('ann_index_endpoint_resource_uri', str),\n",
    "    ('ann_index_endpoint', Artifact),\n",
    "    ('ann_index_endpoint_display_name', str),\n",
    "    ('ann_index_resource_uri', str),\n",
    "]):\n",
    "\n",
    "    import logging\n",
    "    from google.cloud import aiplatform as vertex_ai\n",
    "    from datetime import datetime\n",
    "    import time\n",
    "\n",
    "    vertex_ai.init(\n",
    "        project=project,\n",
    "        location=location,\n",
    "    )\n",
    "    \n",
    "    TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "    vpc_network_resource_uri = f'projects/{project_number}/global/networks/{vpc_network_name}'\n",
    "    logging.info(f\"vpc_network_resource_uri: {vpc_network_resource_uri}\")\n",
    "\n",
    "    ann_index_endpoint = vertex_ai.MatchingEngineIndexEndpoint.create(\n",
    "        display_name=f'{ann_index_endpoint_display_name}',\n",
    "        description=ann_index_endpoint_description,\n",
    "        network=vpc_network_resource_uri,\n",
    "    )\n",
    "    ann_index_endpoint_resource_uri = ann_index_endpoint.resource_name\n",
    "    logging.info(f\"ann_index_endpoint_resource_uri: {ann_index_endpoint_resource_uri}\")\n",
    "\n",
    "    return (\n",
    "        f'{vpc_network_resource_uri}',\n",
    "        f'{ann_index_endpoint_resource_uri}',\n",
    "        ann_index_endpoint,\n",
    "        f'{ann_index_endpoint_display_name}',\n",
    "        f'{ann_index_resource_uri}',\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d429b90f-25dc-47b9-b75e-1175f0bee26b",
   "metadata": {},
   "source": [
    "## Create brute force index endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e00bf2fa-bc5c-4289-b6a0-7cf3b6e525f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/train_pipes/create_brute_index_endpoint_vpc.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_SRC}/{PIPELINES_SUB_DIR}/create_brute_index_endpoint_vpc.py\n",
    "\n",
    "import kfp\n",
    "from typing import Any, Callable, Dict, NamedTuple, Optional, List\n",
    "from kfp.v2.dsl import (\n",
    "    Artifact, Dataset, Input, InputPath, \n",
    "    Model, Output, OutputPath, component, Metrics\n",
    ")\n",
    "@kfp.v2.dsl.component(\n",
    "    base_image=\"python:3.9\",\n",
    "    packages_to_install=[\n",
    "        'google-cloud-aiplatform==1.26.1',\n",
    "        'google-api-core==2.11.0',\n",
    "    ],\n",
    ")\n",
    "def create_brute_index_endpoint_vpc(\n",
    "    bf_index_artifact: Input[Artifact],\n",
    "    project: str,\n",
    "    project_number: str,\n",
    "    location: str,\n",
    "    version: str,\n",
    "    vpc_network_name: str,\n",
    "    brute_index_endpoint_display_name: str,\n",
    "    brute_index_endpoint_description: str,\n",
    "    brute_force_index_resource_uri: str,\n",
    ") -> NamedTuple('Outputs', [\n",
    "    ('vpc_network_resource_uri', str),\n",
    "    ('brute_index_endpoint_resource_uri', str),\n",
    "    ('brute_index_endpoint', Artifact),\n",
    "    ('brute_index_endpoint_display_name', str),\n",
    "    ('brute_force_index_resource_uri', str),\n",
    "]):\n",
    "\n",
    "    import logging\n",
    "    from google.cloud import aiplatform as vertex_ai\n",
    "    from datetime import datetime\n",
    "    import time\n",
    "\n",
    "    vertex_ai.init(\n",
    "        project=project,\n",
    "        location=location,\n",
    "    )\n",
    "    \n",
    "    TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "    vpc_network_resource_uri = f'projects/{project_number}/global/networks/{vpc_network_name}'\n",
    "    logging.info(f\"vpc_network_resource_uri: {vpc_network_resource_uri}\")\n",
    "\n",
    "    brute_index_endpoint = vertex_ai.MatchingEngineIndexEndpoint.create(\n",
    "        display_name=f'{brute_index_endpoint_display_name}',\n",
    "        description=brute_index_endpoint_description,\n",
    "        network=vpc_network_resource_uri,\n",
    "    )\n",
    "    brute_index_endpoint_resource_uri = brute_index_endpoint.resource_name\n",
    "    logging.info(f\"brute_index_endpoint_resource_uri: {brute_index_endpoint_resource_uri}\")\n",
    "\n",
    "    return (\n",
    "      f'{vpc_network_resource_uri}',\n",
    "      f'{brute_index_endpoint_resource_uri}',\n",
    "      brute_index_endpoint,\n",
    "      f'{brute_index_endpoint_display_name}',\n",
    "      f'{brute_force_index_resource_uri}',\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24b4ef9-ca2f-4e3b-8802-dd2a1abb1ddd",
   "metadata": {},
   "source": [
    "## Deploy ANN Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f2a60680-96c2-4aaa-bea9-579bc26f6edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/train_pipes/deploy_ann_index.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_SRC}/{PIPELINES_SUB_DIR}/deploy_ann_index.py\n",
    "\n",
    "import kfp\n",
    "from typing import Any, Callable, Dict, NamedTuple, Optional, List\n",
    "from kfp.v2.dsl import (\n",
    "    Artifact, Dataset, Input, InputPath, \n",
    "    Model, Output, OutputPath, component, Metrics\n",
    ")\n",
    "@kfp.v2.dsl.component(\n",
    "    base_image=\"python:3.9\",\n",
    "    packages_to_install=[\n",
    "        'google-cloud-aiplatform==1.26.1',\n",
    "        'google-api-core==2.11.0',\n",
    "    ]\n",
    ")\n",
    "def deploy_ann_index(\n",
    "    project: str,\n",
    "    location: str,\n",
    "    version: str,\n",
    "    deployed_ann_index_name: str,\n",
    "    ann_index_resource_uri: str,\n",
    "    index_endpoint_resource_uri: str,\n",
    ") -> NamedTuple('Outputs', [\n",
    "    ('index_endpoint_resource_uri', str),\n",
    "    ('ann_index_resource_uri', str),\n",
    "    ('deployed_ann_index_name', str),\n",
    "    ('deployed_ann_index', Artifact),\n",
    "]):\n",
    "  \n",
    "    import logging\n",
    "    from google.cloud import aiplatform as vertex_ai\n",
    "    from datetime import datetime\n",
    "    import time\n",
    "\n",
    "    vertex_ai.init(\n",
    "        project=project,\n",
    "        location=location,\n",
    "    )\n",
    "    \n",
    "    # define vars\n",
    "    TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    # deployed_ann_index_name = deployed_ann_index_name.replace('-', '_')\n",
    "    # logging.info(f\"deployed_ann_index_name: {deployed_ann_index_name}\")\n",
    "    \n",
    "    DEPLOYED_INDEX_NAME = f'{deployed_ann_index_name}-{TIMESTAMP}'\n",
    "    logging.info(f\"DEPLOYED_INDEX_NAME: {DEPLOYED_INDEX_NAME}\")\n",
    "    \n",
    "    # init index\n",
    "    ann_index = vertex_ai.MatchingEngineIndex(\n",
    "      index_name=ann_index_resource_uri\n",
    "    )\n",
    "    ann_index_resource_uri = ann_index.resource_name\n",
    "    logging.info(f\"ann_index_resource_uri: {ann_index_resource_uri}\")\n",
    "\n",
    "    # init index endpoint\n",
    "    index_endpoint = vertex_ai.MatchingEngineIndexEndpoint(\n",
    "      index_endpoint_resource_uri\n",
    "    )\n",
    "    logging.info(f\"index_endpoint: {index_endpoint}\")\n",
    "\n",
    "    # deploy index to endpoint\n",
    "    index_endpoint = index_endpoint.deploy_index(\n",
    "      index=ann_index, \n",
    "      deployed_index_id=DEPLOYED_INDEX_NAME\n",
    "    )\n",
    "\n",
    "    logging.info(f\"index_endpoint.deployed_indexes: {index_endpoint.deployed_indexes}\")\n",
    "    INDEX_ID = index_endpoint.deployed_indexes[0].id\n",
    "    logging.info(f\"INDEX_ID: {INDEX_ID}\")\n",
    "\n",
    "    return (\n",
    "      f'{index_endpoint_resource_uri}',\n",
    "      f'{ann_index_resource_uri}',\n",
    "      f'{deployed_ann_index_name}',\n",
    "      ann_index,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2557b1-3b03-48fc-a658-4b6c1b5cc812",
   "metadata": {},
   "source": [
    "## Deploy brute force Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f073f3ad-c410-4c08-bd50-1b3ffcbe728c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/train_pipes/deploy_brute_index.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_SRC}/{PIPELINES_SUB_DIR}/deploy_brute_index.py\n",
    "\n",
    "import kfp\n",
    "from typing import Any, Callable, Dict, NamedTuple, Optional, List\n",
    "from kfp.v2.dsl import (\n",
    "    Artifact, Dataset, Input, InputPath, \n",
    "    Model, Output, OutputPath, component, Metrics\n",
    ")\n",
    "@kfp.v2.dsl.component(\n",
    "    base_image=\"python:3.9\",\n",
    "    packages_to_install=[\n",
    "        'google-cloud-aiplatform==1.26.1',\n",
    "        'google-api-core==2.11.0',\n",
    "    ],\n",
    ")\n",
    "def deploy_brute_index(\n",
    "    project: str,\n",
    "    location: str,\n",
    "    version: str,\n",
    "    deployed_brute_force_index_name: str,\n",
    "    brute_force_index_resource_uri: str,\n",
    "    index_endpoint_resource_uri: str,\n",
    ") -> NamedTuple('Outputs', [\n",
    "    ('index_endpoint_resource_uri', str),\n",
    "    ('brute_force_index_resource_uri', str),\n",
    "    ('deployed_brute_force_index_name', str),\n",
    "    ('deployed_brute_force_index', Artifact),\n",
    "]):\n",
    "  \n",
    "    import logging\n",
    "    from google.cloud import aiplatform as vertex_ai\n",
    "    from datetime import datetime\n",
    "    import time\n",
    "\n",
    "    vertex_ai.init(\n",
    "        project=project,\n",
    "        location=location,\n",
    "    )\n",
    "    # define vars\n",
    "    TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    # deployed_brute_force_index_name = deployed_brute_force_index_name.replace('-', '_')\n",
    "    # logging.info(f\"deployed_brute_force_index_name: {deployed_brute_force_index_name}\")\n",
    "    \n",
    "    DEPLOYED_INDEX_NAME = f'{deployed_brute_force_index_name}-{TIMESTAMP}'\n",
    "    logging.info(f\"DEPLOYED_INDEX_NAME: {DEPLOYED_INDEX_NAME}\")\n",
    "\n",
    "    # init index\n",
    "    brute_index = vertex_ai.MatchingEngineIndex(\n",
    "        index_name=brute_force_index_resource_uri\n",
    "    )\n",
    "    brute_force_index_resource_uri = brute_index.resource_name\n",
    "    logging.info(f\"brute_force_index_resource_uri: {brute_force_index_resource_uri}\")\n",
    "\n",
    "    # init index endpoint\n",
    "    index_endpoint = vertex_ai.MatchingEngineIndexEndpoint(index_endpoint_resource_uri)\n",
    "    logging.info(f\"index_endpoint: {index_endpoint}\")\n",
    "\n",
    "    # deploy index to endpoint\n",
    "    index_endpoint = index_endpoint.deploy_index(\n",
    "        index=brute_index, \n",
    "        deployed_index_id=DEPLOYED_INDEX_NAME\n",
    "    )\n",
    "\n",
    "    logging.info(f\"index_endpoint.deployed_indexes: {index_endpoint.deployed_indexes}\")\n",
    "    INDEX_ID = index_endpoint.deployed_indexes[0].id\n",
    "    logging.info(f\"INDEX_ID: {INDEX_ID}\")\n",
    "\n",
    "    return (\n",
    "      f'{index_endpoint_resource_uri}',\n",
    "      f'{brute_force_index_resource_uri}',\n",
    "      f'{deployed_brute_force_index_name}', #-{TIMESTAMP}',\n",
    "      brute_index,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95592b63-0b33-4e87-bf81-d44fbba00722",
   "metadata": {},
   "source": [
    "## Model monitoring job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "050e6925-788b-4760-b659-ef212c3270ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/train_pipes/model_monitoring_config.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_SRC}/{PIPELINES_SUB_DIR}/model_monitoring_config.py\n",
    "\n",
    "import kfp\n",
    "from typing import Any, Callable, Dict, NamedTuple, Optional, List\n",
    "from kfp.v2.dsl import (\n",
    "    Artifact, Dataset, Input, InputPath, \n",
    "    Model, Output, OutputPath, component, Metrics\n",
    ")\n",
    "@kfp.v2.dsl.component(\n",
    "    base_image=\"python:3.9\",\n",
    "    packages_to_install=[\n",
    "        'google-cloud-aiplatform==1.26.1',\n",
    "        'google-cloud-pipeline-components',\n",
    "        'google-cloud-storage',\n",
    "        'tensorflow==2.11.0',\n",
    "        'numpy'\n",
    "    ],\n",
    ")\n",
    "def model_monitoring_config(\n",
    "    project: str,\n",
    "    location: str,\n",
    "    version: str,\n",
    "    prefix: str,\n",
    "    emails: str,\n",
    "    train_output_gcs_bucket: str,\n",
    "    # feature_dict: dict, # TODO\n",
    "    bq_dataset: str,\n",
    "    bq_train_table: str,\n",
    "    experiment_name: str,\n",
    "    experiment_run: str,\n",
    "    endpoint: str,\n",
    "):\n",
    "    # TODO - imports\n",
    "    \n",
    "    import logging\n",
    "    from datetime import datetime\n",
    "    import time\n",
    "    import numpy as np\n",
    "    import pickle as pkl\n",
    "    \n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "    # google cloud SDKs\n",
    "    from google.cloud import storage\n",
    "    from google.cloud import aiplatform as vertex_ai\n",
    "    from google.cloud.aiplatform import model_monitoring\n",
    "    \n",
    "    from google.cloud import storage\n",
    "    from google.cloud.storage.bucket import Bucket\n",
    "    from google.cloud.storage.blob import Blob\n",
    "    \n",
    "    from google.protobuf import json_format\n",
    "    from google.protobuf.json_format import Parse\n",
    "    from google.protobuf.struct_pb2 import Value\n",
    "    from google_cloud_pipeline_components.proto.gcp_resources_pb2 import GcpResources\n",
    "    \n",
    "    vertex_ai.init(\n",
    "        project=project,\n",
    "        location=location,\n",
    "    )\n",
    "    storage_client = storage.Client(project=project)\n",
    "    \n",
    "    # ====================================================\n",
    "    # helper functions\n",
    "    # ====================================================\n",
    "    \n",
    "    def download_blob(bucket_name, source_gcs_obj, local_filename):\n",
    "        \"\"\"Uploads a file to the bucket.\"\"\"\n",
    "        # storage_client = storage.Client(project=project_number)\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "        blob = bucket.blob(source_gcs_obj)\n",
    "        blob.download_to_filename(local_filename)\n",
    "        \n",
    "        filehandler = open(f'{local_filename}', 'rb')\n",
    "        loaded_dict = pkl.load(filehandler)\n",
    "        filehandler.close()\n",
    "        \n",
    "        logging.info(f\"File {local_filename} downloaded from gs://{bucket_name}/{source_gcs_obj}\")\n",
    "        \n",
    "        return loaded_dict\n",
    "    \n",
    "    # ====================================================\n",
    "    # get deployed model endpoint\n",
    "    # ====================================================\n",
    "    logging.info(f\"Endpoint = {endpoint}\")\n",
    "    gcp_resources = Parse(endpoint, GcpResources())\n",
    "    logging.info(f\"gcp_resources = {gcp_resources}\")\n",
    "    \n",
    "    _endpoint_resource = gcp_resources.resources[0].resource_uri\n",
    "    logging.info(f\"_endpoint_resource = {_endpoint_resource}\")\n",
    "    \n",
    "    _endpoint_uri = \"/\".join(_endpoint_resource.split(\"/\")[-8:-2])\n",
    "    logging.info(f\"_endpoint_uri = {_endpoint_uri}\")\n",
    "    \n",
    "    # define endpoint resource in component\n",
    "    _endpoint = vertex_ai.Endpoint(_endpoint_uri)\n",
    "    logging.info(f\"_endpoint defined\")\n",
    "    \n",
    "    \n",
    "    USER_EMAILS = [emails]\n",
    "    alert_config = model_monitoring.EmailAlertConfig(USER_EMAILS, enable_logging=True)\n",
    "    \n",
    "    MONITOR_INTERVAL = 1\n",
    "    schedule_config = model_monitoring.ScheduleConfig(monitor_interval=MONITOR_INTERVAL)\n",
    "    \n",
    "    SAMPLE_RATE = 0.8\n",
    "\n",
    "    logging_sampling_strategy = model_monitoring.RandomSampleConfig(sample_rate=SAMPLE_RATE)\n",
    "    \n",
    "    # ===================================================\n",
    "    # feature dict\n",
    "    # ===================================================\n",
    "    QUERY_FILENAME = 'query_feats_dict.pkl'\n",
    "    # FEATURES_PREFIX = f'{experiment_name}/{experiment_run}/features'\n",
    "    GCS_PATH_TO_BLOB = f'{experiment_name}/{experiment_run}/features/{QUERY_FILENAME}'\n",
    "    \n",
    "    loaded_feat_dict = download_blob(\n",
    "        bucket_name=train_output_gcs_bucket,\n",
    "        source_gcs_obj=GCS_PATH_TO_BLOB,\n",
    "        local_filename=QUERY_FILENAME\n",
    "    )\n",
    "    logging.info(f'loaded_feat_dict: {loaded_feat_dict}')\n",
    "    \n",
    "    filehandler = open(QUERY_FILENAME, 'rb')\n",
    "    FEAT_DICT = pkl.load(filehandler)\n",
    "    filehandler.close()\n",
    "\n",
    "    \n",
    "    feature_names = list(FEAT_DICT.keys())\n",
    "\n",
    "    # =========================== #\n",
    "    ##   Feature value drift     ##\n",
    "    # =========================== #\n",
    "    DRIFT_THRESHOLD_VALUE = 0.05\n",
    "    ATTRIBUTION_DRIFT_THRESHOLD_VALUE = 0.05\n",
    "    \n",
    "    drift_thresholds = dict()\n",
    "\n",
    "    for feature in feature_names:\n",
    "        if feature in drift_thresholds:\n",
    "            print(\"feature name already in dict\")\n",
    "        else:\n",
    "            drift_thresholds[feature] = DRIFT_THRESHOLD_VALUE\n",
    "\n",
    "    logging.info(f\"drift_thresholds      : {drift_thresholds}\\n\")\n",
    "    \n",
    "    drift_config = model_monitoring.DriftDetectionConfig(\n",
    "        drift_thresholds=drift_thresholds,\n",
    "        # attribute_drift_thresholds=attr_drift_thresholds,\n",
    "    )\n",
    "\n",
    "    # =========================== #\n",
    "    ##   Feature value skew      ##\n",
    "    # =========================== #\n",
    "    TRAIN_DATA_SOURCE_URI = f\"bq://{project}.{bq_dataset}.{bq_train_table}\"\n",
    "    logging.info(f\"TRAIN_DATA_SOURCE_URI = {TRAIN_DATA_SOURCE_URI}\")\n",
    "    \n",
    "    SKEW_THRESHOLD_VALUE = 0.05\n",
    "    ATTRIBUTION_SKEW_THRESHOLD_VALUE = 0.05\n",
    "    \n",
    "    skew_thresholds = dict()\n",
    "\n",
    "    for feature in feature_names:\n",
    "        if feature in skew_thresholds:\n",
    "            logging.info(\"feature name already in dict\")\n",
    "        else:\n",
    "            skew_thresholds[feature] = SKEW_THRESHOLD_VALUE        \n",
    "    logging.info(f\"skew_thresholds      : {skew_thresholds}\\n\")\n",
    "    \n",
    "    # skew config\n",
    "    skew_config = model_monitoring.SkewDetectionConfig(\n",
    "        data_source=TRAIN_DATA_SOURCE_URI,\n",
    "        # data_format = TRAIN_DATA_FORMAT, # only used if source in GCS\n",
    "        skew_thresholds=skew_thresholds,\n",
    "        # attribute_skew_thresholds=attribute_skew_thresholds,\n",
    "        # target_field=TARGET, # no target; embedding model\n",
    "    )\n",
    "    \n",
    "    # ====================================================\n",
    "    # objective_config\n",
    "    # ====================================================\n",
    "    objective_config = model_monitoring.ObjectiveConfig(\n",
    "        skew_detection_config=skew_config,\n",
    "        drift_detection_config=drift_config,\n",
    "        explanation_config=None,\n",
    "    )\n",
    "    \n",
    "    # ====================================================\n",
    "    # launch monitoring_job\n",
    "    # ====================================================\n",
    "    \n",
    "    JOB_DISPLAY_NAME = f\"mm_pipe_{experiment_run}_{prefix}\"\n",
    "    logging.info(f\"JOB_DISPLAY_NAME: {JOB_DISPLAY_NAME}\")\n",
    "\n",
    "    monitoring_job = vertex_ai.ModelDeploymentMonitoringJob.create(\n",
    "        display_name=JOB_DISPLAY_NAME,\n",
    "        project=project,\n",
    "        location=location,\n",
    "        endpoint=_endpoint,\n",
    "        logging_sampling_strategy=logging_sampling_strategy,\n",
    "        schedule_config=schedule_config,\n",
    "        alert_config=alert_config,\n",
    "        objective_configs=objective_config,\n",
    "    )\n",
    "    \n",
    "    logging.info(f\"monitoring_job: {monitoring_job.resource_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9f8169-9b2c-4632-afaf-2d2ed0631f31",
   "metadata": {},
   "source": [
    "## Test query model endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c88bec31-9f44-4d8b-a2bd-fce98bfe8c7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/train_pipes/test_model_endpoint.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_SRC}/{PIPELINES_SUB_DIR}/test_model_endpoint.py\n",
    "\n",
    "import kfp\n",
    "from typing import Any, Callable, Dict, NamedTuple, Optional, List\n",
    "from kfp.v2.dsl import (\n",
    "    Artifact, Dataset, Input, InputPath, \n",
    "    Model, Output, OutputPath, component, Metrics\n",
    ")\n",
    "@kfp.v2.dsl.component(\n",
    "    base_image=\"python:3.9\",\n",
    "    packages_to_install=[\n",
    "        'google-cloud-aiplatform==1.26.1',\n",
    "        'google-cloud-pipeline-components',\n",
    "        'google-cloud-storage',\n",
    "        'tensorflow==2.11.0',\n",
    "        'numpy'\n",
    "    ],\n",
    ")\n",
    "def test_model_endpoint(\n",
    "    project: str,\n",
    "    location: str,\n",
    "    version: str,\n",
    "    train_output_gcs_bucket: str,\n",
    "    many_test_instances_gcs_filename: str,\n",
    "    experiment_name: str,\n",
    "    experiment_run: str,\n",
    "    endpoint: str, # Input[Artifact],\n",
    "    # feature_dict: dict,\n",
    "    # metrics: Output[Metrics],\n",
    "):\n",
    "    \n",
    "    import logging\n",
    "    from datetime import datetime\n",
    "    import time\n",
    "    import numpy as np\n",
    "    import pickle as pkl\n",
    "    \n",
    "    from google.cloud import aiplatform as vertex_ai\n",
    "    \n",
    "    from google.cloud import storage\n",
    "    from google.cloud.storage.bucket import Bucket\n",
    "    from google.cloud.storage.blob import Blob\n",
    "    \n",
    "    from google.protobuf import json_format\n",
    "    from google.protobuf.json_format import Parse\n",
    "    from google.protobuf.struct_pb2 import Value\n",
    "    from google_cloud_pipeline_components.proto.gcp_resources_pb2 import GcpResources\n",
    "\n",
    "    import tensorflow as tf\n",
    "\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "    vertex_ai.init(\n",
    "        project=project,\n",
    "        location=location,\n",
    "    )\n",
    "    storage_client = storage.Client(project=project)\n",
    "    \n",
    "    # ====================================================\n",
    "    # helper functions\n",
    "    # ====================================================\n",
    "    \n",
    "    def download_blob(bucket_name, source_gcs_obj, local_filename):\n",
    "        \"\"\"Uploads a file to the bucket.\"\"\"\n",
    "        # storage_client = storage.Client(project=project_number)\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "        blob = bucket.blob(source_gcs_obj)\n",
    "        blob.download_to_filename(local_filename)\n",
    "        \n",
    "        filehandler = open(f'{local_filename}', 'rb')\n",
    "        loaded_dict = pkl.load(filehandler)\n",
    "        filehandler.close()\n",
    "        \n",
    "        logging.info(f\"File {local_filename} downloaded from gs://{bucket_name}/{source_gcs_obj}\")\n",
    "        \n",
    "        return loaded_dict\n",
    "    \n",
    "    # ===================================================\n",
    "    # load test instance\n",
    "    # ===================================================\n",
    "    LOCAL_INSTANCE_FILE = 'test_instance_list.pkl'\n",
    "    GCS_PATH_TO_BLOB = f'{experiment_name}/{experiment_run}/{many_test_instances_gcs_filename}'\n",
    "    LOADED_TEST_LIST = f'loaded_{LOCAL_INSTANCE_FILE}'\n",
    "    \n",
    "    loaded_test_instance = download_blob(\n",
    "        bucket_name=train_output_gcs_bucket,\n",
    "        source_gcs_obj=GCS_PATH_TO_BLOB,\n",
    "        local_filename=LOADED_TEST_LIST\n",
    "    )\n",
    "    logging.info(f'loaded_test_instance: {loaded_test_instance}')\n",
    "    \n",
    "    filehandler = open(LOADED_TEST_LIST, 'rb')\n",
    "    LIST_OF_DICTS = pkl.load(filehandler)\n",
    "    filehandler.close()\n",
    "    \n",
    "    logging.info(f'len(LIST_OF_DICTS): {len(LIST_OF_DICTS)}')\n",
    "    \n",
    "    # LIST_OF_DICTS[200]\n",
    "    \n",
    "    # ====================================================\n",
    "    # get deployed model endpoint\n",
    "    # ====================================================\n",
    "    logging.info(f\"Endpoint = {endpoint}\")\n",
    "    gcp_resources = Parse(endpoint, GcpResources())\n",
    "    logging.info(f\"gcp_resources = {gcp_resources}\")\n",
    "    \n",
    "    _endpoint_resource = gcp_resources.resources[0].resource_uri\n",
    "    logging.info(f\"_endpoint_resource = {_endpoint_resource}\")\n",
    "    \n",
    "    _endpoint_uri = \"/\".join(_endpoint_resource.split(\"/\")[-8:-2])\n",
    "    logging.info(f\"_endpoint_uri = {_endpoint_uri}\")\n",
    "    \n",
    "    # define endpoint resource in component\n",
    "    _endpoint = vertex_ai.Endpoint(_endpoint_uri)\n",
    "    logging.info(f\"_endpoint defined\")\n",
    "    \n",
    "    \n",
    "    # ====================================================\n",
    "    # Send predictions\n",
    "    # ====================================================\n",
    "    # TOTAL_ROUNDS = 4\n",
    "    SLEEP_SECONDS = 2 \n",
    "    START=1\n",
    "    END=4\n",
    "\n",
    "    logging.info(f\"testing online endpoint for {END} rounds\")\n",
    "    \n",
    "    for i in range(START, END+1):\n",
    "        \n",
    "        count = 0\n",
    "\n",
    "        for test in LIST_OF_DICTS:\n",
    "            response = _endpoint.predict(instances=[test])\n",
    "\n",
    "            if count > 0 and count % 250 == 0:\n",
    "                logging.info(f\"{count} prediciton requests..\")\n",
    "\n",
    "            count += 1\n",
    "            \n",
    "        logging.info(f\"finsihed round {i} of {END}\")\n",
    "        time.sleep(SLEEP_SECONDS)\n",
    "        \n",
    "    logging.info(f\"endpoint test complete - {count} predictions sent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4942c99-bcd0-4a85-be7b-7bb0d19aab46",
   "metadata": {},
   "source": [
    "## Send skewed traffic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6061d9d3-825a-47e5-ba53-dbae1a9b4b70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/train_pipes/send_skewed_traffic.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_SRC}/{PIPELINES_SUB_DIR}/send_skewed_traffic.py\n",
    "\n",
    "import kfp\n",
    "from typing import Any, Callable, Dict, NamedTuple, Optional, List\n",
    "from kfp.v2.dsl import (\n",
    "    Artifact, Dataset, Input, InputPath, \n",
    "    Model, Output, OutputPath, component, Metrics\n",
    ")\n",
    "@kfp.v2.dsl.component(\n",
    "    base_image=\"python:3.9\",\n",
    "    packages_to_install=[\n",
    "        'google-cloud-aiplatform==1.26.1',\n",
    "        'google-cloud-pipeline-components',\n",
    "        'google-cloud-storage',\n",
    "        'tensorflow==2.11.0',\n",
    "        'numpy'\n",
    "    ],\n",
    ")\n",
    "def send_skewed_traffic(\n",
    "    project: str,\n",
    "    location: str,\n",
    "    version: str,\n",
    "    train_output_gcs_bucket: str,\n",
    "    experiment_name: str,\n",
    "    experiment_run: str,\n",
    "    endpoint: str, # Input[Artifact],\n",
    "    # feature_dict: dict,\n",
    "    # metrics: Output[Metrics],\n",
    "):\n",
    "    \n",
    "    import logging\n",
    "    from datetime import datetime\n",
    "    import time\n",
    "    import numpy as np\n",
    "    import pickle as pkl\n",
    "    \n",
    "    from google.cloud import aiplatform as vertex_ai\n",
    "    \n",
    "    from google.cloud import storage\n",
    "    from google.cloud.storage.bucket import Bucket\n",
    "    from google.cloud.storage.blob import Blob\n",
    "    \n",
    "    from google.protobuf import json_format\n",
    "    from google.protobuf.json_format import Parse\n",
    "    from google.protobuf.struct_pb2 import Value\n",
    "    from google_cloud_pipeline_components.proto.gcp_resources_pb2 import GcpResources\n",
    "\n",
    "    import tensorflow as tf\n",
    "\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "    vertex_ai.init(\n",
    "        project=project,\n",
    "        location=location,\n",
    "    )\n",
    "    storage_client = storage.Client(project=project)\n",
    "    \n",
    "    # ====================================================\n",
    "    # helper functions\n",
    "    # ====================================================\n",
    "    def download_blob(bucket_name, source_gcs_obj, local_filename):\n",
    "        \"\"\"Uploads a file to the bucket.\"\"\"\n",
    "        # storage_client = storage.Client(project=project_number)\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "        blob = bucket.blob(source_gcs_obj)\n",
    "        blob.download_to_filename(local_filename)\n",
    "        \n",
    "        filehandler = open(f'{local_filename}', 'rb')\n",
    "        loaded_dict = pkl.load(filehandler)\n",
    "        filehandler.close()\n",
    "        \n",
    "        logging.info(f\"File {local_filename} downloaded from gs://{bucket_name}/{source_gcs_obj}\")\n",
    "        \n",
    "        return loaded_dict\n",
    "    \n",
    "    # ====================================================\n",
    "    # get deployed model endpoint\n",
    "    # ====================================================\n",
    "    logging.info(f\"Endpoint = {endpoint}\")\n",
    "    gcp_resources = Parse(endpoint, GcpResources())\n",
    "    logging.info(f\"gcp_resources = {gcp_resources}\")\n",
    "    \n",
    "    _endpoint_resource = gcp_resources.resources[0].resource_uri\n",
    "    logging.info(f\"_endpoint_resource = {_endpoint_resource}\")\n",
    "    \n",
    "    _endpoint_uri = \"/\".join(_endpoint_resource.split(\"/\")[-8:-2])\n",
    "    logging.info(f\"_endpoint_uri = {_endpoint_uri}\")\n",
    "    \n",
    "    # define endpoint resource in component\n",
    "    _endpoint = vertex_ai.Endpoint(_endpoint_uri)\n",
    "    logging.info(f\"_endpoint defined\")\n",
    "    \n",
    "    # ===================================================\n",
    "    # load test instance\n",
    "    # ===================================================\n",
    "    LOCAL_INSTANCE_FILE = 'test_instance_list.pkl'\n",
    "    GCS_PATH_TO_BLOB = f'{experiment_name}/{experiment_run}/{many_test_instances_gcs_filename}'\n",
    "    LOADED_TEST_LIST = f'loaded_{LOCAL_INSTANCE_FILE}'\n",
    "    \n",
    "    loaded_test_instance = download_blob(\n",
    "        bucket_name=train_output_gcs_bucket,\n",
    "        source_gcs_obj=GCS_PATH_TO_BLOB,\n",
    "        local_filename=LOADED_TEST_LIST\n",
    "    )\n",
    "    logging.info(f'loaded_test_instance: {loaded_test_instance}')\n",
    "    \n",
    "    filehandler = open(LOADED_TEST_LIST, 'rb')\n",
    "    LIST_OF_DICTS = pkl.load(filehandler)\n",
    "    filehandler.close()\n",
    "    \n",
    "    logging.info(f'len(LIST_OF_DICTS): {len(LIST_OF_DICTS)}')\n",
    "    \n",
    "    # ====================================================\n",
    "    # load skew features stats\n",
    "    # ====================================================\n",
    "    SKEW_FEATURES_STATS_FILE = 'skew_feat_stats.pkl'\n",
    "    GCS_PATH_TO_BLOB = f'{experiment_name}/{experiment_run}/{SKEW_FEATURES_STATS_FILE}'\n",
    "    LOADED_SKEW_FEATURES_STATS_FILE = f\"loaded_{SKEW_FEATURES_STATS_FILE}\"\n",
    "    logging.info(f'loading: {LOADED_SKEW_FEATURES_STATS_FILE}')\n",
    "    \n",
    "    loaded_skew_test_instance = download_blob(\n",
    "        bucket_name=train_output_gcs_bucket,\n",
    "        source_gcs_obj=GCS_PATH_TO_BLOB,\n",
    "        local_filename=LOADED_SKEW_FEATURES_STATS_FILE\n",
    "    )\n",
    "    logging.info(f'loaded_skew_test_instance: {loaded_skew_test_instance}')\n",
    "    \n",
    "    filehandler_v2 = open(LOADED_SKEW_FEATURES_STATS_FILE, 'rb')\n",
    "    SKEW_FEATURES = pkl.load(filehandler_v2)\n",
    "    filehandler_v2.close()\n",
    "    \n",
    "    mean_durations, std_durations = SKEW_FEATURES['pl_duration_ms_new']\n",
    "    mean_num_songs, std_num_songs = SKEW_FEATURES['num_pl_songs_new']\n",
    "    mean_num_artists, std_num_artists = SKEW_FEATURES['num_pl_artists_new']\n",
    "    mean_num_albums, std_num_albums = SKEW_FEATURES['num_pl_albums_new']\n",
    "    \n",
    "    logging.info(f\"std_durations   : {round(std_durations, 0)}\")\n",
    "    logging.info(f\"std_num_songs   : {round(std_num_songs, 0)}\")\n",
    "    logging.info(f\"std_num_artists : {round(std_num_artists, 0)}\")\n",
    "    logging.info(f\"std_num_albums  : {round(std_num_albums, 0)}\\n\")\n",
    "    \n",
    "    def monitoring_test(endpoint, instances, skew_feat_stat, start=2, end=4):\n",
    "\n",
    "        mean_durations, std_durations = skew_feat_stat['pl_duration_ms_new']\n",
    "        mean_num_songs, std_num_songs = skew_feat_stat['num_pl_songs_new']\n",
    "        mean_num_artists, std_num_artists = skew_feat_stat['num_pl_artists_new']\n",
    "        mean_num_albums, std_num_albums = skew_feat_stat['num_pl_albums_new']\n",
    "        \n",
    "        logging.info(f\"std_durations   : {round(std_durations, 0)}\")\n",
    "        logging.info(f\"std_num_songs   : {round(std_num_songs, 0)}\")\n",
    "        logging.info(f\"std_num_artists : {round(std_num_artists, 0)}\")\n",
    "        logging.info(f\"std_num_albums  : {round(std_num_albums, 0)}\\n\")\n",
    "\n",
    "        total_preds = 0\n",
    "\n",
    "        for multiplier in range(start, end+1):\n",
    "\n",
    "            print(f\"multiplier: {multiplier}\")\n",
    "\n",
    "            pred_count = 0\n",
    "\n",
    "            for example in instances:\n",
    "                list_dict = {}\n",
    "\n",
    "                example['pl_duration_ms_new'] = round(std_durations * multiplier, 0)\n",
    "                example['num_pl_songs_new'] = round(std_num_songs * multiplier, 0)\n",
    "                example['num_pl_artists_new'] = round(std_num_artists * multiplier, 0)\n",
    "                example['num_pl_albums_new'] = round(std_num_albums * multiplier, 0)\n",
    "                # list_of_skewed_instances.append(example)\n",
    "\n",
    "                response = endpoint.predict(instances=[example])\n",
    "\n",
    "                if pred_count > 0 and pred_count % 250 == 0:\n",
    "                    print(f\"pred_count: {pred_count}\")\n",
    "\n",
    "                pred_count += 1\n",
    "                total_preds += 1\n",
    "\n",
    "            logging.info(f\"sent {pred_count} pred requests with {multiplier}X multiplier\")\n",
    "\n",
    "        logging.info(f\"sent {total_preds} total pred requests\")\n",
    "        \n",
    "    # send skewed traffic\n",
    "    monitoring_test(\n",
    "        endpoint=_endpoint, \n",
    "        instances=LIST_OF_DICTS,\n",
    "        skew_feat_stat=SKEW_FEATURES,\n",
    "        start=2, \n",
    "        end=8\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8331413f-8639-4c2d-b948-1162d97bec13",
   "metadata": {},
   "source": [
    "## Test index recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "53209e5c-2050-4333-a994-4133b8ee58b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/train_pipes/test_model_index_endpoint.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_SRC}/{PIPELINES_SUB_DIR}/test_model_index_endpoint.py\n",
    "\n",
    "import kfp\n",
    "from typing import Any, Callable, Dict, NamedTuple, Optional, List\n",
    "from kfp.v2.dsl import (\n",
    "    Artifact, Dataset, Input, InputPath, \n",
    "    Model, Output, OutputPath, component, Metrics\n",
    ")\n",
    "@kfp.v2.dsl.component(\n",
    "    base_image=\"python:3.9\",\n",
    "    packages_to_install=[\n",
    "        'google-cloud-aiplatform==1.26.1',\n",
    "        'google-cloud-pipeline-components',\n",
    "        'google-cloud-storage',\n",
    "        'tensorflow==2.11.0',\n",
    "        'numpy'\n",
    "    ],\n",
    ")\n",
    "def test_model_index_endpoint(\n",
    "    project: str,\n",
    "    location: str,\n",
    "    version: str,\n",
    "    train_output_gcs_bucket: str,\n",
    "    test_instances_gcs_filename: str,\n",
    "    experiment_name: str,\n",
    "    experiment_run: str,\n",
    "    # train_dir: str,\n",
    "    # train_dir_prefix: str,\n",
    "    # ann_index_resource_uri: str,\n",
    "    ann_index_endpoint_resource_uri: str,\n",
    "    brute_index_endpoint_resource_uri: str,\n",
    "    gcs_train_script_path: str,\n",
    "    endpoint: str, # Input[Artifact],\n",
    "    metrics: Output[Metrics],\n",
    "):\n",
    "    \n",
    "    import logging\n",
    "    from datetime import datetime\n",
    "    import time\n",
    "    import numpy as np\n",
    "    import pickle as pkl\n",
    "    \n",
    "    import base64\n",
    "\n",
    "    from typing import Dict, List, Union\n",
    "\n",
    "    from google.cloud import aiplatform as vertex_ai\n",
    "    \n",
    "    from google.protobuf import json_format\n",
    "    from google.protobuf.json_format import Parse\n",
    "    from google.protobuf.struct_pb2 import Value\n",
    "\n",
    "    from google_cloud_pipeline_components.proto.gcp_resources_pb2 import GcpResources\n",
    "    \n",
    "    from google.cloud import storage\n",
    "    from google.cloud.storage.bucket import Bucket\n",
    "    from google.cloud.storage.blob import Blob\n",
    "\n",
    "    import tensorflow as tf\n",
    "\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "    vertex_ai.init(\n",
    "        project=project,\n",
    "        location=location,\n",
    "    )\n",
    "    storage_client = storage.Client(project=project)\n",
    "    \n",
    "    # ====================================================\n",
    "    # helper functions\n",
    "    # ====================================================\n",
    "    \n",
    "    def download_blob(bucket_name, source_gcs_obj, local_filename):\n",
    "        \"\"\"Uploads a file to the bucket.\"\"\"\n",
    "        # storage_client = storage.Client(project=project_number)\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "        blob = bucket.blob(source_gcs_obj)\n",
    "        blob.download_to_filename(local_filename)\n",
    "        \n",
    "        filehandler = open(f'{local_filename}', 'rb')\n",
    "        loaded_dict = pkl.load(filehandler)\n",
    "        filehandler.close()\n",
    "        \n",
    "        logging.info(f\"File {local_filename} downloaded from gs://{bucket_name}/{source_gcs_obj}\")\n",
    "        \n",
    "        return loaded_dict\n",
    "    \n",
    "    # ====================================================\n",
    "    # get deployed model endpoint\n",
    "    # ====================================================\n",
    "    logging.info(f\"Endpoint = {endpoint}\")\n",
    "    gcp_resources = Parse(endpoint, GcpResources())\n",
    "    logging.info(f\"gcp_resources = {gcp_resources}\")\n",
    "    \n",
    "    _endpoint_resource = gcp_resources.resources[0].resource_uri\n",
    "    logging.info(f\"_endpoint_resource = {_endpoint_resource}\")\n",
    "    \n",
    "    _endpoint_uri = \"/\".join(_endpoint_resource.split(\"/\")[-8:-2])\n",
    "    logging.info(f\"_endpoint_uri = {_endpoint_uri}\")\n",
    "    \n",
    "    # define endpoint resource in component\n",
    "    _endpoint = vertex_ai.Endpoint(_endpoint_uri)\n",
    "    logging.info(f\"_endpoint defined\")\n",
    "    \n",
    "    # ==============================================================\n",
    "    # helper function for returning endpoint predictions via json\n",
    "    # ==============================================================\n",
    "    \n",
    "    def predict_custom_trained_model_sample(\n",
    "        project: str,\n",
    "        endpoint_id: str,\n",
    "        instances: Dict,\n",
    "        location: str = \"us-central1\",\n",
    "        api_endpoint: str = \"us-central1-aiplatform.googleapis.com\",\n",
    "    ):\n",
    "        \"\"\"\n",
    "        either single instance of type dict or a list of instances.\n",
    "        This client only needs to be created once, and can be reused for multiple requests.\n",
    "        \"\"\"\n",
    "\n",
    "        # The AI Platform services require regional API endpoints.\n",
    "        client_options = {\"api_endpoint\": api_endpoint}\n",
    "        \n",
    "        # Initialize client that will be used to create and send requests.\n",
    "        client = vertex_ai.gapic.PredictionServiceClient(client_options=client_options)\n",
    "        \n",
    "        # The format of each instance should conform to the deployed model's prediction input schema.\n",
    "        instances = instances if type(instances) == list else [instances]\n",
    "        instances = [\n",
    "            json_format.ParseDict(instance_dict, Value()) for instance_dict in instances\n",
    "        ]\n",
    "        \n",
    "        parameters_dict = {}\n",
    "        parameters = json_format.ParseDict(parameters_dict, Value())\n",
    "        \n",
    "        endpoint = client.endpoint_path(\n",
    "            project=project, location=location, endpoint=endpoint_id\n",
    "        )\n",
    "        \n",
    "        response = client.predict(\n",
    "            endpoint=endpoint, instances=instances, parameters=parameters\n",
    "        )\n",
    "        logging.info(f'Response: {response}')\n",
    "        logging.info(f'Deployed Model ID(s): {response.deployed_model_id}')\n",
    "        # The predictions are a google.protobuf.Value representation of the model's predictions.\n",
    "        _predictions = response.predictions\n",
    "        logging.info(f'Response Predictions: {_predictions}')\n",
    "        \n",
    "        return _predictions\n",
    "    \n",
    "    # ===================================================\n",
    "    # load test instance\n",
    "    # ===================================================\n",
    "    LOCAL_TEST_INSTANCE = 'test_instances_dict.pkl'\n",
    "    GCS_PATH_TO_BLOB = f'{experiment_name}/{experiment_run}/{test_instances_gcs_filename}'\n",
    "    LOADED_CANDIDATE_DICT = f'loaded_{LOCAL_TEST_INSTANCE}'\n",
    "    \n",
    "    loaded_test_instance = download_blob(\n",
    "        bucket_name=train_output_gcs_bucket,\n",
    "        source_gcs_obj=GCS_PATH_TO_BLOB,\n",
    "        local_filename=LOADED_CANDIDATE_DICT\n",
    "    )\n",
    "    logging.info(f'loaded_test_instance: {loaded_test_instance}')\n",
    "    \n",
    "    # make prediction request\n",
    "    _endpoint_id = _endpoint_uri.split('/')[-1]\n",
    "    logging.info(f\"_endpoint_id created = {_endpoint_id}\")\n",
    "    prediction_test = predict_custom_trained_model_sample(\n",
    "        project=project,                     \n",
    "        endpoint_id=_endpoint_id,\n",
    "        location=\"us-central1\",\n",
    "        instances=loaded_test_instance\n",
    "    )\n",
    "    \n",
    "    # ===================================================\n",
    "    # Matching Engine\n",
    "    # ===================================================\n",
    "    logging.info(f\"ann_index_endpoint_resource_uri: {ann_index_endpoint_resource_uri}\")\n",
    "    logging.info(f\"brute_index_endpoint_resource_uri: {brute_index_endpoint_resource_uri}\")\n",
    "\n",
    "    deployed_ann_index = vertex_ai.MatchingEngineIndexEndpoint(ann_index_endpoint_resource_uri)\n",
    "    deployed_bf_index = vertex_ai.MatchingEngineIndexEndpoint(brute_index_endpoint_resource_uri)\n",
    "\n",
    "    DEPLOYED_ANN_ID = deployed_ann_index.deployed_indexes[0].id\n",
    "    DEPLOYED_BF_ID = deployed_bf_index.deployed_indexes[0].id\n",
    "    logging.info(f\"DEPLOYED_ANN_ID: {DEPLOYED_ANN_ID}\")\n",
    "    logging.info(f\"DEPLOYED_BF_ID: {DEPLOYED_BF_ID}\")\n",
    "    \n",
    "    logging.info('Retreiving neighbors from ANN index...')\n",
    "    \n",
    "    start = time.time()\n",
    "    ANN_response = deployed_ann_index.match(\n",
    "        deployed_index_id=DEPLOYED_ANN_ID,\n",
    "        queries=prediction_test,\n",
    "        num_neighbors=10\n",
    "    )\n",
    "    elapsed_ann_time = time.time() - start\n",
    "    elapsed_ann_time = round(elapsed_ann_time, 4)\n",
    "    logging.info(f'ANN latency: {elapsed_ann_time} seconds')\n",
    "    \n",
    "    logging.info('Retreiving neighbors from BF index...')\n",
    "    \n",
    "    start = time.time()\n",
    "    BF_response = deployed_bf_index.match(\n",
    "        deployed_index_id=DEPLOYED_BF_ID,\n",
    "        queries=prediction_test,\n",
    "        num_neighbors=10\n",
    "    )\n",
    "    \n",
    "    elapsed_bf_time = time.time() - start\n",
    "    elapsed_bf_time = round(elapsed_bf_time, 4)\n",
    "    logging.info(f'Bruteforce latency: {elapsed_bf_time} seconds')\n",
    "    \n",
    "    # =========================================================\n",
    "    # Calculate recall by determining how many neighbors \n",
    "    # correctly retrieved as compared to the brute-force option\n",
    "    # =========================================================\n",
    "    recalled_neighbors = 0\n",
    "    for tree_ah_neighbors, brute_force_neighbors in zip(\n",
    "        ANN_response, BF_response\n",
    "    ):\n",
    "        tree_ah_neighbor_ids = [neighbor.id for neighbor in tree_ah_neighbors]\n",
    "        brute_force_neighbor_ids = [neighbor.id for neighbor in brute_force_neighbors]\n",
    "\n",
    "        recalled_neighbors += len(\n",
    "            set(tree_ah_neighbor_ids).intersection(brute_force_neighbor_ids)\n",
    "        )\n",
    "\n",
    "    recall = recalled_neighbors / len(\n",
    "        [neighbor for neighbors in BF_response for neighbor in neighbors]\n",
    "    )\n",
    "    \n",
    "    # =========================================================\n",
    "    # Metrics\n",
    "    # =========================================================\n",
    "    reduction = (elapsed_bf_time - elapsed_ann_time) / elapsed_bf_time*100.00\n",
    "    increase  = (elapsed_bf_time - elapsed_ann_time)/elapsed_ann_time*100.00\n",
    "    faster    = elapsed_bf_time / elapsed_ann_time\n",
    "\n",
    "    logging.info(f\"reduction in time         : {round(reduction, 3)}%\")\n",
    "    logging.info(f\"% increase in performance : {round(increase, 3)}%\")\n",
    "    logging.info(f\"how many times faster     : {round(faster, 3)}x faster\")\n",
    "\n",
    "    logging.info(\"Recall: {}\".format(recall * 100.0))\n",
    "    \n",
    "    metrics.log_metric(\"Recall\", (recall * 100.0))\n",
    "    # metrics.log_metric(\"elapsed_query_time\", elapsed_query_time)\n",
    "    metrics.log_metric(\"elapsed_ann_time\", elapsed_ann_time)\n",
    "    metrics.log_metric(\"elapsed_bf_time\", elapsed_bf_time)\n",
    "    metrics.log_metric(\"latency_reduction\", reduction)\n",
    "    metrics.log_metric(\"perf_increase\", increase)\n",
    "    metrics.log_metric(\"x_faster\", faster)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76367d45-43a9-43b1-a475-cc1237696e2e",
   "metadata": {},
   "source": [
    "## Compute config for pipeline steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "18a86934-2700-4ce3-b0c9-673fdce5a430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/train_pipes/pipeline_config.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_SRC}/{PIPELINES_SUB_DIR}/pipeline_config.py\n",
    "\n",
    "CPU_LIMIT='96'\n",
    "MEMORY_LIMIT='624G'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa3eea5-05b5-46a7-ab2e-6ef4b826fcae",
   "metadata": {},
   "source": [
    "# Prepare Job Specs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045adff3-74fc-4e86-ac19-8e2feee61766",
   "metadata": {},
   "source": [
    "## Accelerators and Device Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8a0a9956-fb25-4d7a-9754-97d5868f7ffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORKER_MACHINE_TYPE            : n1-highmem-16\n",
      "REPLICA_COUNT                  : 1\n",
      "ACCELERATOR_TYPE               : NVIDIA_TESLA_T4\n",
      "PER_MACHINE_ACCELERATOR_COUNT  : 1\n",
      "DISTRIBUTE_STRATEGY            : single\n",
      "REDUCTION_SERVER_COUNT         : 0\n",
      "REDUCTION_SERVER_MACHINE_TYPE  : n1-highcpu-16\n"
     ]
    }
   ],
   "source": [
    "from src.two_tower_jt import train_utils\n",
    "\n",
    "gpu_dict = train_utils.get_accelerator_config(\n",
    "    key='t4', \n",
    "    worker_machine_type = 'n1-highmem-16',\n",
    "    reduction_n=0\n",
    ")\n",
    "\n",
    "WORKER_MACHINE_TYPE            = gpu_dict['WORKER_MACHINE_TYPE']\n",
    "REPLICA_COUNT                  = gpu_dict['REPLICA_COUNT']\n",
    "ACCELERATOR_TYPE               = gpu_dict['ACCELERATOR_TYPE']\n",
    "PER_MACHINE_ACCELERATOR_COUNT  = gpu_dict['PER_MACHINE_ACCELERATOR_COUNT']\n",
    "DISTRIBUTE_STRATEGY            = gpu_dict['DISTRIBUTE_STRATEGY']\n",
    "REDUCTION_SERVER_COUNT         = gpu_dict['REDUCTION_SERVER_COUNT']\n",
    "REDUCTION_SERVER_MACHINE_TYPE  = gpu_dict['REDUCTION_SERVER_MACHINE_TYPE'] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83f96f0-3b01-4622-88e9-cb646b3f873a",
   "metadata": {},
   "source": [
    "## Vertex AI Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "28d8b91d-9772-49da-b7a4-4ab33a12c0c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPERIMENT_NAME: tfrs-pipe-v1\n",
      "RUN_NAME: run-20230926-120445\n"
     ]
    }
   ],
   "source": [
    "EXPERIMENT_PREFIX = 'tfrs-pipe'                     # custom identifier for organizing experiments\n",
    "EXPERIMENT_NAME=f'{EXPERIMENT_PREFIX}-{VERSION}'\n",
    "# RUN_NAME = f'run-{time.strftime(\"%Y%m%d-%H%M%S\")}'\n",
    "\n",
    "RUN_NAME = \"run-20230926-120445\"\n",
    "\n",
    "print(f\"EXPERIMENT_NAME: {EXPERIMENT_NAME}\")\n",
    "print(f\"RUN_NAME: {RUN_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66142882-0f35-4e7d-ab5a-c0d6b49d3f44",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Training Config\n",
    "\n",
    "* see [src code](https://github.com/googleapis/python-aiplatform/blob/e7bf0d83d8bb0849a9bce886c958d13f5cbe5fab/google/cloud/aiplatform/utils/worker_spec_utils.py#L153) for worker_pool_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cc68e120-39c8-4974-92a2-228cd0173853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PIPELINE_ROOT_PATH: gs://ndr-v1-hybrid-vertex-bucket/tfrs-pipe-v1/run-20230926-120445/pipeline_root\n"
     ]
    }
   ],
   "source": [
    "# =================================================\n",
    "# trainconfig: gcs locations\n",
    "# =================================================\n",
    "\n",
    "# Stores pipeline executions for each run\n",
    "PIPELINE_ROOT_PATH = f'gs://{BUCKET_NAME}/{EXPERIMENT_NAME}/{RUN_NAME}/pipeline_root'\n",
    "print('PIPELINE_ROOT_PATH: {}'.format(PIPELINE_ROOT_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7104ca-6877-4d25-881f-6a40a4a6be08",
   "metadata": {},
   "source": [
    "### Feature lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b3beb06e-05a8-470a-9d54-ff02a03758a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FEATURES_PREFIX: tfrs-pipe-v1/run-20230926-120445/features\n"
     ]
    }
   ],
   "source": [
    "# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "    \n",
    "import tensorflow as tf\n",
    "\n",
    "FEATURES_PREFIX = f'{EXPERIMENT_NAME}/{RUN_NAME}/features'\n",
    "print(f\"FEATURES_PREFIX: {FEATURES_PREFIX}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb31939-5672-4800-a98a-82484ad2c197",
   "metadata": {},
   "source": [
    "#### candidate features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f9c1846e-e63f-4c63-8c9c-049f99d503c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'track_uri_can': FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n",
       " 'track_name_can': FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n",
       " 'artist_uri_can': FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n",
       " 'artist_name_can': FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n",
       " 'album_uri_can': FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n",
       " 'album_name_can': FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n",
       " 'duration_ms_can': FixedLenFeature(shape=(), dtype=tf.float32, default_value=None),\n",
       " 'track_pop_can': FixedLenFeature(shape=(), dtype=tf.float32, default_value=None),\n",
       " 'artist_pop_can': FixedLenFeature(shape=(), dtype=tf.float32, default_value=None),\n",
       " 'artist_genres_can': FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n",
       " 'artist_followers_can': FixedLenFeature(shape=(), dtype=tf.float32, default_value=None),\n",
       " 'track_danceability_can': FixedLenFeature(shape=(), dtype=tf.float32, default_value=None),\n",
       " 'track_energy_can': FixedLenFeature(shape=(), dtype=tf.float32, default_value=None),\n",
       " 'track_key_can': FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n",
       " 'track_loudness_can': FixedLenFeature(shape=(), dtype=tf.float32, default_value=None),\n",
       " 'track_mode_can': FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n",
       " 'track_speechiness_can': FixedLenFeature(shape=(), dtype=tf.float32, default_value=None),\n",
       " 'track_acousticness_can': FixedLenFeature(shape=(), dtype=tf.float32, default_value=None),\n",
       " 'track_instrumentalness_can': FixedLenFeature(shape=(), dtype=tf.float32, default_value=None),\n",
       " 'track_liveness_can': FixedLenFeature(shape=(), dtype=tf.float32, default_value=None),\n",
       " 'track_valence_can': FixedLenFeature(shape=(), dtype=tf.float32, default_value=None),\n",
       " 'track_tempo_can': FixedLenFeature(shape=(), dtype=tf.float32, default_value=None),\n",
       " 'track_time_signature_can': FixedLenFeature(shape=(), dtype=tf.string, default_value=None)}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CANDIDATE_FEATURES_DICT = feature_utils.get_candidate_features()\n",
    "CANDIDATE_FEATURES_DICT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ab61ac70-9da5-4ed6-b1f8-aecc167e8192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# candidate features\n",
    "CANDIDATE_FILENAME = 'candidate_feats_dict.pkl'\n",
    "CANDIDATE_FEATURES_GCS_OBJ = f'{FEATURES_PREFIX}/{CANDIDATE_FILENAME}'\n",
    "\n",
    "# pickle\n",
    "filehandler = open(f'{CANDIDATE_FILENAME}', 'wb')\n",
    "pkl.dump(CANDIDATE_FEATURES_DICT, filehandler)\n",
    "filehandler.close()\n",
    "\n",
    "# upload to GCS\n",
    "bucket_client = storage_client.bucket(BUCKET_NAME)\n",
    "blob = bucket_client.blob(CANDIDATE_FEATURES_GCS_OBJ)\n",
    "blob.upload_from_filename(CANDIDATE_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674a0108-00f1-4656-a3c6-9d93b8c6892b",
   "metadata": {},
   "source": [
    "#### query features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "84158b9e-45d9-4ed3-802d-a2eab948535a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAX_PLAYLIST_LENGTH=5 # TODO- make consistent with previous notebooks e.g., 5\n",
    "\n",
    "QUERY_FEATURES_DICT = feature_utils.get_all_features(TRACK_HISTORY, ranker=False)\n",
    "# QUERY_FEATURES_DICT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "75b11787-0c10-4f3b-b76f-0b10dbfd5868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query features\n",
    "QUERY_FILENAME = 'query_feats_dict.pkl'\n",
    "QUERY_FEATURES_GCS_OBJ = f'{FEATURES_PREFIX}/{QUERY_FILENAME}'\n",
    "\n",
    "# pickle\n",
    "filehandler = open(f'{QUERY_FILENAME}', 'wb')\n",
    "pkl.dump(QUERY_FEATURES_DICT, filehandler)\n",
    "filehandler.close()\n",
    "\n",
    "# upload to GCS\n",
    "bucket_client = storage_client.bucket(BUCKET_NAME)\n",
    "blob = bucket_client.blob(QUERY_FEATURES_GCS_OBJ)\n",
    "blob.upload_from_filename(QUERY_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f484d9b8-0f91-4c03-83cd-fb47cd80188e",
   "metadata": {},
   "source": [
    "### test instances\n",
    "\n",
    "* create test instances pkl\n",
    "* will copy to pipeline root later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cf9ff3ab-ed67-4b0f-90f9-ed97c1e88a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of instances: 52\n"
     ]
    }
   ],
   "source": [
    "instances = test_instances.TEST_INSTANCE_5\n",
    "print(f\"length of instances: {len(instances)}\")\n",
    "\n",
    "LOCAL_INSTANCES_PKL = \"test_instances_5.pkl\"\n",
    "\n",
    "filehandler = open(f'{LOCAL_INSTANCES_PKL}', 'wb')\n",
    "pkl.dump(instances, filehandler)\n",
    "filehandler.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6587613f-0351-4fa2-8fd2-aeed12b1eab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEATURE_DICT = feature_utils.get_all_features(TRACK_HISTORY, ranker=False)\n",
    "# FEATURE_DICT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95b92ba-10f2-4f8c-80fe-9446da2dbd6b",
   "metadata": {},
   "source": [
    "### train image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8bc940d1-aecb-480b-9b00-cb5c3afcda81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REMOTE_IMAGE_NAME : us-central1-docker.pkg.dev/hybrid-vertex/ndr-v1-spotify/train-v1\n",
      "DOCKERNAME        : tfrs\n"
     ]
    }
   ],
   "source": [
    "# =================================================\n",
    "# train image\n",
    "# =================================================\n",
    "# Existing image URI or name for image to create\n",
    "# IMAGE_URI = f'gcr.io/hybrid-vertex/sp-2tower-tfrs-trainerv6-tr'\n",
    "# DOCKERNAME = 'tfrs'\n",
    "\n",
    "print(f\"REMOTE_IMAGE_NAME : {REMOTE_IMAGE_NAME}\")\n",
    "print(f\"DOCKERNAME        : {DOCKERNAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1c38a4-7207-43a2-92c4-33d821abc448",
   "metadata": {},
   "source": [
    "### train params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "aef27ee8-e094-4293-a14d-23e87f687a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALID_FREQUENCY : 11\n",
      "VALID_STEPS     : 20\n",
      "EPOCH_STEPS     : 2003\n",
      "EMBED_FREQUENCY : 1\n",
      "HIST_FREQUENCY  : 0\n",
      "CHECKPOINT_FREQ : 500\n",
      "UPDATE_FREQ     : 500\n"
     ]
    }
   ],
   "source": [
    "SEED = 1234\n",
    "\n",
    "# =================================================\n",
    "# trainconfig: GPU related\n",
    "# =================================================\n",
    "TF_GPU_THREAD_COUNT  = '8'      # '1' | '4' | '8'\n",
    "\n",
    "# =================================================\n",
    "# trainconfig: data input pipeline\n",
    "# =================================================\n",
    "BLOCK_LENGTH         = 64            # 1, 8, 16, 32, 64\n",
    "NUM_DATA_SHARDS      = 4          # 2, 4, 8, 16, 32, 64\n",
    "# TRAIN_PREFETCH=3\n",
    "\n",
    "# =================================================\n",
    "# trainconfig: training hparams\n",
    "# =================================================\n",
    "NUM_EPOCHS           = 10\n",
    "LEARNING_RATE        = 0.01\n",
    "BATCH_SIZE           = 4096           # 8192, 4096, 2048, 1024, 512 \n",
    "\n",
    "# dropout\n",
    "DROPOUT_RATE         = 0.33\n",
    "\n",
    "# model size\n",
    "EMBEDDING_DIM        = 128\n",
    "PROJECTION_DIM       = int(EMBEDDING_DIM / 4) # 50  \n",
    "LAYER_SIZES          = '[512,256,128]'\n",
    "MAX_TOKENS           = 20000     # vocab\n",
    "\n",
    "# =================================================\n",
    "# trainconfig: train & valid steps\n",
    "# =================================================\n",
    "train_sample_cnt     = 8_205_265 # 8_205_265\n",
    "valid_samples_cnt    = 82_959\n",
    "\n",
    "# validation & evaluation\n",
    "VALID_FREQUENCY      = NUM_EPOCHS + 1 #// 3 # 20\n",
    "VALID_STEPS          = valid_samples_cnt // BATCH_SIZE # 100\n",
    "EPOCH_STEPS          = train_sample_cnt // BATCH_SIZE\n",
    "\n",
    "# =================================================\n",
    "# trainconfig: tensorboard\n",
    "# =================================================\n",
    "EMBED_FREQUENCY      = 1\n",
    "HIST_FREQUENCY       = 0\n",
    "CHECKPOINT_FREQ      = EPOCH_STEPS // 4 # 'epoch'\n",
    "UPDATE_FREQ          = EPOCH_STEPS // 4 # 'epoch'\n",
    "\n",
    "print(f\"VALID_FREQUENCY : {VALID_FREQUENCY}\")\n",
    "print(f\"VALID_STEPS     : {VALID_STEPS}\")\n",
    "print(f\"EPOCH_STEPS     : {EPOCH_STEPS}\")\n",
    "print(f\"EMBED_FREQUENCY : {EMBED_FREQUENCY}\")\n",
    "print(f\"HIST_FREQUENCY  : {HIST_FREQUENCY}\")\n",
    "print(f\"CHECKPOINT_FREQ : {CHECKPOINT_FREQ}\")\n",
    "print(f\"UPDATE_FREQ     : {UPDATE_FREQ}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85669e4e-da9c-438b-accd-6e8a64973efc",
   "metadata": {},
   "source": [
    "### data source\n",
    "\n",
    "**TODO:** update these variables to point to the GCS location where the processed training data is stored "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c29169fd-6a15-48a4-99f1-c4bdfb754f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CANDIDATE_PREFIX: data/v1/candidates\n",
      "TRAIN_DIR_PREFIX: data/v1/valid\n",
      "VALID_DIR_PREFIX: data/v1/valid\n"
     ]
    }
   ],
   "source": [
    "# =================================================\n",
    "# trainconfig: Data sources\n",
    "# =================================================\n",
    "TRAIN_DIR_PREFIX = f'data/{DATA_VERSION}/valid' # train | valid\n",
    "VALID_DIR_PREFIX = f'data/{DATA_VERSION}/valid' \n",
    "CANDIDATE_PREFIX = f'data/{DATA_VERSION}/candidates' \n",
    "\n",
    "# print(f\"BUCKET_DATA_DIR: {BUCKET_DATA_DIR}\")\n",
    "print(f\"CANDIDATE_PREFIX: {CANDIDATE_PREFIX}\")\n",
    "print(f\"TRAIN_DIR_PREFIX: {TRAIN_DIR_PREFIX}\")\n",
    "print(f\"VALID_DIR_PREFIX: {VALID_DIR_PREFIX}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c653b1a-b103-4e65-92e2-05e40148b867",
   "metadata": {},
   "source": [
    "## Gather train args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d8fa7a02-6d4d-43ba-bce4-5289f86db2ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'container_spec': {'args': ['--project=hybrid-vertex',\n",
      "                              '--train_output_gcs_bucket=ndr-v1-hybrid-vertex-bucket',\n",
      "                              '--train_dir=ndr-v1-hybrid-vertex-bucket',\n",
      "                              '--train_dir_prefix=data/v1/valid',\n",
      "                              '--valid_dir=ndr-v1-hybrid-vertex-bucket',\n",
      "                              '--valid_dir_prefix=data/v1/valid',\n",
      "                              '--candidate_file_dir=ndr-v1-hybrid-vertex-bucket',\n",
      "                              '--candidate_files_prefix=data/v1/candidates',\n",
      "                              '--experiment_name=tfrs-pipe-v1',\n",
      "                              '--experiment_run=run-20230926-120445',\n",
      "                              '--num_epochs=10',\n",
      "                              '--batch_size=4096',\n",
      "                              '--embedding_dim=128',\n",
      "                              '--projection_dim=32',\n",
      "                              '--layer_sizes=[512,256,128]',\n",
      "                              '--learning_rate=0.01',\n",
      "                              '--valid_frequency=11',\n",
      "                              '--valid_steps=20',\n",
      "                              '--epoch_steps=2003',\n",
      "                              '--distribute=single',\n",
      "                              '--model_version=v1',\n",
      "                              '--pipeline_version=pipe-v2',\n",
      "                              '--seed=1234',\n",
      "                              '--max_tokens=20000',\n",
      "                              '--embed_frequency=1',\n",
      "                              '--update_frequency=500',\n",
      "                              '--hist_frequency=0',\n",
      "                              '--tf_gpu_thread_count=8',\n",
      "                              '--block_length=64',\n",
      "                              '--num_data_shards=4',\n",
      "                              '--chkpt_freq=500',\n",
      "                              '--dropout_rate=0.33',\n",
      "                              '--profiler',\n",
      "                              '--compute_batch_metrics',\n",
      "                              '--use_cross_layer',\n",
      "                              '--use_dropout'],\n",
      "                     'command': ['python', '-m', 'src.two_tower_jt.task'],\n",
      "                     'image_uri': 'us-central1-docker.pkg.dev/hybrid-vertex/ndr-v1-spotify/train-v1:latest'},\n",
      "  'machine_spec': {'accelerator_count': 1,\n",
      "                   'accelerator_type': 'NVIDIA_TESLA_T4',\n",
      "                   'machine_type': 'n1-highmem-16'},\n",
      "  'replica_count': 1}]\n"
     ]
    }
   ],
   "source": [
    "from util import workerpool_specs\n",
    "\n",
    "WORKER_CMD = [\"python\", \"-m\", \"src.two_tower_jt.task\"]\n",
    "# WORKER_CMD = [\"python\", \"-m\", \"task\"]\n",
    "\n",
    "WORKER_ARGS = [\n",
    "    f'--project={PROJECT_ID}',\n",
    "    f'--train_output_gcs_bucket={BUCKET_NAME}',\n",
    "    f'--train_dir={BUCKET_NAME}',\n",
    "    f'--train_dir_prefix={TRAIN_DIR_PREFIX}',\n",
    "    f'--valid_dir={BUCKET_NAME}',\n",
    "    f'--valid_dir_prefix={VALID_DIR_PREFIX}',\n",
    "    f'--candidate_file_dir={BUCKET_NAME}',\n",
    "    f'--candidate_files_prefix={CANDIDATE_PREFIX}',\n",
    "    f'--experiment_name={EXPERIMENT_NAME}',\n",
    "    f'--experiment_run={RUN_NAME}',\n",
    "    f'--num_epochs={NUM_EPOCHS}',\n",
    "    f'--batch_size={BATCH_SIZE}',\n",
    "    f'--embedding_dim={EMBEDDING_DIM}',\n",
    "    f'--projection_dim={PROJECTION_DIM}',\n",
    "    f'--layer_sizes={LAYER_SIZES}',\n",
    "    f'--learning_rate={LEARNING_RATE}',\n",
    "    f'--valid_frequency={VALID_FREQUENCY}',\n",
    "    f'--valid_steps={VALID_STEPS}',\n",
    "    f'--epoch_steps={EPOCH_STEPS}',\n",
    "    f'--distribute={DISTRIBUTE_STRATEGY}',\n",
    "    f'--model_version={VERSION}',\n",
    "    f'--pipeline_version={PIPELINE_VERSION}',\n",
    "    f'--seed={SEED}',\n",
    "    f'--max_tokens={MAX_TOKENS}',\n",
    "    # f'--tb_resource_name={TB_RESOURCE_NAME}',\n",
    "    f'--embed_frequency={EMBED_FREQUENCY}',\n",
    "    f'--update_frequency={UPDATE_FREQ}',      # TODO - turn on\n",
    "    f'--hist_frequency={HIST_FREQUENCY}',\n",
    "    f'--tf_gpu_thread_count={TF_GPU_THREAD_COUNT}',\n",
    "    f'--block_length={BLOCK_LENGTH}',\n",
    "    f'--num_data_shards={NUM_DATA_SHARDS}',\n",
    "    f'--chkpt_freq={CHECKPOINT_FREQ}',\n",
    "    f'--dropout_rate={DROPOUT_RATE}',\n",
    "    # uncomment these to pass value of True (bool)\n",
    "    # f'--cache_train',                              # caches train_dataset\n",
    "    # f'--evaluate_model',                             # runs model.eval()\n",
    "    # f'--write_embeddings',                         # writes embeddings index in train job\n",
    "    f'--profiler',                                   # runs TB profiler\n",
    "    # f'--set_jit',                                  # enables XLA\n",
    "    f'--compute_batch_metrics',\n",
    "    f'--use_cross_layer',\n",
    "    f'--use_dropout',\n",
    "]\n",
    "\n",
    "WORKER_POOL_SPECS = workerpool_specs.prepare_worker_pool_specs(\n",
    "    image_uri=f\"{REMOTE_IMAGE_NAME}:latest\",\n",
    "    args=WORKER_ARGS,\n",
    "    cmd=WORKER_CMD,\n",
    "    replica_count=REPLICA_COUNT,\n",
    "    machine_type=WORKER_MACHINE_TYPE,\n",
    "    accelerator_count=PER_MACHINE_ACCELERATOR_COUNT,\n",
    "    accelerator_type=ACCELERATOR_TYPE,\n",
    "    reduction_server_count=REDUCTION_SERVER_COUNT,\n",
    "    reduction_server_machine_type=REDUCTION_SERVER_MACHINE_TYPE,\n",
    ")\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(WORKER_POOL_SPECS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e4d826dc-0dfc-44de-99f1-79d68ecd559d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/jw-repo2/spotify_mpd_two_tower\n",
      "gs://ndr-v1-hybrid-vertex-bucket/tfrs-pipe-v1/run-20230926-120445/pipeline_root\n",
      "src\n"
     ]
    }
   ],
   "source": [
    "!export PWD=pwd\n",
    "!export PIPELINE_ROOT_PATH=PIPELINE_ROOT_PATH\n",
    "!export REPO_SRC=REPO_SRC\n",
    "\n",
    "! echo $PWD\n",
    "! echo $PIPELINE_ROOT_PATH\n",
    "! echo $REPO_SRC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caff7b07-e1ad-4dcf-845a-7d2661cc63cd",
   "metadata": {},
   "source": [
    "### Copy train and deployment files to GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f18863e6-1940-468a-adcf-985d879bb21a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gs://ndr-v1-hybrid-vertex-bucket/tfrs-pipe-v1/run-20230926-120445'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from notebook: 06-deploy-query-tower-monitoring\n",
    "MANY_TESTS_FILE          = 'test_instance_list.pkl' \n",
    "SKEW_FEATURES_STATS_FILE = 'skew_feat_stats.pkl'\n",
    "\n",
    "BASE_OUTPUT_DIR = f'gs://{BUCKET_NAME}/{EXPERIMENT_NAME}/{RUN_NAME}'\n",
    "BASE_OUTPUT_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562f9c0e-ad9a-4d0e-a56f-df5d146b7365",
   "metadata": {},
   "source": [
    "#### copy training files for tracking\n",
    "\n",
    "> copy train files to pipeline root\n",
    "\n",
    "* Cloud Build `yaml`\n",
    "* Dockerfile\n",
    "* trainer code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "913e0f5d-abc0-4d78-915e-7a28489e3d23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://ndr-v1-hybrid-vertex-bucket/tfrs-pipe-v1/run-20230926-120445/pipeline_root/Dockerfile_tfrs\n",
      "gs://ndr-v1-hybrid-vertex-bucket/tfrs-pipe-v1/run-20230926-120445/pipeline_root/pipeline_spec.json\n",
      "gs://ndr-v1-hybrid-vertex-bucket/tfrs-pipe-v1/run-20230926-120445/pipeline_root/934903580331/\n",
      "gs://ndr-v1-hybrid-vertex-bucket/tfrs-pipe-v1/run-20230926-120445/pipeline_root/trainer/\n"
     ]
    }
   ],
   "source": [
    "# !gsutil -q cp $REPO_SRC/cloudbuild.yaml $PIPELINE_ROOT_PATH/cloudbuild.yaml\n",
    "! gsutil -q cp $REPO_SRC/Dockerfile_tfrs $PIPELINE_ROOT_PATH/Dockerfile_tfrs\n",
    "! gsutil -q -m cp -r $REPO_SRC/two_tower_jt/* $PIPELINE_ROOT_PATH/trainer\n",
    "\n",
    "# print(f\"Copied files to {PIPELINE_ROOT_PATH}\")\n",
    "! gsutil ls $PIPELINE_ROOT_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006966b0-066f-44da-b1ae-cb399e12e0bf",
   "metadata": {},
   "source": [
    "#### copy deployment files for pipeline use\n",
    "\n",
    "> copy deployment files to pipeline run base output directory\n",
    "\n",
    "* vocabulary (vocab)\n",
    "* test instances\n",
    "* trainer code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "41530012-e7c5-4574-b1e5-e40b51a1f90b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://ndr-v1-hybrid-vertex-bucket/tfrs-pipe-v1/run-20230926-120445/skew_feat_stats.pkl\n",
      "gs://ndr-v1-hybrid-vertex-bucket/tfrs-pipe-v1/run-20230926-120445/test_instance_list.pkl\n",
      "gs://ndr-v1-hybrid-vertex-bucket/tfrs-pipe-v1/run-20230926-120445/test_instances_5.pkl\n",
      "gs://ndr-v1-hybrid-vertex-bucket/tfrs-pipe-v1/run-20230926-120445/train_job_dict.pkl\n",
      "gs://ndr-v1-hybrid-vertex-bucket/tfrs-pipe-v1/run-20230926-120445/vocab_dict.pkl\n",
      "gs://ndr-v1-hybrid-vertex-bucket/tfrs-pipe-v1/run-20230926-120445/candidate-embeddings-20230926-123713/\n",
      "gs://ndr-v1-hybrid-vertex-bucket/tfrs-pipe-v1/run-20230926-120445/candidate-embeddings-20230927-113357/\n",
      "gs://ndr-v1-hybrid-vertex-bucket/tfrs-pipe-v1/run-20230926-120445/features/\n",
      "gs://ndr-v1-hybrid-vertex-bucket/tfrs-pipe-v1/run-20230926-120445/logs/\n",
      "gs://ndr-v1-hybrid-vertex-bucket/tfrs-pipe-v1/run-20230926-120445/model-dir/\n",
      "gs://ndr-v1-hybrid-vertex-bucket/tfrs-pipe-v1/run-20230926-120445/pipeline_root/\n"
     ]
    }
   ],
   "source": [
    "!gsutil -q cp vocab_dict.pkl $BASE_OUTPUT_DIR/vocab_dict.pkl\n",
    "!gsutil -q cp $LOCAL_INSTANCES_PKL $BASE_OUTPUT_DIR/$LOCAL_INSTANCES_PKL\n",
    "!gsutil -q cp $BUCKET_URI/endpoint-tests/$MANY_TESTS_FILE $BASE_OUTPUT_DIR/$MANY_TESTS_FILE\n",
    "!gsutil -q cp $BUCKET_URI/endpoint-tests/$SKEW_FEATURES_STATS_FILE $BASE_OUTPUT_DIR/$SKEW_FEATURES_STATS_FILE\n",
    "\n",
    "# print(f\"Copied files to {BASE_OUTPUT_DIR}\")\n",
    "! gsutil ls $BASE_OUTPUT_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d64bb5-3128-44f6-b652-468a7a325c15",
   "metadata": {},
   "source": [
    "# Build & Submit Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f1eb197b-ea56-4b83-aa92-8198a75711ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PIPELINE_TAG: 2tower-pipe-v2\n",
      "PIPELINE_NAME: tfrs-v1-2tower-pipe-v2\n"
     ]
    }
   ],
   "source": [
    "PIPELINE_TAG = f'2tower-{PIPELINE_VERSION}'\n",
    "print(\"PIPELINE_TAG:\", PIPELINE_TAG)\n",
    "\n",
    "PIPELINE_NAME = f'tfrs-{VERSION}-{PIPELINE_TAG}'.replace('_', '-')\n",
    "print(\"PIPELINE_NAME:\", PIPELINE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89d7d94-7e15-42d6-b38c-f3eab2affb1a",
   "metadata": {},
   "source": [
    "## Create pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5e6f5b61-ab53-4619-b854-8c5a4da210e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.train_pipes import train_custom_model, create_tensorboard, generate_candidates, \\\n",
    "                            create_ann_index, create_brute_force_index, create_ann_index_endpoint_vpc, \\\n",
    "                            create_brute_index_endpoint_vpc, deploy_ann_index, deploy_brute_index, \\\n",
    "                            test_model_index_endpoint, model_monitoring_config, test_model_endpoint, \\\n",
    "                            send_skewed_traffic\n",
    "\n",
    "from src.train_pipes import pipeline_config as cfg\n",
    "\n",
    "@kfp.v2.dsl.pipeline(\n",
    "    name=f'{PIPELINE_NAME}'.replace('_', '-')\n",
    ")\n",
    "def pipeline(\n",
    "    project: str,\n",
    "    project_number: str,\n",
    "    location: str,\n",
    "    service_account: str,\n",
    "    model_version: str,\n",
    "    pipeline_version: str,\n",
    "    train_image_uri: str,\n",
    "    train_output_gcs_bucket: str,\n",
    "    gcs_train_script_path: str,\n",
    "    model_display_name: str,\n",
    "    train_dockerfile_name: str,\n",
    "    train_dir: str,\n",
    "    train_dir_prefix: str,\n",
    "    valid_dir: str,\n",
    "    valid_dir_prefix: str,\n",
    "    candidate_file_dir: str,\n",
    "    candidate_files_prefix: str,\n",
    "    test_instances_gcs_filename: str,\n",
    "    many_test_instances_gcs_filename: str,\n",
    "    # tensorboard_resource_name: str,\n",
    "    experiment_name: str,\n",
    "    experiment_run: str,\n",
    "    register_model_flag: str,\n",
    "    vpc_network_name: str,\n",
    "    generate_new_vocab: bool,\n",
    "    max_playlist_length: int,\n",
    "    max_tokens: int,\n",
    "    ngrams: int,\n",
    "    # new\n",
    "    # feature_dict: dict,\n",
    "    prefix: str,\n",
    "    emails: str,\n",
    "    bq_dataset: str,\n",
    "    bq_train_table: str,\n",
    "):\n",
    "    \n",
    "    from kfp.v2.components import importer_node\n",
    "    from google_cloud_pipeline_components.types import artifact_types\n",
    "            \n",
    "    # ========================================================================\n",
    "    # Managed TB\n",
    "    # ========================================================================\n",
    "    \n",
    "    create_managed_tensorboard_op = (\n",
    "        create_tensorboard.create_tensorboard(\n",
    "            # here\n",
    "            project=project,\n",
    "            location=location,\n",
    "            model_version=model_version,\n",
    "            pipeline_version=pipeline_version,\n",
    "            model_name=model_display_name, \n",
    "            experiment_name=experiment_name,\n",
    "            experiment_run=experiment_run,\n",
    "        )\n",
    "        .set_display_name(\"Managed TB\")\n",
    "        .set_caching_options(True)\n",
    "    )\n",
    "\n",
    "\n",
    "    run_train_task_op = (\n",
    "        train_custom_model.train_custom_model(\n",
    "            project=project,\n",
    "            location=location,\n",
    "            model_version=model_version,\n",
    "            pipeline_version=pipeline_version,\n",
    "            model_name=model_display_name,\n",
    "            worker_pool_specs=WORKER_POOL_SPECS, \n",
    "            train_output_gcs_bucket=train_output_gcs_bucket,\n",
    "            experiment_name=experiment_name,\n",
    "            experiment_run=experiment_run,\n",
    "            training_image_uri=train_image_uri,\n",
    "            tensorboard_resource_name=create_managed_tensorboard_op.outputs['tensorboard_resource_name'], #tensorboard_resource_name, \n",
    "            service_account=service_account,\n",
    "            generate_new_vocab=generate_new_vocab,\n",
    "        )\n",
    "        .set_display_name(\"2Tower Training\")\n",
    "        .set_caching_options(True)\n",
    "        # .after(build_custom_train_image_op)\n",
    "    )\n",
    "    \n",
    "    # ========================================================================\n",
    "    # Import trained Query and Candidate Towers to this DAG (metadata)\n",
    "    # ========================================================================\n",
    "    \n",
    "    import_unmanaged_query_model_task = (\n",
    "        importer_node.importer(\n",
    "            artifact_uri=run_train_task_op.outputs['query_tower_dir_uri'],\n",
    "            artifact_class=artifact_types.UnmanagedContainerModel,\n",
    "            metadata={\n",
    "                'containerSpec': {\n",
    "                    'imageUri': 'us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-11:latest',\n",
    "                },\n",
    "            },\n",
    "        )\n",
    "        .set_display_name(\"Import Query Tower\")\n",
    "        .after(run_train_task_op)\n",
    "        .set_caching_options(True)\n",
    "    )\n",
    "    \n",
    "    import_unmanaged_candidate_model_task = (\n",
    "        importer_node.importer(\n",
    "            artifact_uri=run_train_task_op.outputs['candidate_tower_dir_uri'],\n",
    "            artifact_class=artifact_types.UnmanagedContainerModel,\n",
    "            metadata={\n",
    "                'containerSpec': {\n",
    "                    'imageUri': 'us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-11:latest',\n",
    "                },\n",
    "            },\n",
    "        )\n",
    "        .set_display_name(\"Import Candidate Tower\")\n",
    "        .after(run_train_task_op)\n",
    "        .set_caching_options(True)\n",
    "    )\n",
    "    \n",
    "    # ========================================================================\n",
    "    # Conditional: Upload models to Vertex model registry\n",
    "    # ========================================================================\n",
    "    # with kfp.v2.dsl.Condition(register_model_flag == \"True\", name=\"Register towers\"):\n",
    "        \n",
    "    # here\n",
    "\n",
    "    query_model_upload_op = (\n",
    "        gcc_aip.ModelUploadOp(\n",
    "            project=project,\n",
    "            location=location,\n",
    "            display_name=f'query-tower-{model_display_name}',\n",
    "            unmanaged_container_model=import_unmanaged_query_model_task.outputs[\"artifact\"],\n",
    "            labels={\"tower\": \"query\"},\n",
    "        )\n",
    "        .set_display_name(\"Upload Query Tower\")\n",
    "        .set_caching_options(True)\n",
    "    )\n",
    "\n",
    "    candidate_model_upload_op = (\n",
    "        gcc_aip.ModelUploadOp(\n",
    "            project=project,\n",
    "            location=location,\n",
    "            display_name=f'candidate-tower-{model_display_name}',\n",
    "            unmanaged_container_model=import_unmanaged_candidate_model_task.outputs[\"artifact\"],\n",
    "            labels={\"tower\": \"candidate\"},\n",
    "        )\n",
    "        .set_display_name(\"Upload Query Tower to Vertex\")\n",
    "        .set_caching_options(True)\n",
    "    )\n",
    "\n",
    "    # ========================================================================\n",
    "    # Deploy Query Tower to Endpoint\n",
    "    # ========================================================================\n",
    "    endpoint_create_op = (\n",
    "        gcc_aip.EndpointCreateOp(\n",
    "            project=project,\n",
    "            display_name=f'query-tower-endpoint-{pipeline_version}'\n",
    "        )\n",
    "        .after(query_model_upload_op)\n",
    "        .set_display_name(\"Create Query Endpoint\")\n",
    "        .set_caching_options(True)\n",
    "    )\n",
    "\n",
    "    model_deploy_op = (\n",
    "        gcc_aip.ModelDeployOp(\n",
    "            endpoint=endpoint_create_op.outputs['endpoint'],\n",
    "            model=query_model_upload_op.outputs['model'],\n",
    "            deployed_model_display_name=f'deployed-qtower-{pipeline_version}',\n",
    "            # dedicated_resources_accelerator_type=\"NVIDIA_TESLA_T4\",\n",
    "            # dedicated_resources_accelerator_count=1,\n",
    "            # dedicated_resources_max_replica_count=1,\n",
    "            # dedicated_resources_min_replica_count=1,\n",
    "            dedicated_resources_machine_type=\"n1-standard-16\",\n",
    "            dedicated_resources_min_replica_count=1,\n",
    "            dedicated_resources_max_replica_count=1,\n",
    "            service_account=service_account,\n",
    "        )\n",
    "        .set_display_name(\"Deploy Query Tower\")\n",
    "        .set_caching_options(True)\n",
    "    )\n",
    "\n",
    "    generate_candidates_op = (\n",
    "        generate_candidates.generate_candidates(\n",
    "            project=project,\n",
    "            location=location,\n",
    "            version=model_version,\n",
    "            candidate_tower_dir_uri=run_train_task_op.outputs['candidate_tower_dir_uri'],\n",
    "            train_output_gcs_bucket=train_output_gcs_bucket,\n",
    "            experiment_name=experiment_name,\n",
    "            experiment_run=experiment_run,\n",
    "            candidate_file_dir_bucket=candidate_file_dir,\n",
    "            candidate_file_dir_prefix=candidate_files_prefix,\n",
    "            experiment_run_dir=run_train_task_op.outputs['experiment_run_dir']\n",
    "        )\n",
    "        .set_display_name(\"Generate Candidate emb vectors\")\n",
    "        .set_caching_options(True)\n",
    "        .set_cpu_limit(cfg.CPU_LIMIT)\n",
    "        .set_memory_limit(cfg.MEMORY_LIMIT)\n",
    "    )\n",
    "    \n",
    "    model_monitoring_config_op = (\n",
    "        model_monitoring_config.model_monitoring_config(\n",
    "            project=project,\n",
    "            location=location,\n",
    "            version=model_version,\n",
    "            prefix=prefix,\n",
    "            emails=emails,\n",
    "            train_output_gcs_bucket=train_output_gcs_bucket,\n",
    "            # feature_dict=feature_dict, # TODO\n",
    "            bq_dataset= bq_dataset,\n",
    "            bq_train_table=bq_train_table,\n",
    "            experiment_name=experiment_name,\n",
    "            experiment_run=experiment_run,\n",
    "            endpoint=model_deploy_op.outputs['gcp_resources']\n",
    "        )\n",
    "        .set_display_name(\"set Model Monitoring\")\n",
    "        # .after(XXXX)\n",
    "        .set_caching_options(True)\n",
    "    )\n",
    "    \n",
    "    test_model_endpoint_op = (\n",
    "        test_model_endpoint.test_model_endpoint(\n",
    "            project=project,\n",
    "            location=location,\n",
    "            version=model_version,\n",
    "            train_output_gcs_bucket=train_output_gcs_bucket,\n",
    "            many_test_instances_gcs_filename=many_test_instances_gcs_filename,\n",
    "            experiment_name=experiment_name,\n",
    "            experiment_run=experiment_run,\n",
    "            endpoint=model_deploy_op.outputs['gcp_resources']\n",
    "        )\n",
    "        .set_display_name(\"Test endpoint deployment\")\n",
    "        .after(model_monitoring_config_op)\n",
    "        .set_caching_options(True)\n",
    "    )\n",
    "    \n",
    "    send_skewed_traffic_op = (\n",
    "        send_skewed_traffic.send_skewed_traffic(\n",
    "            project=project,\n",
    "            location=location,\n",
    "            version=model_version,\n",
    "            train_output_gcs_bucket=train_output_gcs_bucket,\n",
    "            experiment_name=experiment_name,\n",
    "            experiment_run=experiment_run,\n",
    "            endpoint=model_deploy_op.outputs['gcp_resources']\n",
    "        )\n",
    "        .set_display_name(\"Send skewed traffic\")\n",
    "        .after(model_monitoring_config_op)\n",
    "        .set_caching_options(True)\n",
    "    )\n",
    "\n",
    "    # ========================================================================\n",
    "    # Create ME indexes\n",
    "    # ========================================================================\n",
    "\n",
    "    create_ann_index_op = (\n",
    "        create_ann_index.create_ann_index(\n",
    "            project=project,\n",
    "            location=location,\n",
    "            version=model_version,\n",
    "            vpc_network_name=vpc_network_name,\n",
    "            emb_index_gcs_uri=generate_candidates_op.outputs['emb_index_gcs_uri'],\n",
    "            dimensions=128, #TODO: parameterize\n",
    "            ann_index_display_name=f'ann_index_{pipeline_version}'.replace('-', '_'),\n",
    "            approximate_neighbors_count=50,\n",
    "            distance_measure_type=\"DOT_PRODUCT_DISTANCE\",\n",
    "            leaf_node_embedding_count=500,\n",
    "            leaf_nodes_to_search_percent=7, \n",
    "            ann_index_description=\"testing ann index for TFRS deployment\",\n",
    "            # ann_index_labels=ann_index_labels,\n",
    "        )\n",
    "        .set_display_name(\"Create ANN Index\")\n",
    "        # .after(XXXX)\n",
    "        .set_caching_options(True)\n",
    "    )\n",
    "\n",
    "    create_brute_force_index_op = (\n",
    "        create_brute_force_index.create_brute_force_index(\n",
    "            project=project,\n",
    "            location=location,\n",
    "            version=model_version,\n",
    "            vpc_network_name=vpc_network_name,\n",
    "            emb_index_gcs_uri=generate_candidates_op.outputs['emb_index_gcs_uri'],\n",
    "            dimensions=128, #TODO: parameterize\n",
    "            brute_force_index_display_name=f'bf_index_{pipeline_version}'.replace('-', '_'),\n",
    "            approximate_neighbors_count=50,\n",
    "            distance_measure_type=\"DOT_PRODUCT_DISTANCE\",\n",
    "            brute_force_index_description=\"testing bf index for TFRS deployment\",\n",
    "            # brute_force_index_labels=brute_force_index_labels,\n",
    "        )\n",
    "        .set_display_name(\"Create BF Index\")\n",
    "        # .after(XXX)\n",
    "        .set_caching_options(True)\n",
    "    )\n",
    "\n",
    "    # ========================================================================\n",
    "    # Create ME index endpoints\n",
    "    # ========================================================================\n",
    "\n",
    "    create_ann_index_endpoint_vpc_op = (\n",
    "        create_ann_index_endpoint_vpc.create_ann_index_endpoint_vpc(\n",
    "            ann_index_artifact=create_ann_index_op.outputs['ann_index'],\n",
    "            project=project,\n",
    "            project_number=project_number,\n",
    "            version=model_version,\n",
    "            location=location,\n",
    "            vpc_network_name=vpc_network_name,\n",
    "            ann_index_endpoint_display_name=f'ann-index-endpoint_{pipeline_version}'.replace('-', '_'),\n",
    "            ann_index_endpoint_description='endpoint for ann index',\n",
    "            ann_index_resource_uri=create_ann_index_op.outputs['ann_index_resource_uri'],\n",
    "        )\n",
    "        .set_display_name(\"Create ANN Index Endpoint\")\n",
    "        # .after(XXX)\n",
    "    )\n",
    "\n",
    "    create_brute_index_endpoint_vpc_op = (\n",
    "        create_brute_index_endpoint_vpc.create_brute_index_endpoint_vpc(\n",
    "            bf_index_artifact=create_brute_force_index_op.outputs['brute_force_index'],\n",
    "            project=project,\n",
    "            project_number=project_number,\n",
    "            version=model_version,\n",
    "            location=location,\n",
    "            vpc_network_name=vpc_network_name,\n",
    "            brute_index_endpoint_display_name=f'bf-index-endpoint_{pipeline_version}'.replace('-', '_'),\n",
    "            brute_index_endpoint_description='endpoint for brute force index',\n",
    "            brute_force_index_resource_uri=create_brute_force_index_op.outputs['brute_force_index_resource_uri'],\n",
    "        )\n",
    "        .set_display_name(\"Create BF Index Endpoint\")\n",
    "        # .after(XXX)\n",
    "    )\n",
    "\n",
    "    # ========================================================================\n",
    "    # Deploy Indexes\n",
    "    # ========================================================================\n",
    "\n",
    "    deploy_ann_index_op = (\n",
    "        deploy_ann_index.deploy_ann_index(\n",
    "            project=project,\n",
    "            location=location,\n",
    "            version=model_version,\n",
    "            deployed_ann_index_name=f'deployedann_{model_version}'.replace('-', '_'), #todo update to letters, numbers, and underscores only\n",
    "            ann_index_resource_uri=create_ann_index_endpoint_vpc_op.outputs['ann_index_resource_uri'],\n",
    "            index_endpoint_resource_uri=create_ann_index_endpoint_vpc_op.outputs['ann_index_endpoint_resource_uri'],\n",
    "        )\n",
    "        .set_display_name(\"Deploy ANN Index\")\n",
    "        .set_caching_options(True)\n",
    "    )\n",
    "\n",
    "    deploy_brute_index_op = (\n",
    "        deploy_brute_index.deploy_brute_index(\n",
    "            project=project,\n",
    "            location=location,\n",
    "            version=model_version,\n",
    "            deployed_brute_force_index_name=f'deployedbf_{model_version}'.replace('-', '_'),\n",
    "            brute_force_index_resource_uri=create_brute_index_endpoint_vpc_op.outputs['brute_force_index_resource_uri'],\n",
    "            index_endpoint_resource_uri=create_brute_index_endpoint_vpc_op.outputs['brute_index_endpoint_resource_uri'],\n",
    "        )\n",
    "        .set_display_name(\"Deploy BF Index\")\n",
    "        .set_caching_options(True)\n",
    "    )\n",
    "\n",
    "    test_model_index_endpoint_op = (\n",
    "        test_model_index_endpoint.test_model_index_endpoint(\n",
    "            project=project,\n",
    "            location=location,\n",
    "            version=model_version,\n",
    "            test_instances_gcs_filename=test_instances_gcs_filename,\n",
    "            gcs_train_script_path=gcs_train_script_path,\n",
    "            train_output_gcs_bucket=train_output_gcs_bucket, \n",
    "            experiment_name=experiment_name,\n",
    "            experiment_run=experiment_run,\n",
    "            ann_index_endpoint_resource_uri=deploy_ann_index_op.outputs['index_endpoint_resource_uri'],\n",
    "            brute_index_endpoint_resource_uri=deploy_brute_index_op.outputs['index_endpoint_resource_uri'],\n",
    "            endpoint=model_deploy_op.outputs['gcp_resources']\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "65c65526-e42c-459c-a3a2-db8c84b6cc43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.7/site-packages/kfp/v2/compiler/compiler.py:1293: FutureWarning: APIs imported from the v1 namespace (e.g. kfp.dsl, kfp.components, etc) will not be supported by the v2 compiler since v2.0.0\n",
      "  category=FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "# ! rm -f custom_container_pipeline_spec.json\n",
    "\n",
    "PIPELINE_JSON_SPEC_LOCAL = \"custom_pipeline_spec.json\"\n",
    "\n",
    "! rm -f $PIPELINE_JSON_SPEC_LOCAL\n",
    "\n",
    "kfp.v2.compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline, package_path=PIPELINE_JSON_SPEC_LOCAL,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8d598e-9456-452f-bb41-6c80feb288a7",
   "metadata": {},
   "source": [
    "### save pipeline spec json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b3c4df38-23c9-449c-bbd2-bb6432d6e6a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PIPELINES_FILEPATH: gs://ndr-v1-hybrid-vertex-bucket/tfrs-pipe-v1/run-20230926-120445/pipeline_root/pipeline_spec.json\n"
     ]
    }
   ],
   "source": [
    "# !gsutil cp custom_container_pipeline_spec.json $PIPELINE_ROOT_PATH/pipeline_spec.json\n",
    "\n",
    "PIPELINES_FILEPATH = f'{PIPELINE_ROOT_PATH}/pipeline_spec.json'\n",
    "print(\"PIPELINES_FILEPATH:\", PIPELINES_FILEPATH)\n",
    "\n",
    "!gsutil -q cp $PIPELINE_JSON_SPEC_LOCAL $PIPELINES_FILEPATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c6dba00a-d42b-40f5-8135-777caaae79a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://ndr-v1-hybrid-vertex-bucket/tfrs-pipe-v1/run-20230926-120445/pipeline_root/Dockerfile_tfrs\n",
      "gs://ndr-v1-hybrid-vertex-bucket/tfrs-pipe-v1/run-20230926-120445/pipeline_root/pipeline_spec.json\n",
      "gs://ndr-v1-hybrid-vertex-bucket/tfrs-pipe-v1/run-20230926-120445/pipeline_root/934903580331/\n",
      "gs://ndr-v1-hybrid-vertex-bucket/tfrs-pipe-v1/run-20230926-120445/pipeline_root/trainer/\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls $PIPELINE_ROOT_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa01119-431a-49ce-bd7d-0acc1ca37e6c",
   "metadata": {},
   "source": [
    "## Submit pipeline to Vertex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "db63e2ee-ab3e-4e08-bc95-91e9b194d469",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_NUMBER=PROJECT_NUM\n",
    "vpc_network_name = VPC_NETWORK_NAME\n",
    "# VERTEX_SA = 'notebooksa@hybrid-vertex.iam.gserviceaccount.com'\n",
    "\n",
    "TRAIN_APP_CODE_PATH = f'{PIPELINE_ROOT_PATH}/trainer'\n",
    "\n",
    "job = vertex_ai.PipelineJob(\n",
    "    display_name=PIPELINE_NAME,\n",
    "    template_path=PIPELINES_FILEPATH,\n",
    "    pipeline_root=f'{PIPELINE_ROOT_PATH}',\n",
    "    failure_policy='fast', # slow | fast\n",
    "    # enable_caching=False,\n",
    "    parameter_values={\n",
    "        'project': PROJECT_ID,\n",
    "        'project_number': PROJECT_NUM,\n",
    "        'location': REGION,\n",
    "        'model_version': VERSION,\n",
    "        'pipeline_version': PIPELINE_VERSION,\n",
    "        'model_display_name': MODEL_ROOT_NAME,\n",
    "        'vpc_network_name':vpc_network_name,\n",
    "        # 'pipeline_tag': PIPELINE_TAG,\n",
    "        'gcs_train_script_path': TRAIN_APP_CODE_PATH,\n",
    "        'train_image_uri': f\"{REMOTE_IMAGE_NAME}\",\n",
    "        'train_output_gcs_bucket': BUCKET_NAME,\n",
    "        'train_dir': BUCKET_NAME,\n",
    "        'train_dir_prefix': TRAIN_DIR_PREFIX,\n",
    "        'valid_dir': BUCKET_NAME,\n",
    "        'valid_dir_prefix': VALID_DIR_PREFIX,\n",
    "        'candidate_file_dir': BUCKET_NAME,\n",
    "        'candidate_files_prefix': CANDIDATE_PREFIX,\n",
    "        'test_instances_gcs_filename': LOCAL_INSTANCES_PKL,\n",
    "        'many_test_instances_gcs_filename': MANY_TESTS_FILE,\n",
    "        # 'tensorboard_resource_name': TB_RESOURCE_NAME,\n",
    "        'train_dockerfile_name': DOCKERNAME,\n",
    "        'experiment_name': EXPERIMENT_NAME,\n",
    "        'experiment_run': RUN_NAME,\n",
    "        'service_account': VERTEX_SA,\n",
    "        'register_model_flag': 'True',\n",
    "        'generate_new_vocab': False,\n",
    "        'max_playlist_length': TRACK_HISTORY,\n",
    "        'max_tokens': 20000,\n",
    "        'ngrams': 2,\n",
    "        # 'feature_dict': FEATURE_DICT,\n",
    "        'prefix': PREFIX,\n",
    "        'emails': \"jordantotten@google.com\",\n",
    "        'bq_dataset': BQ_DATASET,\n",
    "        'bq_train_table': BQ_TABLE_TRAIN,\n",
    "    },\n",
    ")\n",
    "    \n",
    "job.run(\n",
    "    sync=False,\n",
    "    service_account=VERTEX_SA,\n",
    "    network=f'projects/{PROJECT_NUM}/global/networks/{VPC_NETWORK_NAME}'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb75704-0393-48c1-8281-de18bd55fd71",
   "metadata": {},
   "source": [
    "#### clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "deea05b0-4a31-4b8c-941b-e0dd79979f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! rm -rf custom_pipeline_spec.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f231ba7b-6208-4b97-9d03-ee66614e62d3",
   "metadata": {},
   "source": [
    "**Finished**"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-10.m103",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-10:m103"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
