{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24f0eb21",
   "metadata": {},
   "source": [
    "#### Step 0: Dependencies\n",
    "\n",
    "Run this one time when starting, then restart the kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbb59bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pandas pandas-gbq==0.12.0 --user"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68da0ab1",
   "metadata": {},
   "source": [
    "# Data prep\n",
    "\n",
    "## In this notebook we will load the songs from the zip file, and perform transoformations to prepare the data for two-tower training\n",
    "Steps\n",
    "1. Extract from the zip file\n",
    "2. Upload to BQ\n",
    "3. Enrich features for the playlist songs\n",
    "4. Cross-join songs with features (excpected rows = n_songs x n_playlists)\n",
    "5. Remove after-the-fact (later position songs) from the newly generated samples\n",
    "6. Create a clean train table, and flatten structs or use arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e04cd08",
   "metadata": {},
   "source": [
    "#### Unzip the file and uplaod to BQ\n",
    "[Source of data if you want to download zip](https://www.aicrowd.com/challenges/spotify-million-playlist-dataset-challenge/dataset_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77d08efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your variables for your project, region, and dataset name\n",
    "SOURCE_BUCKET = 'gs://spotify-builtin-2t/source_data/'\n",
    "PROJECT_ID = 'hybrid-vertex'\n",
    "REGION = 'us-central1'\n",
    "bq_dataset = 'mdp_eda_test'\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0276cc9-832f-466b-88d9-56c967fe29bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "\n",
    "bigquery_client = bigquery.Client(project=PROJECT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fd3725a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !gsutil cp {SOURCE_BUCKET}spotify_million_playlist_dataset.zip .\n",
    "# !unzip spotify_million_playlist_dataset.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420750c4",
   "metadata": {},
   "source": [
    "#### This step can take up to xxxxxxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0888f988",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "data_files = os.listdir('data')\n",
    "\n",
    "#make sure there is not already existing data in the playlists table\n",
    "#loops over json files - converts to pandas then upload/appends\n",
    "for filename in data_files:\n",
    "    with open(f'data/{filename}') as f:\n",
    "        json_dict = json.load(f)\n",
    "        df = pd.DataFrame(json_dict['playlists'])\n",
    "        df.to_gbq(\n",
    "        destination_table=f'{bq_dataset}.playlists', \n",
    "        project_id=PROJECT_ID, # TODO: param\n",
    "        location=REGION, \n",
    "        progress_bar=False, \n",
    "        reauth=True, \n",
    "        if_exists='append'\n",
    "        ) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338b113f",
   "metadata": {},
   "source": [
    "### Import bigquery and run parameterized queries to shape the data\n",
    "\n",
    "This query formats the json strings to be read as Bigquery structs, to be manipulated in subsequent queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e9b7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "json_extract_query = f\"\"\"create or replace table `{PROJECT_ID}.{bq_dataset}.playlists_nested` as (\n",
    "with json_parsed as (SELECT * except(tracks), JSON_EXTRACT_ARRAY(tracks) as json_data FROM `{PROJECT_ID}.{bq_dataset}.playlists` )\n",
    "\n",
    "select json_parsed.* except(json_data),\n",
    "ARRAY(SELECT AS STRUCT\n",
    "JSON_EXTRACT_SCALAR(json_data, \"$.pos\") as pos, \n",
    "JSON_EXTRACT_SCALAR(json_data, \"$.artist_name\") as artist_name,\n",
    "JSON_EXTRACT_SCALAR(json_data, \"$.track_uri\") as track_uri,\n",
    "JSON_EXTRACT_SCALAR(json_data, \"$.artist_uri\") as artist_uri,\n",
    "JSON_EXTRACT_SCALAR(json_data, \"$.track_name\") as track_name,\n",
    "JSON_EXTRACT_SCALAR(json_data, \"$.album_uri\") as album_uri,\n",
    "JSON_EXTRACT_SCALAR(json_data, \"$.duration_ms\") as duration_ms,\n",
    "JSON_EXTRACT_SCALAR(json_data, \"$.album_name\") as album_name\n",
    "from json_parsed.json_data\n",
    ") as tracks,\n",
    "from json_parsed) \"\"\"\n",
    "\n",
    "bigquery_client.query(json_extract_query).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2890a40",
   "metadata": {},
   "source": [
    "## Now enrich the playlist songs with the new features\n",
    "\n",
    "`unique_track_features` - create from file\n",
    "\n",
    "+\n",
    "\n",
    "`unique_artist_features` - create from file\n",
    "\n",
    "These are additional files where features were added "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6875ecec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 295860 rows.\n"
     ]
    }
   ],
   "source": [
    "# TODO(developer): Set table_id to the ID of the table to create.\n",
    "# table_id = \"your-project.your_dataset.your_table_name\"\n",
    "\n",
    "job_config = bigquery.LoadJobConfig(\n",
    "    autodetect=True,\n",
    "    source_format=bigquery.SourceFormat.NEWLINE_DELIMITED_JSON,\n",
    ")\n",
    "uri = f\"{SOURCE_BUCKET}unique_artists_features.json\"\n",
    "\n",
    "table_id = f\"{PROJECT_ID}.{bq_dataset}.unique_artists_features\"\n",
    "\n",
    "load_job = bigquery_client.load_table_from_uri(\n",
    "    uri,\n",
    "    table_id,\n",
    "    location=REGION,  # Must match the destination dataset location.\n",
    "    job_config=job_config,\n",
    ")  # Make an API request.\n",
    "\n",
    "load_job.result()  # Waits for the job to complete.\n",
    "\n",
    "destination_table = bigquery_client.get_table(table_id)\n",
    "print(\"Loaded {} rows.\".format(destination_table.num_rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b8eb60f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2262292 rows.\n"
     ]
    }
   ],
   "source": [
    "# TODO(developer): Set table_id to the ID of the table to create.\n",
    "# table_id = \"your-project.your_dataset.your_table_name\"\n",
    "\n",
    "job_config = bigquery.LoadJobConfig(\n",
    "    autodetect=True,\n",
    "    source_format=bigquery.SourceFormat.NEWLINE_DELIMITED_JSON,\n",
    ")\n",
    "uri = f\"{SOURCE_BUCKET}unique_track_features.json\"\n",
    "\n",
    "table_id = f\"{PROJECT_ID}.{bq_dataset}.unique_track_features\"\n",
    "\n",
    "load_job = bigquery_client.load_table_from_uri(\n",
    "    uri,\n",
    "    table_id,\n",
    "    location=REGION,  # Must match the destination dataset location.\n",
    "    job_config=job_config,\n",
    ")  # Make an API request.\n",
    "\n",
    "load_job.result()  # Waits for the job to complete.\n",
    "\n",
    "destination_table = bigquery_client.get_table(table_id)\n",
    "print(\"Loaded {} rows.\".format(destination_table.num_rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b3066d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 31.1 ms, sys: 221 Âµs, total: 31.3 ms\n",
      "Wall time: 48.6 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<google.cloud.bigquery.table._EmptyRowIterator at 0x7f47581f7430>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "enrich_query = f\"\"\"CREATE OR REPLACE TABLE\n",
    "  `{PROJECT_ID}.{bq_dataset}.enriched_data` AS (\n",
    "    SELECT\n",
    "    a.* except(tracks),\n",
    "      ARRAY(\n",
    "    SELECT\n",
    "      AS STRUCT CAST(track.pos AS int64) AS pos_can,\n",
    "      case when track.artist_name = '' then 'NONE' else track.artist_name end AS artist_name_can,\n",
    "      case when track.track_uri = '' then 'NONE' else track.track_uri  end AS track_uri_can,\n",
    "      case when track.track_name = '' then 'NONE' else track.track_name  end AS track_name_can,\n",
    "      case when track.album_uri = '' then 'NONE' else track.album_uri  end AS album_uri_can,\n",
    "      case when track.artist_uri = '' then 'NONE' else track.artist_uri  end AS artist_uri_can,\n",
    "      CAST(track.duration_ms AS float64) / 1.0 AS duration_ms_can,\n",
    "      case when track.album_name = '' then 'NONE' else track.album_name end AS album_name_can,\n",
    "      CAST(IFNULL(tf.track_pop, 0.0) as float64) / 1.0 AS track_pop_can,\n",
    "      CAST(IFNULL(af.artist_pop, 0.0) as float64) / 1.0  AS artist_pop_can,\n",
    "      case when af.artist_genres[OFFSET(0)] = '' then ['NONE'] else af.artist_genres end AS artist_genres_can,\n",
    "      CAST(IFNULL(af.artist_followers, 0.0) as float64) / 1.0 AS artist_followers_can\n",
    "    FROM\n",
    "      UNNEST(tracks) as track\n",
    "    INNER JOIN\n",
    "      `{PROJECT_ID}.{bq_dataset}.unique_track_features` AS tf --track features\n",
    "    ON\n",
    "      (track.track_uri = tf.track_uri)\n",
    "    INNER JOIN\n",
    "      `{PROJECT_ID}.{bq_dataset}.unique_artists_features` AS af --artist features\n",
    "      ON\n",
    "      (track.artist_uri = af.artist_uri)\n",
    "      ) AS tracks\n",
    "  FROM \n",
    "  `{PROJECT_ID}.{bq_dataset}.playlists_nested` as a)\"\"\"\n",
    "\n",
    "bigquery_client.query(enrich_query).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d933a3b9",
   "metadata": {},
   "source": [
    "## Cross join + get rid of after-the-fact `pos` data in playlist\n",
    "\n",
    "cross_join_songxplaylist_struct_query\n",
    "\n",
    "`hybrid-vertex.spotify_train_3.ordered_position_training`\n",
    "\n",
    "We create a data structure that creates unique song-playlist combos (every possible via cross-join). There is also a portion of pulling the last song in the playlist as the \"seed track\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e136cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "cross_join_query = f\"\"\"\n",
    "  CREATE OR REPLACE TABLE\n",
    "  `{PROJECT_ID}.{bq_dataset}.ordered_position_training` AS (\n",
    "  WITH\n",
    "    -- get every combination of song and its parent playlist\n",
    "    unnest_cross AS(\n",
    "    SELECT\n",
    "      b.*,\n",
    "      CONCAT(b.pid,\"-\",track.pos_can) AS pid_pos_id,\n",
    "      CAST(track.pos_can AS int64) AS pos_can,\n",
    "      track.artist_name_can ,\n",
    "      track.track_uri_can ,\n",
    "      track.album_uri_can,\n",
    "      track.track_name_can ,\n",
    "      track.artist_uri_can ,\n",
    "      CAST(track.duration_ms_can AS float64) AS duration_ms_can,\n",
    "      track.album_name_can ,\n",
    "      track.track_pop_can ,\n",
    "      track.artist_pop_can,\n",
    "      track.artist_genres_can ,\n",
    "      track.artist_followers_can \n",
    "    FROM (\n",
    "      SELECT\n",
    "        * EXCEPT(duration_ms)\n",
    "      FROM\n",
    "        `{PROJECT_ID}.{bq_dataset}.enriched_data`) AS b\n",
    "    CROSS JOIN\n",
    "      UNNEST(tracks) AS track)\n",
    "  SELECT\n",
    "    a.* EXCEPT(tracks,\n",
    "      num_tracks,\n",
    "      num_artists,\n",
    "      num_albums,\n",
    "      num_followers,\n",
    "      num_edits),\n",
    "    ARRAY(\n",
    "    SELECT\n",
    "      AS STRUCT CAST(track.pos_can AS int64) AS pos_pl,\n",
    "      track.artist_name_can AS artist_name_pl,\n",
    "      track.track_uri_can AS track_uri_pl,\n",
    "      track.track_name_can AS track_name_pl,\n",
    "      track.album_uri_can AS album_uri_pl,\n",
    "      track.artist_uri_can AS artist_uri_pl,\n",
    "      CAST(track.duration_ms_can AS float64) AS duration_ms_pl,\n",
    "      track.album_name_can AS album_name_pl,\n",
    "      track.track_pop_can AS track_pop_pl,\n",
    "      track.artist_pop_can AS artist_pop_pl,\n",
    "      track.artist_genres_can AS artist_genres_pl,\n",
    "      track.artist_followers_can AS artist_followers_pl,\n",
    "    FROM\n",
    "      UNNEST(tracks) AS track\n",
    "    WHERE\n",
    "      CAST(track.pos_can AS int64) < a.pos_can ORDER BY CAST(track.pos_can AS int64)) AS seed_playlist_tracks,\n",
    "    ----- seed track part\n",
    "    trx.pos_can AS pos_seed_track,\n",
    "    trx.artist_name_can AS artist_name_seed_track,\n",
    "    trx.artist_uri_can AS artist_uri_seed_track,\n",
    "    trx.track_name_can AS track_name_seed_track,\n",
    "    trx.track_uri_can AS track_uri_seed_track,\n",
    "    trx.album_name_can AS album_name_seed_track,\n",
    "    trx.album_uri_can AS album_uri_seed_track,\n",
    "    trx.duration_ms_can AS duration_seed_track,\n",
    "    trx.track_pop_can AS track_pop_seed_track,\n",
    "    trx.artist_pop_can AS artist_pop_seed_track,\n",
    "    trx.artist_genres_can as artist_genres_seed_track,\n",
    "    trx.artist_followers_can as artist_followers_seed_track\n",
    "  FROM\n",
    "    unnest_cross AS a -- with statement\n",
    "    ,\n",
    "    UNNEST(a.tracks) AS trx\n",
    "  WHERE\n",
    "    CAST(trx.pos_can AS int64) = a.pos_can-1);\n",
    "    \"\"\"\n",
    "\n",
    "bigquery_client.query(cross_join_query).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48a1393",
   "metadata": {},
   "source": [
    "## Update the playlist metadata with the new samples created above\n",
    "\n",
    "Trainv3-clean-track-features\n",
    "\n",
    "Get new metadata for the tracks now that there are updated track counts, durations, etc...\n",
    "\n",
    "`hybrid-vertex.spotify_train_3.train`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7985b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 52.1 ms, sys: 7.18 ms, total: 59.3 ms\n",
      "Wall time: 3min 4s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<google.cloud.bigquery.table._EmptyRowIterator at 0x7fdb52da8850>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "get_new_metadata_query = f\"\"\"\n",
    "create or replace table `{PROJECT_ID}.{bq_dataset}.train_dif_artist` as (\n",
    "WITH\n",
    "  playlist_features_clean AS (\n",
    "  SELECT\n",
    "    pid_pos_id,\n",
    "    SUM(trx.duration_ms_pl) / 1.0 AS duration_ms_seed_pl,\n",
    "    COUNT(1) / 1.0 AS n_songs_pl,\n",
    "    COUNT(DISTINCT trx.artist_name_pl) / 1.0 AS num_artists_pl,\n",
    "    COUNT(DISTINCT trx.album_uri_pl) /1.0 AS num_albums_pl,\n",
    "  FROM\n",
    "    `{PROJECT_ID}.{bq_dataset}.ordered_position_training`,\n",
    "    UNNEST(seed_playlist_tracks) AS trx\n",
    "  GROUP BY\n",
    "    pid_pos_id)\n",
    "    \n",
    "SELECT\n",
    "  a.* except(artist_genres_can, artist_genres_seed_track, track_pop_can, artist_pop_can, artist_followers_can,\n",
    "            track_pop_seed_track, artist_pop_seed_track),\n",
    "  b.* except(pid_pos_id),\n",
    "  IFNULL(a.artist_genres_can[OFFSET(0)], \"NONE\") as artist_genres_can,\n",
    "  IFNULL(a.artist_genres_seed_track[OFFSET(0)], \"NONE\") as artist_genres_seed_track,\n",
    "  IFNULL(a.track_pop_can, 0.0) / 1.0 as  track_pop_can, \n",
    "  IFNULL(a.artist_pop_can, 0.0) / 1.0 as artist_pop_can,\n",
    "  IFNULL(a.artist_followers_can, 0.0) / 1.0 as artist_followers_can,\n",
    "  IFNULL(a.track_pop_seed_track, 0.0) / 1.0 as track_pop_seed_track,\n",
    "  IFNULL(a.artist_pop_seed_track, 0.0) / 1.0 as artist_pop_seed_track,\n",
    "  \n",
    "FROM\n",
    "  `{PROJECT_ID}.{bq_dataset}.ordered_position_training` a\n",
    "INNER JOIN\n",
    "  playlist_features_clean b\n",
    "ON\n",
    "  a.pid_pos_id = b.pid_pos_id \n",
    "  WHERE album_uri_can != album_uri_seed_track and artist_uri_seed_track != artist_uri_can)\n",
    "  \"\"\"\n",
    "\n",
    "bigquery_client.query(get_new_metadata_query).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4517477d",
   "metadata": {},
   "source": [
    "## For TFRecords\n",
    "Get rid of structs by creating new table with arrays from playlist_seed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "da1c786f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 38 ms, sys: 8.46 ms, total: 46.4 ms\n",
      "Wall time: 53.3 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<google.cloud.bigquery.table._EmptyRowIterator at 0x7fdb45b4d690>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "train_flatten_query = f\"\"\"\n",
    "create or replace table `{PROJECT_ID}.{bq_dataset}.train_flatten_dif_artist` as (\n",
    "SELECT a.* except(seed_playlist_tracks, description),\n",
    "    RAND() as random,\n",
    "    IFNULL(a.description, \"\") as description_pl,\n",
    "    ARRAY(select t.pos_pl from UNNEST(seed_playlist_tracks) t) as pos_pl,\n",
    "    ARRAY(select t.artist_name_pl from UNNEST(seed_playlist_tracks) t) as artist_name_pl,\n",
    "    ARRAY(select t.track_uri_pl from UNNEST(seed_playlist_tracks) t) as track_uri_pl,\n",
    "    ARRAY(select t.track_name_pl from UNNEST(seed_playlist_tracks) t) as track_name_pl,\n",
    "    ARRAY(select t.duration_ms_pl from UNNEST(seed_playlist_tracks) t) as duration_ms_songs_pl,\n",
    "    ARRAY(select t.album_name_pl from UNNEST(seed_playlist_tracks) t) as album_name_pl,\n",
    "    ARRAY(select cast(t.artist_pop_pl as FLOAT64) from UNNEST(seed_playlist_tracks) t) as artist_pop_pl,\n",
    "    ARRAY(select t.artist_followers_pl from UNNEST(seed_playlist_tracks) t) as artists_followers_pl,\n",
    "    ARRAY(select case when t.track_pop_pl is null then 0. else t.track_pop_pl end from UNNEST(seed_playlist_tracks) t) as track_pop_pl,\n",
    "    ARRAY(select t.artist_genres_pl[OFFSET(0)] from UNNEST(seed_playlist_tracks) t) as artist_genres_pl\n",
    "    from `{PROJECT_ID}.{bq_dataset}.train_dif_artist` a\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "bigquery_client.query(train_flatten_query).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbde2e62",
   "metadata": {},
   "source": [
    "## We will use this table to export jsonl in the next notebook\n",
    "\n",
    "Links on built in two tower and data [requriements](https://cloud.google.com/vertex-ai/docs/matching-engine/train-embeddings-two-tower#training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5e7039a7-928a-4906-8b0e-6b6f4e06fd1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20 ms, sys: 3.23 ms, total: 23.2 ms\n",
      "Wall time: 20.3 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<google.cloud.bigquery.table._EmptyRowIterator at 0x7fdb45b43ad0>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "VALIDATION_P = 0.1\n",
    "\n",
    "validation_creation = f\"\"\"\n",
    "CREATE OR REPLACE TABLE\n",
    "  `{PROJECT_ID}.{bq_dataset}.train_flatten_dif_artist_valid` AS (\n",
    "    SELECT * except(random)\n",
    "  FROM\n",
    "    `{PROJECT_ID}.{bq_dataset}.train_flatten_dif_artist` where random < {VALIDATION_P})\"\"\"\n",
    "\n",
    "bigquery_client.query(validation_creation).result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e0f11c67-99f5-41a0-847c-7c8badf157da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.1 ms, sys: 11.6 ms, total: 26.7 ms\n",
      "Wall time: 35.3 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<google.cloud.bigquery.table._EmptyRowIterator at 0x7fdb45cf30d0>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "VALIDATION_P = 0.1\n",
    "\n",
    "validation_creation = f\"\"\"\n",
    "CREATE OR REPLACE TABLE\n",
    "  `{PROJECT_ID}.{bq_dataset}.train_flatten_dif_artist_train` AS (\n",
    "    SELECT * except(random)\n",
    "  FROM\n",
    "    `{PROJECT_ID}.{bq_dataset}.train_flatten_dif_artist` where random >= {VALIDATION_P})\"\"\"\n",
    "\n",
    "bigquery_client.query(validation_creation).result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4439a72b-553f-44aa-abe2-89193781edfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExtractJob<project=hybrid-vertex, location=us-central1, id=686bf6ab-7b5b-4dd7-846d-6c5831b836c5>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Export train\n",
    "\n",
    "BUCKET = 'gs://spotify-builtin-2t'\n",
    "PROJECT = PROJECT_ID\n",
    "DATASET_ID = bq_dataset\n",
    "TABLE = 'train_flatten_dif_artist_train'\n",
    "LOCATION = REGION\n",
    "\n",
    "from google.cloud import bigquery\n",
    "client = bigquery.Client()\n",
    "\n",
    "destination_uri = f\"{BUCKET}/train_data_parquet_dif_artist_train/*.snappy.parquet\"\n",
    "dataset_ref = bigquery.DatasetReference(PROJECT, DATASET_ID)\n",
    "table_ref = dataset_ref.table(TABLE)\n",
    "job_config = bigquery.job.ExtractJobConfig()\n",
    "job_config.destination_format = bigquery.DestinationFormat.PARQUET\n",
    "extract_job = client.extract_table(\n",
    "    table_ref,\n",
    "    destination_uri,\n",
    "    job_config=job_config,\n",
    "    # Location must match that of the source table.\n",
    "    location=LOCATION,\n",
    ")  # API request\n",
    "extract_job.result()  # Waits for job to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450a12e4-3625-4712-8e3a-d1e89177bf7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Export validation\n",
    "\n",
    "BUCKET = 'gs://spotify-builtin-2t'\n",
    "PROJECT = PROJECT_ID\n",
    "DATASET_ID = bq_dataset\n",
    "TABLE = 'train_flatten_dif_artist_valid'\n",
    "LOCATION = REGION\n",
    "\n",
    "from google.cloud import bigquery\n",
    "client = bigquery.Client()\n",
    "\n",
    "destination_uri = f\"{BUCKET}/train_data_parquet_dif_artist_valid/*.snappy.parquet\"\n",
    "dataset_ref = bigquery.DatasetReference(PROJECT, DATASET_ID)\n",
    "table_ref = dataset_ref.table(TABLE)\n",
    "job_config = bigquery.job.ExtractJobConfig()\n",
    "job_config.destination_format = bigquery.DestinationFormat.PARQUET\n",
    "extract_job = client.extract_table(\n",
    "    table_ref,\n",
    "    destination_uri,\n",
    "    job_config=job_config,\n",
    "    # Location must match that of the source table.\n",
    "    location=LOCATION,\n",
    ")  # API request\n",
    "extract_job.result()  # Waits for job to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19b9804",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-9.m95",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-9:m95"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
