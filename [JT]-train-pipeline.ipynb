{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "152d43dc-e7c4-4085-b214-b576e1b4a94e",
   "metadata": {},
   "source": [
    "# Training pipeline for TFRS  2tower model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58440717-f517-42f4-bfb1-d22676d04044",
   "metadata": {},
   "source": [
    "```\n",
    "tensorflow==2.10.1\n",
    "tensorflow-cloud==0.1.16\n",
    "tensorflow-datasets==4.6.0\n",
    "tensorflow-estimator==2.10.0\n",
    "tensorflow-hub==0.12.0\n",
    "tensorflow-io==0.27.0\n",
    "tensorflow-io-gcs-filesystem==0.27.0\n",
    "tensorflow-metadata==1.8.0\n",
    "tensorflow-probability==0.18.0\n",
    "tensorflow-recommenders==0.7.2\n",
    "tensorflow-serving-api==2.8.3\n",
    "tensorflow-transform==1.8.0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6a2a17-2a69-447f-a050-4e5ec3d261fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install kfp\n",
    "# !pip install google-cloud-pipeline-components "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43201bb1-8b40-4ae4-a415-b15eee0323cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! python3 -c \"import kfp; print('KFP SDK version: {}'.format(kfp.__version__))\"\n",
    "# ! python3 -c \"import google_cloud_pipeline_components; print('google_cloud_pipeline_components version: {}'.format(google_cloud_pipeline_components.__version__))\"\n",
    "# ! python3 -c \"import google.cloud.aiplatform; print('aiplatform SDK version: {}'.format(google.cloud.aiplatform.__version__))\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e3d319c-84e1-4cd2-8882-0f390bc21e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ID: hybrid-vertex\n",
      "PROJECT_NUM: 934903580331\n",
      "LOCATION: us-central1\n"
     ]
    }
   ],
   "source": [
    "GCP_PROJECTS = !gcloud config get-value project\n",
    "PROJECT_ID = GCP_PROJECTS[0]\n",
    "PROJECT_NUM = !gcloud projects list --filter=\"$PROJECT_ID\" --format=\"value(PROJECT_NUMBER)\"\n",
    "PROJECT_NUM = PROJECT_NUM[0]\n",
    "LOCATION = 'us-central1'\n",
    "\n",
    "print(f\"PROJECT_ID: {PROJECT_ID}\")\n",
    "print(f\"PROJECT_NUM: {PROJECT_NUM}\")\n",
    "print(f\"LOCATION: {LOCATION}\")\n",
    "\n",
    "VERTEX_SA = '934903580331-compute@developer.gserviceaccount.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68a4fb97-bed6-4533-92d3-dbc79e134f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "from time import time\n",
    "import pandas as pd\n",
    "# disable INFO and DEBUG logging everywhere\n",
    "import logging\n",
    "import time\n",
    "from pprint import pprint\n",
    "\n",
    "logging.disable(logging.WARNING)\n",
    "\n",
    "from google.cloud import aiplatform as vertex_ai\n",
    "from google.cloud import storage\n",
    "\n",
    "# Pipelines\n",
    "from typing import Any, Callable, Dict, NamedTuple, Optional, List\n",
    "from google_cloud_pipeline_components import aiplatform as gcc_aip\n",
    "from google_cloud_pipeline_components.types import artifact_types\n",
    "\n",
    "# Kubeflow SDK\n",
    "# TODO: fix these\n",
    "from kfp.v2 import dsl\n",
    "import kfp\n",
    "import kfp.v2.dsl\n",
    "from kfp.v2.google import client as pipelines_client\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output,\n",
    "                        OutputPath, component)\n",
    "\n",
    "storage_client = storage.Client(project=PROJECT_ID)\n",
    "\n",
    "vertex_ai.init(project=PROJECT_ID,location=LOCATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd32b85b-b44a-4e17-9c23-98fad8efe064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL_ROOT_NAME: sp-2tower-tfrs-jtv13-v11\n"
     ]
    }
   ],
   "source": [
    "# PREFIX = 'spotify-2tower'\n",
    "APP='sp'\n",
    "MODEL_TYPE='2tower'\n",
    "FRAMEWORK = 'tfrs'\n",
    "MODEL_VERSION = 'jtv13'\n",
    "PIPELINE_VERSION = 'v11'\n",
    "MODEL_ROOT_NAME = f'{APP}-{MODEL_TYPE}-{FRAMEWORK}-{MODEL_VERSION}-{PIPELINE_VERSION}'\n",
    "\n",
    "print(f\"MODEL_ROOT_NAME: {MODEL_ROOT_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e816a896-4015-4d0b-8a39-04d9a5a5b924",
   "metadata": {},
   "source": [
    "## Write Train files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "022ba9e5-95fb-47b2-8718-76687ad0dcc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Make the training subfolder\n",
    "# ! rm -rf {REPO_DOCKER_PATH_PREFIX}/trainer\n",
    "# ! mkdir {REPO_DOCKER_PATH_PREFIX}/trainer\n",
    "# ! touch {REPO_DOCKER_PATH_PREFIX}/trainer/__init__.py\n",
    "\n",
    "REPO_DOCKER_PATH_PREFIX = 'src'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c02857dd-731b-40ba-ac58-74286774fbf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMAGE_URI: gcr.io/hybrid-vertex/sp-2tower-tfrs-jtv13-v11-training\n"
     ]
    }
   ],
   "source": [
    "# Docker definitions for training\n",
    "IMAGE_NAME = f'{MODEL_ROOT_NAME}-training'\n",
    "IMAGE_URI = f'gcr.io/{PROJECT_ID}/{IMAGE_NAME}'\n",
    "\n",
    "DOCKERNAME = 'tfrs'\n",
    "REPO_DOCKER_PATH_PREFIX = 'src'\n",
    "MACHINE_TYPE ='e2-highcpu-32'\n",
    "FILE_LOCATION = './src'\n",
    "\n",
    "print(f\"IMAGE_URI: {IMAGE_URI}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "46e4a95f-40f2-47b4-af3f-2d49cccd82d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/cloudbuild.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/cloudbuild.yaml\n",
    "\n",
    "steps:\n",
    "- name: 'gcr.io/cloud-builders/docker'\n",
    "  args: ['build', '-t', '$_IMAGE_URI', '$_FILE_LOCATION', '-f', '$_FILE_LOCATION/Dockerfile.$_DOCKERNAME']\n",
    "images:\n",
    "- '$_IMAGE_URI'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad807fb4-b7a4-4ce0-967c-deb73f32536b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/two_tower_jt/train_config.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/two_tower_jt/train_config.py\n",
    "\n",
    "PROJECT_ID = 'hybrid-vertex'\n",
    "NEW_ADAPTS = 'True'\n",
    "USE_CROSS_LAYER = False\n",
    "USE_DROPOUT = 'False'\n",
    "SEED = 1234\n",
    "MAX_PLAYLIST_LENGTH = 5         # this should improve performance vs 375\n",
    "EMBEDDING_DIM = 128   \n",
    "PROJECTION_DIM = 25  \n",
    "SEED = 1234\n",
    "DROPOUT_RATE = 0.33\n",
    "MAX_TOKENS = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d80ee63-a858-4ffe-9f6d-39e17afb486c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/two_tower_jt/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/two_tower_jt/requirements.txt\n",
    "google-cloud-aiplatform>=1.20.0\n",
    "tensorflow-recommenders==0.7.2\n",
    "tensorboard==2.10.1\n",
    "tensorboard-data-server==0.6.1\n",
    "tensorboard-plugin-profile==2.11.1\n",
    "tensorflow-io==0.27.0\n",
    "google-cloud-aiplatform[cloud_profiler]>=1.20.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2b81e499-9dd2-42c6-a2af-91f6849a5b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/Dockerfile.tfrs\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/Dockerfile.{DOCKERNAME}\n",
    "\n",
    "# FROM tensorflow/tensorflow:2.10.1-gpu\n",
    "FROM gcr.io/deeplearning-platform-release/tf-gpu.2-10\n",
    "\n",
    "WORKDIR /src\n",
    "\n",
    "# Copies the trainer code to the docker image.\n",
    "COPY two_tower_jt/* two_tower_jt/ \n",
    "\n",
    "RUN pip install -r two_tower_jt/requirements.txt\n",
    "\n",
    "RUN apt update && apt -y install nvtop\n",
    "\n",
    "# # Sets up the entry point to invoke the trainer.\n",
    "# # ENTRYPOINT [\"python\", \"-m\", \"two_tower_jt.task\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85623af4-764f-4316-bde2-d4ae8cbbdf60",
   "metadata": {},
   "source": [
    "## Build Custom Train Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c76bfb28-6fd5-4d96-8d17-ba5e8212a43b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOCKERNAME: tfrs\n",
      "IMAGE_URI: gcr.io/hybrid-vertex/sp-2tower-tfrs-jtv13-v11-training\n",
      "FILE_LOCATION: ./src\n",
      "MACHINE_TYPE: e2-highcpu-32\n"
     ]
    }
   ],
   "source": [
    "print(f\"DOCKERNAME: {DOCKERNAME}\")\n",
    "print(f\"IMAGE_URI: {IMAGE_URI}\")\n",
    "print(f\"FILE_LOCATION: {FILE_LOCATION}\")\n",
    "print(f\"MACHINE_TYPE: {MACHINE_TYPE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6b85202c-6495-4ff4-82d0-f0ffcc709725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/jw-repo/spotify_mpd_two_tower\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "351cbc55-3efc-48c1-ac1b-5917d10faaf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m/home/jupyter/jw-repo/spotify_mpd_two_tower/src\u001b[00m\n",
      "├── Dockerfile.tfrs\n",
      "├── cloudbuild.yaml\n",
      "├── \u001b[01;34mtrain_pipes\u001b[00m\n",
      "│   ├── build_custom_image.py\n",
      "│   └── train_custom_model.py\n",
      "├── \u001b[01;34mtwo_tower_jt\u001b[00m\n",
      "│   ├── __init__.py\n",
      "│   ├── \u001b[01;34m__pycache__\u001b[00m\n",
      "│   │   ├── __init__.cpython-37.pyc\n",
      "│   │   ├── train_config.cpython-37.pyc\n",
      "│   │   ├── two_tower.cpython-37.pyc\n",
      "│   │   └── two_tower_lite.cpython-37.pyc\n",
      "│   ├── data-pipeline.py\n",
      "│   ├── interactive_train.py\n",
      "│   ├── requirements.txt\n",
      "│   ├── task.py\n",
      "│   ├── train_config.py\n",
      "│   ├── two_tower.py\n",
      "│   └── two_tower_lite.py\n",
      "└── \u001b[01;34mvocab_pipes\u001b[00m\n",
      "    ├── \u001b[01;34m__pycache__\u001b[00m\n",
      "    ├── adapt_fixed_text_layer_vocab.py\n",
      "    ├── adapt_ragged_text_layer_vocab.py\n",
      "    ├── config.py\n",
      "    └── create_master_vocab.py\n",
      "\n",
      "5 directories, 20 files\n"
     ]
    }
   ],
   "source": [
    "!tree /home/jupyter/jw-repo/spotify_mpd_two_tower/src"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fcdae75-a4cb-476f-8d15-89c2f69ea417",
   "metadata": {},
   "source": [
    "### Optionally include a `.gcloudignore` file \n",
    "\n",
    "* limits the files submitted to Cloud Build\n",
    "* see [gcloudignore](https://cloud.google.com/sdk/gcloud/reference/topic/gcloudignore) for details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1433a749-df25-4862-8f27-b9c84442b1ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [gcloudignore/enabled].\n"
     ]
    }
   ],
   "source": [
    "! gcloud config set gcloudignore/enabled true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2c430b9f-556f-40ec-bb68-1ff8d2d9bd9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting .gcloudignore\n"
     ]
    }
   ],
   "source": [
    "%%writefile .gcloudignore\n",
    ".gcloudignore\n",
    "/local_files/\n",
    "/img/\n",
    "*.pkl\n",
    "*.png\n",
    ".git\n",
    ".github\n",
    ".ipynb_checkpoints/*\n",
    "*__pycache__\n",
    "spotipy_secret_creds.py\n",
    "candidate_embs_local_v6_20230112-180944.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d6c3cd53-5595-4524-88cd-a2f6b4c31c7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !gcloud meta list-files-for-upload\n",
    "# !ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "94546ac1-8b61-4fff-981a-9dae7f976fdb",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 61 file(s) totalling 1.8 MiB before compression.\n",
      "Uploading tarball of [.] to [gs://hybrid-vertex_cloudbuild/source/1673606563.320645-46ebda4c43014d47b0b155bd5656694f.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/hybrid-vertex/locations/global/builds/bcca1ea0-0d76-4630-927e-fa1b077cc00e].\n",
      "Logs are available at [ https://console.cloud.google.com/cloud-build/builds/bcca1ea0-0d76-4630-927e-fa1b077cc00e?project=934903580331 ].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"bcca1ea0-0d76-4630-927e-fa1b077cc00e\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://hybrid-vertex_cloudbuild/source/1673606563.320645-46ebda4c43014d47b0b155bd5656694f.tgz#1673606563965303\n",
      "Copying gs://hybrid-vertex_cloudbuild/source/1673606563.320645-46ebda4c43014d47b0b155bd5656694f.tgz#1673606563965303...\n",
      "/ [1 files][374.1 KiB/374.1 KiB]                                                \n",
      "Operation completed over 1 objects/374.1 KiB.\n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  179.7kB\n",
      "Step 1/5 : FROM gcr.io/deeplearning-platform-release/tf-gpu.2-10\n",
      "latest: Pulling from deeplearning-platform-release/tf-gpu.2-10\n",
      "eaead16dc43b: Pulling fs layer\n",
      "66505a2e0e5b: Pulling fs layer\n",
      "a655a22c474a: Pulling fs layer\n",
      "a296e3e832e8: Pulling fs layer\n",
      "c1c3ca40938b: Pulling fs layer\n",
      "aa8799b38d43: Pulling fs layer\n",
      "7a83033ad051: Pulling fs layer\n",
      "e508c64fd858: Pulling fs layer\n",
      "c711afa7022f: Pulling fs layer\n",
      "eced3f37c25e: Pulling fs layer\n",
      "8e7602fdce33: Pulling fs layer\n",
      "ff60b2bba5f8: Pulling fs layer\n",
      "4f4fb700ef54: Pulling fs layer\n",
      "ebd800816a17: Pulling fs layer\n",
      "a798f9ec3f11: Pulling fs layer\n",
      "9c1c6c434334: Pulling fs layer\n",
      "c2ccb3ba2311: Pulling fs layer\n",
      "76a2e4e12ef8: Pulling fs layer\n",
      "b336ce8db20f: Pulling fs layer\n",
      "53e0c73b7a09: Pulling fs layer\n",
      "f1615116c881: Pulling fs layer\n",
      "b885cde9b114: Pulling fs layer\n",
      "0949c75a4dde: Pulling fs layer\n",
      "55cd1e2d875c: Pulling fs layer\n",
      "f6e57611c727: Pulling fs layer\n",
      "35252cfce065: Pulling fs layer\n",
      "f4777e8e02cd: Pulling fs layer\n",
      "c1c3ca40938b: Waiting\n",
      "aa8799b38d43: Waiting\n",
      "7a83033ad051: Waiting\n",
      "e508c64fd858: Waiting\n",
      "c711afa7022f: Waiting\n",
      "eced3f37c25e: Waiting\n",
      "8e7602fdce33: Waiting\n",
      "ff60b2bba5f8: Waiting\n",
      "4f4fb700ef54: Waiting\n",
      "ebd800816a17: Waiting\n",
      "a798f9ec3f11: Waiting\n",
      "a296e3e832e8: Waiting\n",
      "9c1c6c434334: Waiting\n",
      "c2ccb3ba2311: Waiting\n",
      "76a2e4e12ef8: Waiting\n",
      "b336ce8db20f: Waiting\n",
      "b885cde9b114: Waiting\n",
      "53e0c73b7a09: Waiting\n",
      "f6e57611c727: Waiting\n",
      "0949c75a4dde: Waiting\n",
      "f1615116c881: Waiting\n",
      "55cd1e2d875c: Waiting\n",
      "35252cfce065: Waiting\n",
      "3cb270247f6f: Pulling fs layer\n",
      "8fd949e557d0: Pulling fs layer\n",
      "2dc11453d343: Pulling fs layer\n",
      "3cb270247f6f: Waiting\n",
      "ff76e3b4874b: Pulling fs layer\n",
      "92415ea2bbf1: Pulling fs layer\n",
      "3e5afccf09a2: Pulling fs layer\n",
      "e1ebff6b8521: Pulling fs layer\n",
      "92415ea2bbf1: Waiting\n",
      "ff76e3b4874b: Waiting\n",
      "527af5ff702b: Pulling fs layer\n",
      "3e5afccf09a2: Waiting\n",
      "17e5239c7022: Pulling fs layer\n",
      "ca1bf562c349: Pulling fs layer\n",
      "527af5ff702b: Waiting\n",
      "e1ebff6b8521: Waiting\n",
      "17e5239c7022: Waiting\n",
      "a655a22c474a: Download complete\n",
      "eaead16dc43b: Verifying Checksum\n",
      "eaead16dc43b: Download complete\n",
      "a296e3e832e8: Verifying Checksum\n",
      "a296e3e832e8: Download complete\n",
      "c1c3ca40938b: Verifying Checksum\n",
      "c1c3ca40938b: Download complete\n",
      "7a83033ad051: Verifying Checksum\n",
      "7a83033ad051: Download complete\n",
      "eaead16dc43b: Pull complete\n",
      "66505a2e0e5b: Verifying Checksum\n",
      "66505a2e0e5b: Download complete\n",
      "c711afa7022f: Download complete\n",
      "66505a2e0e5b: Pull complete\n",
      "a655a22c474a: Pull complete\n",
      "a296e3e832e8: Pull complete\n",
      "c1c3ca40938b: Pull complete\n",
      "e508c64fd858: Download complete\n",
      "aa8799b38d43: Verifying Checksum\n",
      "aa8799b38d43: Download complete\n",
      "ff60b2bba5f8: Download complete\n",
      "4f4fb700ef54: Verifying Checksum\n",
      "4f4fb700ef54: Download complete\n",
      "8e7602fdce33: Verifying Checksum\n",
      "8e7602fdce33: Download complete\n",
      "ebd800816a17: Verifying Checksum\n",
      "ebd800816a17: Download complete\n",
      "9c1c6c434334: Verifying Checksum\n",
      "9c1c6c434334: Download complete\n",
      "c2ccb3ba2311: Verifying Checksum\n",
      "c2ccb3ba2311: Download complete\n",
      "a798f9ec3f11: Verifying Checksum\n",
      "a798f9ec3f11: Download complete\n",
      "b336ce8db20f: Verifying Checksum\n",
      "b336ce8db20f: Download complete\n",
      "76a2e4e12ef8: Verifying Checksum\n",
      "76a2e4e12ef8: Download complete\n",
      "f1615116c881: Verifying Checksum\n",
      "f1615116c881: Download complete\n",
      "b885cde9b114: Verifying Checksum\n",
      "b885cde9b114: Download complete\n",
      "0949c75a4dde: Verifying Checksum\n",
      "0949c75a4dde: Download complete\n",
      "55cd1e2d875c: Verifying Checksum\n",
      "55cd1e2d875c: Download complete\n",
      "f6e57611c727: Download complete\n",
      "35252cfce065: Download complete\n",
      "f4777e8e02cd: Verifying Checksum\n",
      "f4777e8e02cd: Download complete\n",
      "3cb270247f6f: Verifying Checksum\n",
      "3cb270247f6f: Download complete\n",
      "8fd949e557d0: Verifying Checksum\n",
      "8fd949e557d0: Download complete\n",
      "2dc11453d343: Download complete\n",
      "53e0c73b7a09: Download complete\n",
      "92415ea2bbf1: Verifying Checksum\n",
      "92415ea2bbf1: Download complete\n",
      "3e5afccf09a2: Verifying Checksum\n",
      "3e5afccf09a2: Download complete\n",
      "eced3f37c25e: Verifying Checksum\n",
      "eced3f37c25e: Download complete\n",
      "e1ebff6b8521: Verifying Checksum\n",
      "e1ebff6b8521: Download complete\n",
      "17e5239c7022: Verifying Checksum\n",
      "17e5239c7022: Download complete\n",
      "ca1bf562c349: Verifying Checksum\n",
      "ca1bf562c349: Download complete\n",
      "ff76e3b4874b: Verifying Checksum\n",
      "ff76e3b4874b: Download complete\n",
      "aa8799b38d43: Pull complete\n",
      "7a83033ad051: Pull complete\n",
      "527af5ff702b: Verifying Checksum\n",
      "527af5ff702b: Download complete\n",
      "e508c64fd858: Pull complete\n",
      "c711afa7022f: Pull complete\n",
      "eced3f37c25e: Pull complete\n",
      "8e7602fdce33: Pull complete\n",
      "ff60b2bba5f8: Pull complete\n",
      "4f4fb700ef54: Pull complete\n",
      "ebd800816a17: Pull complete\n",
      "a798f9ec3f11: Pull complete\n",
      "9c1c6c434334: Pull complete\n",
      "c2ccb3ba2311: Pull complete\n",
      "76a2e4e12ef8: Pull complete\n",
      "b336ce8db20f: Pull complete\n",
      "53e0c73b7a09: Pull complete\n",
      "f1615116c881: Pull complete\n",
      "b885cde9b114: Pull complete\n",
      "0949c75a4dde: Pull complete\n",
      "55cd1e2d875c: Pull complete\n",
      "f6e57611c727: Pull complete\n",
      "35252cfce065: Pull complete\n",
      "f4777e8e02cd: Pull complete\n",
      "3cb270247f6f: Pull complete\n",
      "8fd949e557d0: Pull complete\n",
      "2dc11453d343: Pull complete\n",
      "ff76e3b4874b: Pull complete\n",
      "92415ea2bbf1: Pull complete\n",
      "3e5afccf09a2: Pull complete\n",
      "e1ebff6b8521: Pull complete\n",
      "527af5ff702b: Pull complete\n",
      "17e5239c7022: Pull complete\n",
      "ca1bf562c349: Pull complete\n",
      "Digest: sha256:e171cfa693c84f1812d98312935ab1bff167f2aee913e94cf22147193aff8447\n",
      "Status: Downloaded newer image for gcr.io/deeplearning-platform-release/tf-gpu.2-10:latest\n",
      " ---> 12e7fe8a4d96\n",
      "Step 2/5 : WORKDIR /src\n",
      " ---> Running in ea171de71eba\n",
      "Removing intermediate container ea171de71eba\n",
      " ---> 316074600880\n",
      "Step 3/5 : COPY two_tower_jt/* two_tower_jt/\n",
      " ---> 42976e39de8e\n",
      "Step 4/5 : RUN pip install -r two_tower_jt/requirements.txt\n",
      " ---> Running in 02f9292d4b02\n",
      "Collecting google-cloud-aiplatform>=1.20.0\n",
      "  Downloading google_cloud_aiplatform-1.20.0-py2.py3-none-any.whl (2.3 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.3/2.3 MB 27.4 MB/s eta 0:00:00\n",
      "Collecting tensorflow-recommenders==0.7.2\n",
      "  Downloading tensorflow_recommenders-0.7.2-py3-none-any.whl (89 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 89.3/89.3 kB 15.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: tensorboard==2.10.1 in /opt/conda/lib/python3.7/site-packages (from -r two_tower_jt/requirements.txt (line 3)) (2.10.1)\n",
      "Requirement already satisfied: tensorboard-data-server==0.6.1 in /opt/conda/lib/python3.7/site-packages (from -r two_tower_jt/requirements.txt (line 4)) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-profile==2.11.1 in /opt/conda/lib/python3.7/site-packages (from -r two_tower_jt/requirements.txt (line 5)) (2.11.1)\n",
      "Collecting tensorflow-io==0.27.0\n",
      "  Downloading tensorflow_io-0.27.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (25.0 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 25.0/25.0 MB 69.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: tensorflow>=2.9.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow-recommenders==0.7.2->-r two_tower_jt/requirements.txt (line 2)) (2.10.1)\n",
      "Requirement already satisfied: absl-py>=0.1.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow-recommenders==0.7.2->-r two_tower_jt/requirements.txt (line 2)) (1.3.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard==2.10.1->-r two_tower_jt/requirements.txt (line 3)) (1.8.1)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard==2.10.1->-r two_tower_jt/requirements.txt (line 3)) (1.51.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard==2.10.1->-r two_tower_jt/requirements.txt (line 3)) (65.5.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard==2.10.1->-r two_tower_jt/requirements.txt (line 3)) (3.4.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard==2.10.1->-r two_tower_jt/requirements.txt (line 3)) (2.15.0)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard==2.10.1->-r two_tower_jt/requirements.txt (line 3)) (1.21.6)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard==2.10.1->-r two_tower_jt/requirements.txt (line 3)) (2.28.1)\n",
      "Requirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.7/site-packages (from tensorboard==2.10.1->-r two_tower_jt/requirements.txt (line 3)) (0.38.4)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard==2.10.1->-r two_tower_jt/requirements.txt (line 3)) (2.1.2)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /opt/conda/lib/python3.7/site-packages (from tensorboard==2.10.1->-r two_tower_jt/requirements.txt (line 3)) (3.19.6)\n",
      "Requirement already satisfied: gviz-api>=1.9.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard-plugin-profile==2.11.1->-r two_tower_jt/requirements.txt (line 5)) (1.10.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard-plugin-profile==2.11.1->-r two_tower_jt/requirements.txt (line 5)) (1.16.0)\n",
      "Collecting tensorflow-io-gcs-filesystem==0.27.0\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.27.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.4/2.4 MB 76.1 MB/s eta 0:00:00\n",
      "Collecting google-cloud-resource-manager<3.0.0dev,>=1.3.3\n",
      "  Downloading google_cloud_resource_manager-1.8.0-py2.py3-none-any.whl (235 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 235.3/235.3 kB 32.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform>=1.20.0->-r two_tower_jt/requirements.txt (line 1)) (1.34.0)\n",
      "Collecting packaging<22.0.0dev,>=14.3\n",
      "  Downloading packaging-21.3-py3-none-any.whl (40 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 40.8/40.8 kB 6.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: google-cloud-storage<3.0.0dev,>=1.32.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform>=1.20.0->-r two_tower_jt/requirements.txt (line 1)) (2.7.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform>=1.20.0->-r two_tower_jt/requirements.txt (line 1)) (1.22.1)\n",
      "Collecting google-cloud-bigquery<3.0.0dev,>=1.15.0\n",
      "  Downloading google_cloud_bigquery-2.34.4-py2.py3-none-any.whl (206 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 206.6/206.6 kB 30.0 MB/s eta 0:00:00\n",
      "Collecting werkzeug>=1.0.1\n",
      "  Downloading Werkzeug-2.0.3-py3-none-any.whl (289 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 289.2/289.2 kB 35.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform>=1.20.0->-r two_tower_jt/requirements.txt (line 1)) (1.57.0)\n",
      "Requirement already satisfied: grpcio-status<2.0dev,>=1.33.2 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform>=1.20.0->-r two_tower_jt/requirements.txt (line 1)) (1.48.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard==2.10.1->-r two_tower_jt/requirements.txt (line 3)) (5.2.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard==2.10.1->-r two_tower_jt/requirements.txt (line 3)) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard==2.10.1->-r two_tower_jt/requirements.txt (line 3)) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard==2.10.1->-r two_tower_jt/requirements.txt (line 3)) (1.3.1)\n",
      "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform>=1.20.0->-r two_tower_jt/requirements.txt (line 1)) (2.8.2)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform>=1.20.0->-r two_tower_jt/requirements.txt (line 1)) (2.4.0)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform>=1.20.0->-r two_tower_jt/requirements.txt (line 1)) (2.3.2)\n",
      "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /opt/conda/lib/python3.7/site-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform>=1.20.0->-r two_tower_jt/requirements.txt (line 1)) (0.12.4)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard==2.10.1->-r two_tower_jt/requirements.txt (line 3)) (5.1.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging<22.0.0dev,>=14.3->google-cloud-aiplatform>=1.20.0->-r two_tower_jt/requirements.txt (line 1)) (3.0.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard==2.10.1->-r two_tower_jt/requirements.txt (line 3)) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard==2.10.1->-r two_tower_jt/requirements.txt (line 3)) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard==2.10.1->-r two_tower_jt/requirements.txt (line 3)) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard==2.10.1->-r two_tower_jt/requirements.txt (line 3)) (1.26.13)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.9.0->tensorflow-recommenders==0.7.2->-r two_tower_jt/requirements.txt (line 2)) (0.2.0)\n",
      "Requirement already satisfied: keras<2.11,>=2.10.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.9.0->tensorflow-recommenders==0.7.2->-r two_tower_jt/requirements.txt (line 2)) (2.10.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.9.0->tensorflow-recommenders==0.7.2->-r two_tower_jt/requirements.txt (line 2)) (3.7.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.9.0->tensorflow-recommenders==0.7.2->-r two_tower_jt/requirements.txt (line 2)) (4.4.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.9.0->tensorflow-recommenders==0.7.2->-r two_tower_jt/requirements.txt (line 2)) (2.1.1)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.9.0->tensorflow-recommenders==0.7.2->-r two_tower_jt/requirements.txt (line 2)) (1.14.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.9.0->tensorflow-recommenders==0.7.2->-r two_tower_jt/requirements.txt (line 2)) (1.6.3)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.9.0->tensorflow-recommenders==0.7.2->-r two_tower_jt/requirements.txt (line 2)) (0.4.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.9.0->tensorflow-recommenders==0.7.2->-r two_tower_jt/requirements.txt (line 2)) (14.0.6)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.9.0->tensorflow-recommenders==0.7.2->-r two_tower_jt/requirements.txt (line 2)) (3.3.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.11,>=2.10.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.9.0->tensorflow-recommenders==0.7.2->-r two_tower_jt/requirements.txt (line 2)) (2.10.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.9.0->tensorflow-recommenders==0.7.2->-r two_tower_jt/requirements.txt (line 2)) (22.12.6)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.9.0->tensorflow-recommenders==0.7.2->-r two_tower_jt/requirements.txt (line 2)) (1.1.2)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.7/site-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform>=1.20.0->-r two_tower_jt/requirements.txt (line 1)) (1.5.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard==2.10.1->-r two_tower_jt/requirements.txt (line 3)) (3.11.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard==2.10.1->-r two_tower_jt/requirements.txt (line 3)) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard==2.10.1->-r two_tower_jt/requirements.txt (line 3)) (3.2.2)\n",
      "Installing collected packages: werkzeug, tensorflow-io-gcs-filesystem, packaging, tensorflow-io, google-auth-oauthlib, google-cloud-resource-manager, google-cloud-bigquery, tensorflow-recommenders, google-cloud-aiplatform\n",
      "  Attempting uninstall: werkzeug\n",
      "    Found existing installation: Werkzeug 2.1.2\n",
      "    Uninstalling Werkzeug-2.1.2:\n",
      "      Successfully uninstalled Werkzeug-2.1.2\n",
      "  Attempting uninstall: tensorflow-io-gcs-filesystem\n",
      "    Found existing installation: tensorflow-io-gcs-filesystem 0.28.0\n",
      "    Uninstalling tensorflow-io-gcs-filesystem-0.28.0:\n",
      "      Successfully uninstalled tensorflow-io-gcs-filesystem-0.28.0\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 22.0\n",
      "    Uninstalling packaging-22.0:\n",
      "      Successfully uninstalled packaging-22.0\n",
      "  Attempting uninstall: tensorflow-io\n",
      "    Found existing installation: tensorflow-io 0.28.0\n",
      "    Uninstalling tensorflow-io-0.28.0:\n",
      "      Successfully uninstalled tensorflow-io-0.28.0\n",
      "  Attempting uninstall: google-auth-oauthlib\n",
      "    Found existing installation: google-auth-oauthlib 0.8.0\n",
      "    Uninstalling google-auth-oauthlib-0.8.0:\n",
      "      Successfully uninstalled google-auth-oauthlib-0.8.0\n",
      "  Attempting uninstall: google-cloud-bigquery\n",
      "    Found existing installation: google-cloud-bigquery 3.4.1\n",
      "    Uninstalling google-cloud-bigquery-3.4.1:\n",
      "      Successfully uninstalled google-cloud-bigquery-3.4.1\n",
      "\u001b[91mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow-transform 1.11.0 requires pyarrow<7,>=6, but you have pyarrow 7.0.0 which is incompatible.\n",
      "\u001b[0mSuccessfully installed google-auth-oauthlib-0.4.6 google-cloud-aiplatform-1.20.0 google-cloud-bigquery-2.34.4 google-cloud-resource-manager-1.8.0 packaging-21.3 tensorflow-io-0.27.0 tensorflow-io-gcs-filesystem-0.27.0 tensorflow-recommenders-0.7.2 werkzeug-2.0.3\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mRemoving intermediate container 02f9292d4b02\n",
      " ---> 3029aa1020ae\n",
      "Step 5/5 : RUN apt update && apt -y install nvtop\n",
      " ---> Running in 1494a4fd5e98\n",
      "\u001b[91m\n",
      "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
      "\n",
      "\u001b[0mGet:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease [1581 B]\n",
      "Get:2 http://packages.cloud.google.com/apt gcsfuse-focal InRelease [5389 B]\n",
      "Get:3 https://packages.cloud.google.com/apt cloud-sdk InRelease [6751 B]\n",
      "Get:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  Packages [831 kB]\n",
      "Get:5 https://packages.cloud.google.com/apt google-fast-socket InRelease [5405 B]\n",
      "Get:6 http://packages.cloud.google.com/apt gcsfuse-focal/main amd64 Packages [1799 B]\n",
      "Get:7 https://packages.cloud.google.com/apt cloud-sdk/main amd64 Packages [372 kB]\n",
      "Get:8 https://packages.cloud.google.com/apt google-fast-socket/main amd64 Packages [428 B]\n",
      "Get:9 http://archive.ubuntu.com/ubuntu focal InRelease [265 kB]\n",
      "Get:10 http://archive.ubuntu.com/ubuntu focal-updates InRelease [114 kB]\n",
      "Get:11 http://archive.ubuntu.com/ubuntu focal-backports InRelease [108 kB]\n",
      "Get:12 http://archive.ubuntu.com/ubuntu focal/universe amd64 Packages [11.3 MB]\n",
      "Get:13 http://security.ubuntu.com/ubuntu focal-security InRelease [114 kB]\n",
      "Get:14 http://security.ubuntu.com/ubuntu focal-security/multiverse amd64 Packages [27.7 kB]\n",
      "Get:15 http://security.ubuntu.com/ubuntu focal-security/main amd64 Packages [2423 kB]\n",
      "Get:16 http://security.ubuntu.com/ubuntu focal-security/restricted amd64 Packages [1862 kB]\n",
      "Get:17 http://security.ubuntu.com/ubuntu focal-security/universe amd64 Packages [979 kB]\n",
      "Get:18 http://archive.ubuntu.com/ubuntu focal/main amd64 Packages [1275 kB]\n",
      "Get:19 http://archive.ubuntu.com/ubuntu focal/restricted amd64 Packages [33.4 kB]\n",
      "Get:20 http://archive.ubuntu.com/ubuntu focal/multiverse amd64 Packages [177 kB]\n",
      "Get:21 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [1281 kB]\n",
      "Get:22 http://archive.ubuntu.com/ubuntu focal-updates/multiverse amd64 Packages [31.2 kB]\n",
      "Get:23 http://archive.ubuntu.com/ubuntu focal-updates/restricted amd64 Packages [1986 kB]\n",
      "Get:24 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [2896 kB]\n",
      "Get:25 http://archive.ubuntu.com/ubuntu focal-backports/main amd64 Packages [55.2 kB]\n",
      "Get:26 http://archive.ubuntu.com/ubuntu focal-backports/universe amd64 Packages [28.6 kB]\n",
      "Fetched 26.2 MB in 4min 11s (104 kB/s)\n",
      "Reading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "25 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
      "\u001b[91m\n",
      "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
      "\u001b[0mReading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "The following additional packages will be installed:\n",
      "  libnvidia-compute-418 libnvidia-compute-430 libnvidia-compute-525\n",
      "The following NEW packages will be installed:\n",
      "  libnvidia-compute-418 libnvidia-compute-430 libnvidia-compute-525 nvtop\n",
      "0 upgraded, 4 newly installed, 0 to remove and 25 not upgraded.\n",
      "Need to get 50.3 MB of archives.\n",
      "After this operation, 234 MB of additional disk space will be used.\n",
      "Get:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  libnvidia-compute-525 525.60.13-0ubuntu1 [50.2 MB]\n",
      "Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  libnvidia-compute-430 525.60.13-0ubuntu1 [6912 B]\n",
      "Get:3 http://archive.ubuntu.com/ubuntu focal/restricted amd64 libnvidia-compute-418 amd64 430.50-0ubuntu3 [6936 B]\n",
      "Get:4 http://archive.ubuntu.com/ubuntu focal/multiverse amd64 nvtop amd64 1.0.0-1ubuntu2 [26.8 kB]\n",
      "\u001b[91mdebconf: unable to initialize frontend: Dialog\n",
      "debconf: (TERM is not set, so the dialog frontend is not usable.)\n",
      "debconf: falling back to frontend: Readline\n",
      "\u001b[0m\u001b[91mdebconf: unable to initialize frontend: Readline\n",
      "debconf: (This frontend requires a controlling tty.)\n",
      "debconf: falling back to frontend: Teletype\n",
      "\u001b[0m\u001b[91mdpkg-preconfigure: unable to re-open stdin: \n",
      "\u001b[0mFetched 50.3 MB in 1s (38.5 MB/s)\n",
      "Selecting previously unselected package libnvidia-compute-525:amd64.\n",
      "(Reading database ... 90802 files and directories currently installed.)\n",
      "Preparing to unpack .../libnvidia-compute-525_525.60.13-0ubuntu1_amd64.deb ...\n",
      "Unpacking libnvidia-compute-525:amd64 (525.60.13-0ubuntu1) ...\n",
      "Selecting previously unselected package libnvidia-compute-430:amd64.\n",
      "Preparing to unpack .../libnvidia-compute-430_525.60.13-0ubuntu1_amd64.deb ...\n",
      "Unpacking libnvidia-compute-430:amd64 (525.60.13-0ubuntu1) ...\n",
      "Selecting previously unselected package libnvidia-compute-418:amd64.\n",
      "Preparing to unpack .../libnvidia-compute-418_430.50-0ubuntu3_amd64.deb ...\n",
      "Unpacking libnvidia-compute-418:amd64 (430.50-0ubuntu3) ...\n",
      "Selecting previously unselected package nvtop.\n",
      "Preparing to unpack .../nvtop_1.0.0-1ubuntu2_amd64.deb ...\n",
      "Unpacking nvtop (1.0.0-1ubuntu2) ...\n",
      "Setting up libnvidia-compute-525:amd64 (525.60.13-0ubuntu1) ...\n",
      "Setting up libnvidia-compute-430:amd64 (525.60.13-0ubuntu1) ...\n",
      "Setting up libnvidia-compute-418:amd64 (430.50-0ubuntu3) ...\n",
      "Setting up nvtop (1.0.0-1ubuntu2) ...\n",
      "Processing triggers for man-db (2.9.1-1) ...\n",
      "Processing triggers for libc-bin (2.31-0ubuntu9.9) ...\n",
      "Removing intermediate container 1494a4fd5e98\n",
      " ---> b7c9fe74618b\n",
      "Successfully built b7c9fe74618b\n",
      "Successfully tagged gcr.io/hybrid-vertex/sp-2tower-tfrs-jtv13-v11-training:latest\n",
      "PUSH\n",
      "Pushing gcr.io/hybrid-vertex/sp-2tower-tfrs-jtv13-v11-training\n",
      "The push refers to repository [gcr.io/hybrid-vertex/sp-2tower-tfrs-jtv13-v11-training]\n",
      "169dbd12f00f: Preparing\n",
      "01c736593d62: Preparing\n",
      "1974b9d8bee3: Preparing\n",
      "4e47c32f73cf: Preparing\n",
      "a800e87117e9: Preparing\n",
      "86924e35cc59: Preparing\n",
      "7b66a1be304f: Preparing\n",
      "afc3c82235cd: Preparing\n",
      "a64ef3222ad5: Preparing\n",
      "e9fd5c76f4b2: Preparing\n",
      "83273d72b080: Preparing\n",
      "17d3bfcc5741: Preparing\n",
      "739e5e43a72a: Preparing\n",
      "86924e35cc59: Waiting\n",
      "7e66bd8b5b8d: Preparing\n",
      "7b66a1be304f: Waiting\n",
      "10535f2fe91d: Preparing\n",
      "38aaf0adbb42: Preparing\n",
      "9192d6b86332: Preparing\n",
      "2b55ab636d0a: Preparing\n",
      "afc3c82235cd: Waiting\n",
      "f14c2d2eb39e: Preparing\n",
      "060f5a6593f6: Preparing\n",
      "e9fd5c76f4b2: Waiting\n",
      "74115b901496: Preparing\n",
      "81ac346eb187: Preparing\n",
      "83273d72b080: Waiting\n",
      "7365f548d2be: Preparing\n",
      "8d14dba85309: Preparing\n",
      "17d3bfcc5741: Waiting\n",
      "fc09c92af34e: Preparing\n",
      "6e36bffef97d: Preparing\n",
      "739e5e43a72a: Waiting\n",
      "31a4ee0dfc1f: Preparing\n",
      "988e9d26d6ee: Preparing\n",
      "7e66bd8b5b8d: Waiting\n",
      "a64ef3222ad5: Waiting\n",
      "5f70bf18a086: Preparing\n",
      "10535f2fe91d: Waiting\n",
      "b763c1c026b0: Preparing\n",
      "38aaf0adbb42: Waiting\n",
      "9f24c36cbd64: Preparing\n",
      "74115b901496: Waiting\n",
      "99799594069a: Preparing\n",
      "9192d6b86332: Waiting\n",
      "d812321bae2e: Preparing\n",
      "81ac346eb187: Waiting\n",
      "2b55ab636d0a: Waiting\n",
      "e39fe845e186: Preparing\n",
      "0c0255fe70c0: Preparing\n",
      "f14c2d2eb39e: Waiting\n",
      "7365f548d2be: Waiting\n",
      "3f68638ae737: Preparing\n",
      "fc09c92af34e: Waiting\n",
      "060f5a6593f6: Waiting\n",
      "6aa81253de72: Preparing\n",
      "8d14dba85309: Waiting\n",
      "25e27b6ba1ab: Preparing\n",
      "6e36bffef97d: Waiting\n",
      "31a4ee0dfc1f: Waiting\n",
      "2e699e937b48: Preparing\n",
      "3f09125d08e2: Preparing\n",
      "9f24c36cbd64: Waiting\n",
      "988e9d26d6ee: Waiting\n",
      "f4462d5b2da2: Preparing\n",
      "d812321bae2e: Waiting\n",
      "99799594069a: Waiting\n",
      "5f70bf18a086: Waiting\n",
      "b763c1c026b0: Waiting\n",
      "0c0255fe70c0: Waiting\n",
      "2e699e937b48: Waiting\n",
      "3f68638ae737: Waiting\n",
      "f4462d5b2da2: Waiting\n",
      "3f09125d08e2: Waiting\n",
      "25e27b6ba1ab: Waiting\n",
      "6aa81253de72: Waiting\n",
      "a800e87117e9: Layer already exists\n",
      "86924e35cc59: Layer already exists\n",
      "7b66a1be304f: Layer already exists\n",
      "afc3c82235cd: Layer already exists\n",
      "a64ef3222ad5: Layer already exists\n",
      "e9fd5c76f4b2: Layer already exists\n",
      "83273d72b080: Layer already exists\n",
      "17d3bfcc5741: Layer already exists\n",
      "1974b9d8bee3: Pushed\n",
      "739e5e43a72a: Layer already exists\n",
      "4e47c32f73cf: Pushed\n",
      "7e66bd8b5b8d: Layer already exists\n",
      "10535f2fe91d: Layer already exists\n",
      "38aaf0adbb42: Layer already exists\n",
      "9192d6b86332: Layer already exists\n",
      "2b55ab636d0a: Layer already exists\n",
      "f14c2d2eb39e: Layer already exists\n",
      "74115b901496: Layer already exists\n",
      "060f5a6593f6: Layer already exists\n",
      "81ac346eb187: Layer already exists\n",
      "7365f548d2be: Layer already exists\n",
      "8d14dba85309: Layer already exists\n",
      "fc09c92af34e: Layer already exists\n",
      "6e36bffef97d: Layer already exists\n",
      "988e9d26d6ee: Layer already exists\n",
      "5f70bf18a086: Layer already exists\n",
      "31a4ee0dfc1f: Layer already exists\n",
      "b763c1c026b0: Layer already exists\n",
      "9f24c36cbd64: Layer already exists\n",
      "99799594069a: Layer already exists\n",
      "d812321bae2e: Layer already exists\n",
      "e39fe845e186: Layer already exists\n",
      "0c0255fe70c0: Layer already exists\n",
      "3f68638ae737: Layer already exists\n",
      "6aa81253de72: Layer already exists\n",
      "25e27b6ba1ab: Layer already exists\n",
      "2e699e937b48: Layer already exists\n",
      "f4462d5b2da2: Layer already exists\n",
      "3f09125d08e2: Layer already exists\n",
      "01c736593d62: Pushed\n",
      "169dbd12f00f: Pushed\n",
      "latest: digest: sha256:85cf5acee4a0b19abe5e2bc0c6c8242bfc98f6bcd20b54355599a2892a0ae582 size: 8930\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                       IMAGES                                                            STATUS\n",
      "bcca1ea0-0d76-4630-927e-fa1b077cc00e  2023-01-13T10:42:44+00:00  8M14S     gs://hybrid-vertex_cloudbuild/source/1673606563.320645-46ebda4c43014d47b0b155bd5656694f.tgz  gcr.io/hybrid-vertex/sp-2tower-tfrs-jtv13-v11-training (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "! gcloud builds submit --config src/cloudbuild.yaml \\\n",
    "    --substitutions _DOCKERNAME=$DOCKERNAME,_IMAGE_URI=$IMAGE_URI,_FILE_LOCATION=$FILE_LOCATION \\\n",
    "    --timeout=2h \\\n",
    "    --machine-type=$MACHINE_TYPE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c889eb1-b663-4491-8bbf-4247d87b1762",
   "metadata": {},
   "source": [
    "# Pipeline Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3868029b-b1ce-45a9-aade-ebdfec8f110f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jupyter/jw-repo/spotify_mpd_two_tower'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aa1292ae-b51d-42bc-83c5-996e827c2633",
   "metadata": {},
   "outputs": [],
   "source": [
    "REPO_DOCKER_PATH_PREFIX = 'src'\n",
    "PIPELINES_SUB_DIR = 'train_pipes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6893f359-64de-458e-a964-d42ce581fd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -rf {REPO_DOCKER_PATH_PREFIX}/{PIPELINES_SUB_DIR}\n",
    "! mkdir {REPO_DOCKER_PATH_PREFIX}/{PIPELINES_SUB_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3fedd1-0897-4e32-bbd5-1dd6bbc12158",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Build Custom Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "335711ed-4425-409f-aebc-5fbdcbfedea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/train_pipes/build_custom_image.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/{PIPELINES_SUB_DIR}/build_custom_image.py\n",
    "\n",
    "import kfp\n",
    "from typing import Any, Callable, Dict, NamedTuple, Optional, List\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output,\n",
    "                        OutputPath, component, Metrics)\n",
    "@kfp.v2.dsl.component(\n",
    "    base_image=\"gcr.io/google.com/cloudsdktool/cloud-sdk:latest\",\n",
    "    packages_to_install=[\n",
    "        \"google-cloud-build\"\n",
    "    ],\n",
    ")\n",
    "def build_custom_image(\n",
    "    project: str,\n",
    "    artifact_gcs_path: str,\n",
    "    docker_name: str,\n",
    "    app_dir_name: str,\n",
    "    custom_image_uri: str,\n",
    ") -> NamedTuple('Outputs', [\n",
    "    ('custom_image_uri', str),\n",
    "]):\n",
    "    # TODO: make output Artifact for image_uri\n",
    "    \"\"\"\n",
    "    custom pipeline component to build custom image using\n",
    "    Cloud Build, the training/serving application code, and dependencies\n",
    "    defined in the Dockerfile\n",
    "    \"\"\"\n",
    "    \n",
    "    import logging\n",
    "    import os\n",
    "\n",
    "    from google.cloud.devtools import cloudbuild_v1 as cloudbuild\n",
    "    from google.protobuf.duration_pb2 import Duration\n",
    "\n",
    "    # initialize client for cloud build\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "    build_client = cloudbuild.services.cloud_build.CloudBuildClient()\n",
    "    \n",
    "    # parse step inputs to get path to Dockerfile and training application code\n",
    "    _gcs_dockerfile_path = os.path.join(artifact_gcs_path, f\"{docker_name}\") # Dockerfile.XXXXX\n",
    "    _gcs_script_dir_path = os.path.join(artifact_gcs_path, f\"{app_dir_name}/\") # \"trainer/\"\n",
    "    \n",
    "    logging.info(f\"_gcs_dockerfile_path: {_gcs_dockerfile_path}\")\n",
    "    logging.info(f\"_gcs_script_dir_path: {_gcs_script_dir_path}\")\n",
    "    \n",
    "    # define build steps to pull the training code and Dockerfile\n",
    "    # and build/push the custom training container image\n",
    "    build = cloudbuild.Build()\n",
    "    build.steps = [\n",
    "        {\n",
    "            \"name\": \"gcr.io/cloud-builders/gsutil\",\n",
    "            \"args\": [\"cp\", \"-r\", _gcs_script_dir_path, \".\"],\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"gcr.io/cloud-builders/gsutil\",\n",
    "            \"args\": [\"cp\", _gcs_dockerfile_path, \"Dockerfile\"],\n",
    "        },\n",
    "        # enabling Kaniko cache in a Docker build that caches intermediate\n",
    "        # layers and pushes image automatically to Container Registry\n",
    "        # https://cloud.google.com/build/docs/kaniko-cache\n",
    "        # {\n",
    "        #     \"name\": \"gcr.io/kaniko-project/executor:latest\",\n",
    "        #     # \"name\": \"gcr.io/kaniko-project/executor:v1.8.0\",        # TODO; downgraded to avoid error in build\n",
    "        #     # \"args\": [f\"--destination={training_image_uri}\", \"--cache=true\"],\n",
    "        #     \"args\": [f\"--destination={training_image_uri}\", \"--cache=false\"],\n",
    "        # },\n",
    "        {\n",
    "            \"name\": \"gcr.io/cloud-builders/docker\",\n",
    "            \"args\": ['build','-t', f'{custom_image_uri}', '.'],\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"gcr.io/cloud-builders/docker\",\n",
    "            \"args\": ['push', f'{custom_image_uri}'], \n",
    "        },\n",
    "    ]\n",
    "    # override default timeout of 10min\n",
    "    timeout = Duration()\n",
    "    timeout.seconds = 7200\n",
    "    build.timeout = timeout\n",
    "\n",
    "    # create build\n",
    "    operation = build_client.create_build(project_id=project, build=build)\n",
    "    logging.info(\"IN PROGRESS:\")\n",
    "    logging.info(operation.metadata)\n",
    "\n",
    "    # get build status\n",
    "    result = operation.result()\n",
    "    logging.info(\"RESULT:\", result.status)\n",
    "\n",
    "    # return step outputs\n",
    "    return (\n",
    "        custom_image_uri,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8843984-b680-4c46-a456-b55291643e14",
   "metadata": {},
   "source": [
    "## Custom train job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bc26d281-a72f-4825-a5f8-33ff35d2fb03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/train_pipes/train_custom_model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/{PIPELINES_SUB_DIR}/train_custom_model.py\n",
    "\n",
    "import kfp\n",
    "from typing import Any, Callable, Dict, NamedTuple, Optional, List\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output,\n",
    "                        OutputPath, component, Metrics)\n",
    "@kfp.v2.dsl.component(\n",
    "    base_image='python:3.9',\n",
    "    packages_to_install=[\n",
    "        'google-cloud-aiplatform==1.18.1',\n",
    "        # 'tensorflow==2.9.2',\n",
    "        # 'tensorflow-recommenders==0.7.0',\n",
    "        'numpy',\n",
    "        'google-cloud-storage',\n",
    "    ],\n",
    "    # output_component_file=\"./pipelines/train_custom_model.yaml\",\n",
    ")\n",
    "def train_custom_model(\n",
    "    project: str,\n",
    "    model_version: str,\n",
    "    pipeline_version: str,\n",
    "    model_name: str, \n",
    "    worker_pool_specs: dict,\n",
    "    # vocab_dict_uri: str, \n",
    "    train_output_gcs_bucket: str,                         # change to workdir?\n",
    "    training_image_uri: str,\n",
    "    tensorboard_resource_name: str,\n",
    "    service_account: str,\n",
    "    experiment_name: str,\n",
    "    experiment_run: str,\n",
    ") -> NamedTuple('Outputs', [\n",
    "    ('job_dict_uri', str),\n",
    "    ('query_tower_dir_uri', str),\n",
    "    ('candidate_tower_dir_uri', str),\n",
    "    # ('candidate_index_dir_uri', str),\n",
    "]):\n",
    "    \n",
    "    import logging\n",
    "    import numpy as np\n",
    "    import pickle as pkl\n",
    "    \n",
    "    from google.cloud import aiplatform as vertex_ai\n",
    "    from google.cloud import storage\n",
    "    \n",
    "    vertex_ai.init(\n",
    "        project=project,\n",
    "        location='us-central1',\n",
    "        experiment=experiment_name,\n",
    "    )\n",
    "    \n",
    "    storage_client = storage.Client(project=project)\n",
    "    \n",
    "    JOB_NAME = f'train-{model_name}'\n",
    "    logging.info(f'JOB_NAME: {JOB_NAME}')\n",
    "    \n",
    "    BASE_OUTPUT_DIR = f'gs://{train_output_gcs_bucket}/{experiment_name}/{experiment_run}'\n",
    "    logging.info(f'BASE_OUTPUT_DIR: {BASE_OUTPUT_DIR}')\n",
    "    \n",
    "    # logging.info(f'vocab_dict_uri: {vocab_dict_uri}')\n",
    "    \n",
    "    logging.info(f'tensorboard_resource_name: {tensorboard_resource_name}')\n",
    "    logging.info(f'service_account: {service_account}')\n",
    "    logging.info(f'worker_pool_specs: {worker_pool_specs}')\n",
    "    \n",
    "    # ====================================================\n",
    "    # Launch Vertex job\n",
    "    # ====================================================\n",
    "  \n",
    "    job = vertex_ai.CustomJob(\n",
    "        display_name=JOB_NAME,\n",
    "        worker_pool_specs=worker_pool_specs,\n",
    "        base_output_dir=BASE_OUTPUT_DIR,\n",
    "        staging_bucket=f\"{BASE_OUTPUT_DIR}/staging\",\n",
    "    )\n",
    "    \n",
    "    logging.info(f'Submitting train job to Vertex AI...')\n",
    "\n",
    "    # try:\n",
    "    #     job.run(\n",
    "    #         tensorboard=tensorboard_resource_name,\n",
    "    #         service_account=f'{service_account}',\n",
    "    #         restart_job_on_worker_restart=False,\n",
    "    #         enable_web_access=True,\n",
    "    #         sync=False,\n",
    "    #     )\n",
    "    # except Exception as e:\n",
    "    #     # may fail in multi-worker to find startup script\n",
    "    #     logging.info(e)\n",
    "    \n",
    "    job.run(\n",
    "        tensorboard=tensorboard_resource_name,\n",
    "        service_account=f'{service_account}',\n",
    "        restart_job_on_worker_restart=False,\n",
    "        enable_web_access=True,\n",
    "        sync=False,\n",
    "    )\n",
    "        \n",
    "    # wait for job to complete\n",
    "    job.wait()\n",
    "    \n",
    "    # ====================================================\n",
    "    # Save job details\n",
    "    # ====================================================\n",
    "    \n",
    "    train_job_dict = job.to_dict()\n",
    "    logging.info(f'train_job_dict: {train_job_dict}')\n",
    "    \n",
    "    # pkl dict to GCS\n",
    "    logging.info(f\"Write pickled dict to GCS...\")\n",
    "    TRAIN_DICT_LOCAL = f'train_job_dict.pkl'\n",
    "    TRAIN_DICT_GCS_OBJ = f'{experiment_name}/{experiment_run}/{TRAIN_DICT_LOCAL}' # destination folder prefix and blob name\n",
    "    \n",
    "    logging.info(f\"TRAIN_DICT_LOCAL: {TRAIN_DICT_LOCAL}\")\n",
    "    logging.info(f\"TRAIN_DICT_GCS_OBJ: {TRAIN_DICT_GCS_OBJ}\")\n",
    "\n",
    "    # pickle\n",
    "    filehandler = open(f'{TRAIN_DICT_LOCAL}', 'wb')\n",
    "    pkl.dump(train_job_dict, filehandler)\n",
    "    filehandler.close()\n",
    "    \n",
    "    # upload to GCS\n",
    "    bucket_client = storage_client.bucket(train_output_gcs_bucket)\n",
    "    blob = bucket_client.blob(TRAIN_DICT_GCS_OBJ)\n",
    "    blob.upload_from_filename(TRAIN_DICT_LOCAL)\n",
    "    \n",
    "    job_dict_uri = f'gs://{train_output_gcs_bucket}/{TRAIN_DICT_GCS_OBJ}'\n",
    "    logging.info(f\"{TRAIN_DICT_LOCAL} uploaded to {job_dict_uri}\")\n",
    "    \n",
    "    # ====================================================\n",
    "    # Model and index artifact uris\n",
    "    # ====================================================\n",
    "    \n",
    "    # \"gs://jt-tfrs-output-v2/pipe-dev-2tower-tfrs-jtv10/run-20221228-172834/model-dir/candidate_model\n",
    "    # \"gs://jt-tfrs-output-v2/pipe-dev-2tower-tfrs-jtv10/run-20221228-172834/model-dir/candidate_tower\"\n",
    "    \n",
    "    query_tower_dir_uri = f\"gs://{train_output_gcs_bucket}/{experiment_name}/{experiment_run}/model-dir/query_model\" \n",
    "    candidate_tower_dir_uri = f\"gs://{train_output_gcs_bucket}/{experiment_name}/{experiment_run}/model-dir/candidate_model\"\n",
    "    # candidate_index_dir_uri = f\"gs://{output_dir_gcs_bucket_name}/{experiment_name}/{experiment_run}/candidate_model\"\n",
    "    \n",
    "    logging.info(f'query_tower_dir_uri: {query_tower_dir_uri}')\n",
    "    logging.info(f'candidate_tower_dir_uri: {candidate_tower_dir_uri}')\n",
    "    # logging.info(f'candidate_index_dir_uri: {candidate_index_dir_uri}')\n",
    "    \n",
    "    return (\n",
    "        f'{job_dict_uri}',\n",
    "        f'{query_tower_dir_uri}',\n",
    "        f'{candidate_tower_dir_uri}',\n",
    "        # f'{candidate_index_dir_uri}',\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa3eea5-05b5-46a7-ab2e-6ef4b826fcae",
   "metadata": {},
   "source": [
    "# Prepare Job Specs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1cfa03-1db5-44ac-9e19-b855a5c63975",
   "metadata": {},
   "source": [
    "## Vertex Train: workerpool specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2286562e-ff21-46e2-bfa4-14e732ddadef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_worker_pool_specs(\n",
    "    image_uri,\n",
    "    args,\n",
    "    cmd,\n",
    "    replica_count=1,\n",
    "    machine_type=\"n1-standard-16\",\n",
    "    accelerator_count=1,\n",
    "    accelerator_type=\"ACCELERATOR_TYPE_UNSPECIFIED\",\n",
    "    reduction_server_count=0,\n",
    "    reduction_server_machine_type=\"n1-highcpu-16\",\n",
    "    reduction_server_image_uri=\"us-docker.pkg.dev/vertex-ai-restricted/training/reductionserver:latest\",\n",
    "):\n",
    "\n",
    "    if accelerator_count > 0:\n",
    "        machine_spec = {\n",
    "            \"machine_type\": machine_type,\n",
    "            \"accelerator_type\": accelerator_type,\n",
    "            \"accelerator_count\": accelerator_count,\n",
    "        }\n",
    "    else:\n",
    "        machine_spec = {\"machine_type\": machine_type}\n",
    "\n",
    "    container_spec = {\n",
    "        \"image_uri\": image_uri,\n",
    "        \"args\": args,\n",
    "        \"command\": cmd,\n",
    "    }\n",
    "\n",
    "    chief_spec = {\n",
    "        \"replica_count\": 1,\n",
    "        \"machine_spec\": machine_spec,\n",
    "        \"container_spec\": container_spec,\n",
    "    }\n",
    "\n",
    "    worker_pool_specs = [chief_spec]\n",
    "    if replica_count > 1:\n",
    "        workers_spec = {\n",
    "            \"replica_count\": replica_count - 1,\n",
    "            \"machine_spec\": machine_spec,\n",
    "            \"container_spec\": container_spec,\n",
    "        }\n",
    "        worker_pool_specs.append(workers_spec)\n",
    "    if reduction_server_count > 1:\n",
    "        workers_spec = {\n",
    "            \"replica_count\": reduction_server_count,\n",
    "            \"machine_spec\": {\n",
    "                \"machine_type\": reduction_server_machine_type,\n",
    "            },\n",
    "            \"container_spec\": {\"image_uri\": reduction_server_image_uri},\n",
    "        }\n",
    "        worker_pool_specs.append(workers_spec)\n",
    "\n",
    "    return worker_pool_specs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045adff3-74fc-4e86-ac19-8e2feee61766",
   "metadata": {},
   "source": [
    "## Accelerators and Device Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a8a47f70-c364-4122-a9c6-abda876a0068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Single machine, single GPU\n",
    "WORKER_MACHINE_TYPE = 'a2-highgpu-1g'\n",
    "REPLICA_COUNT = 1\n",
    "ACCELERATOR_TYPE = 'NVIDIA_TESLA_A100'\n",
    "PER_MACHINE_ACCELERATOR_COUNT = 1\n",
    "REDUCTION_SERVER_COUNT = 0                                                      \n",
    "REDUCTION_SERVER_MACHINE_TYPE = \"n1-highcpu-16\"\n",
    "DISTRIBUTE_STRATEGY = 'single'\n",
    "\n",
    "# # # Single Machine; multiple GPU\n",
    "# WORKER_MACHINE_TYPE = 'a2-highgpu-4g' # a2-ultragpu-4g\n",
    "# REPLICA_COUNT = 1\n",
    "# ACCELERATOR_TYPE = 'NVIDIA_TESLA_A100'\n",
    "# PER_MACHINE_ACCELERATOR_COUNT = 4\n",
    "# REDUCTION_SERVER_COUNT = 0                                                      \n",
    "# REDUCTION_SERVER_MACHINE_TYPE = \"n1-highcpu-16\"\n",
    "# DISTRIBUTE_STRATEGY = 'mirrored'\n",
    "\n",
    "# # # # Multiple Machine; 1 GPU per machine\n",
    "# WORKER_MACHINE_TYPE = 'a2-highgpu-2g' # a2-ultragpu-4g\n",
    "# REPLICA_COUNT = 2\n",
    "# ACCELERATOR_TYPE = 'NVIDIA_TESLA_A100'\n",
    "# PER_MACHINE_ACCELERATOR_COUNT = 2\n",
    "# REDUCTION_SERVER_COUNT = 4                                                      \n",
    "# REDUCTION_SERVER_MACHINE_TYPE = \"n1-highcpu-16\"\n",
    "# DISTRIBUTE_STRATEGY = 'multiworker'\n",
    "\n",
    "# # # Multiple Machines, 1 GPU per Machine\n",
    "# WORKER_MACHINE_TYPE = 'n1-standard-16'\n",
    "# REPLICA_COUNT = 9\n",
    "# ACCELERATOR_TYPE = 'NVIDIA_TESLA_T4'\n",
    "# PER_MACHINE_ACCELERATOR_COUNT = 1\n",
    "# REDUCTION_SERVER_COUNT = 10                                                      \n",
    "# REDUCTION_SERVER_MACHINE_TYPE = \"n1-highcpu-16\"\n",
    "# DISTRIBUTE_STRATEGY = 'multiworker'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83f96f0-3b01-4622-88e9-cb646b3f873a",
   "metadata": {},
   "source": [
    "## Vertex AI Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "28d8b91d-9772-49da-b7a4-4ab33a12c0c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPERIMENT_NAME: test-full-v14-data-jtv13\n",
      "RUN_NAME: run-20230113-112521\n"
     ]
    }
   ],
   "source": [
    "EXPERIMENT_PREFIX = 'test-full-v14-data'                                                  # custom identifier for organizing experiments\n",
    "# EXPERIMENT_NAME=f'{EXPERIMENT_PREFIX}-{MODEL_TYPE}-{FRAMEWORK}-{MODEL_VERSION}'\n",
    "EXPERIMENT_NAME=f'{EXPERIMENT_PREFIX}-{MODEL_VERSION}'\n",
    "RUN_NAME = f'run-{time.strftime(\"%Y%m%d-%H%M%S\")}'\n",
    "\n",
    "print(f\"EXPERIMENT_NAME: {EXPERIMENT_NAME}\")\n",
    "print(f\"RUN_NAME: {RUN_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f2fed1-9d86-41cf-add4-4e25f6d93030",
   "metadata": {},
   "source": [
    "## Managed Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e7a56de9-f390-4c3f-9aad-24c09145afff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TB_RESOURCE_NAME: projects/934903580331/locations/us-central1/tensorboards/3550569336693325824\n"
     ]
    }
   ],
   "source": [
    "# use existing TB instance\n",
    "# TB_RESOURCE_NAME = 'projects/934903580331/locations/us-central1/tensorboards/636071874714927104'\n",
    "\n",
    "# # create new TB instance\n",
    "TENSORBOARD_DISPLAY_NAME=f\"{EXPERIMENT_PREFIX}-v1\"\n",
    "tensorboard = vertex_ai.Tensorboard.create(display_name=TENSORBOARD_DISPLAY_NAME, project=PROJECT_ID, location=\"us-central1\")\n",
    "TB_RESOURCE_NAME = tensorboard.resource_name\n",
    "\n",
    "\n",
    "print(f\"TB_RESOURCE_NAME: {TB_RESOURCE_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "29a04819-3787-4dc1-af59-0d2ee6e3de6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test-full-v14-data-v1'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensorboard.display_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66142882-0f35-4e7d-ab5a-c0d6b49d3f44",
   "metadata": {},
   "source": [
    "## Training Config\n",
    "\n",
    "* see [src code](https://github.com/googleapis/python-aiplatform/blob/e7bf0d83d8bb0849a9bce886c958d13f5cbe5fab/google/cloud/aiplatform/utils/worker_spec_utils.py#L153) for worker_pool_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1d2ee483-5376-49d5-a743-303e7f90d587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_steps: 2003\n",
      "val_steps: 20\n"
     ]
    }
   ],
   "source": [
    "# train_samples = 60_733_427\n",
    "# valid_samples = 613_001\n",
    "\n",
    "train_samples = 8205265\n",
    "valid_samples = 82959\n",
    "\n",
    "batch_size = 4096\n",
    "\n",
    "train_steps = train_samples // batch_size\n",
    "val_steps = valid_samples // batch_size\n",
    "\n",
    "print(f\"train_steps: {train_steps}\")\n",
    "print(f\"val_steps: {val_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d8fa7a02-6d4d-43ba-bce4-5289f86db2ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PIPELINE_ROOT_PATH: gs://jt-tfrs-central-v2/test-full-v14-data-jtv13/run-20230113-112521/pipeline_root\n",
      "IMAGE_URI: gcr.io/hybrid-vertex/sp-2tower-tfrs-jtv13-v11-training\n",
      "[{'container_spec': {'args': ['--project=hybrid-vertex',\n",
      "                              '--train_output_gcs_bucket=jt-tfrs-central-v2',\n",
      "                              '--train_dir=spotify-data-regimes',\n",
      "                              '--train_dir_prefix=jtv14-8m/train_v14',\n",
      "                              '--valid_dir=spotify-data-regimes',\n",
      "                              '--valid_dir_prefix=jtv14-8m/valid_v14',\n",
      "                              '--candidate_file_dir=spotify-data-regimes',\n",
      "                              '--candidate_files_prefix=jtv14-8m/candidates',\n",
      "                              '--experiment_name=test-full-v14-data-jtv13',\n",
      "                              '--experiment_run=run-20230113-112521',\n",
      "                              '--num_epochs=10',\n",
      "                              '--batch_size=4096',\n",
      "                              '--embedding_dim=128',\n",
      "                              '--projection_dim=50',\n",
      "                              '--layer_sizes=[64,32]',\n",
      "                              '--learning_rate=0.01',\n",
      "                              '--valid_frequency=51',\n",
      "                              '--valid_steps=20',\n",
      "                              '--epoch_steps=2003',\n",
      "                              '--distribute=single',\n",
      "                              '--model_version=jtv13',\n",
      "                              '--pipeline_version=v11',\n",
      "                              '--seed=1234',\n",
      "                              '--max_tokens=20000',\n",
      "                              '--tb_resource_name=projects/934903580331/locations/us-central1/tensorboards/3550569336693325824',\n",
      "                              '--embed_frequency=0',\n",
      "                              '--hist_frequency=0',\n",
      "                              '--tf_gpu_thread_count=8',\n",
      "                              '--block_length=64',\n",
      "                              '--num_data_shards=4',\n",
      "                              '--chkpt_freq=epoch',\n",
      "                              '--profiler'],\n",
      "                     'command': ['python', 'two_tower_jt/task.py'],\n",
      "                     'image_uri': 'gcr.io/hybrid-vertex/sp-2tower-tfrs-jtv13-v11-training'},\n",
      "  'machine_spec': {'accelerator_count': 1,\n",
      "                   'accelerator_type': 'NVIDIA_TESLA_A100',\n",
      "                   'machine_type': 'a2-highgpu-1g'},\n",
      "  'replica_count': 1}]\n"
     ]
    }
   ],
   "source": [
    "train_sample_cnt = 8_205_265\n",
    "valid_samples_cnt = 82_959\n",
    "\n",
    "# ====================================================\n",
    "# Pipeline output repo \n",
    "# ====================================================\n",
    "OUTPUT_BUCKET = 'jt-tfrs-central-v2'\n",
    "OUTPUT_GCS_URI =f'gs://{OUTPUT_BUCKET}'\n",
    "\n",
    "# Stores pipeline executions for each run\n",
    "# PIPELINE_ROOT_PATH = f'gs://{OUTPUT_BUCKET}/{MODEL_ROOT_NAME}/pipeline_root'\n",
    "PIPELINE_ROOT_PATH = f'gs://{OUTPUT_BUCKET}/{EXPERIMENT_NAME}/{RUN_NAME}/pipeline_root'\n",
    "print('PIPELINE_ROOT_PATH: {}'.format(PIPELINE_ROOT_PATH))\n",
    "\n",
    "# =================================================\n",
    "# Data sources\n",
    "# =================================================\n",
    "CANDIDATE_FILE_DIR = 'spotify-data-regimes'\n",
    "# CANDIDATE_PREFIX = 'jtv10/candidates'\n",
    "CANDIDATE_PREFIX = 'jtv14-8m/candidates'\n",
    "\n",
    "TRAIN_DIR = 'spotify-data-regimes' \n",
    "# TRAIN_DIR_PREFIX = 'jtv10/train_v9' # full 65m\n",
    "# TRAIN_DIR_PREFIX = 'jtv10/valid_v9'  # test small data\n",
    "TRAIN_DIR_PREFIX = 'jtv14-8m/train_v14' # full 8m\n",
    "\n",
    "VALID_DIR = 'spotify-data-regimes' \n",
    "# VALID_DIR_PREFIX = 'jtv10/valid_v9'\n",
    "VALID_DIR_PREFIX = 'jtv14-8m/valid_v14' \n",
    "\n",
    "# =================================================\n",
    "# train image\n",
    "# =================================================\n",
    "# Existing image URI or name for image to create\n",
    "IMAGE_URI = f\"{IMAGE_URI}\"\n",
    "print(f\"IMAGE_URI: {IMAGE_URI}\")\n",
    "\n",
    "# =================================================\n",
    "# train job config\n",
    "# =================================================\n",
    "SEED = 1234\n",
    "TF_GPU_THREAD_COUNT='8' # '1' | '4' | '8'\n",
    "\n",
    "BLOCK_LENGTH = 64 # 1, 8, 16, 32, 64\n",
    "\n",
    "NUM_DATA_SHARDS=4 # 2, 4, 8, 16, 32, 64\n",
    "\n",
    "# training hparams\n",
    "NUM_EPOCHS = 10\n",
    "BATCH_SIZE = 4096  # 4096, 2048, 1024, 512 \n",
    "LEARNING_RATE = 0.01\n",
    "\n",
    "# regularization\n",
    "DROPOUT_RATE = 0.33\n",
    "\n",
    "# model size\n",
    "EMBEDDING_DIM = 128\n",
    "PROJECTION_DIM = 50\n",
    "LAYER_SIZES = '[64,32]'\n",
    "MAX_TOKENS = 20000 # vocab\n",
    "# MAX_PADDING = 375\n",
    "\n",
    "# validation & evaluation\n",
    "VALID_FREQUENCY = 51\n",
    "VALID_STEPS = valid_samples_cnt // BATCH_SIZE # 100\n",
    "EPOCH_STEPS = train_sample_cnt // BATCH_SIZE\n",
    "\n",
    "# tensorboard\n",
    "EMBED_FREQUENCY=0\n",
    "HISTOGRAM_FREQUENCY=0\n",
    "CHECKPOINT_FREQ='epoch'\n",
    "\n",
    "WORKER_CMD = [\"python\", \"two_tower_jt/task.py\"]\n",
    "# WORKER_CMD [\"python\", \"-m\", \"trainer.task\"]\n",
    "\n",
    "WORKER_ARGS = [\n",
    "    f'--project={PROJECT_ID}',\n",
    "    f'--train_output_gcs_bucket={OUTPUT_BUCKET}',\n",
    "    f'--train_dir={TRAIN_DIR}',\n",
    "    f'--train_dir_prefix={TRAIN_DIR_PREFIX}',\n",
    "    f'--valid_dir={VALID_DIR}',\n",
    "    f'--valid_dir_prefix={VALID_DIR_PREFIX}',\n",
    "    f'--candidate_file_dir={CANDIDATE_FILE_DIR}',\n",
    "    f'--candidate_files_prefix={CANDIDATE_PREFIX}',\n",
    "    f'--experiment_name={EXPERIMENT_NAME}',\n",
    "    f'--experiment_run={RUN_NAME}',\n",
    "    f'--num_epochs={NUM_EPOCHS}',\n",
    "    f'--batch_size={BATCH_SIZE}',\n",
    "    f'--embedding_dim={EMBEDDING_DIM}',\n",
    "    f'--projection_dim={PROJECTION_DIM}',\n",
    "    f'--layer_sizes={LAYER_SIZES}',\n",
    "    f'--learning_rate={LEARNING_RATE}',\n",
    "    f'--valid_frequency={VALID_FREQUENCY}',\n",
    "    f'--valid_steps={VALID_STEPS}',\n",
    "    f'--epoch_steps={EPOCH_STEPS}',\n",
    "    f'--distribute={DISTRIBUTE_STRATEGY}',\n",
    "    f'--model_version={MODEL_VERSION}',\n",
    "    f'--pipeline_version={PIPELINE_VERSION}',\n",
    "    f'--seed={SEED}',\n",
    "    f'--max_tokens={MAX_TOKENS}',\n",
    "    f'--tb_resource_name={TB_RESOURCE_NAME}',\n",
    "    f'--embed_frequency={EMBED_FREQUENCY}',\n",
    "    f'--hist_frequency={HISTOGRAM_FREQUENCY}',\n",
    "    f'--tf_gpu_thread_count={TF_GPU_THREAD_COUNT}',\n",
    "    f'--block_length={BLOCK_LENGTH}',\n",
    "    f'--num_data_shards={NUM_DATA_SHARDS}',\n",
    "    f'--chkpt_freq={CHECKPOINT_FREQ}',\n",
    "    # f'--cache_train', # uncomment to cache train_dataset\n",
    "    # f'--evaluate_model',  # uncomment to run model.eval()\n",
    "    # f'--write_embeddings', # uncomment to write embeddings index in train job\n",
    "    f'--profiler',\n",
    "    # f'--set_jit',\n",
    "]\n",
    "\n",
    "WORKER_POOL_SPECS = prepare_worker_pool_specs(\n",
    "    image_uri=IMAGE_URI,\n",
    "    args=WORKER_ARGS,\n",
    "    cmd=WORKER_CMD,\n",
    "    replica_count=REPLICA_COUNT,\n",
    "    machine_type=WORKER_MACHINE_TYPE,\n",
    "    accelerator_count=PER_MACHINE_ACCELERATOR_COUNT,\n",
    "    accelerator_type=ACCELERATOR_TYPE,\n",
    "    reduction_server_count=REDUCTION_SERVER_COUNT,\n",
    "    reduction_server_machine_type=REDUCTION_SERVER_MACHINE_TYPE,\n",
    ")\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(WORKER_POOL_SPECS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e4d826dc-0dfc-44de-99f1-79d68ecd559d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/jw-repo/spotify_mpd_two_tower\n",
      "gs://jt-tfrs-central-v2/test-full-v14-data-jtv13/run-20230113-112521/pipeline_root\n",
      "src\n"
     ]
    }
   ],
   "source": [
    "!export PWD=pwd\n",
    "!export PIPELINE_ROOT_PATH=PIPELINE_ROOT_PATH\n",
    "!export REPO_DOCKER_PATH_PREFIX=REPO_DOCKER_PATH_PREFIX\n",
    "\n",
    "! echo $PWD\n",
    "! echo $PIPELINE_ROOT_PATH\n",
    "! echo $REPO_DOCKER_PATH_PREFIX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caff7b07-e1ad-4dcf-845a-7d2661cc63cd",
   "metadata": {},
   "source": [
    "### copy train package to GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "913e0f5d-abc0-4d78-915e-7a28489e3d23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://src/cloudbuild.yaml [Content-Type=application/octet-stream]...\n",
      "/ [1 files][  178.0 B/  178.0 B]                                                \n",
      "Operation completed over 1 objects/178.0 B.                                      \n",
      "Copying file://src/Dockerfile.tfrs [Content-Type=application/octet-stream]...\n",
      "/ [1 files][  387.0 B/  387.0 B]                                                \n",
      "Operation completed over 1 objects/387.0 B.                                      \n",
      "Copying file://src/two_tower_jt/__init__.py [Content-Type=text/x-python]...\n",
      "Copying file://src/two_tower_jt/__pycache__/train_config.cpython-37.pyc [Content-Type=application/x-python-code]...\n",
      "Copying file://src/two_tower_jt/__pycache__/two_tower_lite.cpython-37.pyc [Content-Type=application/x-python-code]...\n",
      "Copying file://src/two_tower_jt/__pycache__/__init__.cpython-37.pyc [Content-Type=application/x-python-code]...\n",
      "Copying file://src/two_tower_jt/__pycache__/two_tower.cpython-37.pyc [Content-Type=application/x-python-code]...\n",
      "Copying file://src/two_tower_jt/data-pipeline.py [Content-Type=text/x-python]...\n",
      "Copying file://src/two_tower_jt/interactive_train.py [Content-Type=text/x-python]...\n",
      "Copying file://src/two_tower_jt/requirements.txt [Content-Type=text/plain]...   \n",
      "Copying file://src/two_tower_jt/task.py [Content-Type=text/x-python]...         \n",
      "Copying file://src/two_tower_jt/two_tower_lite.py [Content-Type=text/x-python]...\n",
      "Copying file://src/two_tower_jt/train_config.py [Content-Type=text/x-python]...\n",
      "Copying file://src/two_tower_jt/two_tower.py [Content-Type=text/x-python]...    \n",
      "/ [12/12 files][162.2 KiB/162.2 KiB] 100% Done                                  \n",
      "Operation completed over 12 objects/162.2 KiB.                                   \n",
      "\n",
      " Copied training package and Dockerfile to gs://jt-tfrs-central-v2/test-full-v14-data-jtv13/run-20230113-112521/pipeline_root\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# copy training Dockerfile\n",
    "!gsutil cp $REPO_DOCKER_PATH_PREFIX/cloudbuild.yaml $PIPELINE_ROOT_PATH/cloudbuild.yaml\n",
    "!gsutil cp $REPO_DOCKER_PATH_PREFIX/Dockerfile.tfrs $PIPELINE_ROOT_PATH/Dockerfile.tfrs\n",
    "\n",
    "# # # copy training application code\n",
    "! gsutil -m cp -r $REPO_DOCKER_PATH_PREFIX/two_tower_jt/* $PIPELINE_ROOT_PATH/trainer\n",
    "\n",
    "print(f\"\\n Copied training package and Dockerfile to {PIPELINE_ROOT_PATH}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f94fccbf-ca7f-4326-b22d-e119fcee168c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://jt-tfrs-central-v2/test-full-v14-data-jtv13/run-20230113-112521/pipeline_root/trainer/:\n",
      "         0  2023-01-13T11:32:28Z  gs://jt-tfrs-central-v2/test-full-v14-data-jtv13/run-20230113-112521/pipeline_root/trainer/__init__.py\n",
      "       835  2023-01-13T11:32:28Z  gs://jt-tfrs-central-v2/test-full-v14-data-jtv13/run-20230113-112521/pipeline_root/trainer/data-pipeline.py\n",
      "        44  2023-01-13T11:32:28Z  gs://jt-tfrs-central-v2/test-full-v14-data-jtv13/run-20230113-112521/pipeline_root/trainer/interactive_train.py\n",
      "       219  2023-01-13T11:32:28Z  gs://jt-tfrs-central-v2/test-full-v14-data-jtv13/run-20230113-112521/pipeline_root/trainer/requirements.txt\n",
      "     24848  2023-01-13T11:32:28Z  gs://jt-tfrs-central-v2/test-full-v14-data-jtv13/run-20230113-112521/pipeline_root/trainer/task.py\n",
      "       277  2023-01-13T11:32:28Z  gs://jt-tfrs-central-v2/test-full-v14-data-jtv13/run-20230113-112521/pipeline_root/trainer/train_config.py\n",
      "     56895  2023-01-13T11:32:28Z  gs://jt-tfrs-central-v2/test-full-v14-data-jtv13/run-20230113-112521/pipeline_root/trainer/two_tower.py\n",
      "     44861  2023-01-13T11:32:28Z  gs://jt-tfrs-central-v2/test-full-v14-data-jtv13/run-20230113-112521/pipeline_root/trainer/two_tower_lite.py\n",
      "\n",
      "gs://jt-tfrs-central-v2/test-full-v14-data-jtv13/run-20230113-112521/pipeline_root/trainer/__pycache__/:\n",
      "       159  2023-01-13T11:32:28Z  gs://jt-tfrs-central-v2/test-full-v14-data-jtv13/run-20230113-112521/pipeline_root/trainer/__pycache__/__init__.cpython-37.pyc\n",
      "       425  2023-01-13T11:32:28Z  gs://jt-tfrs-central-v2/test-full-v14-data-jtv13/run-20230113-112521/pipeline_root/trainer/__pycache__/train_config.cpython-37.pyc\n",
      "     20015  2023-01-13T11:32:28Z  gs://jt-tfrs-central-v2/test-full-v14-data-jtv13/run-20230113-112521/pipeline_root/trainer/__pycache__/two_tower.cpython-37.pyc\n",
      "     17523  2023-01-13T11:32:28Z  gs://jt-tfrs-central-v2/test-full-v14-data-jtv13/run-20230113-112521/pipeline_root/trainer/__pycache__/two_tower_lite.cpython-37.pyc\n",
      "TOTAL: 12 objects, 166101 bytes (162.21 KiB)\n"
     ]
    }
   ],
   "source": [
    "! gsutil ls -Rl $PIPELINE_ROOT_PATH/trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d64bb5-3128-44f6-b652-468a7a325c15",
   "metadata": {},
   "source": [
    "# Build & Submit Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f1eb197b-ea56-4b83-aa92-8198a75711ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PIPELINE_TAG: 2tower-v11\n",
      "PIPELINE_NAME: trainer-jtv13-2tower-v11\n"
     ]
    }
   ],
   "source": [
    "PIPELINE_TAG = f'2tower-{PIPELINE_VERSION}'\n",
    "print(\"PIPELINE_TAG:\", PIPELINE_TAG)\n",
    "\n",
    "PIPELINE_NAME = f'trainer-{MODEL_VERSION}-{PIPELINE_TAG}'.replace('_', '-')\n",
    "print(\"PIPELINE_NAME:\", PIPELINE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89d7d94-7e15-42d6-b38c-f3eab2affb1a",
   "metadata": {},
   "source": [
    "## Create pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5e6f5b61-ab53-4619-b854-8c5a4da210e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.train_pipes import build_custom_image, train_custom_model\n",
    "\n",
    "@kfp.v2.dsl.pipeline(\n",
    "    name=f'{PIPELINE_NAME}'.replace('_', '-')\n",
    ")\n",
    "def pipeline(\n",
    "    project: str,\n",
    "    project_number: str,\n",
    "    location: str,\n",
    "    service_account: str,\n",
    "    model_version: str,\n",
    "    pipeline_version: str,\n",
    "    train_image_uri: str,\n",
    "    train_output_gcs_bucket: str,\n",
    "    gcs_train_script_path: str,\n",
    "    model_display_name: str,\n",
    "    train_dockerfile_name: str,\n",
    "    train_dir: str,\n",
    "    train_dir_prefix: str,\n",
    "    valid_dir: str,\n",
    "    valid_dir_prefix: str,\n",
    "    candidate_file_dir: str,\n",
    "    candidate_files_prefix: str,\n",
    "    tensorboard_resource_name: str,\n",
    "    experiment_name: str,\n",
    "    experiment_run: str,\n",
    "    register_model: bool,\n",
    "):\n",
    "    \n",
    "    from kfp.v2.components import importer_node\n",
    "    from google_cloud_pipeline_components.types import artifact_types\n",
    "    \n",
    "    # ========================================================================\n",
    "    # Build Custom Train Image\n",
    "    # ========================================================================\n",
    "    \n",
    "    # build_custom_train_image_op = (\n",
    "    #     build_custom_train_image.build_custom_train_image(\n",
    "    #         project=project,\n",
    "    #         gcs_train_script_path=gcs_train_script_path,\n",
    "    #         training_image_uri=train_image_uri,\n",
    "    #         train_dockerfile_name=train_dockerfile_name,\n",
    "    #     )\n",
    "    #     .set_display_name(\"Build custom train image\")\n",
    "    #     .set_caching_options(False)\n",
    "    # )\n",
    "\n",
    "\n",
    "    run_train_task_op = (\n",
    "        train_custom_model.train_custom_model(\n",
    "            project=project,\n",
    "            model_version=model_version,\n",
    "            pipeline_version=pipeline_version,\n",
    "            model_name=model_display_name,\n",
    "            worker_pool_specs=WORKER_POOL_SPECS, \n",
    "            train_output_gcs_bucket=train_output_gcs_bucket,\n",
    "            # vocab_dict_uri=build_vocabs_string_lookups_op.outputs['vocab_gcs_uri'],\n",
    "            experiment_name=experiment_name,\n",
    "            experiment_run=experiment_run,\n",
    "            training_image_uri=train_image_uri,                                                                       # build_custom_train_image_op.outputs['training_image_uri'],\n",
    "            tensorboard_resource_name=tensorboard_resource_name, # create_tensorboard_op.outputs['tensorboard_resource_name'],\n",
    "            service_account=service_account,\n",
    "        )\n",
    "        .set_display_name(\"2Tower Training\")\n",
    "        .set_caching_options(True)\n",
    "        # .after(build_custom_train_image_op)\n",
    "    )\n",
    "    \n",
    "    # ========================================================================\n",
    "    # Import trained Query and Candidate Towers to this DAG (metadata)\n",
    "    # ========================================================================\n",
    "    \n",
    "    import_unmanaged_query_model_task = (\n",
    "        importer_node.importer(\n",
    "            artifact_uri=run_train_task_op.outputs['query_tower_dir_uri'],\n",
    "            artifact_class=artifact_types.UnmanagedContainerModel,\n",
    "            metadata={\n",
    "                'containerSpec': {\n",
    "                    'imageUri': 'us-docker.pkg.dev/vertex-ai/prediction/tf2-gpu.2-10:latest',\n",
    "                },\n",
    "            },\n",
    "        )\n",
    "        .set_display_name(\"Import Query Tower\")\n",
    "        .after(run_train_task_op)\n",
    "        .set_caching_options(True)\n",
    "    )\n",
    "    \n",
    "    import_unmanaged_candidate_model_task = (\n",
    "        importer_node.importer(\n",
    "            artifact_uri=run_train_task_op.outputs['candidate_tower_dir_uri'],\n",
    "            artifact_class=artifact_types.UnmanagedContainerModel,\n",
    "            metadata={\n",
    "                'containerSpec': {\n",
    "                    'imageUri': 'us-docker.pkg.dev/vertex-ai/prediction/tf2-gpu.2-10:latest',\n",
    "                },\n",
    "            },\n",
    "        )\n",
    "        .set_display_name(\"Import Candidate Tower\")\n",
    "        .after(run_train_task_op)\n",
    "        .set_caching_options(True)\n",
    "    )\n",
    "    \n",
    "    # ========================================================================\n",
    "    # Conditional: Upload models to Vertex model registry\n",
    "    # ========================================================================\n",
    "    with kfp.v2.dsl.Condition(register_model == \"True\", name=\"Create New Endpoint\"):\n",
    "        \n",
    "        query_model_upload_op = (\n",
    "            gcc_aip.ModelUploadOp(\n",
    "                project=project,\n",
    "                location=location,\n",
    "                display_name=f'query-tower-{model_display_name}',\n",
    "                unmanaged_container_model=import_unmanaged_query_model_task.outputs[\"artifact\"],\n",
    "                labels={\"tower\": \"query\"},\n",
    "            )\n",
    "            .set_display_name(\"Upload Query Tower\")\n",
    "            .set_caching_options(True)\n",
    "        )\n",
    "        \n",
    "        candidate_model_upload_op = (\n",
    "            gcc_aip.ModelUploadOp(\n",
    "                project=project,\n",
    "                location=location,\n",
    "                display_name=f'candidate-tower-{model_display_name}',\n",
    "                unmanaged_container_model=import_unmanaged_candidate_model_task.outputs[\"artifact\"],\n",
    "                labels={\"tower\": \"candidate\"},\n",
    "            )\n",
    "            .set_display_name(\"Upload Query Tower to Vertex\")\n",
    "            .set_caching_options(True)\n",
    "        )\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "65c65526-e42c-459c-a3a2-db8c84b6cc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! rm -f custom_container_pipeline_spec.json\n",
    "\n",
    "PIPELINE_JSON_SPEC_LOCAL = \"custom_pipeline_spec.json\"\n",
    "\n",
    "! rm -f $PIPELINE_JSON_SPEC_LOCAL\n",
    "\n",
    "kfp.v2.compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline, package_path=PIPELINE_JSON_SPEC_LOCAL,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8d598e-9456-452f-bb41-6c80feb288a7",
   "metadata": {},
   "source": [
    "### save pipeline spec json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b3c4df38-23c9-449c-bbd2-bb6432d6e6a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PIPELINES_FILEPATH: gs://jt-tfrs-central-v2/test-full-v14-data-jtv13/run-20230113-112521/pipeline_root/pipeline_spec.json\n",
      "Copying file://custom_pipeline_spec.json [Content-Type=application/json]...\n",
      "/ [1 files][ 36.6 KiB/ 36.6 KiB]                                                \n",
      "Operation completed over 1 objects/36.6 KiB.                                     \n"
     ]
    }
   ],
   "source": [
    "# !gsutil cp custom_container_pipeline_spec.json $PIPELINE_ROOT_PATH/pipeline_spec.json\n",
    "\n",
    "PIPELINES_FILEPATH = f'{PIPELINE_ROOT_PATH}/pipeline_spec.json'\n",
    "print(\"PIPELINES_FILEPATH:\", PIPELINES_FILEPATH)\n",
    "\n",
    "!gsutil cp $PIPELINE_JSON_SPEC_LOCAL $PIPELINES_FILEPATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c6dba00a-d42b-40f5-8135-777caaae79a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://jt-tfrs-central-v2/test-full-v14-data-jtv13/run-20230113-112521/pipeline_root/Dockerfile.tfrs\n",
      "gs://jt-tfrs-central-v2/test-full-v14-data-jtv13/run-20230113-112521/pipeline_root/cloudbuild.yaml\n",
      "gs://jt-tfrs-central-v2/test-full-v14-data-jtv13/run-20230113-112521/pipeline_root/pipeline_spec.json\n",
      "gs://jt-tfrs-central-v2/test-full-v14-data-jtv13/run-20230113-112521/pipeline_root/trainer/\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls $PIPELINE_ROOT_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa01119-431a-49ce-bd7d-0acc1ca37e6c",
   "metadata": {},
   "source": [
    "## Submit pipeline to Vertex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9e110b33-7f68-4575-a1a6-e0515727505e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'trainer-jtv13-2tower-v11'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PIPELINE_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "db63e2ee-ab3e-4e08-bc95-91e9b194d469",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_NUMBER='934903580331'\n",
    "vpc_network_name = 'ucaip-haystack-vpc-network'\n",
    "# SERVICE_ACCOUNT = '934903580331-compute@developer.gserviceaccount.com'\n",
    "SERVICE_ACCOUNT = 'notebooksa@hybrid-vertex.iam.gserviceaccount.com'\n",
    "\n",
    "TRAIN_APP_CODE_PATH = f'{PIPELINE_ROOT_PATH}/trainer'\n",
    "\n",
    "job = vertex_ai.PipelineJob(\n",
    "    display_name=PIPELINE_NAME,\n",
    "    template_path=PIPELINES_FILEPATH,\n",
    "    pipeline_root=f'{PIPELINE_ROOT_PATH}',\n",
    "    failure_policy='fast', # slow | fast\n",
    "    # enable_caching=False,\n",
    "    parameter_values={\n",
    "        'project': PROJECT_ID,\n",
    "        'project_number': PROJECT_NUMBER,\n",
    "        'location': LOCATION,\n",
    "        'model_version': MODEL_VERSION,\n",
    "        'pipeline_version': PIPELINE_VERSION,\n",
    "        'model_display_name': MODEL_ROOT_NAME,\n",
    "        # 'pipeline_tag': PIPELINE_TAG,\n",
    "        'gcs_train_script_path': TRAIN_APP_CODE_PATH,\n",
    "        'train_image_uri': f\"{IMAGE_URI}\",\n",
    "        'train_output_gcs_bucket': OUTPUT_BUCKET,\n",
    "        'train_dir': TRAIN_DIR,\n",
    "        'train_dir_prefix': TRAIN_DIR_PREFIX,\n",
    "        'valid_dir': VALID_DIR,\n",
    "        'valid_dir_prefix': VALID_DIR_PREFIX,\n",
    "        'candidate_file_dir': CANDIDATE_FILE_DIR,\n",
    "        'candidate_files_prefix': CANDIDATE_PREFIX,\n",
    "        'tensorboard_resource_name': TB_RESOURCE_NAME,\n",
    "        'train_dockerfile_name': DOCKERNAME,\n",
    "        'experiment_name': EXPERIMENT_NAME,\n",
    "        'experiment_run': RUN_NAME,\n",
    "        'service_account': SERVICE_ACCOUNT,\n",
    "        'register_model': False,\n",
    "    },\n",
    ")\n",
    "\n",
    "job.run(\n",
    "    sync=False,\n",
    "    service_account=SERVICE_ACCOUNT,\n",
    "    network=f'projects/{PROJECT_NUMBER}/global/networks/{vpc_network_name}'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb75704-0393-48c1-8281-de18bd55fd71",
   "metadata": {},
   "source": [
    "#### clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "deea05b0-4a31-4b8c-941b-e0dd79979f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! rm -rf custom_pipeline_spec.json"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m97",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m97"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
