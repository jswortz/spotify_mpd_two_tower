{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "152d43dc-e7c4-4085-b214-b576e1b4a94e",
   "metadata": {},
   "source": [
    "# Training pipeline for TFRS  2tower model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58440717-f517-42f4-bfb1-d22676d04044",
   "metadata": {},
   "source": [
    "```\n",
    "tensorflow==2.10.1\n",
    "tensorflow-cloud==0.1.16\n",
    "tensorflow-datasets==4.6.0\n",
    "tensorflow-estimator==2.10.0\n",
    "tensorflow-hub==0.12.0\n",
    "tensorflow-io==0.27.0\n",
    "tensorflow-io-gcs-filesystem==0.27.0\n",
    "tensorflow-metadata==1.8.0\n",
    "tensorflow-probability==0.18.0\n",
    "tensorflow-recommenders==0.7.2\n",
    "tensorflow-serving-api==2.8.3\n",
    "tensorflow-transform==1.8.0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6a2a17-2a69-447f-a050-4e5ec3d261fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install kfp\n",
    "# !pip install google-cloud-pipeline-components "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43201bb1-8b40-4ae4-a415-b15eee0323cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! python3 -c \"import kfp; print('KFP SDK version: {}'.format(kfp.__version__))\"\n",
    "# ! python3 -c \"import google_cloud_pipeline_components; print('google_cloud_pipeline_components version: {}'.format(google_cloud_pipeline_components.__version__))\"\n",
    "# ! python3 -c \"import google.cloud.aiplatform; print('aiplatform SDK version: {}'.format(google.cloud.aiplatform.__version__))\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e3d319c-84e1-4cd2-8882-0f390bc21e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ID: hybrid-vertex\n",
      "PROJECT_NUM: 934903580331\n",
      "LOCATION: us-central1\n"
     ]
    }
   ],
   "source": [
    "GCP_PROJECTS = !gcloud config get-value project\n",
    "PROJECT_ID = GCP_PROJECTS[0]\n",
    "PROJECT_NUM = !gcloud projects list --filter=\"$PROJECT_ID\" --format=\"value(PROJECT_NUMBER)\"\n",
    "PROJECT_NUM = PROJECT_NUM[0]\n",
    "LOCATION = 'us-central1'\n",
    "\n",
    "print(f\"PROJECT_ID: {PROJECT_ID}\")\n",
    "print(f\"PROJECT_NUM: {PROJECT_NUM}\")\n",
    "print(f\"LOCATION: {LOCATION}\")\n",
    "\n",
    "VERTEX_SA = '934903580331-compute@developer.gserviceaccount.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68a4fb97-bed6-4533-92d3-dbc79e134f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "from time import time\n",
    "import pandas as pd\n",
    "# disable INFO and DEBUG logging everywhere\n",
    "import logging\n",
    "import time\n",
    "from pprint import pprint\n",
    "\n",
    "logging.disable(logging.WARNING)\n",
    "\n",
    "from google.cloud import aiplatform as vertex_ai\n",
    "from google.cloud import storage\n",
    "\n",
    "# Pipelines\n",
    "from typing import Any, Callable, Dict, NamedTuple, Optional, List\n",
    "from google_cloud_pipeline_components import aiplatform as gcc_aip\n",
    "from google_cloud_pipeline_components.types import artifact_types\n",
    "\n",
    "# Kubeflow SDK\n",
    "# TODO: fix these\n",
    "from kfp.v2 import dsl\n",
    "import kfp\n",
    "import kfp.v2.dsl\n",
    "from kfp.v2.google import client as pipelines_client\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output,\n",
    "                        OutputPath, component)\n",
    "\n",
    "storage_client = storage.Client(project=PROJECT_ID)\n",
    "\n",
    "vertex_ai.init(project=PROJECT_ID,location=LOCATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd32b85b-b44a-4e17-9c23-98fad8efe064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL_ROOT_NAME: sp-2tower-tfrs-jtv10-v0\n"
     ]
    }
   ],
   "source": [
    "# PREFIX = 'spotify-2tower'\n",
    "APP='sp'\n",
    "MODEL_TYPE='2tower'\n",
    "FRAMEWORK = 'tfrs'\n",
    "MODEL_VERSION = 'jtv10'\n",
    "PIPELINE_VERSION = 'v0'\n",
    "MODEL_ROOT_NAME = f'{APP}-{MODEL_TYPE}-{FRAMEWORK}-{MODEL_VERSION}-{PIPELINE_VERSION}'\n",
    "\n",
    "print(f\"MODEL_ROOT_NAME: {MODEL_ROOT_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e816a896-4015-4d0b-8a39-04d9a5a5b924",
   "metadata": {},
   "source": [
    "## Write Train files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "022ba9e5-95fb-47b2-8718-76687ad0dcc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Make the training subfolder\n",
    "# ! rm -rf {REPO_DOCKER_PATH_PREFIX}/trainer\n",
    "# ! mkdir {REPO_DOCKER_PATH_PREFIX}/trainer\n",
    "# ! touch {REPO_DOCKER_PATH_PREFIX}/trainer/__init__.py\n",
    "\n",
    "REPO_DOCKER_PATH_PREFIX = 'src'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c02857dd-731b-40ba-ac58-74286774fbf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMAGE_URI: gcr.io/hybrid-vertex/sp-2tower-tfrs-jtv10-v0-training\n"
     ]
    }
   ],
   "source": [
    "# Docker definitions for training\n",
    "IMAGE_NAME = f'{MODEL_ROOT_NAME}-training'\n",
    "IMAGE_URI = f'gcr.io/{PROJECT_ID}/{IMAGE_NAME}'\n",
    "\n",
    "DOCKERNAME = 'tfrs'\n",
    "REPO_DOCKER_PATH_PREFIX = 'src'\n",
    "MACHINE_TYPE ='e2-highcpu-32'\n",
    "FILE_LOCATION = './src'\n",
    "\n",
    "print(f\"IMAGE_URI: {IMAGE_URI}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46e4a95f-40f2-47b4-af3f-2d49cccd82d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/cloudbuild.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/cloudbuild.yaml\n",
    "\n",
    "steps:\n",
    "- name: 'gcr.io/cloud-builders/docker'\n",
    "  args: ['build', '-t', '$_IMAGE_URI', '$_FILE_LOCATION', '-f', '$_FILE_LOCATION/Dockerfile.$_DOCKERNAME']\n",
    "images:\n",
    "- '$_IMAGE_URI'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad807fb4-b7a4-4ce0-967c-deb73f32536b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/two_tower_jt/train_config.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/two_tower_jt/train_config.py\n",
    "\n",
    "PROJECT_ID = 'hybrid-vertex'\n",
    "NEW_ADAPTS = 'True'\n",
    "USE_CROSS_LAYER = False\n",
    "USE_DROPOUT = 'False'\n",
    "SEED = 1234\n",
    "MAX_PLAYLIST_LENGTH = 5         # this should improve performance vs 375\n",
    "EMBEDDING_DIM = 128   \n",
    "PROJECTION_DIM = 50  \n",
    "SEED = 1234\n",
    "DROPOUT_RATE = 0.33\n",
    "MAX_TOKENS = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d80ee63-a858-4ffe-9f6d-39e17afb486c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/two_tower_jt/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/two_tower_jt/requirements.txt\n",
    "google-cloud-aiplatform==1.18.0\n",
    "tensorflow-recommenders==0.7.2\n",
    "tensorboard==2.10.1\n",
    "tensorboard-data-server==0.6.1\n",
    "tensorboard-plugin-profile==2.8.0\n",
    "scann\n",
    "google-cloud-aiplatform[cloud_profiler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b81e499-9dd2-42c6-a2af-91f6849a5b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/Dockerfile.tfrs\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/Dockerfile.{DOCKERNAME}\n",
    "\n",
    "FROM tensorflow/tensorflow:2.10.1-gpu\n",
    "\n",
    "WORKDIR /src\n",
    "\n",
    "# Copies the trainer code to the docker image.\n",
    "COPY two_tower_jt/* two_tower_jt/ \n",
    "\n",
    "RUN pip install -r two_tower_jt/requirements.txt\n",
    "\n",
    "# # Sets up the entry point to invoke the trainer.\n",
    "# # ENTRYPOINT [\"python\", \"-m\", \"two_tower_jt.task\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85623af4-764f-4316-bde2-d4ae8cbbdf60",
   "metadata": {},
   "source": [
    "## Build Custom Train Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c76bfb28-6fd5-4d96-8d17-ba5e8212a43b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOCKERNAME: tfrs\n",
      "IMAGE_URI: gcr.io/hybrid-vertex/sp-2tower-tfrs-jtv10-v0-training\n",
      "FILE_LOCATION: ./src\n",
      "MACHINE_TYPE: e2-highcpu-32\n"
     ]
    }
   ],
   "source": [
    "print(f\"DOCKERNAME: {DOCKERNAME}\")\n",
    "print(f\"IMAGE_URI: {IMAGE_URI}\")\n",
    "print(f\"FILE_LOCATION: {FILE_LOCATION}\")\n",
    "print(f\"MACHINE_TYPE: {MACHINE_TYPE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b85202c-6495-4ff4-82d0-f0ffcc709725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/jw-repo/spotify_mpd_two_tower\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "351cbc55-3efc-48c1-ac1b-5917d10faaf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m/home/jupyter/jw-repo/spotify_mpd_two_tower/src\u001b[00m\n",
      "├── Dockerfile.tfrs\n",
      "├── cloudbuild.yaml\n",
      "├── \u001b[01;34mtwo_tower_jt\u001b[00m\n",
      "│   ├── __init__.py\n",
      "│   ├── \u001b[01;34m__pycache__\u001b[00m\n",
      "│   │   ├── __init__.cpython-37.pyc\n",
      "│   │   └── two_tower.cpython-37.pyc\n",
      "│   ├── interactive_train.py\n",
      "│   ├── requirements.txt\n",
      "│   ├── task.py\n",
      "│   ├── train_config.py\n",
      "│   └── two_tower.py\n",
      "└── \u001b[01;34mvocab_pipes\u001b[00m\n",
      "    ├── \u001b[01;34m__pycache__\u001b[00m\n",
      "    │   ├── adapt_fixed_text_layer_vocab.cpython-37.pyc\n",
      "    │   ├── adapt_ragged_text_layer_vocab.cpython-37.pyc\n",
      "    │   ├── config.cpython-37.pyc\n",
      "    │   └── create_master_vocab.cpython-37.pyc\n",
      "    ├── adapt_fixed_text_layer_vocab.py\n",
      "    ├── adapt_ragged_text_layer_vocab.py\n",
      "    ├── config.py\n",
      "    └── create_master_vocab.py\n",
      "\n",
      "4 directories, 18 files\n"
     ]
    }
   ],
   "source": [
    "!tree /home/jupyter/jw-repo/spotify_mpd_two_tower/src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "94546ac1-8b61-4fff-981a-9dae7f976fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 1171 file(s) totalling 47.6 GiB before compression.\n",
      "^C\n",
      "\n",
      "\n",
      "Command killed by keyboard interrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! gcloud builds submit --config src/cloudbuild.yaml \\\n",
    "    --substitutions _DOCKERNAME=$DOCKERNAME,_IMAGE_URI=$IMAGE_URI,_FILE_LOCATION=$FILE_LOCATION \\\n",
    "    --timeout=2h \\\n",
    "    --machine-type=$MACHINE_TYPE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c889eb1-b663-4491-8bbf-4247d87b1762",
   "metadata": {},
   "source": [
    "# Pipeline Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3868029b-b1ce-45a9-aade-ebdfec8f110f",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1292ae-b51d-42bc-83c5-996e827c2633",
   "metadata": {},
   "outputs": [],
   "source": [
    "REPO_DOCKER_PATH_PREFIX = 'src'\n",
    "PIPELINES_SUB_DIR = 'train_pipes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6893f359-64de-458e-a964-d42ce581fd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -rf {REPO_DOCKER_PATH_PREFIX}/{PIPELINES_SUB_DIR}\n",
    "! mkdir {REPO_DOCKER_PATH_PREFIX}/{PIPELINES_SUB_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3fedd1-0897-4e32-bbd5-1dd6bbc12158",
   "metadata": {},
   "source": [
    "## Build Custom Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335711ed-4425-409f-aebc-5fbdcbfedea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/{PIPELINES_SUB_DIR}/build_custom_image.py\n",
    "\n",
    "import kfp\n",
    "from typing import Any, Callable, Dict, NamedTuple, Optional, List\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output,\n",
    "                        OutputPath, component, Metrics)\n",
    "@kfp.v2.dsl.component(\n",
    "    base_image=\"gcr.io/google.com/cloudsdktool/cloud-sdk:latest\",\n",
    "    packages_to_install=[\n",
    "        \"google-cloud-build\"\n",
    "    ],\n",
    ")\n",
    "def build_custom_image(\n",
    "    project: str,\n",
    "    artifact_gcs_path: str,\n",
    "    docker_name: str,\n",
    "    app_dir_name: str,\n",
    "    custom_image_uri: str,\n",
    ") -> NamedTuple('Outputs', [\n",
    "    ('custom_image_uri', str),\n",
    "]):\n",
    "    # TODO: make output Artifact for image_uri\n",
    "    \"\"\"\n",
    "    custom pipeline component to build custom image using\n",
    "    Cloud Build, the training/serving application code, and dependencies\n",
    "    defined in the Dockerfile\n",
    "    \"\"\"\n",
    "    \n",
    "    import logging\n",
    "    import os\n",
    "\n",
    "    from google.cloud.devtools import cloudbuild_v1 as cloudbuild\n",
    "    from google.protobuf.duration_pb2 import Duration\n",
    "\n",
    "    # initialize client for cloud build\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "    build_client = cloudbuild.services.cloud_build.CloudBuildClient()\n",
    "    \n",
    "    # parse step inputs to get path to Dockerfile and training application code\n",
    "    _gcs_dockerfile_path = os.path.join(artifact_gcs_path, f\"{docker_name}\") # Dockerfile.XXXXX\n",
    "    _gcs_script_dir_path = os.path.join(artifact_gcs_path, f\"{app_dir_name}/\") # \"trainer/\"\n",
    "    \n",
    "    logging.info(f\"_gcs_dockerfile_path: {_gcs_dockerfile_path}\")\n",
    "    logging.info(f\"_gcs_script_dir_path: {_gcs_script_dir_path}\")\n",
    "    \n",
    "    # define build steps to pull the training code and Dockerfile\n",
    "    # and build/push the custom training container image\n",
    "    build = cloudbuild.Build()\n",
    "    build.steps = [\n",
    "        {\n",
    "            \"name\": \"gcr.io/cloud-builders/gsutil\",\n",
    "            \"args\": [\"cp\", \"-r\", _gcs_script_dir_path, \".\"],\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"gcr.io/cloud-builders/gsutil\",\n",
    "            \"args\": [\"cp\", _gcs_dockerfile_path, \"Dockerfile\"],\n",
    "        },\n",
    "        # enabling Kaniko cache in a Docker build that caches intermediate\n",
    "        # layers and pushes image automatically to Container Registry\n",
    "        # https://cloud.google.com/build/docs/kaniko-cache\n",
    "        # {\n",
    "        #     \"name\": \"gcr.io/kaniko-project/executor:latest\",\n",
    "        #     # \"name\": \"gcr.io/kaniko-project/executor:v1.8.0\",        # TODO; downgraded to avoid error in build\n",
    "        #     # \"args\": [f\"--destination={training_image_uri}\", \"--cache=true\"],\n",
    "        #     \"args\": [f\"--destination={training_image_uri}\", \"--cache=false\"],\n",
    "        # },\n",
    "        {\n",
    "            \"name\": \"gcr.io/cloud-builders/docker\",\n",
    "            \"args\": ['build','-t', f'{custom_image_uri}', '.'],\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"gcr.io/cloud-builders/docker\",\n",
    "            \"args\": ['push', f'{custom_image_uri}'], \n",
    "        },\n",
    "    ]\n",
    "    # override default timeout of 10min\n",
    "    timeout = Duration()\n",
    "    timeout.seconds = 7200\n",
    "    build.timeout = timeout\n",
    "\n",
    "    # create build\n",
    "    operation = build_client.create_build(project_id=project, build=build)\n",
    "    logging.info(\"IN PROGRESS:\")\n",
    "    logging.info(operation.metadata)\n",
    "\n",
    "    # get build status\n",
    "    result = operation.result()\n",
    "    logging.info(\"RESULT:\", result.status)\n",
    "\n",
    "    # return step outputs\n",
    "    return (\n",
    "        custom_image_uri,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8843984-b680-4c46-a456-b55291643e14",
   "metadata": {},
   "source": [
    "## Custom train job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc26d281-a72f-4825-a5f8-33ff35d2fb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/{PIPELINES_SUB_DIR}/train_custom_model.py\n",
    "\n",
    "import kfp\n",
    "from typing import Any, Callable, Dict, NamedTuple, Optional, List\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output,\n",
    "                        OutputPath, component, Metrics)\n",
    "@kfp.v2.dsl.component(\n",
    "    base_image='python:3.9',\n",
    "    packages_to_install=[\n",
    "        'google-cloud-aiplatform==1.17.0',\n",
    "        'tensorflow==2.9.2',\n",
    "        'tensorflow-recommenders==0.7.0',\n",
    "        'numpy',\n",
    "        'google-cloud-storage',\n",
    "    ],\n",
    "    # output_component_file=\"./pipelines/train_custom_model.yaml\",\n",
    ")\n",
    "def train_custom_model(\n",
    "    project: str,\n",
    "    model_version: str,\n",
    "    pipeline_version: str,\n",
    "    model_name: str, \n",
    "    worker_pool_specs: dict,\n",
    "    # vocab_dict_uri: str, \n",
    "    train_output_gcs_bucket: str,                         # change to workdir?\n",
    "    training_image_uri: str,\n",
    "    tensorboard_resource_name: str,\n",
    "    service_account: str,\n",
    "    experiment_name: str,\n",
    "    experiment_run: str,\n",
    ") -> NamedTuple('Outputs', [\n",
    "    ('query_tower_dir_uri', str),\n",
    "    ('candidate_tower_dir_uri', str),\n",
    "    # ('candidate_index_dir_uri', str),\n",
    "]):\n",
    "    \n",
    "    from google.cloud import aiplatform as vertex_ai\n",
    "    import logging\n",
    "    import numpy as np\n",
    "    \n",
    "    vertex_ai.init(\n",
    "        project=project,\n",
    "        location='us-central1',\n",
    "    )\n",
    "    \n",
    "    JOB_NAME = f'train-{model_name}'\n",
    "    logging.info(f'JOB_NAME: {JOB_NAME}')\n",
    "    \n",
    "    BASE_OUTPUT_DIR = f'gs://{train_output_gcs_bucket}/{experiment_name}/{experiment_run}'\n",
    "    logging.info(f'BASE_OUTPUT_DIR: {BASE_OUTPUT_DIR}')\n",
    "    \n",
    "    # logging.info(f'vocab_dict_uri: {vocab_dict_uri}')\n",
    "    \n",
    "    logging.info(f'tensorboard_resource_name: {tensorboard_resource_name}')\n",
    "    logging.info(f'service_account: {service_account}')\n",
    "    logging.info(f'worker_pool_specs: {worker_pool_specs}')\n",
    "  \n",
    "    job = vertex_ai.CustomJob(\n",
    "        display_name=JOB_NAME,\n",
    "        worker_pool_specs=worker_pool_specs,\n",
    "        staging_bucket=BASE_OUTPUT_DIR,\n",
    "    )\n",
    "    \n",
    "    logging.info(f'Submitting train job to Vertex AI...')\n",
    "\n",
    "    job.run(\n",
    "        tensorboard=tensorboard_resource_name,\n",
    "        service_account=f'{service_account}',\n",
    "        restart_job_on_worker_restart=False,\n",
    "        enable_web_access=True,\n",
    "        sync=False,\n",
    "    )\n",
    "    \n",
    "    # TODO: this is hardcoded, but created in train job\n",
    "    query_tower_dir_uri = f\"gs://{train_output_gcs_bucket}/{experiment_name}/{experiment_run}/model-dir/query_tower\" \n",
    "    candidate_tower_dir_uri = f\"gs://{train_output_gcs_bucket}/{experiment_name}/{experiment_run}/model-dir/candidate_tower\"\n",
    "    # candidate_index_dir_uri = f\"gs://{output_dir_gcs_bucket_name}/{experiment_name}/{experiment_run}/candidate-index\"\n",
    "    \n",
    "    return (\n",
    "        f'{query_tower_dir_uri}',\n",
    "        f'{candidate_tower_dir_uri}',\n",
    "        # f'{candidate_index_dir_uri}',\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa3eea5-05b5-46a7-ab2e-6ef4b826fcae",
   "metadata": {},
   "source": [
    "## Prepare Job Specs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1cfa03-1db5-44ac-9e19-b855a5c63975",
   "metadata": {},
   "source": [
    "### Vertex Train: workerpool specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2286562e-ff21-46e2-bfa4-14e732ddadef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_worker_pool_specs(\n",
    "    image_uri,\n",
    "    args,\n",
    "    cmd,\n",
    "    replica_count=1,\n",
    "    machine_type=\"n1-standard-16\",\n",
    "    accelerator_count=1,\n",
    "    accelerator_type=\"ACCELERATOR_TYPE_UNSPECIFIED\",\n",
    "    reduction_server_count=0,\n",
    "    reduction_server_machine_type=\"n1-highcpu-16\",\n",
    "    reduction_server_image_uri=\"us-docker.pkg.dev/vertex-ai-restricted/training/reductionserver:latest\",\n",
    "):\n",
    "\n",
    "    if accelerator_count > 0:\n",
    "        machine_spec = {\n",
    "            \"machine_type\": machine_type,\n",
    "            \"accelerator_type\": accelerator_type,\n",
    "            \"accelerator_count\": accelerator_count,\n",
    "        }\n",
    "    else:\n",
    "        machine_spec = {\"machine_type\": machine_type}\n",
    "\n",
    "    container_spec = {\n",
    "        \"image_uri\": image_uri,\n",
    "        \"args\": args,\n",
    "        \"command\": cmd,\n",
    "    }\n",
    "\n",
    "    chief_spec = {\n",
    "        \"replica_count\": 1,\n",
    "        \"machine_spec\": machine_spec,\n",
    "        \"container_spec\": container_spec,\n",
    "    }\n",
    "\n",
    "    worker_pool_specs = [chief_spec]\n",
    "    if replica_count > 1:\n",
    "        workers_spec = {\n",
    "            \"replica_count\": replica_count - 1,\n",
    "            \"machine_spec\": machine_spec,\n",
    "            \"container_spec\": container_spec,\n",
    "        }\n",
    "        worker_pool_specs.append(workers_spec)\n",
    "    if reduction_server_count > 1:\n",
    "        workers_spec = {\n",
    "            \"replica_count\": reduction_server_count,\n",
    "            \"machine_spec\": {\n",
    "                \"machine_type\": reduction_server_machine_type,\n",
    "            },\n",
    "            \"container_spec\": {\"image_uri\": reduction_server_image_uri},\n",
    "        }\n",
    "        worker_pool_specs.append(workers_spec)\n",
    "\n",
    "    return worker_pool_specs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045adff3-74fc-4e86-ac19-8e2feee61766",
   "metadata": {},
   "source": [
    "### Accelerators and Device Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a47f70-c364-4122-a9c6-abda876a0068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Single machine, single GPU\n",
    "WORKER_MACHINE_TYPE = 'a2-highgpu-1g'\n",
    "REPLICA_COUNT = 1\n",
    "ACCELERATOR_TYPE = 'NVIDIA_TESLA_A100'\n",
    "PER_MACHINE_ACCELERATOR_COUNT = 1\n",
    "REDUCTION_SERVER_COUNT = 0                                                      \n",
    "REDUCTION_SERVER_MACHINE_TYPE = \"n1-highcpu-16\"\n",
    "DISTRIBUTE_STRATEGY = 'single'\n",
    "\n",
    "# # # Single Machine; multiple GPU\n",
    "# WORKER_MACHINE_TYPE = 'a2-highgpu-4g' # a2-ultragpu-4g\n",
    "# REPLICA_COUNT = 1\n",
    "# ACCELERATOR_TYPE = 'NVIDIA_TESLA_A100'\n",
    "# PER_MACHINE_ACCELERATOR_COUNT = 4\n",
    "# REDUCTION_SERVER_COUNT = 0                                                      \n",
    "# REDUCTION_SERVER_MACHINE_TYPE = \"n1-highcpu-16\"\n",
    "# DISTRIBUTE_STRATEGY = 'mirrored'\n",
    "\n",
    "# # # Multiple Machines, 1 GPU per Machine\n",
    "# WORKER_MACHINE_TYPE = 'n1-standard-16'\n",
    "# REPLICA_COUNT = 9\n",
    "# ACCELERATOR_TYPE = 'NVIDIA_TESLA_T4'\n",
    "# PER_MACHINE_ACCELERATOR_COUNT = 1\n",
    "# REDUCTION_SERVER_COUNT = 10                                                      \n",
    "# REDUCTION_SERVER_MACHINE_TYPE = \"n1-highcpu-16\"\n",
    "# DISTRIBUTE_STRATEGY = 'multiworker'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66142882-0f35-4e7d-ab5a-c0d6b49d3f44",
   "metadata": {},
   "source": [
    "### Training Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fa7a02-6d4d-43ba-bce4-5289f86db2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Pipeline output repo \n",
    "# ====================================================\n",
    "OUTPUT_BUCKET = 'jt-tfrs-output-v2'\n",
    "OUTPUT_GCS_URI =f'gs://{OUTPUT_BUCKET}'\n",
    "\n",
    "EXPERIMENT_PREFIX = 'pipe-dev'                                                  # custom identifier for organizing experiments\n",
    "EXPERIMENT_NAME=f'{EXPERIMENT_PREFIX}-{MODEL_TYPE}-{FRAMEWORK}-{MODEL_VERSION}'\n",
    "RUN_NAME = f'run-{time.strftime(\"%Y%m%d-%H%M%S\")}'\n",
    "\n",
    "# Stores pipeline executions for each run\n",
    "PIPELINE_ROOT_PATH = f'gs://{OUTPUT_BUCKET}/{MODEL_ROOT_NAME}/pipeline_root'\n",
    "print('PIPELINE_ROOT_PATH: {}'.format(PIPELINE_ROOT_PATH))\n",
    "\n",
    "TB_RESOURCE_NAME = 'projects/934903580331/locations/us-central1/tensorboards/7336372589079560192'\n",
    "\n",
    "# =================================================\n",
    "# Data sources\n",
    "# =================================================\n",
    "CANDIDATE_FILE_DIR = 'spotify-data-regimes'\n",
    "CANDIDATE_PREFIX = 'candidates/' \n",
    "\n",
    "TRAIN_DIR = 'spotify-data-regimes' \n",
    "# TRAIN_DIR_PREFIX = 'train_v9/'\n",
    "TRAIN_DIR_PREFIX = 'valid_v9/'\n",
    "\n",
    "VALID_DIR = 'spotify-data-regimes' \n",
    "VALID_DIR_PREFIX = 'valid_v9/'\n",
    "\n",
    "# =================================================\n",
    "# train image\n",
    "# =================================================\n",
    "# Existing image URI or name for image to create\n",
    "# IMAGE_URI = 'gcr.io/hybrid-vertex/sp-2tower-tfrs-v15-v0-training'\n",
    "# IMAGE_NAME = f'{MODEL_NAME}-training'\n",
    "# IMAGE_URI = f'gcr.io/{PROJECT}/{IMAGE_NAME}'\n",
    "\n",
    "# =================================================\n",
    "# train job config\n",
    "# =================================================\n",
    "SEED = 1234\n",
    "VALID_FREQUENCY = 10\n",
    "# VALID_SIZE = 20_000\n",
    "\n",
    "MAX_TOKENS = 20000\n",
    "\n",
    "NUM_EPOCHS = 1\n",
    "BATCH_SIZE = 1024*16 # batch_size = 1024*16\n",
    "LEARNING_RATE = 0.01\n",
    "\n",
    "# MAX_PADDING = 375\n",
    "EMBEDDING_DIM = 128\n",
    "PROJECTION_DIM = 50\n",
    "\n",
    "DROPOUT_RATE = 0.33\n",
    "LAYER_SIZES = '[64,32]'\n",
    "\n",
    "WORKER_CMD = [\"python\", \"trainer/task.py\"]\n",
    "# WORKER_CMD [\"python\", \"-m\", \"trainer.task\"]\n",
    "\n",
    "WORKER_ARGS = [\n",
    "    f'--project={PROJECT_ID}',\n",
    "    f'--train_output_gcs_bucket={OUTPUT_BUCKET}',\n",
    "    f'--train_dir={TRAIN_DIR}',\n",
    "    f'--train_dir_prefix={TRAIN_DIR_PREFIX}',\n",
    "    f'--valid_dir={VALID_DIR}',\n",
    "    f'--valid_dir_prefix={VALID_DIR_PREFIX}',\n",
    "    f'--candidate_file_dir={CANDIDATE_FILE_DIR}',\n",
    "    f'--candidate_files_prefix={CANDIDATE_PREFIX}',\n",
    "    f'--experiment_name={EXPERIMENT_NAME}',\n",
    "    f'--experiment_run={RUN_NAME}',\n",
    "    f'--num_epochs={NUM_EPOCHS}',\n",
    "    f'--batch_size={BATCH_SIZE}',\n",
    "    f'--embedding_dim={EMBEDDING_DIM}',\n",
    "    f'--projection_dim={PROJECTION_DIM}',\n",
    "    f'--layer_sizes={LAYER_SIZES}',\n",
    "    f'--learning_rate={LEARNING_RATE}',\n",
    "    # f'--valid_frequency={VALID_FREQUENCY}',\n",
    "    f'--distribute={DISTRIBUTE_STRATEGY}',\n",
    "    f'--model_version={MODEL_VERSION}',\n",
    "    f'--pipeline_version={PIPELINE_VERSION}',\n",
    "    f'--seed={SEED}',\n",
    "    f'--max_tokens={MAX_TOKENS}',\n",
    "    f'--tb_resource_name={TB_RESOURCE_NAME}',\n",
    "]\n",
    "\n",
    "WORKER_POOL_SPECS = prepare_worker_pool_specs(\n",
    "    image_uri=IMAGE_URI,\n",
    "    args=WORKER_ARGS,\n",
    "    cmd=WORKER_CMD,\n",
    "    replica_count=REPLICA_COUNT,\n",
    "    machine_type=WORKER_MACHINE_TYPE,\n",
    "    accelerator_count=PER_MACHINE_ACCELERATOR_COUNT,\n",
    "    accelerator_type=ACCELERATOR_TYPE,\n",
    "    reduction_server_count=REDUCTION_SERVER_COUNT,\n",
    "    reduction_server_machine_type=REDUCTION_SERVER_MACHINE_TYPE,\n",
    ")\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(WORKER_POOL_SPECS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d826dc-0dfc-44de-99f1-79d68ecd559d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export PWD=pwd\n",
    "!export PIPELINE_ROOT_PATH=PIPELINE_ROOT_PATH\n",
    "\n",
    "! echo $PWD\n",
    "! echo $PIPELINE_ROOT_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913e0f5d-abc0-4d78-915e-7a28489e3d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # copy training Dockerfile\n",
    "# ! gsutil cp $PWD/src/Dockerfile.tfrs $BUCKET_URI/$VERSION/src\n",
    "!gsutil cp $PWD/Dockerfile.tfrs $PIPELINE_ROOT_PATH\n",
    "!gsutil cp $PWD/cloudbuild.yaml $PIPELINE_ROOT_PATH\n",
    "\n",
    "# # # copy training application code\n",
    "! gsutil -m cp -r $PWD/two_tower_jt/* $PIPELINE_ROOT_PATH/trainer\n",
    "\n",
    "print(f\"Copied training application code and Dockerfile to {PIPELINE_ROOT_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d64bb5-3128-44f6-b652-468a7a325c15",
   "metadata": {},
   "source": [
    "## Build Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1eb197b-ea56-4b83-aa92-8198a75711ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINE_TAG = f'2tower-recsys-{PIPELINE_VERSION}'\n",
    "print(\"PIPELINE_TAG:\", PIPELINE_TAG)\n",
    "\n",
    "PIPELINE_NAME = f'modeling-{MODEL_VERSION}-{PIPELINE_TAG}'.replace('_', '-')\n",
    "print(\"PIPELINE_NAME:\", PIPELINE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6f5b61-ab53-4619-b854-8c5a4da210e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.pipelines import build_custom_image, train_custom_model\n",
    "\n",
    "@kfp.v2.dsl.pipeline(\n",
    "    name=f'{PIPELINE_NAME}'.replace('_', '-')\n",
    ")\n",
    "def pipeline(\n",
    "    project: str,\n",
    "    project_number: str,\n",
    "    location: str,\n",
    "    service_account: str,\n",
    "    model_version: str,\n",
    "    pipeline_version: str,\n",
    "    train_image_uri: str,\n",
    "    train_output_gcs_bucket: str,\n",
    "    gcs_train_script_path: str,\n",
    "    model_display_name: str,\n",
    "    train_dockerfile_name: str,\n",
    "    train_dir: str,\n",
    "    train_dir_prefix: str,\n",
    "    valid_dir: str,\n",
    "    valid_dir_prefix: str,\n",
    "    candidate_file_dir: str,\n",
    "    candidate_files_prefix: str,\n",
    "    experiment_name: str,\n",
    "):\n",
    "    \n",
    "    from kfp.v2.components import importer_node\n",
    "    from google_cloud_pipeline_components.types import artifact_types\n",
    "    \n",
    "    # ========================================================================\n",
    "    # Build Custom Train Image\n",
    "    # ========================================================================\n",
    "    \n",
    "    # build_custom_train_image_op = (\n",
    "    #     build_custom_train_image.build_custom_train_image(\n",
    "    #         project=project,\n",
    "    #         gcs_train_script_path=gcs_train_script_path,\n",
    "    #         training_image_uri=train_image_uri,\n",
    "    #         train_dockerfile_name=train_dockerfile_name,\n",
    "    #     )\n",
    "    #     .set_display_name(\"Build custom train image\")\n",
    "    #     .set_caching_options(False)\n",
    "    # )\n",
    "\n",
    "\n",
    "    run_train_task_op = (\n",
    "        train_custom_model.train_custom_model(\n",
    "            project=project,\n",
    "            model_version=model_version,\n",
    "            pipeline_version=pipeline_version,\n",
    "            model_name=model_display_name,\n",
    "            worker_pool_specs=WORKER_POOL_SPECS, \n",
    "            train_output_gcs_bucket=train_output_gcs_bucket,\n",
    "            # vocab_dict_uri=build_vocabs_string_lookups_op.outputs['vocab_gcs_uri'],\n",
    "            experiment_name=experiment_name,\n",
    "            experiment_run=experiment_run,\n",
    "            training_image_uri=train_image_uri,                                                                       # build_custom_train_image_op.outputs['training_image_uri'],\n",
    "            tensorboard_resource_name=\"projects/934903580331/locations/us-central1/tensorboards/3482390819678191616\", # create_tensorboard_op.outputs['tensorboard_resource_name'],\n",
    "            service_account=service_account,\n",
    "        )\n",
    "        .set_display_name(\"2Tower Training\")\n",
    "        .set_caching_options(True)\n",
    "        # .after(build_custom_train_image_op)\n",
    "    )\n",
    "    \n",
    "    # ========================================================================\n",
    "    # Upload Query and Candidate Towers\n",
    "    # ========================================================================\n",
    "    \n",
    "    import_unmanaged_query_model_task = (\n",
    "        importer_node.importer(\n",
    "            artifact_uri=run_train_task_op.outputs['query_tower_dir_uri'],\n",
    "            artifact_class=artifact_types.UnmanagedContainerModel,\n",
    "            metadata={\n",
    "                'containerSpec': {\n",
    "                    'imageUri': 'us-docker.pkg.dev/vertex-ai/prediction/tf2-gpu.2-10:latest',\n",
    "                },\n",
    "            },\n",
    "        )\n",
    "        .set_display_name(\"Import Query Tower\")\n",
    "        .after(run_train_task_op)\n",
    "        .set_caching_options(True)\n",
    "    )\n",
    "    \n",
    "    query_model_upload_op = (\n",
    "        gcc_aip.ModelUploadOp(\n",
    "            project=project,\n",
    "            location=location,\n",
    "            display_name=f'query-tower-{model_display_name}',\n",
    "            unmanaged_container_model=import_unmanaged_query_model_task.outputs[\"artifact\"],\n",
    "            labels={\"tower\": \"query\"},\n",
    "        )\n",
    "        .after(import_unmanaged_query_model_task)\n",
    "        .set_display_name(\"Upload Query Tower\")\n",
    "        .set_caching_options(True)\n",
    "    )\n",
    "    \n",
    "    import_unmanaged_candidate_model_task = (\n",
    "        importer_node.importer(\n",
    "            artifact_uri=run_train_task_op.outputs['candidate_tower_dir_uri'],\n",
    "            artifact_class=artifact_types.UnmanagedContainerModel,\n",
    "            metadata={\n",
    "                'containerSpec': {\n",
    "                    'imageUri': 'us-docker.pkg.dev/vertex-ai/prediction/tf2-gpu.2-10:latest',\n",
    "                },\n",
    "            },\n",
    "        )\n",
    "        .set_display_name(\"Import Candidate Tower\")\n",
    "        .after(run_train_task_op)\n",
    "        .set_caching_options(True)\n",
    "    )\n",
    "\n",
    "    candidate_model_upload_op = (\n",
    "        gcc_aip.ModelUploadOp(\n",
    "            project=project,\n",
    "            location=location,\n",
    "            display_name=f'candidate-tower-{model_display_name}',\n",
    "            unmanaged_container_model=import_unmanaged_candidate_model_task.outputs[\"artifact\"],\n",
    "            labels={\"tower\": \"candidate\"},\n",
    "        )\n",
    "        # .after(import_unmanaged_query_model_task)\n",
    "        .set_display_name(\"Upload Query Tower to Vertex\")\n",
    "        .set_caching_options(True)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c65526-e42c-459c-a3a2-db8c84b6cc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -f pipelines/custom_container_pipeline_spec.json\n",
    "\n",
    "PIPELINE_JSON_SPEC_PATH = \"./src/pipelines/custom_container_pipeline_spec.json\"\n",
    "\n",
    "kfp.v2.compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline, package_path=PIPELINE_JSON_SPEC_PATH,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8d598e-9456-452f-bb41-6c80feb288a7",
   "metadata": {},
   "source": [
    "## Submit Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655bb924-f650-4e3c-8866-1ac410f74487",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_NUMBER='934903580331'\n",
    "vpc_network_name = 'ucaip-haystack-vpc-network'\n",
    "SERVICE_ACCOUNT = '934903580331-compute@developer.gserviceaccount.com'\n",
    "\n",
    "# ========================================================================\n",
    "# Submit Pipeline Job\n",
    "# ========================================================================\n",
    "# # \"gs://spotify-tfrecords-blog/tfrecords_v1/train/output-00000-of-00796.tfrecord\"\n",
    "# # gs://spotify-tfrs-dir/small-dataset/output-00000-of-00796.tfrecord\n",
    "\n",
    "overwrite = True\n",
    "    \n",
    "if not PIPELINES.get('train') or overwrite:\n",
    "    response = pipeline_client.create_run_from_job_spec(\n",
    "        job_spec_path=PIPELINE_JSON_SPEC_PATH,\n",
    "        enable_caching=True,\n",
    "        network=f'projects/{PROJECT_NUMBER}/global/networks/{vpc_network_name}',            #\n",
    "        # service_account=SERVICE_ACCOUNT,                                                 \n",
    "        parameter_values={\n",
    "            'project': PROJECT_ID,\n",
    "            'project_number': PROJECT_NUMBER,\n",
    "            'location': LOCATION,\n",
    "            'model_version': MODEL_VERSION,\n",
    "            'pipeline_version': PIPELINE_VERSION,\n",
    "            'model_display_name': MODEL_ROOT_NAME,\n",
    "            'pipeline_tag': PIPELINE_TAG,\n",
    "            'gcs_train_script_path': TRAIN_APP_CODE_PATH,\n",
    "            'train_image_uri': IMAGE_URI,\n",
    "            'train_output_gcs_bucket': OUTPUT_BUCKET,\n",
    "            'train_dir': TRAIN_DIR,\n",
    "            'train_dir_prefix': TRAIN_DIR_PREFIX,\n",
    "            'valid_dir': VALID_DIR,\n",
    "            'valid_dir_prefix': VALID_DIR_PREFIX,\n",
    "            'candidate_file_dir': CANDIDATE_FILE_DIR,\n",
    "            'candidate_files_prefix': CANDIDATE_PREFIX,\n",
    "            # 'vpc_network_name': vpc_network_name,\n",
    "            'train_dockerfile_name': DOCKERNAME_TRAIN,\n",
    "            'experiment_name': EXPERIMENT_NAME,\n",
    "            'experiment_run': RUN_NAME,\n",
    "            'service_account': SERVICE_ACCOUNT,\n",
    "        },\n",
    "        pipeline_root=f'{PIPELINE_ROOT_PATH}/{PIPELINE_VERSION}',\n",
    "    )\n",
    "    PIPELINES['train'] = response['name']"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m97",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m97"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
