{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2804db33-2adb-4845-a6bb-087f168eb6a5",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "# Build baseline tfrs model \n",
    "\n",
    "* source code for model can be found in `src/`\n",
    "\n",
    "This notebook constructs the two tower model and saves the model to GCS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "059c0d02-8de1-446e-9b2c-7366dfff042d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aiplatform SDK version: 1.26.1\n"
     ]
    }
   ],
   "source": [
    "# !pip install tensorflow-recommenders google-cloud-aiplatform --user\n",
    "! python3 -c \"import google.cloud.aiplatform; print('aiplatform SDK version: {}'.format(google.cloud.aiplatform.__version__))\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d2a891-1129-480d-ad11-f8f78ce062b8",
   "metadata": {},
   "source": [
    "## Load env config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93f5e488-ed81-4b49-99fa-63dea67ab6c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREFIX = ndr-v1\n"
     ]
    }
   ],
   "source": [
    "# naming convention for all cloud resources\n",
    "VERSION        = \"v1\"                  # TODO\n",
    "PREFIX         = f'ndr-{VERSION}'      # TODO\n",
    "\n",
    "print(f\"PREFIX = {PREFIX}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a6fbc7e-1dba-46c8-9abf-380fc80042e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PROJECT_ID               = \"hybrid-vertex\"\n",
      "PROJECT_NUM              = \"934903580331\"\n",
      "LOCATION                 = \"us-central1\"\n",
      "\n",
      "REGION                   = \"us-central1\"\n",
      "BQ_LOCATION              = \"US\"\n",
      "VPC_NETWORK_NAME         = \"ucaip-haystack-vpc-network\"\n",
      "\n",
      "VERTEX_SA                = \"934903580331-compute@developer.gserviceaccount.com\"\n",
      "\n",
      "PREFIX                   = \"ndr-v1\"\n",
      "VERSION                  = \"v1\"\n",
      "\n",
      "APP                      = \"sp\"\n",
      "MODEL_TYPE               = \"2tower\"\n",
      "FRAMEWORK                = \"tfrs\"\n",
      "DATA_VERSION             = \"v1\"\n",
      "TRACK_HISTORY            = \"5\"\n",
      "\n",
      "BUCKET_NAME              = \"ndr-v1-hybrid-vertex-bucket\"\n",
      "BUCKET_URI               = \"gs://ndr-v1-hybrid-vertex-bucket\"\n",
      "SOURCE_BUCKET            = \"spotify-million-playlist-dataset\"\n",
      "\n",
      "DATA_GCS_PREFIX          = \"data\"\n",
      "DATA_PATH                = \"gs://ndr-v1-hybrid-vertex-bucket/data\"\n",
      "VOCAB_SUBDIR             = \"vocabs\"\n",
      "VOCAB_FILENAME           = \"vocab_dict.pkl\"\n",
      "\n",
      "CANDIDATE_PREFIX         = \"candidates\"\n",
      "TRAIN_DIR_PREFIX         = \"train\"\n",
      "VALID_DIR_PREFIX         = \"valid\"\n",
      "\n",
      "VPC_NETWORK_FULL         = \"projects/934903580331/global/networks/ucaip-haystack-vpc-network\"\n",
      "\n",
      "BQ_DATASET               = \"spotify_e2e_test\"\n",
      "\n",
      "REPO_SRC                 = \"src\"\n",
      "PIPELINES_SUB_DIR        = \"feature_pipes\"\n",
      "\n",
      "REPOSITORY               = \"ndr-v1-spotify\"\n",
      "IMAGE_NAME               = \"train-v1\"\n",
      "REMOTE_IMAGE_NAME        = \"us-central1-docker.pkg.dev/hybrid-vertex/ndr-v1-spotify/train-v1\"\n",
      "DOCKERNAME               = \"tfrs\"\n",
      "\n",
      "SERVING_IMAGE_URI_CPU    = \"us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-11:latest\"\n",
      "SERVING_IMAGE_URI_GPU    = \"us-docker.pkg.dev/vertex-ai/prediction/tf2-gpu.2-11:latest\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# staging GCS\n",
    "GCP_PROJECTS             = !gcloud config get-value project\n",
    "PROJECT_ID               = GCP_PROJECTS[0]\n",
    "\n",
    "# GCS bucket and paths\n",
    "BUCKET_NAME              = f'{PREFIX}-{PROJECT_ID}-bucket'\n",
    "BUCKET_URI               = f'gs://{BUCKET_NAME}'\n",
    "\n",
    "config = !gsutil cat {BUCKET_URI}/config/notebook_env.py\n",
    "print(config.n)\n",
    "exec(config.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9289ca69-cd66-4161-88b2-db58e9218b91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =============================== #\n",
    "# included in env-setup.ipynb     #\n",
    "# =============================== #\n",
    "\n",
    "# REGION           = 'us-central1' \n",
    "# BUCKET_NAME      = 'matching-engine-content'    # location to store output\n",
    "# BUCKET_URI       = f'gs://{BUCKET_NAME}'\n",
    "# DATA_VERSION     = \"v1-0-0\"                     # version tag for dataflow pipeline\n",
    "# TRAIN_DIR_PREFIX = f'valid'                       # subset: valid_v9 | train_v9\n",
    "# VALID_DIR_PREFIX = f'valid'                       # valid_v9 | train_v9\n",
    "# CANDIDATE_PREFIX = f'candidates' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2d6dd36-f5ae-40cb-b76e-480b9782cd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "os.environ['TF_GPU_THREAD_MODE']='gpu_private'\n",
    "os.environ['TF_GPU_ALLOCATOR']='cuda_malloc_async'\n",
    "os.environ[\"CLOUD_ML_PROJECT_ID\"] = PROJECT_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef3417e6-72a8-48e2-ba9d-8fd207ab6229",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "\n",
    "import logging\n",
    "import time\n",
    "from pprint import pprint\n",
    "\n",
    "# tensorflow\n",
    "import tensorflow as tf\n",
    "import tensorflow_recommenders as tfrs\n",
    "\n",
    "from google.cloud import aiplatform as vertex_ai\n",
    "from google.cloud import storage\n",
    "storage_client = storage.Client(project=PROJECT_ID)\n",
    "\n",
    "from src.two_tower_jt import two_tower as tt\n",
    "from src.two_tower_jt import train_utils as train_utils\n",
    "# from src.two_tower_jt import feature_sets\n",
    "\n",
    "from util import feature_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e214c04a-e77a-4e26-a5ea-b54285957b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5015b9c5-f6f8-46b3-998c-9ceb0ca65962",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create Dataset for local training and testing\n",
    "\n",
    "Inspect the contents of the directory - you can change parameters in the header of the `two_tower.py` script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2e48504-5598-4f8d-b996-9f0f9192e471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34msrc/two_tower_jt\u001b[00m\n",
      "├── __init__.py\n",
      "├── \u001b[01;34m__pycache__\u001b[00m\n",
      "│   ├── __init__.cpython-37.pyc\n",
      "│   ├── feature_sets.cpython-37.pyc\n",
      "│   ├── test_instances.cpython-37.pyc\n",
      "│   ├── train_config.cpython-37.pyc\n",
      "│   ├── train_utils.cpython-37.pyc\n",
      "│   ├── train_utils_v1.cpython-37.pyc\n",
      "│   └── two_tower.cpython-37.pyc\n",
      "├── feature_sets.py\n",
      "├── interactive_train.py\n",
      "├── requirements.txt\n",
      "├── task.py\n",
      "├── test_instances.py\n",
      "├── train_config.py\n",
      "├── train_utils.py\n",
      "└── two_tower.py\n",
      "\n",
      "1 directory, 16 files\n"
     ]
    }
   ],
   "source": [
    "!tree src/two_tower_jt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9d8dc5-a796-4295-a1da-5474827ef859",
   "metadata": {},
   "source": [
    "## Create Dataset objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c26cf3b-6f23-476f-9806-27dd9697d22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "options = tf.data.Options()\n",
    "options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.AUTO\n",
    "\n",
    "batch_size = 1024 #*16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984676ac-e3bc-4c34-9875-3113130e418b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Create Train dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6827209-b916-4291-8f56-6afdb33c26ee",
   "metadata": {},
   "source": [
    "### data input pipeline \n",
    "\n",
    "> interleave --> map --> batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b03553df-06ea-4e76-b766-a8171acb2c8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_OptionsDataset element_spec={'album_name_can': TensorSpec(shape=(None,), dtype=tf.string, name=None), 'album_name_pl': TensorSpec(shape=(None, 5), dtype=tf.string, name=None), 'album_uri_can': TensorSpec(shape=(None,), dtype=tf.string, name=None), 'album_uri_pl': TensorSpec(shape=(None, 5), dtype=tf.string, name=None), 'artist_followers_can': TensorSpec(shape=(None,), dtype=tf.float32, name=None), 'artist_genres_can': TensorSpec(shape=(None,), dtype=tf.string, name=None), 'artist_genres_pl': TensorSpec(shape=(None, 5), dtype=tf.string, name=None), 'artist_name_can': TensorSpec(shape=(None,), dtype=tf.string, name=None), 'artist_name_pl': TensorSpec(shape=(None, 5), dtype=tf.string, name=None), 'artist_pop_can': TensorSpec(shape=(None,), dtype=tf.float32, name=None), 'artist_pop_pl': TensorSpec(shape=(None, 5), dtype=tf.float32, name=None), 'artist_uri_can': TensorSpec(shape=(None,), dtype=tf.string, name=None), 'artist_uri_pl': TensorSpec(shape=(None, 5), dtype=tf.string, name=None), 'artists_followers_pl': TensorSpec(shape=(None, 5), dtype=tf.float32, name=None), 'duration_ms_can': TensorSpec(shape=(None,), dtype=tf.float32, name=None), 'duration_ms_songs_pl': TensorSpec(shape=(None, 5), dtype=tf.float32, name=None), 'num_pl_albums_new': TensorSpec(shape=(None,), dtype=tf.float32, name=None), 'num_pl_artists_new': TensorSpec(shape=(None,), dtype=tf.float32, name=None), 'num_pl_songs_new': TensorSpec(shape=(None,), dtype=tf.float32, name=None), 'pl_collaborative_src': TensorSpec(shape=(None,), dtype=tf.string, name=None), 'pl_duration_ms_new': TensorSpec(shape=(None,), dtype=tf.float32, name=None), 'pl_name_src': TensorSpec(shape=(None,), dtype=tf.string, name=None), 'track_acousticness_can': TensorSpec(shape=(None,), dtype=tf.float32, name=None), 'track_acousticness_pl': TensorSpec(shape=(None, 5), dtype=tf.float32, name=None), 'track_danceability_can': TensorSpec(shape=(None,), dtype=tf.float32, name=None), 'track_danceability_pl': TensorSpec(shape=(None, 5), dtype=tf.float32, name=None), 'track_energy_can': TensorSpec(shape=(None,), dtype=tf.float32, name=None), 'track_energy_pl': TensorSpec(shape=(None, 5), dtype=tf.float32, name=None), 'track_instrumentalness_can': TensorSpec(shape=(None,), dtype=tf.float32, name=None), 'track_instrumentalness_pl': TensorSpec(shape=(None, 5), dtype=tf.float32, name=None), 'track_key_can': TensorSpec(shape=(None,), dtype=tf.string, name=None), 'track_key_pl': TensorSpec(shape=(None, 5), dtype=tf.string, name=None), 'track_liveness_can': TensorSpec(shape=(None,), dtype=tf.float32, name=None), 'track_liveness_pl': TensorSpec(shape=(None, 5), dtype=tf.float32, name=None), 'track_loudness_can': TensorSpec(shape=(None,), dtype=tf.float32, name=None), 'track_loudness_pl': TensorSpec(shape=(None, 5), dtype=tf.float32, name=None), 'track_mode_can': TensorSpec(shape=(None,), dtype=tf.string, name=None), 'track_mode_pl': TensorSpec(shape=(None, 5), dtype=tf.string, name=None), 'track_name_can': TensorSpec(shape=(None,), dtype=tf.string, name=None), 'track_name_pl': TensorSpec(shape=(None, 5), dtype=tf.string, name=None), 'track_pop_can': TensorSpec(shape=(None,), dtype=tf.float32, name=None), 'track_pop_pl': TensorSpec(shape=(None, 5), dtype=tf.float32, name=None), 'track_speechiness_can': TensorSpec(shape=(None,), dtype=tf.float32, name=None), 'track_speechiness_pl': TensorSpec(shape=(None, 5), dtype=tf.float32, name=None), 'track_tempo_can': TensorSpec(shape=(None,), dtype=tf.float32, name=None), 'track_tempo_pl': TensorSpec(shape=(None, 5), dtype=tf.float32, name=None), 'track_time_signature_can': TensorSpec(shape=(None,), dtype=tf.string, name=None), 'track_time_signature_pl': TensorSpec(shape=(None, 5), dtype=tf.string, name=None), 'track_uri_can': TensorSpec(shape=(None,), dtype=tf.string, name=None), 'track_uri_pl': TensorSpec(shape=(None, 5), dtype=tf.string, name=None), 'track_valence_can': TensorSpec(shape=(None,), dtype=tf.float32, name=None), 'track_valence_pl': TensorSpec(shape=(None, 5), dtype=tf.float32, name=None)}>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_files = []\n",
    "for blob in storage_client.list_blobs(f\"{BUCKET_NAME}\", prefix=f'data/{DATA_VERSION}/{TRAIN_DIR_PREFIX}/'):\n",
    "    if '.tfrecords' in blob.name:\n",
    "        train_files.append(blob.public_url.replace(\"https://storage.googleapis.com/\", \"gs://\"))\n",
    "\n",
    "# train_files    \n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_files).prefetch(\n",
    "    tf.data.AUTOTUNE,\n",
    ")\n",
    "\n",
    "train_dataset = train_dataset.interleave(\n",
    "    train_utils.full_parse,\n",
    "    cycle_length=tf.data.AUTOTUNE, \n",
    "    num_parallel_calls=tf.data.AUTOTUNE,\n",
    "    deterministic=False,\n",
    ").map(\n",
    "    feature_sets.parse_tfrecord, \n",
    "    num_parallel_calls=tf.data.AUTOTUNE\n",
    ").batch(\n",
    "    batch_size \n",
    ").prefetch(\n",
    "    tf.data.AUTOTUNE,\n",
    ").with_options(\n",
    "    options\n",
    ")\n",
    "\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d0ccf9-4602-49b8-bde7-dbb962d6cc25",
   "metadata": {},
   "source": [
    "### Create Validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4983881f-099b-43d6-b296-381cf3e1d874",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_files = []\n",
    "for blob in storage_client.list_blobs(f\"{BUCKET_NAME}\", prefix=f'data/{DATA_VERSION}/{VALID_DIR_PREFIX}/'):\n",
    "    if '.tfrecords' in blob.name:\n",
    "        valid_files.append(blob.public_url.replace(\"https://storage.googleapis.com/\", \"gs://\"))\n",
    "\n",
    "\n",
    "valid_dataset = tf.data.Dataset.from_tensor_slices(valid_files).prefetch(\n",
    "    tf.data.AUTOTUNE,\n",
    ")\n",
    "\n",
    "valid_dataset = valid_dataset.interleave(\n",
    "    train_utils.full_parse,\n",
    "    num_parallel_calls=tf.data.AUTOTUNE,\n",
    "    cycle_length=tf.data.AUTOTUNE, \n",
    "    deterministic=False,\n",
    ").map(\n",
    "    feature_sets.parse_tfrecord, \n",
    "    num_parallel_calls=tf.data.AUTOTUNE\n",
    ").batch(\n",
    "    batch_size\n",
    ").prefetch(\n",
    "    tf.data.AUTOTUNE,\n",
    ").with_options(\n",
    "    options\n",
    ")\n",
    "\n",
    "# valid_dataset = valid_dataset.cache() #1gb machine mem + 400 MB in candidate ds (src/two-tower.py)\n",
    "\n",
    "# valid_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf15fc9-5d2d-470c-83d6-29354ff813d9",
   "metadata": {},
   "source": [
    "### Create Candidates dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b115e4b4-f616-4314-ac65-b956bbc2d9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_files = []\n",
    "for blob in storage_client.list_blobs(f\"{BUCKET_NAME}\", prefix=f'data/{DATA_VERSION}/{CANDIDATE_PREFIX}/'):\n",
    "    if '.tfrecords' in blob.name:\n",
    "        candidate_files.append(blob.public_url.replace(\"https://storage.googleapis.com/\", \"gs://\"))\n",
    "\n",
    "candidate_dataset = tf.data.Dataset.from_tensor_slices(candidate_files)\n",
    "\n",
    "parsed_candidate_dataset = candidate_dataset.interleave(\n",
    "    train_utils.full_parse,\n",
    "    cycle_length=tf.data.AUTOTUNE, \n",
    "    num_parallel_calls=tf.data.AUTOTUNE,\n",
    "    deterministic=False\n",
    ").map(\n",
    "    feature_sets.parse_candidate_tfrecord_fn, \n",
    "    num_parallel_calls=tf.data.AUTOTUNE\n",
    ").with_options(\n",
    "    options\n",
    ")\n",
    "\n",
    "parsed_candidate_dataset = parsed_candidate_dataset.cache() #400 MB on machine mem\n",
    "# parsed_candidate_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f0e25957-1097-457a-aa8d-967f8cb031fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'album_name_can': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'Thanatophobia'], dtype=object)>,\n",
      " 'album_uri_can': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'spotify:album:5GBUYg5EqeDI0CuszAvDzj'], dtype=object)>,\n",
      " 'artist_followers_can': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([27438.], dtype=float32)>,\n",
      " 'artist_genres_can': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b\"'indie garage rock'\"], dtype=object)>,\n",
      " 'artist_name_can': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'Worn-Tin'], dtype=object)>,\n",
      " 'artist_pop_can': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([40.], dtype=float32)>,\n",
      " 'artist_uri_can': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'spotify:artist:7j8ds7BnqaEKuz1a1GN0J9'], dtype=object)>,\n",
      " 'duration_ms_can': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([216923.], dtype=float32)>,\n",
      " 'track_acousticness_can': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.655], dtype=float32)>,\n",
      " 'track_danceability_can': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.321], dtype=float32)>,\n",
      " 'track_energy_can': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.555], dtype=float32)>,\n",
      " 'track_instrumentalness_can': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.752], dtype=float32)>,\n",
      " 'track_key_can': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'4.0'], dtype=object)>,\n",
      " 'track_liveness_can': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.117], dtype=float32)>,\n",
      " 'track_loudness_can': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([-9.589], dtype=float32)>,\n",
      " 'track_mode_can': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'1'], dtype=object)>,\n",
      " 'track_name_can': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'Doug'], dtype=object)>,\n",
      " 'track_pop_can': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([14.], dtype=float32)>,\n",
      " 'track_speechiness_can': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.0323], dtype=float32)>,\n",
      " 'track_tempo_can': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([129.537], dtype=float32)>,\n",
      " 'track_time_signature_can': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'4'], dtype=object)>,\n",
      " 'track_uri_can': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'spotify:track:2XZ3bL3ROk605SPpy6Dn9C'], dtype=object)>,\n",
      " 'track_valence_can': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.21], dtype=float32)>}\n"
     ]
    }
   ],
   "source": [
    "# check dataset output\n",
    "for x in parsed_candidate_dataset.batch(1).take(1):\n",
    "    pprint(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa2e448-f833-421f-977d-f52b0db198c2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Local Training\n",
    "\n",
    "Compile the model\n",
    "Review the details of the model layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051a914a-e3d4-4c5c-a42a-9d39a6bde63c",
   "metadata": {},
   "source": [
    "## Adapt Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd6fa77-6401-47bc-b198-14c1891ddcac",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Adapt the text vectorizors - copy/paste to run one time\n",
    "\n",
    "We are accessing the `TextVectorizor` layers in the model via the layer print-outs above\n",
    "\n",
    "```python\n",
    "# adpat the text vectorizors\n",
    "\n",
    "MAX_PLAYLIST_LENGTH = 5 # this is set upstream by the BigQuery max length\n",
    "    \n",
    "model.query_tower.layers[0].layers[0].adapt(\n",
    "    train_dataset.unbatch().batch(10000).map(lambda x: x['pl_name_src']))\n",
    "print('pl_name_src adapts complete')\n",
    "model.query_tower.layers[7].layers[0].adapt(\n",
    "    train_dataset.unbatch().batch(10000).map(lambda x: tf.reshape(x['track_name_pl'], [-1, MAX_PLAYLIST_LENGTH, 1])))\n",
    "print('track_name_pl adapts complete')\n",
    "model.query_tower.layers[9].layers[0].adapt(\n",
    "    train_dataset.unbatch().batch(10000).map(lambda x: tf.reshape(x['artist_name_pl'], [-1, MAX_PLAYLIST_LENGTH, 1]))) \n",
    "print('artist_name_pl adapts complete')\n",
    "model.query_tower.layers[11].layers[0].adapt(\n",
    "    train_dataset.unbatch().batch(10000).map(lambda x: tf.reshape(x['album_name_pl'], [-1, MAX_PLAYLIST_LENGTH, 1])))\n",
    "print('album_name_pl adapts complete')\n",
    "model.query_tower.layers[12].layers[0].adapt(\n",
    "    train_dataset.unbatch().batch(10000).map(lambda x: tf.reshape(x['artist_genres_pl'], [-1, MAX_PLAYLIST_LENGTH, 1])))\n",
    "print('artist_genres_pl adapts complete')\n",
    "# model.query_tower.layers[13].layers[0].adapt(\n",
    "#     train_dataset.unbatch().batch(10000).map(lambda x: tf.reshape(x['tracks_playlist_titles_pl'], [-1, MAX_PLAYLIST_LENGTH, 1])))\n",
    "# print('tracks_playlist_titles_pl adapts complete')\n",
    "\n",
    "model.candidate_tower.layers[1].layers[0].adapt(\n",
    "    train_dataset.unbatch().batch(10000).map(lambda x: x['track_name_can'])) \n",
    "print('track_name_can adapts complete')\n",
    "model.candidate_tower.layers[3].layers[0].adapt(\n",
    "    train_dataset.unbatch().batch(10000).map(lambda x: x['artist_name_can'])) \n",
    "print('artist_name_can adapts complete')\n",
    "model.candidate_tower.layers[5].layers[0].adapt(\n",
    "    train_dataset.unbatch().batch(10000).map(lambda x: x['album_name_can'])) \n",
    "print('album_name_can adapts complete')\n",
    "model.candidate_tower.layers[9].layers[0].adapt(\n",
    "    train_dataset.unbatch().batch(10000).map(lambda x: x['artist_genres_can'])) \n",
    "print('artist_genres_can adapts complete')\n",
    "# model.candidate_tower.layers[11].layers[0].adapt(\n",
    "#     train_dataset.unbatch().batch(10000).map(lambda x: x['track_pl_titles_can'])) \n",
    "# print('track_pl_titles_can adapts complete')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c78fde-5d37-4fb9-a4b7-d4d4d9fd2648",
   "metadata": {
    "tags": []
   },
   "source": [
    "### save vocab dict \n",
    "\n",
    "Save the vocab dictionary for later so you will not have to adapt\n",
    "\n",
    "```python\n",
    "vocab_dict = {\n",
    "    'pl_name_src' : model.query_tower.layers[0].layers[0].get_vocabulary(),\n",
    "    'track_name_pl' : model.query_tower.layers[7].layers[0].get_vocabulary(),\n",
    "    'artist_name_pl' : model.query_tower.layers[9].layers[0].get_vocabulary(),\n",
    "    'album_name_pl' : model.query_tower.layers[11].layers[0].get_vocabulary(),\n",
    "    'artist_genres_pl' : model.query_tower.layers[12].layers[0].get_vocabulary(),\n",
    "    # 'tracks_playlist_titles_pl' : model.query_tower.layers[13].layers[0].get_vocabulary(),\n",
    "\n",
    "    'track_name_can' : model.candidate_tower.layers[1].layers[0].get_vocabulary(),\n",
    "    'artist_name_can' : model.candidate_tower.layers[3].layers[0].get_vocabulary(),\n",
    "    'album_name_can' : model.candidate_tower.layers[5].layers[0].get_vocabulary(),\n",
    "    'artist_genres_can' : model.candidate_tower.layers[9].layers[0].get_vocabulary(),\n",
    "    # 'track_pl_titles_can' : model.candidate_tower.layers[11].layers[0].get_vocabulary(),\n",
    "}\n",
    "```\n",
    "\n",
    "```python\n",
    "import pickle as pkl\n",
    "\n",
    "filehandler = open('vocab_dict.pkl', 'wb')\n",
    "pkl.dump(vocab_dict, filehandler)\n",
    "\n",
    "filehandler.close()\n",
    "\n",
    "tt.upload_blob('two-tower-models', 'vocab_dict.pkl', 'vocabs/vocab_dict.pkl', `PROJECT_ID`)\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83159b76-b529-4297-ae89-443e15da477b",
   "metadata": {},
   "source": [
    "### load saved vocab dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be17f6f9-1305-4d7d-ac8c-a5777e79e891",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.system('gsutil cp gs://two-tower-models/vocabs/vocab_dict.pkl .')  # TODO - paramterize\n",
    "\n",
    "filehandler = open('vocab_dict.pkl', 'rb')\n",
    "VOCAB_DICT = pkl.load(filehandler)\n",
    "filehandler.close()\n",
    "\n",
    "# VOCAB_DICT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1873a9d8-7dbd-4168-b47c-a9196b6793aa",
   "metadata": {},
   "source": [
    "## Build and Compile model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513a8d10-395b-4cb2-9980-85569a880fce",
   "metadata": {},
   "source": [
    "### TODO: config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "535a6c91-ab26-49a1-8140-4fef84d55c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CROSS_LAYER = True\n",
    "USE_DROPOUT = True\n",
    "SEED = 1234\n",
    "MAX_PLAYLIST_LENGTH = 15\n",
    "EMBEDDING_DIM = 128   \n",
    "PROJECTION_DIM = 25  \n",
    "SEED = 1234\n",
    "DROPOUT_RATE = 0.33\n",
    "MAX_TOKENS = 20000\n",
    "LAYER_SIZES=[256,128]\n",
    "\n",
    "LR = .1\n",
    "opt = tf.keras.optimizers.Adagrad(LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9ec5d0c8-a4ae-451c-9056-fe5d9a8e4964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    }
   ],
   "source": [
    "model = tt.TheTwoTowers(\n",
    "    layer_sizes=LAYER_SIZES, \n",
    "    vocab_dict=VOCAB_DICT, \n",
    "    parsed_candidate_dataset=parsed_candidate_dataset,\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    projection_dim=PROJECTION_DIM,\n",
    "    seed=SEED,\n",
    "    use_cross_layer=USE_CROSS_LAYER,\n",
    "    use_dropout=USE_DROPOUT,\n",
    "    dropout_rate=DROPOUT_RATE,\n",
    "    # max_playlist_length=MAX_PLAYLIST_LENGTH,\n",
    "    max_tokens=MAX_TOKENS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1468308c-ca39-43c8-a09f-1e08f8a1307c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ec331ba6-0b38-4089-9432-097c43ed3bb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<src.two_tower_jt.two_tower.TheTwoTowers at 0x7f582c0bf150>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3414bbf-43af-4e95-96db-95b36a980ce9",
   "metadata": {},
   "source": [
    "### inspect layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "11401be6-0f5f-4454-997a-862a58df2a81",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Playlist (query) Tower:\n",
      "0 pl_name_src_text_embedding\n",
      "1 pl_collaborative_emb_model\n",
      "2 pl_duration_ms_new_emb_model\n",
      "3 num_pl_songs_new_emb_model\n",
      "4 num_pl_artists_new_emb_model\n",
      "5 num_pl_albums_new_emb_model\n",
      "6 track_uri_pl_emb_model\n",
      "7 track_name_pl_emb_model\n",
      "8 artist_uri_pl_emb_model\n",
      "9 artist_name_pl_emb_model\n",
      "10 album_uri_pl_emb_model\n",
      "11 album_name_pl_emb_model\n",
      "12 artist_genres_pl_emb_model\n",
      "13 duration_ms_songs_pl_emb_model\n",
      "14 track_pop_pl_emb_model\n",
      "15 artist_pop_pl_emb_model\n",
      "16 artists_followers_pl_emb_model\n",
      "17 track_danceability_pl_emb_model\n",
      "18 track_energy_pl_emb_model\n",
      "19 track_key_pl_emb_model\n",
      "20 track_loudness_pl_emb_model\n",
      "21 track_mode_pl_emb_model\n",
      "22 track_speechiness_pl_emb_model\n",
      "23 track_acousticness_pl_emb_model\n",
      "24 track_instrumentalness_pl_emb_model\n",
      "25 track_liveness_pl_emb_model\n",
      "26 track_valence_pl_emb_model\n",
      "27 track_tempo_pl_emb_model\n",
      "28 time_signature_pl_emb_model\n",
      "29 pl_cross_layer\n",
      "30 pl_dense_layers\n"
     ]
    }
   ],
   "source": [
    "## Quick look at the layers\n",
    "print(\"Playlist (query) Tower:\")\n",
    "\n",
    "for i, l in enumerate(model.query_tower.layers):\n",
    "    print(i, l.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0d4fb669-7c98-448a-a2bf-1559b0a4d67e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Track (candidate) Tower:\n",
      "0 track_uri_can_emb_model\n",
      "1 track_name_can_emb_model\n",
      "2 artist_uri_can_emb_model\n",
      "3 artist_name_can_emb_model\n",
      "4 album_uri_can_emb_model\n",
      "5 album_name_can_emb_model\n",
      "6 duration_ms_can_emb_model\n",
      "7 track_pop_can_emb_model\n",
      "8 artist_pop_can_emb_model\n",
      "9 artist_genres_can_emb_model\n",
      "10 artists_followers_can_emb_model\n",
      "11 track_danceability_can_emb_model\n",
      "12 track_energy_can_emb_model\n",
      "13 track_key_can_emb_model\n",
      "14 track_loudness_can_emb_model\n",
      "15 track_mode_can_emb_model\n",
      "16 track_speechiness_can_emb_model\n",
      "17 track_acousticness_can_emb_model\n",
      "18 track_instrumentalness_can_emb_model\n",
      "19 track_liveness_can_emb_model\n",
      "20 track_valence_can_emb_model\n",
      "21 track_tempo_can_emb_model\n",
      "22 time_signature_can_emb_model\n",
      "23 can_cross_layer\n",
      "24 candidate_dense_layers\n"
     ]
    }
   ],
   "source": [
    "print(\"Track (candidate) Tower:\")\n",
    "for i, l in enumerate(model.candidate_tower.layers):\n",
    "    print(i, l.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4de78f-a955-43a2-9940-b8d9f8aa4911",
   "metadata": {},
   "source": [
    "### setup Vertex Exeperiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "68c84c47-bd82-4ca3-9f37-d3160e3ca376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUN_NAME: run-20230919-150451\n",
      "LOG_DIR: gs://ndr-v1-hybrid-vertex-bucket/local-train-v1/run-20230919-150451/tb-logs\n"
     ]
    }
   ],
   "source": [
    "EXPERIMENT_NAME = f'local-train-v1'\n",
    "\n",
    "invoke_time = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "RUN_NAME = f'run-{invoke_time}'\n",
    "\n",
    "LOG_DIR = f\"gs://{BUCKET_NAME}/{EXPERIMENT_NAME}/{RUN_NAME}/tb-logs\"\n",
    "\n",
    "vertex_ai.init(\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    "    experiment=EXPERIMENT_NAME\n",
    ")\n",
    "\n",
    "print(f\"RUN_NAME: {RUN_NAME}\")\n",
    "print(f\"LOG_DIR: {LOG_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425a17df-2e2a-41e5-a0c6-ac2abc2ad135",
   "metadata": {
    "tags": []
   },
   "source": [
    "### setup Tensorboard callbacks\n",
    "\n",
    "Setup tensorboard below so training is visible and we can inspect the graph\n",
    "\n",
    "**TODO:** clean up notebook section \n",
    "\n",
    "> *Note:* While profiling does not work for managed Tensorboard at this time, you can inspect the profiler with an [inline Tensorboard in another notebook](https://www.tensorflow.org/tensorboard/tensorboard_in_notebooks). You may be prompted to install the tensorflow profiler library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8238845-82bb-45a2-98b6-df8fb7589d90",
   "metadata": {},
   "source": [
    "#### Managed Tensorboard Resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6c0f152f-930f-4640-ab6d-7b540e6084b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Tensorboard\n",
      "Create Tensorboard backing LRO: projects/934903580331/locations/us-central1/tensorboards/7680176680945582080/operations/1954146279286112256\n",
      "Tensorboard created. Resource name: projects/934903580331/locations/us-central1/tensorboards/7680176680945582080\n",
      "To use this Tensorboard in another session:\n",
      "tb = aiplatform.Tensorboard('projects/934903580331/locations/us-central1/tensorboards/7680176680945582080')\n",
      "TB_RESOURCE_NAME: projects/934903580331/locations/us-central1/tensorboards/7680176680945582080\n"
     ]
    }
   ],
   "source": [
    "# use existing TB instance\n",
    "# TB_RESOURCE_NAME = 'projects/934903580331/locations/us-central1/tensorboards/472921941339013120'\n",
    "\n",
    "# # create new TB instance\n",
    "TENSORBOARD_DISPLAY_NAME=f\"{EXPERIMENT_NAME}\"\n",
    "tensorboard = vertex_ai.Tensorboard.create(display_name=TENSORBOARD_DISPLAY_NAME)\n",
    "TB_RESOURCE_NAME = tensorboard.resource_name\n",
    "\n",
    "\n",
    "print(f\"TB_RESOURCE_NAME: {TB_RESOURCE_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6d62d539-6e29-4783-a358-1321f9521528",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_upload_logs_to_manged_tb_command(ttl_hrs, oneshot=\"false\"):\n",
    "    \"\"\"\n",
    "    Run this and copy/paste the command into terminal to have \n",
    "    upload the tensorboard logs from this machine to the managed tb instance\n",
    "    Note that the log dir is at the granularity of the run to help select the proper\n",
    "    timestamped run in Tensorboard\n",
    "    You can also run this in one-shot mode after training is done \n",
    "    to upload all tb objects at once\n",
    "    \"\"\"\n",
    "    return(f\"\"\"tb-gcp-uploader --tensorboard_resource_name={TB_RESOURCE_NAME} \\\n",
    "      --logdir={LOG_DIR} \\\n",
    "      --experiment_name={EXPERIMENT_NAME} \\\n",
    "      --one_shot={oneshot} \\\n",
    "      --event_file_inactive_secs={60*60*ttl_hrs}\"\"\")\n",
    "\n",
    "# tensorboard callback\n",
    "class UploadTBLogsBatchEnd(tf.keras.callbacks.Callback):\n",
    "    '''\n",
    "    ecapsulates one-shot log uploader via a custom callback\n",
    "\n",
    "    '''\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        os.system(\n",
    "            get_upload_logs_to_manged_tb_command(\n",
    "                ttl_hrs = 5, oneshot=\"true\"\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360a034b-7959-4bd0-bf9c-45da1b695055",
   "metadata": {},
   "source": [
    "#### train config\n",
    "\n",
    "* consider experiment and experiment-run naming convention so names don't collide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d2eeffe1-efc0-4a2d-b168-6f10a43b1e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 2\n",
    "VALID_FREQUENCY = 5\n",
    "HIST_FREQ = 0\n",
    "EMBED_FREQ = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50177537-964d-4f49-9770-a880dd47fe10",
   "metadata": {},
   "source": [
    "### Train model\n",
    "\n",
    "* train model in-notebook\n",
    "* write metrics to Vertex AI Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5d9f592c-913c-45a9-a9eb-3ae1becefddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "10/10 [==============================] - ETA: 0s - batch_categorical_accuracy_at_10: 0.0100 - batch_categorical_accuracy_at_50: 0.0493 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 9758.3831 - regularization_loss: 0.0000e+00 - total_loss: 9758.3831  View your Tensorboard at https://us-central1.tensorboard.googleusercontent.com/experiment/projects+934903580331+locations+us-central1+tensorboards+7680176680945582080+experiments+local-train-v1\n",
      "\u001b[1m[2023-09-19T15:06:06]\u001b[0m Started scanning logdir.\n",
      "\u001b[1m[2023-09-19T15:06:07]\u001b[0m Total uploaded: 8 scalars, 0 tensors, 1 binary objects (1.1 MB)\n",
      "10/10 [==============================] - 61s 4s/step - batch_categorical_accuracy_at_10: 0.0100 - batch_categorical_accuracy_at_50: 0.0493 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 9517.5381 - regularization_loss: 0.0000e+00 - total_loss: 9517.5381\n",
      "Epoch 2/2\n",
      "10/10 [==============================] - ETA: 0s - batch_categorical_accuracy_at_10: 0.0104 - batch_categorical_accuracy_at_50: 0.0490 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 7105.0629 - regularization_loss: 0.0000e+00 - total_loss: 7105.0629"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0919 15:06:38.495153 140220551669568 uploader.py:389] Please consider uploading to a new experiment instead of an existing one, as the former allows for better upload performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View your Tensorboard at https://us-central1.tensorboard.googleusercontent.com/experiment/projects+934903580331+locations+us-central1+tensorboards+7680176680945582080+experiments+local-train-v1\n",
      "\u001b[1m[2023-09-19T15:06:38]\u001b[0m Started scanning logdir.\n",
      "\u001b[1m[2023-09-19T15:06:42]\u001b[0m Total uploaded: 16 scalars, 0 tensors, 1 binary objects (1.1 MB)\n",
      "10/10 [==============================] - 35s 4s/step - batch_categorical_accuracy_at_10: 0.0104 - batch_categorical_accuracy_at_50: 0.0490 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 7105.3249 - regularization_loss: 0.0000e+00 - total_loss: 7105.3249\n"
     ]
    }
   ],
   "source": [
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "        log_dir=LOG_DIR,\n",
    "        histogram_freq=HIST_FREQ, \n",
    "        write_graph=True,\n",
    "        embeddings_freq=EMBED_FREQ,\n",
    "        # profile_batch=(20,50) #run profiler on steps 20-40 - enable this line if you want to run profiler from the utils/ notebook\n",
    "    )\n",
    "\n",
    "#start the timer and training\n",
    "start_time = time.time()\n",
    "\n",
    "layer_history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=valid_dataset,\n",
    "    validation_freq=VALID_FREQUENCY,\n",
    "    epochs=NUM_EPOCHS,\n",
    "    steps_per_epoch=10,\n",
    "    validation_steps=100,\n",
    "    callbacks=[\n",
    "        tensorboard_callback,\n",
    "        UploadTBLogsBatchEnd()\n",
    "    ], \n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "val_keys = [v for v in layer_history.history.keys()]\n",
    "runtime_mins = int((end_time - start_time) / 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29be04e5-d666-4c3d-af06-4f39ebf42cdc",
   "metadata": {},
   "source": [
    "### log Vertex Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "277cc62b-fa19-4f62-bca1-4aad6a2ac131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Associating projects/934903580331/locations/us-central1/metadataStores/default/contexts/local-train-v1-run-20230919-150451 to Experiment: local-train-v1\n"
     ]
    }
   ],
   "source": [
    "vertex_ai.start_run(RUN_NAME, tensorboard=TB_RESOURCE_NAME)\n",
    "\n",
    "vertex_ai.log_params(\n",
    "    {\n",
    "        \"layers\": str(LAYER_SIZES), \n",
    "        \"learning_rate\": LR,\n",
    "        \"num_epochs\": NUM_EPOCHS,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"valid_freq\": VALID_FREQUENCY,\n",
    "    }\n",
    ")\n",
    "\n",
    "# gather the metrics for the last epoch to be saved in metrics\n",
    "metrics_dict = {\"train-time-minutes\": runtime_mins}\n",
    "\n",
    "_ = [metrics_dict.update({key: layer_history.history[key][-1]}) for key in val_keys]\n",
    "\n",
    "vertex_ai.log_metrics(metrics_dict)\n",
    "\n",
    "vertex_ai.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "54ef68ef-2644-4009-92b3-6ec84d704128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total runtime: 1 minutes\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total runtime: {runtime_mins} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6b7373ed-c4d8-4618-b42f-ec3dbda6ffee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train-time-minutes': 1,\n",
       " 'batch_categorical_accuracy_at_10': 0.01035156287252903,\n",
       " 'batch_categorical_accuracy_at_50': 0.04902343824505806,\n",
       " 'factorized_top_k/top_10_categorical_accuracy': 0.0,\n",
       " 'factorized_top_k/top_50_categorical_accuracy': 0.0,\n",
       " 'factorized_top_k/top_100_categorical_accuracy': 0.0,\n",
       " 'loss': 7107.9453125,\n",
       " 'regularization_loss': 0,\n",
       " 'total_loss': 7107.9453125}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get metrics for the Vertex Experiment\n",
    "metrics_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466dd9cf-70f6-475b-ab28-d17dfffe214a",
   "metadata": {},
   "source": [
    "### Save each tower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e9e83d88-43e1-4ebe-922d-bd4765a6abd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, dense_1_layer_call_fn, dense_1_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: gs://ndr-v1-hybrid-vertex-bucket/local-train-v1/run-20230919-150451/model-dir/query_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: gs://ndr-v1-hybrid-vertex-bucket/local-train-v1/run-20230919-150451/model-dir/query_model/assets\n",
      "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, dense_1_layer_call_fn, dense_1_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: gs://ndr-v1-hybrid-vertex-bucket/local-train-v1/run-20230919-150451/model-dir/candidate_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: gs://ndr-v1-hybrid-vertex-bucket/local-train-v1/run-20230919-150451/model-dir/candidate_model/assets\n"
     ]
    }
   ],
   "source": [
    "#save towers (models)\n",
    "tf.saved_model.save(model.query_tower, export_dir=f\"gs://{BUCKET_NAME}/{EXPERIMENT_NAME}/{RUN_NAME}/model-dir/query_model\")\n",
    "tf.saved_model.save(model.candidate_tower, export_dir=f\"gs://{BUCKET_NAME}/{EXPERIMENT_NAME}/{RUN_NAME}/model-dir/candidate_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2450b70a-d6dd-44e9-b7ce-dc439594d0bc",
   "metadata": {},
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2e0f08e0-7fb7-48d2-99d2-640cd7405b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82/82 [==============================] - 32943s 400s/step - batch_categorical_accuracy_at_10: 0.0113 - batch_categorical_accuracy_at_50: 0.0496 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 1.2054e-05 - factorized_top_k/top_100_categorical_accuracy: 3.6162e-05 - loss: 6953.0374 - regularization_loss: 0.0000e+00 - total_loss: 6953.0374\n",
      "elapsed_mins: 549\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "eval_dict_v1 = model.evaluate(valid_dataset, return_dict=True)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "elapsed_mins = int((end_time - start_time) / 60)\n",
    "print(f\"elapsed_mins: {elapsed_mins}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5f9b0c75-7eaf-466c-b452-680addc20a72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_categorical_accuracy_at_10': 0.011282681487500668,\n",
       " 'batch_categorical_accuracy_at_50': 0.049578707665205,\n",
       " 'factorized_top_k/top_10_categorical_accuracy': 0.0,\n",
       " 'factorized_top_k/top_50_categorical_accuracy': 1.2054147191520315e-05,\n",
       " 'factorized_top_k/top_100_categorical_accuracy': 3.616244066506624e-05,\n",
       " 'loss': 40.9329719543457,\n",
       " 'regularization_loss': 0,\n",
       " 'total_loss': 40.9329719543457}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dict_v1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa43294b-3a5d-482c-a0fc-ea37f1d6cb78",
   "metadata": {},
   "source": [
    "### Efficient eval\n",
    "\n",
    "* approximate with scann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6cca1457-4917-4aa3-81c1-6227e4bea23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "scann = tfrs.layers.factorized_top_k.ScaNN(\n",
    "    num_reordering_candidates=500,\n",
    "    num_leaves_to_search=30\n",
    ")\n",
    "scann.index_from_dataset(\n",
    "    candidates=parsed_candidate_dataset.batch(128).cache().map(\n",
    "        lambda x: (\n",
    "            x['track_uri_can'], \n",
    "            model.candidate_tower(x)\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "elapsed_scann_mins = int((end_time - start_time) / 60)\n",
    "print(f\"elapsed_scann_mins: {elapsed_scann_mins}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0607f90f-843e-4d11-b05c-5085d6e61910",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "model.task.factorized_metrics = tfrs.metrics.FactorizedTopK(\n",
    "    candidates=scann\n",
    ")\n",
    "model.compile()\n",
    "\n",
    "scann_result = model.evaluate(\n",
    "    valid_dataset, \n",
    "    return_dict=True, \n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "elapsed_scann_eval_mins = int((end_time - start_time) / 60)\n",
    "print(f\"elapsed_scann_eval_mins: {elapsed_scann_eval_mins}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5022d991-70ae-4e54-bc6a-5705df1924d7",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Evaluating the train job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c6e1cd-0e65-450b-9490-2f497ce65744",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### You can access the experiment from the console via the experiment name you just declared:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1b2721-0c8b-4d59-905d-c3ac776045b6",
   "metadata": {},
   "source": [
    "<img src=\"./img/experiment-console.png\" \n",
    "     align=\"center\" \n",
    "     width=\"750\"\n",
    "     height=\"750\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda2ba57-5fd1-4493-8d6b-3b14b41aa159",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### After opening the TensorBoard instance:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7345a1b6-5c94-45b7-9181-8111b4fe4466",
   "metadata": {},
   "source": [
    "<img src=\"./img/tensorboard.png\" \n",
    "     align=\"center\" \n",
    "     width=\"750\"\n",
    "     height=\"750\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c18098-6613-4ebd-a780-fe92f2bbcb1a",
   "metadata": {},
   "source": [
    "<img src=\"./img/tb-metrics.png\" \n",
    "     align=\"center\" \n",
    "     width=\"750\"\n",
    "     height=\"750\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77f4fe8-226c-4c11-939f-04fc71300f7a",
   "metadata": {},
   "source": [
    "<img src=\"./img/tb-loss.png\" \n",
    "     align=\"center\" \n",
    "     width=\"750\"\n",
    "     height=\"750\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7ff295-17b4-4ff4-98cb-3d022384ca7a",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Also, while this is running - check out the Tensorboard profiler in `utils`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f82997-27f3-44c4-82a7-75a3211bcf43",
   "metadata": {},
   "source": [
    "<img src=\"./img/tb-profiler.png\" \n",
    "     align=\"center\" \n",
    "     width=\"750\"\n",
    "     height=\"750\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbfc3e2-2786-476f-a4bf-d052370ecacc",
   "metadata": {},
   "source": [
    "#### Run `nvtop` - check out the installation script in `utils` - `install_nvtop.sh`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f78adf-a6b4-4581-803e-252882c82a58",
   "metadata": {},
   "source": [
    "<img src=\"./img/nvtop-optimized.png\" \n",
    "     align=\"center\" \n",
    "     width=\"750\"\n",
    "     height=\"750\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ea4dd5-2003-472a-a501-19e029cd14df",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Save the candidate embeddings to GCS for use in Matching Engine later\n",
    "These will be the files we use for the index\n",
    "\n",
    "This does the following\n",
    "1) Create a tf pipeline to convert embeddings to numpy\n",
    "2) Serialize the candidate song emgeddings with the song_uri index and save to gcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ee8e4e21-c560-4148-93ea-d30f5a946861",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_embeddings = parsed_candidate_dataset.batch(10000).map(\n",
    "    lambda x: [\n",
    "        x['track_uri_can'],\n",
    "        train_utils.tf_if_null_return_zero(\n",
    "            model.candidate_tower(x)\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3a1b298d-167d-4041-a8a6-dbfe94229ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# candidate_embeddings\n",
    "# len(list(candidate_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8c7d452a-3137-41a5-a48a-92e686abafde",
   "metadata": {},
   "outputs": [],
   "source": [
    "CANDIDATE_EMB_JSON = 'candidate_embeddings.json'\n",
    "\n",
    "# Save to the required format\n",
    "# make sure you start out with a clean empty file for the append write\n",
    "\n",
    "# !rm $CANDIDATE_EMB_JSON > /dev/null\n",
    "!touch $CANDIDATE_EMB_JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "84921234-92fb-45c2-b6fe-c932360da226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed_mins: 3\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "for batch in candidate_embeddings:\n",
    "    songs, embeddings = batch\n",
    "    with open(CANDIDATE_EMB_JSON, 'a') as f:\n",
    "        for song, emb in zip(songs.numpy(), embeddings.numpy()):\n",
    "            f.write('{\"id\":\"' + str(song) + '\",\"embedding\":[' + \",\".join(str(x) for x in list(emb)) + ']}')\n",
    "            f.write(\"\\n\")\n",
    "            \n",
    "end_time = time.time()\n",
    "\n",
    "elapsed_mins = int((end_time - start_time) / 60)\n",
    "print(f\"elapsed_mins: {elapsed_mins}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b369e1ff-e135-486d-9bff-b4bbc2c028c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3885, 128), dtype=float32, numpy=\n",
       "array([[ 0.72976977, -0.6685658 , -1.3039492 , ...,  1.0261438 ,\n",
       "         2.1801417 , -0.1081075 ],\n",
       "       [ 0.72705394, -0.66507214, -1.3133432 , ...,  1.0277908 ,\n",
       "         2.1814442 , -0.11300553],\n",
       "       [ 0.73046064, -0.66539216, -1.3096614 , ...,  1.0261513 ,\n",
       "         2.179594  , -0.1100015 ],\n",
       "       ...,\n",
       "       [ 0.7235094 , -0.66666913, -1.3144187 , ...,  1.0275027 ,\n",
       "         2.1822722 , -0.11019386],\n",
       "       [ 0.7158847 , -0.67372656, -1.3102845 , ...,  1.0365474 ,\n",
       "         2.180481  , -0.10880204],\n",
       "       [ 0.7246471 , -0.66796744, -1.3110669 , ...,  1.0285532 ,\n",
       "         2.1808298 , -0.11085889]], dtype=float32)>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ced7ce65-b1e5-49b9-824e-3cabedd656f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File candidate_embeddings.json uploaded to local-train-v1/run-20230919-150451/candidates/candidate_embeddings.json.\n"
     ]
    }
   ],
   "source": [
    "train_utils.upload_blob(\n",
    "    bucket_name=BUCKET_NAME,\n",
    "    source_file_name=CANDIDATE_EMB_JSON,\n",
    "    destination_blob_name=f'{EXPERIMENT_NAME}/{RUN_NAME}/candidates/candidate_embeddings.json',\n",
    "    project_id = PROJECT_ID\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab8cb39-707d-4dc9-9d80-32b198b5cdc3",
   "metadata": {
    "tags": []
   },
   "source": [
    "Do a quick line count from terminal - should look like this:\n",
    "\n",
    "```\n",
    " wc -l candidate_embeddings.json \n",
    " \n",
    " 2249561 candidate_embeddings.json\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ddc8d3-ea1c-4b6b-af2f-e4f272ec3f1a",
   "metadata": {},
   "source": [
    "<img src=\"./img/embeddings.png\" \n",
    "     align=\"center\" \n",
    "     width=\"750\"\n",
    "     height=\"750\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0844a4b-084f-4e4d-8f3e-d8d920ec24ec",
   "metadata": {},
   "source": [
    "### Getting test instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8fbc0d8d-6056-4801-91ce-cec5757d4b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(list(valid_dataset.unbatch()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a79c6fa1-3cb1-480a-b1ec-c411870f2f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(list(train_dataset.unbatch()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "728b2fb7-f01d-4804-9416-e52373cd5749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'album_name_can': b'Brett Young', 'album_name_pl': array([b'The Fighters', b'Love Story', b'The First Time',\n",
      "       b'Now That I Know Your Name', b'Brett Young'], dtype=object), 'album_uri_can': b'spotify:album:6MTPPsqX8KnmDWmnQmbxJ5', 'album_uri_pl': array([b'spotify:album:7dr4GfexRAIruQMG4GanB2',\n",
      "       b'spotify:album:04n1d6ioeumQRlAj0JBID2',\n",
      "       b'spotify:album:2H6xesu7JV1YEO1FvwxKZG',\n",
      "       b'spotify:album:7yzqRhliyDR9Xl2looejao',\n",
      "       b'spotify:album:6MTPPsqX8KnmDWmnQmbxJ5'], dtype=object), 'artist_followers_can': 973783.0, 'artist_genres_can': b\"'contemporary country', 'country', 'country road'\", 'artist_genres_pl': array([b\"'contemporary country', 'country', 'country pop', 'country rap', 'country road', 'modern country rock'\",\n",
      "       b\"'contemporary country', 'country', 'country road'\",\n",
      "       b\"'contemporary country', 'country', 'pop'\",\n",
      "       b\"'contemporary country', 'country pop', 'deep talent show'\",\n",
      "       b\"'contemporary country', 'country', 'country road'\"], dtype=object), 'artist_name_can': b'Brett Young', 'artist_name_pl': array([b'LOCASH', b'Tim McGraw', b'Kelsea Ballerini', b'Jordan Rager',\n",
      "       b'Brett Young'], dtype=object), 'artist_pop_can': 71.0, 'artist_pop_pl': array([54., 73., 77., 34., 71.], dtype=float32), 'artist_uri_can': b'spotify:artist:0fiWOxhsBsQQvFDtxUQWo0', 'artist_uri_pl': array([b'spotify:artist:5IcGbIAgdns0R5EJKHMjCQ',\n",
      "       b'spotify:artist:6roFdX1y5BYSbp60OTJWMd',\n",
      "       b'spotify:artist:3RqBeV12Tt7A8xH3zBDDUF',\n",
      "       b'spotify:artist:4XoYsecWoLIoV66HEbt365',\n",
      "       b'spotify:artist:0fiWOxhsBsQQvFDtxUQWo0'], dtype=object), 'artists_followers_pl': array([ 335017., 4294580.,  985764.,   28209.,  973783.], dtype=float32), 'duration_ms_can': 224853.0, 'duration_ms_songs_pl': array([208863., 215226., 200040., 204506., 207186.], dtype=float32), 'num_pl_albums_new': 22.0, 'num_pl_artists_new': 19.0, 'num_pl_songs_new': 58.0, 'pl_collaborative_src': b'false', 'pl_duration_ms_new': 12244066.0, 'pl_name_src': b'country lovin', 'track_acousticness_can': 0.386, 'track_acousticness_pl': array([0.196 , 0.0516, 0.0164, 0.0442, 0.145 ], dtype=float32), 'track_danceability_can': 0.607, 'track_danceability_pl': array([0.494, 0.506, 0.515, 0.582, 0.586], dtype=float32), 'track_energy_can': 0.536, 'track_energy_pl': array([0.927, 0.746, 0.6  , 0.892, 0.568], dtype=float32), 'track_instrumentalness_can': 0.0, 'track_instrumentalness_pl': array([0.00e+00, 0.00e+00, 7.24e-06, 0.00e+00, 0.00e+00], dtype=float32), 'track_key_can': b'10.0', 'track_key_pl': array([b'1.0', b'6.0', b'1.0', b'2.0', b'1.0'], dtype=object), 'track_liveness_can': 0.102, 'track_liveness_pl': array([0.153 , 0.0841, 0.192 , 0.0545, 0.0858], dtype=float32), 'track_loudness_can': -7.306, 'track_loudness_pl': array([-3.014, -7.172, -6.051, -5.569, -5.369], dtype=float32), 'track_mode_can': b'1', 'track_mode_pl': array([b'1', b'1', b'1', b'1', b'1'], dtype=object), 'track_name_can': b\"In Case You Didn't Know\", 'track_name_pl': array([b'I Love This Life', b'Just to See You Smile', b'Peter Pan',\n",
      "       b'Now That I Know Your Name', b'Like I Loved You'], dtype=object), 'track_pop_can': 72.0, 'track_pop_pl': array([38.,  0., 42., 31., 63.], dtype=float32), 'track_speechiness_can': 0.0305, 'track_speechiness_pl': array([0.0312, 0.0297, 0.0547, 0.0309, 0.03  ], dtype=float32), 'track_tempo_can': 147.986, 'track_tempo_pl': array([111.977,  76.025,  94.315, 135.935,  76.473], dtype=float32), 'track_time_signature_can': b'4', 'track_time_signature_pl': array([b'4', b'4', b'4', b'4', b'4'], dtype=object), 'track_uri_can': b'spotify:track:10M2Ex445zw585Ducldzkw', 'track_uri_pl': array([b'spotify:track:1ZT2SegEiHQz8EeBwe3TH1',\n",
      "       b'spotify:track:1VuFPJ1EI9mtc6FmK3RMFL',\n",
      "       b'spotify:track:5522Ut7YYZSv2TjLMLIpEU',\n",
      "       b'spotify:track:06AVpLCcfRBHuDyQosPfR3',\n",
      "       b'spotify:track:3T6MAweakkNrMYs8jZIWtg'], dtype=object), 'track_valence_can': 0.434, 'track_valence_pl': array([0.549, 0.527, 0.227, 0.699, 0.64 ], dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "for tensor_dict in valid_dataset.unbatch().skip(1000).take(1):\n",
    "    td_keys = tensor_dict.keys()\n",
    "    list_dict = {}\n",
    "    for k in td_keys:\n",
    "        list_dict.update({k: tensor_dict[k].numpy()})\n",
    "    print(list_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1318107a-0747-4b7a-9f17-fcf83c975de7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'album_name_can': b'Brett Young',\n",
       " 'album_name_pl': array([b'The Fighters', b'Love Story', b'The First Time',\n",
       "        b'Now That I Know Your Name', b'Brett Young'], dtype=object),\n",
       " 'album_uri_can': b'spotify:album:6MTPPsqX8KnmDWmnQmbxJ5',\n",
       " 'album_uri_pl': array([b'spotify:album:7dr4GfexRAIruQMG4GanB2',\n",
       "        b'spotify:album:04n1d6ioeumQRlAj0JBID2',\n",
       "        b'spotify:album:2H6xesu7JV1YEO1FvwxKZG',\n",
       "        b'spotify:album:7yzqRhliyDR9Xl2looejao',\n",
       "        b'spotify:album:6MTPPsqX8KnmDWmnQmbxJ5'], dtype=object),\n",
       " 'artist_followers_can': 973783.0,\n",
       " 'artist_genres_can': b\"'contemporary country', 'country', 'country road'\",\n",
       " 'artist_genres_pl': array([b\"'contemporary country', 'country', 'country pop', 'country rap', 'country road', 'modern country rock'\",\n",
       "        b\"'contemporary country', 'country', 'country road'\",\n",
       "        b\"'contemporary country', 'country', 'pop'\",\n",
       "        b\"'contemporary country', 'country pop', 'deep talent show'\",\n",
       "        b\"'contemporary country', 'country', 'country road'\"], dtype=object),\n",
       " 'artist_name_can': b'Brett Young',\n",
       " 'artist_name_pl': array([b'LOCASH', b'Tim McGraw', b'Kelsea Ballerini', b'Jordan Rager',\n",
       "        b'Brett Young'], dtype=object),\n",
       " 'artist_pop_can': 71.0,\n",
       " 'artist_pop_pl': array([54., 73., 77., 34., 71.], dtype=float32),\n",
       " 'artist_uri_can': b'spotify:artist:0fiWOxhsBsQQvFDtxUQWo0',\n",
       " 'artist_uri_pl': array([b'spotify:artist:5IcGbIAgdns0R5EJKHMjCQ',\n",
       "        b'spotify:artist:6roFdX1y5BYSbp60OTJWMd',\n",
       "        b'spotify:artist:3RqBeV12Tt7A8xH3zBDDUF',\n",
       "        b'spotify:artist:4XoYsecWoLIoV66HEbt365',\n",
       "        b'spotify:artist:0fiWOxhsBsQQvFDtxUQWo0'], dtype=object),\n",
       " 'artists_followers_pl': array([ 335017., 4294580.,  985764.,   28209.,  973783.], dtype=float32),\n",
       " 'duration_ms_can': 224853.0,\n",
       " 'duration_ms_songs_pl': array([208863., 215226., 200040., 204506., 207186.], dtype=float32),\n",
       " 'num_pl_albums_new': 22.0,\n",
       " 'num_pl_artists_new': 19.0,\n",
       " 'num_pl_songs_new': 58.0,\n",
       " 'pl_collaborative_src': b'false',\n",
       " 'pl_duration_ms_new': 12244066.0,\n",
       " 'pl_name_src': b'country lovin',\n",
       " 'track_acousticness_can': 0.386,\n",
       " 'track_acousticness_pl': array([0.196 , 0.0516, 0.0164, 0.0442, 0.145 ], dtype=float32),\n",
       " 'track_danceability_can': 0.607,\n",
       " 'track_danceability_pl': array([0.494, 0.506, 0.515, 0.582, 0.586], dtype=float32),\n",
       " 'track_energy_can': 0.536,\n",
       " 'track_energy_pl': array([0.927, 0.746, 0.6  , 0.892, 0.568], dtype=float32),\n",
       " 'track_instrumentalness_can': 0.0,\n",
       " 'track_instrumentalness_pl': array([0.00e+00, 0.00e+00, 7.24e-06, 0.00e+00, 0.00e+00], dtype=float32),\n",
       " 'track_key_can': b'10.0',\n",
       " 'track_key_pl': array([b'1.0', b'6.0', b'1.0', b'2.0', b'1.0'], dtype=object),\n",
       " 'track_liveness_can': 0.102,\n",
       " 'track_liveness_pl': array([0.153 , 0.0841, 0.192 , 0.0545, 0.0858], dtype=float32),\n",
       " 'track_loudness_can': -7.306,\n",
       " 'track_loudness_pl': array([-3.014, -7.172, -6.051, -5.569, -5.369], dtype=float32),\n",
       " 'track_mode_can': b'1',\n",
       " 'track_mode_pl': array([b'1', b'1', b'1', b'1', b'1'], dtype=object),\n",
       " 'track_name_can': b\"In Case You Didn't Know\",\n",
       " 'track_name_pl': array([b'I Love This Life', b'Just to See You Smile', b'Peter Pan',\n",
       "        b'Now That I Know Your Name', b'Like I Loved You'], dtype=object),\n",
       " 'track_pop_can': 72.0,\n",
       " 'track_pop_pl': array([38.,  0., 42., 31., 63.], dtype=float32),\n",
       " 'track_speechiness_can': 0.0305,\n",
       " 'track_speechiness_pl': array([0.0312, 0.0297, 0.0547, 0.0309, 0.03  ], dtype=float32),\n",
       " 'track_tempo_can': 147.986,\n",
       " 'track_tempo_pl': array([111.977,  76.025,  94.315, 135.935,  76.473], dtype=float32),\n",
       " 'track_time_signature_can': b'4',\n",
       " 'track_time_signature_pl': array([b'4', b'4', b'4', b'4', b'4'], dtype=object),\n",
       " 'track_uri_can': b'spotify:track:10M2Ex445zw585Ducldzkw',\n",
       " 'track_uri_pl': array([b'spotify:track:1ZT2SegEiHQz8EeBwe3TH1',\n",
       "        b'spotify:track:1VuFPJ1EI9mtc6FmK3RMFL',\n",
       "        b'spotify:track:5522Ut7YYZSv2TjLMLIpEU',\n",
       "        b'spotify:track:06AVpLCcfRBHuDyQosPfR3',\n",
       "        b'spotify:track:3T6MAweakkNrMYs8jZIWtg'], dtype=object),\n",
       " 'track_valence_can': 0.434,\n",
       " 'track_valence_pl': array([0.549, 0.527, 0.227, 0.699, 0.64 ], dtype=float32)}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf02e74-3543-47d3-bebc-3c565bdbcf33",
   "metadata": {},
   "source": [
    "**Finished**"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-10.m103",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-10:m103"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
